A Combined Shared Memory and TCP/IP Implementation of the SPEEDES Communications LibraryDr. Ronald Van IwaardenDr. Jeffrey S. SteinmanGary BlankMetron Incorporated512 Via De La Valle, Suite 301Solana Beach, CA  92075(619) 792-8904 HYPERLINK mailto:vaniwaar@ca.metsci.com vaniwaar@ca.metsci.com,  HYPERLINK mailto:steinman@ca.metsci.com steinman@ca.metsci.com, blank@ca.metsci.comKeywordsSPEEDES, C++, communication libraries, parallel programmingAbstract: SpeedesComm is a new library of functions to which provides the basic services needed for parallel programming. It is intended to be smaller and lighter than other well-known libraries such as MPI or PVM. This paper defines the basic interfaces, describes an implementation using shared memory and TCP/IP, and provides benchmarks of this implementation on various architectures.1. IntroductionParallel programming is necessary to solve many large problems today and one of the largest obstacles in developing a parallel program is building the required communications libraries and their interfaces. To this end, many different libraries and interfaces have been developed in order to relieve the parallel programmer of this burden. Two of the more popular interfaces are MPI [5,6,7] and PVM [8], both of which are used in a wide variety of fields to solve many large scale problems.One advantage and problem with these interfaces is that they are large and diverse. Their size allows one to solve some very complicated problems but it also can make them difficult to port and can bloat the executables. Overcoming these disadvantages is the motivation behind SpeedesComm.This research produced a parallel programming library that is light and simple but still provides most of the basic services required for parallel programming and has clean C++ interfaces. While this library does not contain all the functionality of diverse libraries such as MPI or PVM, it is able to handle most problems that arise in parallel programming.  It also is a solid teaching tool, allowing one to quickly begin wrestling with fundamental issues in parallel programming such as deadlock and load balancing.2. Programming InterfacesSpeedesComm consists of just 25 functions: one for initialization, two for examining the state of the SpeedesComm library, 11 for synchronization, and another 11 for sending and receiving messages. Also included are two C++ classes that implement heterogeneous integers and doubles for communication between IEEE little endian and big endian machines.2.1 Initialization and state messagesThe following function initializes SpeedesComm:void SpComm_StartUp(int localNodes, int totalNodes, int groupID, char *infile)where localNodes, totalNodes, and groupID are the number of nodes on this machine, the number of total nodes, and the group id of this SpeedesComm group. The group id is an additional number that can separate different applications using the same SpeedesComm interface at the same time (i.e. there can be two or more groups in which messages can only be sent to nodes within the group). The infile argument is an input file that gives these values if a is set to 0.There are then two calls for examining the state of SpeedesComm:int SpComm_GetNumNodes()int SpComm_GetNodeId()The first returns the total number of nodes in the parallel system and the second returns an integer from 0 to #nodes-1, representing the node id of the current process.2.2 Synchronization callsThere are 11 different functions to provide reductions (reduce data from each node to a single value/group of values) and synchronization between SpeedesComm processes, 9 of which are blocking calls and two are non-blocking.  A blocking function call stops the invoking process until it completes its entire task; a non-blocking function just initiates a task, and then allows the invoking process to proceed.  The blocking calls are:void SpComm_BarrierSync()int SpComm_GlobalMin(int ival)int SpComm_GlobalMax(int ival)int SpComm_GlobalSum(int ival)double SpComm_GlobalMin(double dval)double SpComm_GlobalMax(double dval)double SpComm_GlobalSum(double dval)SpSimTime SpComm_GlobalMin(SpSimTime simTime)SpSimTime SpComm_GlobalMax(SpSimTime simTime)The above functions work as follows: a SpeedesComm process makes a call (SpComm_GlobalSum, for example) and then blocks (waits) until all the other SpeedesComm processes make the same call, at which point all processes are permitted to proceed. The names are relatively self-explanatory. The SpComm_GlobalSum functions sum up all the values passed in and returns that sum; the SpComm_GlobalMax functions find the maximum of all the values and return this maximum; the SpComm_GlobalMin functions find the minimum of each of the values and return that value. SpComm_BarrierSync() simply halts each process until everyone has synchronized and then releases them all. SpSimTime is a class that is useful for discrete event simulation. SpSimTime is a generalization of the typical floating-point or double-precision representation of time that includes a double value corresponding to an actual time, along with four integer priorities that that are used to break ties thus insuring repeatability in simulations. SpSimTime supports construction from a double (priorities are initialized to 0), demotion to a double (returns simply the floating point time) and also supports the usual comparison operators with lexicographic comparisons performed on the time represented as a double followed by four integer tie-breaking fields.SpeedesComm also provides two unique non-blocking synchronization calls. These are:void SpComm_EnterFuzzyBarrier()int SpComm_ExitFuzzyBarrier()SpComm_EnterFuzzyBarrier() announces that the calling node has entered the fuzzy barrier. A node can then make repeated calls to SpComm_ExitFuzzyBarrier() to check if all nodes have entered the barrier. This call will return 0 if everyone has not yet entered the fuzzy barrier and will return 1 if everyone has entered the barrier.2.3 Message PassingSpeedesComm provides 6 send functions and 4 receive functions. These are further broken down into standard message passing calls and coordinated message passing calls. The standard send calls are:void SpComm_Send(int dest, int type, int nbytes, void *buff)void SpComm_Send(SpDestination *dest, int type, int nbytes, void *buff)void SpComm_Send(int type, int nbytes, void *buff)which are uni-cast, multi-cast, and broadcast calls respectively. In the first send service, dest is the message destination node number, type is the message type (0-255), nbytes is the size of the message in bytes, and buff is a pointer to the message data being sent. In the second send service, dest is a C++ class, SpDestination, that contains a list of recipients. In the third send service, the arguments have the same meaning as in the first; no destination list is required since a broadcast just delivers the message to all nodes.SpDestination supports several methods for setting nodes and iterating through nodes.  The primary methods that must be supported are:void Set(int node)void Reset(int node)int GetFirstNode()int GetNextNode().There are three standard receive calls:void SpComm_Receive(void)void *SpComm_Receive(int type, int &nbytes)void *SpComm_GetPendingMessage(int type, int &nbytes)SpComm_Receive() checks to see if any messages are waiting for the node and then receives these messages into incoming queues. SpComm_Receive(t, n) first checks to see if any messages of type t are queued up. If so, it returns this message with the size in the parameter n. If not, it checks to see if there are any messages of type t waiting to be received, receives that message and returns it again with the size in the parameter n. Finally, if it can find no messages of type t, it returns NULL with n set to 0. SpComm_GetPendingMessage(t, n) checks the incoming queues for a message of type t. If none are available, it returns NULL with n set to 0. Otherwise, it returns the message with n set to the size of the message returned.The coordinated message send and receives are similar to the non-blocking reduction. There are three send calls, similar to the standard send calls:void SpComm_CoordinatedSend(int dest, int nbytes, void *buff)void SpComm_CoordinatedSend(SpDestination *dest, int nbytes, void *buff)void SpComm_CoordinatedSend(int nbytes, void *buff)Again, these correspond to uni-cast, multi-cast, and broadcast calls where dest can be an integer value representing the node number or a set of node numbers, nbytes is the size of the message and buff is a pointer to the actual message being sent.There is a single coordinated receive call:void *SpComm_CoordinatedReceive(int &nbytes)that returns a pointer to the message and sets nbytes to the message length if there is a message for this node. If there are any messages in transit for this node, this call blocks until those messages arrive and it can return them. If this node has received all the coordinated messages destined for it, the call returns NULL.All the receive calls return char * buffers that have been cast to void * and allocated with new. This means that the user should use delete[] to free this memory when it is no longer needed. It should also be noted that it it is an error to make a synchronization call while one is in the middle of a coordinated send/receive cycle.Since all of the send calls are non-blocking, it is assumed that all messages are actually sent upon return from these send calls. In the event one is unable to send the message and the send call queues the message for later delivery, any call to either SpComm_Receive(void) or SpComm_CoordinatedReceive(int&) will attempt to send any queued messages trying to receive messages.3. An ImplementationThis research created an implementation of SpeedesComm using a combination of TCP/IP and shared memory. TCP/IP is used to send messages between separate computers and shared memory is used to send messages within a single SMP (Shared Memory Processor) box. This library has been extensively tested and is presently in use in SPEEDES [2,3,4]. This research assumed that each process is going to have an entire CPU dedicated to it. This means that the process can wait in polling loops in the reductions and message passing which can lead to extremely fast implementations for the shared memory communications (as described below).3.1 Fast Shared Memory Reductions without SemaphoresMany libraries for parallel programming rely on semaphores in order to synchronize different processes running on the same shared memory machine. A drawback to this is that semaphores can be unacceptably slow. If one is willing to poll on a location, extremely fast reductions can be achieved. Figure  SEQ Figure \* ARABIC 1: Synchronization design for shared memory.Figure 1 shows the process graphically but let’s consider an example.  Suppose we want to compute the global maximum of the values 8, 2, 5, 9, and 7 for nodes 0-4.  Each node fills the value in its slot on the leftmost column of the graph, sets a toggle in that box to signal that it is finished, and proceeds to the next box.  In this case, node 1 proceeds and then looks to see if node 2 and node 3 have set their toggles; when they have, node 1 computes the maximum of node 2 and node 3’s values (max(5, 9) = 9), fills that value in its box, sets the toggle to signal node 0 that it is done, and then proceeds to the next box. Similarly, when node 0 finds that itself and node 1 have completed the first level, it takes the maximum (max(8, 2) = 8), places it in the next box, and sets the toggle. Finally, Node 2 detects when node 4’s value (7) is ready, moves it into its own box, and sets the toggle.  This completes the first level of the computation; local maximum values have been placed into the boxes in the second column and toggles have been set, indicating that these boxes are ready for the next level.  In the second level, node 0 begins by noticing that itself and node 1 have completed the first level; it takes the maximum of 8 and 9, places 9 in the next box and sets the toggle.  Node 1 only has to consider one box and places the number 7 in its box, sets the toggle and waits on the final box.  Finally, node 0 notices that nodes 0 and 1 are done, takes the maximum of 7 and 9 and places the value of 9 in its box. Node 0 then grabs the maximum value calculated on the network nodes, figures the greater of that value and its current max, and places it in the final box. Then, after it sets its toggle, all nodes are released to continue their computations.Reductions such as mins, and maxes are performed the same way, with each node filling in its toggle as well as its value for the reduction at the 0th stage. Each node then performs the reduction on its two children and fills in that value immediately before filling in its toggle and proceeding to the next level of the reduction. Similarly, node 0 provides its reduction value to the network and then receives its value back. This assumes that the network provides some sort of reduction mechanism (perhaps a comm server) but this reduction could take place at a specific node instead.Fuzzy barriers are performed by using the same structure but never blocking if the child nodes have not filled in their toggles.  The SpComm_ExitFuzzyBarrier() call recalls where the SpComm_EnterFuzzyBarrier() call left off and attempts to finish the reduction, returning 1 if the end is reached and 0 otherwise.It should be noted that the time for a reduction should go up as the log2 of the number of computers. This leads to excellent scalability when one is using only shared memory.3.2 Fast Shared Memory Message Passing without SemaphoresSimilar to reductions, message passing is also performed without semaphores in order to get the maximum speed possible.To achieve this, one shared memory segment is allocated for each node and then divided into two pieces, one for the actual message passing and the other as a sort of mail box pointer. Consider Figure 2. In this figure, Node 0 has four messages (A through D) waiting to be picked up and has six messages (two from node 1 and four from node 2) that it needs to pick up. Node 1 has three outgoing messages (A through C) and three messages it needs to pick up (one from node 0 and two from node 2).To send a message, a node needs to perform two tasks. First, the node checks to see if it has enough space in its circular buffer to write the message. For example, in Figure 2, suppose node 0 has a message of size 80 to send and that its circular buffer has room for 1500 bytes. Since the last message ends at 960 bytes, node 0 has room to send the message and so node 0 write the message into the buffer (it actually needs more than 80 bytes but this will be addressed later). If node 0 does not have enough room in the circular buffer, node 0 tries to purge out any read messages in the buffer and then tries again. If node 0 still cannot write the message, the message is queued for later delivery.Second, node 0 needs to announce this message to the remote node. Again consider  REF _Ref409494594 \* MERGEFORMAT Figure 2 and suppose this 80 byte message needs to go to node 1. Node 0 checks its mailbox in Node 1’s shared memory segment. Notice that there are 5 ‘-1’s in the Node 0 row for Node 1’s shared memory segment. Node 0 goes to the -1 just after the last message that node 1 has not picked up (this is the third slot) and fills in the number 960 (Node 0 could also store this information for later use so that it does not need to search for the next available slot). This indicates that Node 1 now needs to pick up two messages from Node 0 which happen to be the first and the last message in Node 0’s circular buffer.Figure  SEQ Figure \* ARABIC 2: Message passing shared memory segments.It is possible that a node cannot announce the message to the remote node. In this case, an announcement is queued up, and before sending any further messages to that node, announcements are made for any previously written messages. Performing the tasks in this order insures in order deliver of all the messages (even though in order delivery is not a requirement of SpeedesComm.In order to understand how message receiving operates, one needs to understand a bit more about the structure of each of the messages. This is shown in  REF _Ref409494671 \* MERGEFORMAT Figure 3. Each message is written with a number of headers in front of it. For example, if the message were to be sent to 5 different destinations, rather than writing 5 copies of the message in the current node’s circular buffer, just one message is written with 5 headers in front of the message. Read is a flag that indicates whether or not this message has been read from that header, type is the message type, Nbytes is the size of the message, and offset indicates the offset from this header to the actual message. Figure  SEQ Figure \* ARABIC 3: Message headers for shared memory message passing.Again, consider sending an 80 byte message from node 0 to nodes 1 and 2. A node would check to see if 80+2*sizeof(MessageHeader)bytes are available in Node 0’s circular buffer. If so, the node writes in the headers followed by the message. The first header has the offset set to 2*sizeof(MessageHeader) and the second header has the offset set to sizeof(MessageHeader). Nbytes is set to 80, Read is set to unread, and type is set to the message type.Reading is then done in several steps. First, a node loops through the other nodes’ mailboxes in the current node’s shared memory segment. Once this node finds a mailbox that has messages for it, this node starts from one beyond the last mailbox slot to be read, and finds where this node can pick up the next message. Again, consider  REF _Ref409494594 \* MERGEFORMAT Figure 2. Node 0 notices that it has four messages waiting to be picked up from node 2. Node 0 remembers it last picked up a message in the fourth mailbox slot so it goes to the fifth slot and sees that it has a message at position 672 of Node 2’s circular buffer.Node 0 then goes to position 672 of Node 2’s circular buffer, moves offset places to the actual message. Node 0 processes the message and then changes Read to unread at position 672, freeing up some space in Node 2’s circular buffer.Finally, Node 0 returns to Node 2’s mailbox in Node 0’s shared memory segment and mark the fifth position with a -1 which indicates we have picked up the message. Node 0 can then move on and pick up the message indicated in the sixth slot at position 968.In SpeedesComm, read messages are copied into a segment of memory allocated by new. These messages are then queued until a message of that type is requested by a later SpComm_GetPendingMessage(...) call, a SpComm_Recieve(int, int&), or a SpComm_CoordinatedReceive(...) is made for that particular type of message.It is always possible that an application may try to send a message larger than the size of the circular buffer. When this happens, this implementation first sends a short message indicating that a multi part message is coming and provides the size of the full message. The message is then sent in pieces until the entire message has been sent.3.3 Network InterfaceBy creating a shared memory solution, this implementation also allows for a network interface while retaining the speed of shared memory when communications could go through shared memory. The network interface has some of the same calls, eliminates many, and adds in a small number of additional calls. There are a few calls directed towards general initialization:void SpComm_Net_Initialize()void SpComm_Net_Terminate()void SpComm_Pre_Fork_Net_GetIDs(	int **localNodeIds, int &numBoxes, int **nodeBoxNumber, int &boxNumber)void SpComm_Post_Fork_Net_GetIDs(	int **localNodeIds, int &numBoxes, int **nodeBoxNumber, int &boxNumber)SpComm_Net_Initialize() and SpComm_Net_Terminate() initialize and terminate any network communications that may be required. The Pre_Fork and Post_Fork calls perform similar operations but the difference is that they are executed before or after the fork() call in the shared memory initialization.The ...NET_GETIDs() calls are designed to set up mapping tables to describe how the network is linked. For example, consider a situation with two boxes, three CPU's on one box and two CPU's on the other. In order to know if a particular message should be sent over the network or through shared memory, the shared memory code needs a table such as  REF _Ref452254168 \h  \* MERGEFORMAT Table 1 (see also  REF _Ref409503669 \* MERGEFORMAT Figure 4):Table  SEQ Table \* ARABIC 1: Node and Box IDs.Global IDLocal IDLoc Node0Box #00001100202132004121In this case, the box with 3 CPU's was assigned global node ID's 0, 1, and 3 while the other box was assigned global node ID's 2 and 4. Box number is needed to determine if both nodes are on the same box and local node zero is important so that the shared memory code can reduce network communication traffic for broadcasts and multicasts and complete reductions through each local node 0. Finally, once a message is sent to a node, a given node wants to be able to convert global node id's to local node id's (used solely for shared memory) so that that node can pass the message to the appropriate destination in the case of a multicast or a broadcast.3.4 Network ReductionsSix calls are used for reductions:void SpComm_Net_Enter_nbRedop()int SpComm_Net_nbRedop()void SpComm_Net_Redop()void SpComm_Net_Sum(int *ival, double *dval, SpSimTime *simTime, int comPath)void SpComm_Net_Max(int *ival, double *dval, SpSimTime *simTime, int comPath)void SpComm_Net_Min(int *ival, double *dval, SpSimTime *simTime, int comPath)SpComm_Net_Enter_nbRedop() simply makes a request to enter the fuzzy barrier (non blocking reduction) and then SpComm_Net_nbRedop() queries the network to see if the fuzzy barrier is finished and returns 1 if completed, 0 otherwise. SpComm_Net_Redop() blocks until all nodes have synchronized and then returns, allowing each machine to proceed.It is assumed that network traffic is significantly slower than shared memory traffic and that the cost difference of sending a small message versus sending a slightly larger message is minimal. For these reasons, the reductions were reduced to just three with the tradeoff of a slight increase in network communications. SpComm_Net_Sum() finds the sum of the ival, dval, and SpSimTime and returns the sum in the same variables. SimilarlySpComm_Net_Max() and SpComm_Net_Min() respectively find the minimum and maximum of their arguments.3.5 Message PassingThe number of message sending calls can be reduced by using special message types to correspond to the coordinated sends and receives. There are then only 5 different calls:void SpComm_Net_Send(int dest, int type, int nbytes, void *buff)void SpComm_Net_Send(SpDestination *dest, int type, int nbytes, void *buff)void SpComm_Net_Send(int *dest, int type, int nbytes, void *buff)void SpComm_Net_Send(int type, int nbytes, void *buff)void SpComm_Net_Receive()The first call naturally sends a message to a single destination. The second sends a message to multiple destinations where the SpDestination class is as described in earlier sections. The third call also does a multisend but the destinations are contained in an array. The 0th element of this array contains the number of destinations N and then elements 1 through N contain each of the destinations. The fourth call does a broadcast to all nodes except for those on the originating box. The single receive call attempts to receive all the messages it can from the network and then route them on to their appropriate destinations on the current box.3.6 TCP/IP implementationTCP/IP is handled through the use of a communications server with all communications going through the server. This solution easily takes care of some issues along with creating potential problems in other areas.Every SpeedesComm process creates a socket connection to the SpeedesServer that resides at a known socket on a given machine. All reductions are done in the server and all messages are passed through the server. The client side always uses blocking send() calls and uses non blocking receive() calls. Multi-cast and broadcast messages are sent to the server and then the server resends a single message to local node zero of each box that contains a destination. That node then routes the message to the necessary destinations through shared memory.The server always uses non-blocking send() and receive() calls and cycles between trying to read messages and trying to send messages. This allows the server to switch to another client if the present client either cannot receive or has stopped sending in the middle of a message. This prevents the deadlocking caused by a client doing a blocking send(). If a client stops sending in the middle of a message, the server will move on to another client and begin receiving from it or will try to send any messages it has queued up, eventually returning to the original client (who may have completed its send). This also solves the problem of a client trying to send to another client that is busy and having to wait. Additionally, clients only have to check one socket for incoming messages rather than scanning through a list of sockets for each node not on its box.A significant potential problem with this implementation is that the SpeedesServer may become a bottleneck in the communications. This follows directly from the question of the scalability of the system. As one can see from the tests given later, this is not a significant problem for small (less than 20) numbers of nodes. It will also not be an issue when run over a serial network such as Ethernet.Figure  SEQ Figure \* ARABIC 4: TCP/IP message passing using client/server approach (GID = Global ID, LID = Local ID).4. Data Types for Heterogeneous ComputingMany different solutions have been proposed and used to manage the data-marshaling problem when one attempts to run parallel programs across a network of heterogeneous computing. A common technique is to pack the messages using some common format such as XDR [1]. This requires greater effort on behalf of the developer since each message must meticulously packed before it is sent and unpacked after it has been received. This is true even if the message contains a large set of items, only small, unknown subsets of which are of interest to the receiver. To avoid this problem and to allow one to simply memcpy() the structure both on sending and receiving, this research has created two classes that handle data marshaling for integers and doubles. If one constructs the message data out from these types, no packing of the message needs to be performed.These data types are called NET_INT and NET_DOUBLE. In normal arithmetic expressions, NET_INTs and NET_DOUBLEs behave just like integers and doubles;  but when shipped to a machine that has a different endian scheme, it automatically converts itself to the local representation.class NET_INT {	private:		char status[4];		int Value;	public:		NET_INT(int);		operator int() const;		operator --();		operator ++();		...		operator <<=();};The abstraction states that the true value of the NET_INT is returned and that the data type is not modified upon access. The status data member contains the architecture type of the machine on which this data type was last accessed. If it is then accessed on another machine with a different architecture (perhaps little endian versus big endian), then Value is changed to reflect the new machine’s architecture and the value in status is changed and the status slot is updated to the current architecture. On subsequent accesses, Value will no longer need to be changed and can be returned.This requires that a branch be evaluated each time that the data is accessed and will, naturally, slow down the execution of a program. It is hoped that this will not be a significant impact when this code is run on a modern CPU using branch prediction and multiple pipelines of execution.NET_DOUBLEs have been implemented in a similar fashion except using 8 bytes for status in order to guarantee alignment on 64bit machines. If one is running on a homogeneous network of machines, then one can put the line#define HETEROGENEOUS_DATA_TYPE 0at the top of NetTypes.H which results in type-defining NET_INT to int and NET_DOUBLE to double, thus reverting to “normal” operations.5. BenchmarksThis implementation of SpeedesComm was tested on several platforms. The code was run on a single dual Pentium Pro 200 machine running Linux, on a network of 8 such machines connected by 10baseT Ethernet a 20 processor SGI Power Challenge and a 64 processor SGI Origin 2000.In Figure 5, the latency and bandwidth as a function of the message size is presented when using two of the Linux boxes for clients and a third box for the comm server. Latency was determined by “Ping-ponging” a single message back and forth between two nodes and computing the average time to send. Bandwidth was determined by repeatedly sending a single message from one node to another and then determining the total number of bytes sent over the given time interval.The latency grows as expected, remaining constant for small messages and then growing exponentially once the messages began to exceed the size of the TCP/IP packets. The bandwidth grows quite naturally up until 256 bytes, doubling when the size of the message doubles. A largeFigure  SEQ Figure \* ARABIC 5: Linux TCP/IP performanceleap is achieved at 512 bytes that could correspond to matching the size of the TCP/IP packets well. After that point, the growth grows linearly and appears to top out at approximately 6Mb/sec The same tests were performed using two nodes of the same Linux boxes but shared memory was used for the communication medium. The results are presented in Figure 6.Figure  SEQ Figure \* ARABIC 6: Linux shared memory performance.The change in performance of the shared memory communications behaves similar to that of the TCP/IP when the message size is varied. Latency is relatively constant for small messages and then grows exponentially with larger messages. Bandwidth grows linearly, doubling with the increasing message size until a breakpoint is reached and then it grows more slowly until a maximum of around 225Mb/sec is achieved.Figure 7: Time for a shared memory reduction as the number of nodes increase on a 20 processor SGI power challenge. The first chart is using shared memory while the second chart is TCP/IP.The time required for a reduction was then explored. Using the 20 processor SGI Onyx, the number of processors used was increased and the average time per reduction was measured. These times are presented in Figure 7. Figure 8 Shared memory reductions with a 64 processor SGI Origin 2000The analysis of the method used to complete a reduction implied that the time to complete a reduction would increase with the log2 of the number of nodes. The tests on the SGI Onyx seem to imply quite strongly that the time required for a reduction actually increases linearly with the number of nodes.  When the same reductions are performed on a 64 processor SGI Origin 2000 with 250mhz R10000 processors, exactly the same sort of behavior is seen (see Figure 8) with the same likely explanation.There are several possible explanations for this behavior. First, the memory access could be serialized which limits the minimal time for a reduction to a linear function of the number of nodes. Second, whenever a node fills in its portion of the reduction tree, this would then invalidate the cached value of the reduction tree contained by the other CPU’s. This would lead to cache thrashing and the linear growth that was observed. For the final tests, latency and bandwidth were explored using two nodes of the SGI Origin 2000.  For these tests, very similar data was obtained as on the other machines.  Bandwidth grows very quickly until a peak is reached and then levels out for the remaining message sizes.  Similarly, the latency times grow exponentially with the size of the message.Figure 9: Message bandwidth and latency for the SGI Origin 20006.ConclusionsThe SpeedesComm library is a small but effective library for parallel programming. It provides the basic services required for most parallel applications and is still small enough to be easily implemented in several paradigms.The presented implementation uses standard UNIX Sys V calls for shared memory and standard BSD socket calls that, making it highly portable. To date, it has been successfully compiled and run on many operating systems including HPUX, Linux, Solaris (Sun and x86), as well OS/2, and Windows NT. This bodes well for porting to even more esoteric operating systems. The plans for future work include an implementation over MPI (due out in 3/99) along with other machine-specific ports. It is hoped that the SpeedesComm interface will be implemented in more optimal methods on various machines leading to higher performance on those architectures.7. References [1]	XDR: External Data Representation Standard request for comments, http://www.census.gov/rfc/rfc1014.txt.gz[2]	Steinman J. 1994, "SPEEDES User's Guide Beta 2.0." The MITRE Corporation and The Jet Propulsion Laboratory.[3]	Steinman, J. 1998, "Scalable Distributed Military Simulations Using the SPEEDES object-Oriented Simulation Framework." In proceedings of the Object Oriented Simulation Conference, (OOS’98), Pages 3-23.[4]	Steinman J., Nicol D., Wilson L., and Lee C. 1995, "Global Virtual Time and Distributed Synchronization." In proceedings of the 1995 Parallel And Distributed Simulation Conference (PADS95), Pages 139-148.[5]	Draft Document for the Real-Time Message Passing Interface (MPI/RT) Standard (5 March 1999), RealTime Message Passing Interface (MPI/RT) Forum.[6]	Gropp W., Lusk E., and Skjellum A., “Using MPI: Portable Programming with the Message-Passing Interface.” (The MIT Press, 1994).[7]	Snir, et. al. , “MPI: The Complete Reference.” (The MIT Press, 1996).[8]	Geist A., Beguelin A., Dongarra J. 1994, “PVM: Parallel Virtual Machine : a Users’ Guide and Tutorial for Networked Parallel Computing (Scientific and Engineering Computation Series).” MIT Press.[9] 	Spirakis P., Gibbons A. (Editors) “Lectures on Parallel Computation.” Cambridge International Series on Parallel Computation, No. 4, Cambridge University Press, 1993.Author BiographiesRONALD VAN IWAARDEN is an analyst for Metron, Inc. (HPCD) and is currently the SPEEDES onsite support representative for War Game 2000 at the Joint National Test Facility.JEFFERY STEINMAN is a division manager for the High Performance Computing Division (HPCD) of Metron Inc. and wrote an early version of the shared memory library presented.GARY BLANK is a senior software analyst for Metron, Inc. (HPCD). Gary developed the client server library used here and is currently writing a high performance RTI for use with SPEEDES.