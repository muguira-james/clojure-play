Verification, Validation and Accreditation of Legacy Software – Can We Get There From Here?Michael A. WhiteBMH Associates, Inc.5425 Robin Hood Road, Suite 201Norfolk, VA 23513-2441757-857-5670, ext. 243white@bmh.com Keywords:Verification, Validation, Accreditation, VV&A, Modeling and Simulation, M&S, legacy softwareABSTRACT: Within the Department of Defense Modeling and Simulation (M&S) Community, Verification, Validation and Accreditation (VV&A) of software products has received a lot of “lip service”.  Everyone seems to know about VV&A, but there remains a vast chasm between the lofty ideals of concept and the “where the rubber meets the road” application.  Managers charged with overseeing the development of software products are necessarily concerned with the resources required to produce a credible, quality representation of the real world.  Misunderstandings concerning basic definitions of terms and the challenges of conducting VV&A of existing software further complicate the process. This paper presents a curriculum outline with examples of implementation for training managers in the procedures for VV&A of DoD-related legacy software.  It covers the importance of VV&A to quality software development and provides a thorough review of pertinent VV&A terminology.  It shows how VV&A activities can be practically integrated within the software development process to save time, money and increase quality, paying particular attention to those areas that are unique to re-use and enhancement of legacy systems.  Finally, it emphasizes the importance of understanding and executing VV&A responsibilities at various levels of program management and development.  For too long, VV&A has been relegated to the position of being an after-thought, talked about, but not properly implemented.  The curriculum outline described in this paper should provide a useful road map for software development managers to get there from here.ForewordBooks on software development processes abound.  Much less has been written on the topic of verification, validation and accreditation (VV&A).  Furthermore, divergent opinions exist between the Department of Defense (DoD) and the commercial software developers on basic definitions associated with the VV&A process.  For instance, one non-DoD definition of validation is “The process of evaluating software at the end of the software development process to ensure compliance with software requirements” [1].  The DoD definitions of verification and validation are: “The process of determining that a model or simulation accurately represents the developer’s conceptual description and specifications” and “The process of determining the extent to which a model or simulation accurately represents the real world from the perspective of the intended use(s)” [3, 4]While they may differ on terminology, both camps agree that the main problem is to provide quality software that meets the customer’s needs within budgetary constraints. The point of divergence is not so much on the objective as it is which path to take to solve the problem and what to call that path.  One publication representing the commercial software development school-of-thought, states, “Customer satisfaction is the ultimate measure of software systems development process value.  If the product fails to yield products satisfying to the customer, the process needs repair” [5].  The authors go on to explain how planning, implementing and maturing organizational culture and processes can achieve the goal.  The purpose of this paper is not to explore the merits of either viewpoint, but to focus on the process of VV&A as it relates to the development of models and simulations for use within DoD.  A collateral purpose is to offer an outline for training managers tasked with Modeling and Simulation (M&S) development and implementation for the DoD.  A few explanatory paragraphs are in order before we launch into our discussion.NOTE: The author chose to use the DMSO RPG as a basis for a VV&A instructional guide.  Although the document is under revision, it contains sufficiently detailed information to serve as a springboard for thought-provoking discussion.What is VV&A?The following definitions are provided to set the stage for our discussion of the VV&A process. These definitions reflect the DoD position on VV&A as defined in DoD Directive 5000.59, Modeling and Simulation (M&S) Management, and DoD Instruction 5000.61, M&S VV&A.Model: A physical, mathematical, or otherwise logical representation of a system, entity, phenomenon, or process.Simulation: A method for implementing a model over time.Modeling and Simulation (M&S): The use of models, including emulators, prototypes and stimulators, either statically or over time, to develop data as a basis for making managerial or technical decisions.  The terms "modeling" and "simulation" are often used interchangeably.Conceptual Model: A statement of the content and internal representations which are the user’s and developer’s combined concept of the model.  It includes logic and algorithms and explicitly recognizes assumptions and limitations.Verification: The process of determining that a model or simulation accurately represents the developer’s conceptual description and specifications.Validation: The process of determining the extent to which a model or simulation accurately represents the real world from the perspective of the intended use(s).Accreditation: An official determination that a model or simulation is acceptable for a specific purpose.From the preceding list of definitions, we conclude that verified software does what it was designed to do, validated software performs close enough to the “real world” to meet its intended use and accredited software has the “official stamp of approval” that it meets the customer’s specific needs.Why Do VV&A?The short answer is to ensure the product meets the customer’s requirements.  One software developer stated the answer thusly: “Many dynamics contribute to the overall success or failure of a simulation system development.  If success is defined as delivering the capability that fulfills the customer’s needs it should be program management’s goal to clearly understand customer requirements then insure the software developer adequately addresses these requirements across the life cycle of the development.  We have to assume these goals are not easily achievable given the number of simulation systems delivered with deficiencies that impact the customer’s ability to employ the simulation for its intended purpose.”A more pointed answer for software that supports the DoD is: “V&V of models, simulations, and data are essential to ensure that simulations correctly fulfill their specific purpose at the lowest possible cost.  Only then will simulation models gain the confidence of user organizations that M&S outcomes are representative of the real world. V&V should be performed during the development of M&S and as part of M&S life-cycle management. Users must also properly accredit or certify each model, simulation, or data set as a prerequisite to its employment for each specific application.”So, why do VV&A?  VV&A results in increased confidence in M&S use.  Properly documented VV&A activities support re-usability in future applications.  Managers employing solid VV&A practices are better able to control costs and reduce risk associated with correcting mistakes after software is developed.  Models and simulations subjected to the VV&A discipline during development provide better analysis.  And of course, DoD instructions tell us we must do it.Where Does VV&A Fit in the M&S Process?The RPG points out that VV&A is an integral part of the M&S Process.  Once M&S is selected as the best means of addressing a problem, the VV&A process begins and continues throughout the M&S Life Cycle. Does VV&A begin after requirements and acceptability criteria are developed, or does the process actually begin earlier?  An emerging school of thought contends that it actually begins with requirements’ analysis, followed closely by development of acceptability criteria and metrics.  The mental discipline required to analyze requirements against an intended use will pay immediate and longer-term dividends by keeping the development team’s efforts focused.  If the customer (Application Sponsor) and the developer can come to some sort of agreement early in the development process, on what the product will look like and how well it will perform, the likelihood of an acceptable model/simulation greatly increases.Who does VV&A?As the DoD definition of verification indicates the developer usually performs verification, since verification is “the process of determining that a model implementation accurately represents the developer’s conceptual description and specifications.”  However, validation of combat or warfighting models and simulations is not necessarily the developer’s forte.  M&S behavior the developer considers adequate, may not meet the “real world” likeness required by the warfighter who wants to use it for training.   The following analogy illustrates this point and why it is important to have a mutual understanding of what the customer really wants (requirements and acceptance criteria).  From an engineering standpoint a new blue Volvo may be quite satisfactory (valid) as a means of transportation (intended use), but if what the user really wanted was a red Porsche, the customer probably will not be satisfied with the Volvo.Who are the Key Players?According to DoDI 5000.61, the following are the key players and the accepted official definitions of the positions:M&S Application Sponsor—The organization that utilizes the results or products from a specific application of a model or simulationAccreditation Agent—The organization designated by the application sponsor to conduct an accreditation assessment of an M&S applicationM&S Developer—The organization responsible for managing or overseeing models and simulations developed by a DoD Component, contractor, or Federally Funded Research and Development CenterValidation (or Verification) Agent—The organization designated by the M&S application sponsor to perform validation (or verification) of a model, simulation, or federation of models and/or simulations.Common Misperceptions About VV&AVV&A is no substitute for sound analysis.  Done properly, VV&A will identify M&S weaknesses but does nothing to correct them or provide workarounds.  Accreditation is not a one-size-fits-all check in the box.  It is a decision that a specific simulation is suitable to use for a specific application.VV&A is never completed, since it is virtually impossible to verify or validate software under every possible situation, condition or application for which the simulation might possibly be used.  And since a model or simulation cannot be verified or validated one time for every conceivable use, it follows that it cannot be accredited one time for all occasions for the same reasons.Principles of Verification, Validation and AccreditationThere is no such thing as an absolutely valid model.VV&A should be an integral part of the entire M&S life cycle.A well-formulated problem is essential to the acceptability and accreditation of M&S results.Credibility can be claimed only for the intended use of the model or simulation and for the prescribed conditions under which it has been tested.M&S validation does not guarantee the credibility and acceptability of analytical results derived from the use of simulation.V&V of each submodel or federate does not imply overall simulation or federation credibility and vice versa.Accreditation is not a binary choice.VV&A is both an art and a science, requiring creativity and insight.The success of any VV&A effort is directly affected by the analyst.VV&A must be planned and documented.V&V requires some level of independence to minimize the effects of developer bias.Successful VV&A requires data that have been verified, validated, and certified.IntroductionAs every manager knows, there always seems to be too little time to accomplish too much work.  Because of this, a common approach among the software development cognoscenti often seems to be: “ Now is not the time to sit with the customer and my software development staff to mutually agree upon what needs to be done next.  Besides, my customer is paying me to figure out what he really needs.  All I need to do is demonstrate the system to him after we are finished coding – and then he will know what he really wants.”  Of course this approach could lead to the following customer comment upon delivery: “I told you what I wanted you to do, and I assumed that you understood what I meant.  But what you just delivered is not what I asked for!” [5].  Although this may be a common approach, it often does not meet the customer’s needs and barring a series of fortuitous coincidences, results in wasted time, money and talent.  It is abundantly clear that the problem is widely recognized and from the Defense Modeling and Simulation Office’s perspective, the solution is equally evident.  To produce software the customer can use, we must properly perform VV&A and it should be an integrated part of the software development process.  Software development managers reading this are probably asking themselves, But how?  To answer that, we will employ a road trip analogy.  In real life, before we embark on a trip, we usually identify some factors about our proposed trip.  We then consult a map, rely on our own experience or consult someone who has made the trip to get directions and make a plan.  For M&S VV&A, we should pose and answer a few inter-related questions: Where do we start?  Where are we? Where do we want to go?  What’s the best way to get there?  How much will it cost?  What are the risks? How do we mitigate risks?On the Road to VV&ATo answer these questions, it is a good idea to look at existing guidance.  The Defense Modeling and Simulation Office (DMSO) VV&A Recommended Practices Guide (RPG) provides a practical approach to comply with current DoD policy and serves as the lynchpin for the curriculum this paper describes.Where do we start?To paraphrase Julie Andrews’ song from “The Sound of Music”, we must start at the very beginning … a very good place to start.  The DMSO RPG points out that modeling and simulation is a problem-solving tool.  Logically, the “very beginning” of M&S development is to identify a problem that needs to be solved. Once the problem is stated, we must establish requirements for a solution to the problem and decide the best approach to solving the problem.  Since it is the topic of our discussion, let’s assume that modeling and simulation is the best application approach. Once we have identified the requirements and determined that M&S is the best approach, we immediately can begin to write code, correct?  WRONG!Where do we want to go?Where is the “very beginning” of VV&A?  To determine our destination, we need to know the model or simulation’s intended use (training, experimentation, analysis, or as a basis for acquisition)?  What does the model or simulation need to do to meet the intended use?  How well does it need to do it?  To answer these questions, the first thing we must do is to analyze the requirements.  This is the phase of software development where we determine the set of real world objects that must be represented in the simulation and the attributes and behaviors that will support the customer’s intended use.  Does VV&A begin here?  Absolutely!  The first step of VV&A is to determine the bounds of the problem. We do this by analyzing requirements to ensure that we understand what the customer wants.  We develop acceptability criteria to specify the elements that will meet the requirements and metrics to provide a means of quantifying these elements.  These elements and metrics reflect whether we intend to model an object or activity explicitly or merely the effects of an object or activity.  Metrics consist of measures of performance (MOPs) or measures of effectiveness (MOEs) and parameters for the metrics.  Before we travel too much further down the road, we need the customer to verify that the list of “things” along with the associated MOPs/MOEs adequately meet their requirements for a specific intended use.  We do not want to model everything in the world nor do we want to exclude elements that are critical to meeting the customer’s requirements for an intended use. The JSIMS VV&A Plan states, “The risk of not bounding the simulation is one of increased cost; i.e., we emulate the real world, or because we don’t have sufficient funding, we build the wrong things at the wrong fidelity and resolution” [8].  Therefore, requirements’ analysis is necessary, whether we are building a completely new simulation or adapting an existing legacy system for a different application.  The mental discipline required to analyze requirements against an intended use will pay immediate and longer-term dividends by keeping the development team’s efforts focused on the objective.  If the customer (user) and the developer can come to some sort of agreement early in the development process, on what the product should look like and how well it should perform, the likelihood of an acceptable model/simulation greatly increases.Where are we?Now can we start writing code?  Not quite yet.  Going back to our road trip analogy, we have to identify where we are before we can plan our trip. Although we have determined that modeling and simulation is the best solution to solve our customer’s problem, what type of M&S do we use?  Our options include developing a brand new simulation specifically focused on the user’s requirements, reusing an existing (legacy) simulation as is, or modifying a legacy system to meet user requirements.  In this case, our thorough analysis of the user’s requirements leads us to recognize that developing a new simulation will not be necessary, and also we find that we have a legacy system that meets some, but not all of the user’s requirements.  Since DoD Policy is to maximize reuse of existing systems, we opt to modify a legacy system.  Although the following may appear to be a “no-brainer”, when we decide to use a legacy system, it is imperative that we determine what we have and how much of it can be re-used for the new application. The first step is to look at previous VV&A efforts, if any, to determine whether the simulation has been validated, and if so for what intended use.  If the intended use for which the simulation was validated and accredited (assuming it was) is similar to our current customer’s intended use, we are in luck!  Even if previous VV&A work doesn’t match our intended use, we can reuse some documentation, such as conceptual models or high level design, thus reducing the amount of new work and saving money. A good place to look to determine how much we can re-use is the archives from previous VV&A efforts.  Documentation should include intended use, acceptability criteria, testing methodology and results, and VV&A decisions.  Also included in the VV&A package should be conceptual models, high level and detailed design documentation and software source code, all extremely helpful in determining the system’s suitability for the intended use.  From this information, we have the basis for comparing our legacy system’s capabilities and our customer’s requirements.  This comparison yields an understanding of the system’s capabilities and shortfalls.  Of course, if no previous VV&A information exists, we’ll need to construct our own VV&A packages. A caveat is in order.  We must ensure that the data is accurate enough and that the models are of sufficient fidelity (life-like) and resolution (level of detail) for our intended use.  The preceding analysis is no insignificant effort.  But, if we don’t get it right up front, the cost is likely to be much greater, showing up later in the process as corrective actions.What’s the best way to get there?OK, we have analyzed the requirements as well as the capabilities and shortfalls of a candidate legacy system.  Now can we start writing code?  Again, the answer is not yet.  We first need to establish a detailed plan for our modification, development and VV&A efforts.  Fortunately, we have laid a solid foundation for our plans by our requirements’ analysis, review of previous VV&A documentation, development of acceptability criteria and metrics.  Our plan should include traceability of requirements through all stages of development.  This accomplishes two important goals: it keeps us focused on our customer’s requirements and provides an audit trail for future VV&A.Once we (application sponsor, developer and validation or verification agent) agree on a vision of the end-result, the developer can define the low-level requirements (detailed design) of the system and the software.  The DMSO RPG calls the resultant document the Software Requirements Specification (SRS).  The SRS defines hardware, software, networks, protocols and personnel needed to execute the model or simulation.  The RPG warns that final software coding before completing the specification is not good practice, but hastens to add that this does not preclude rapid software prototyping or parallel development of selected high-risk code during the specification development.As we plan, some key considerations include identifying resources and availability, defining the schedule and developing supporting plans.  Supporting plans include VV&A testing, configuration management, quality assurance, reporting procedures, formats and schedules.  We have completed our plans.  We have our resources available and scheduled.  The supporting plans are in place.  Now we can start writing code.  As mentioned earlier, VV&A must be an integral part of the development process, if we are to produce quality software that meets the customer’s (application sponsor) needs.  VV&A testing, performed incrementally throughout the development process culminates in an accreditation report submitted to the Accreditation Agent for approval.How much will it cost?Perhaps the question is better put, how much will cost if I don’t do VV&A? The RPG, citing historical data, reports extremes of 5 percent on the low end and 17.5 percent on the upper end of the spectrum, although the mode ranges between 10 and 12 percent.  Of course, the RPG figures consider VV&A as a distinct and separate process when, in fact, VV&A activities are integral to normal software development practices.  Simply put, it is a “pay me now or pay me later” proposition.  Whether we call it VV&A or some other name, the key to customer satisfaction is figuring out what we need the models and simulation to do, how well we want them to do it (resolution and fidelity) and the amount of risk the customer is willing to tolerate.What are the risks?The JSIMS VV&A Plan addresses this question head-on.  “In general, the risk of not properly performing verification is that all of the requirements may not be inserted into the simulation or that elements not required are inserted at the expense of other elements.  Likewise, the risk of not properly performing validation is that JSIMS as a system might not realistically represent the real-world resulting in inappropriate training, mission rehearsal, education, planning and analysis, and doctrine development.  Any corrective actions that would be necessary after fielding a non-validated JSIMS would probably require extensive redesign and re-engineering with a concomitant investment in additional resources by the EAs” [8].We must identify, quantify and prioritize those risks associated with the areas of representation, behavior and interactions of the elements of each model.  But before we can identify risks, we need to determine the elements of the model or simulation needed to meet user requirements.  Each category is evaluated based on the combined impact of two parts.  Risk categories are as follows:Representation:Part 1: impact on overall expected description, distinction, or appearance of simulation caused if model is not represented or not represented wellPart 2: impact on model's description, distinction, or appearance caused if element is not represented or not represented wellBehavior:Part 1: impact on overall expected use of simulation caused if model's behaviors are not accurate or are not presentPart 2: impact on a model's behavior or operation caused if element is not accurate or not presentInteraction:Part 1: impact on overall expected use of simulation caused if model's interactions with other models are not accurate or not presentPart 2: impact on model's ability to interact with other models or elements if element is not accurate or not presentHow do we mitigate risks?A common saw says that knowledge is power.  To increase knowledge, we have a number of tools at our disposal.  One such tool that has been developed to increase our knowledge (and understanding) of risk, is a risk analysis template.  The template consists of a listing of elements judged to be important to the model or simulation we plan to build.  The elements are formulated based on the customer’s requirements, using a subject matter expert’s (SME) best judgement.  The resulting risk analysis actually forms the nucleus of the requirements’ analysis discussed earlier and provides the basis for development of acceptability criteria.Using the risk analysis table for each element of acceptability, we consider the consequence to the overall simulation of an element being missing, poor or wrong and its impact on the model's representation, behaviors, and interactions if an element is missing, improper, or incomplete.  We assign a value to each element in the three categories discussed above, evaluated on a scale of 1 to 10 (least to most).  We then calculate the final score.  An average of the three grades yields a final priority.What do we do with the risk analysis, once we have completed it?  To interpret the score, bear in mind that the risk analysis includes some degree of subjective appraisal.  For this reason, it is not significant if one element is a few decimal places higher than another element.  The elements should be considered according to how they fall out by groups.  For example, in a given model, one might notice that a group of similar elements all average 8 to 9.  These are important.  In the same model, a second group of similar elements may only average 4 to 5.  These are less important.  Given a fixed quantity of time and money, effort should be focused on the first group of elements.  By focusing on the high priority elements, first, we effectively optimize our resources.SUMMARYThis paper has endeavored to show how to apply the fundamentals of VV&A as outlined in the DMSO RPG.  The following definitions are key to correctly interpreting DoD guidance.  From the examples of practical application enumerated throughout this paper we understand the importance of VV&A to quality software development.  We have discussed some methods and activities that mangers may implement to practically integrate VV&A into the software development process to save time, money and increase quality.  We have paid particular attention to those areas that are unique to re-use and enhancement of legacy systems.  Altogether we have laid out a roadmap that enables us to “get there from here”.Key VV&A definitions:Verification—The process of determining that a model implementation accurately represents the developer’s conceptual description and specificationsValidation—The process of determining the manner and degree to which a model is an accurate representation of the real world from the perspective of the intended uses of the modelAccreditation—The official certification that a model or simulation is acceptable for use for a specific purposePractical ConsiderationsWe do VV&A for a couple of reasons.  First of all, because the process helps us establish the credibility of a model or simulation for a specific intended use.  And secondly, because the DoD says we must if we are developing models or simulations for them.VV&A is an integral part of M&S development.  The process reduces software development time, effort and resource expenditures by helping us to focus our efforts on the elements that meet the application sponsor’s requirements.  How much does VV&A cost?  In reality, nothing, since VV&A activities are integral to normally accepted software development practices. The best way to determine the scope of VV&A activities you need is to conduct a thorough analysis of the application sponsor’s requirements and to ensure that acceptability criteria are adequately defined.  Among many analytical tools available is the requirements’ analysis template.  Realistically, a list of requirements and their associated acceptability criteria should be agreed upon and verified by the customer.  Otherwise, you run the risk of doing a great job of providing the wrong thing.An Outline of Verification, Validation and Accreditation (VV&A) CurriculumCurriculum OverviewThe proposed curriculum can be presented in five sessions over a three-day period.  Each session requires some advance reading to enable the students to actively participate in the training.  Since the audience for the curriculum is likely to be relatively heterogeneous, comprising differing perspectives and various levels of experience, the curriculum is rather broad in scope.The first day is divided into two blocks of instruction, covering the first three chapters of the DMSO RPG.  Starting with basic definitions, the first block of instruction endeavors to familiarize the training audience with VV&A fundamentals, dispel some common misconceptions and discuss twelve VV&A principles listed in the DMSO RPG.  The second block of instruction focuses on familiarizing the training audience with VV&A processes, the M&S life cycle and provides a thorough review of VV&A terminology.  The second day also is divided into two blocks of instruction and covers Chapters 4-6 of the DMSO RPG.  The first block introduces the training audience to the 76 verification and validation techniques and 18 statistical validation techniques described in the VV&A RPG and provides guidelines for their use.  The second block presents the training audience with the concept, process and role of accreditation as well as addressing common VV&A reporting formats.On the third day the instructor employs the seminar/group discussion method of instruction. The final block incorporates case studies and provides a forum for discovery of practical applications for VV&A of new and legacy software systems.  ********************************************Block of Instruction #1: Title: Introduction to Software Verification, Validation and AccreditationInstructional Method: Lecture/DiscussionApproximate Class Duration: Three hours********************************************“V&V of models, simulations, and data are essential to ensure that simulations correctly fulfill their specific purpose at the lowest possible cost.  Only then will simulation models gain the confidence of user organizations that M&S outcomes are representative of the real world. V&V should be performed during the development of M&S and as part of M&S life-cycle management. Users must also properly accredit or certify each model, simulation, or data set as a prerequisite to its employment for each specific application.” (DoD 5000.59-P, Ch. 4, para F.3.a. Oct 95)********************************************Purpose:  Familiarize the training audience with VV&A fundamentals, dispel some common misconceptions and discuss twelve VV&A principles.Preparation:  Students will read chapters 1 and 2 of the DoD Verification, Validation and Accreditation (VV&A) Recommended Practices Guide (RPG).Topics: What Is VV&A?Why Do VV&A?Where Does VV&A Fit in the M&S Life Cycle?Common Misconceptions Concerning VV&APractical ConsiderationsPrinciples of VV&AMaterials:Definitions SlidesInstructor’s SlidesBreaks: 10 Minutes Every Hour****************************************************************************************Block of Instruction #2Title: VV&A ProcessesInstructional Method: Lecture/DiscussionApproximate Class Duration: Three hours********************************************According to US Army Field Manual (FM) 100-1, the Army formally adopted a set of Principles of War in 1921 that endure today.  With only minor modification to the definitions, two of these principles are most appropriate to VV&A efforts associated with Software Development.  They are:Objective: Direct every military operation toward a clearly defined, decisive, and attainable objective.  Whether planning to develop new software or to reuse/enhance legacy software, program managers and developers must clearly understand user requirements and the intended purpose and they all must agree on what will comprise the military, user and simulation domains. Unity of Command: For every objective, seek unity of command and unity of effort.  Once all have agreed on the desired end state, there must be someone in charge who will ensure that the efforts of all remain focused on the objective.********************************************Purpose:  Familiarize the training audience with VV&A processes, the M&S life cycle and provide a thorough review of VV&A terminology.Preparation:  Students will read chapter 3 of the DoD VV&A RPG.Topics:Background and OverviewDefinitionsThe M&S Life CycleThe VV&A Process in the M&S Life CycleMaterials:	DMSO Glossary – Handout	Instructor SlidesQuiz:Breaks: 10 Minutes Every Hour****************************************************************************************Block of Instruction #3Title: Verification and Validation TechniquesInstructional Method: Lecture/DiscussionApproximate Class Duration: Four hours********************************************Purpose:  Familiarize the training audience with the 76 verification and validation techniques and 18 statistical validation techniques described in the VV&A RPG and provide guidelines for their use.Preparation:  Students will read chapter 4 of the DoD VV&A RPG.Topics: Informal V&V TechniquesStatic V&V TechniquesDynamic V&V TechniquesFormal V&V TechniquesGuidelines for Using V&V TechniquesMaterials:	Taxonomy Slide	Guidelines Slides	Instructor’s SlidesQuizBreaks: 10 Minutes Every Hour****************************************************************************************Block of Instruction #4Title: Accreditation and ReportsInstructional Method: Lecture/DiscussionApproximate Class Duration: Two hours********************************************Purpose:  Familiarize the training audience with the concept, process and role of accreditation as well as with common VV&A reporting formats.Preparation:  Students will read chapters 5 and 6 of the DoD VV&A RPG.Topics:Definition and BackgroundAccreditation’s Role in the Software Application Development ProcessProcess to Support AccreditationRolesVV&A Reports in the M&S Application Life CycleSummaryMaterials:	Instructor’s Slides	Reports - Handout	QuizBreaks: 10 Minutes Every Hour****************************************************************************************Block of Instruction #5Title:  Case Studies and Practical ApplicationInstructional Method: Lecture/DiscussionApproximate Class Duration: Three hours********************************************Purpose:  Provide forum for discovery of practical applications for VV&A of new and legacy software systems.Preparation:  Students will read JSIMS White Paper, JCOS V&V PowerPoint presentation and Mr. Harvey’s paper on V&V from the program management perspective.  Topics:Be prepared to discuss how each approach either meets or fails to meet the policy guidelines as specified in the VV&A RPG.Materials:	Mr. Harvey’s V&V Paper	JSIMS VV&A White Paper	JCOS V&V Presentation	Mr. Kollmorgen’s Data Certification Paper	Mr. Kollmorgen’s  Data Certification SlidesBreaks: 10 Minutes Every Hour******************************************** References[1]	Boehm, B.W., “Software Life Cycle factors”, in Handbook of Software Engineering, Vick, C.R. and Ramamoorthy, C.V., Eds., Van Nostrand Reinhold Company, New York, new york, 1984, 494-518.[2]	“Department of Defense Verification, Validation and Accreditation (VV&A) Recommended Practices Guide”, November 1996.[3]	DoD Directive 5000.59, “DoD Modeling and Simulation (M&S) Management”, January 4, 1994.[4]	DoD Instruction 5000.61, “DoD Modeling and Simulation (M&S) Verification, Validation and Accreditation (VV&A)”, April 29, 1996.[5]	Donaldson, S.E., and Siegel, S.G., “Cultivating Successful Software Development A Practitioner’s View”, Prentice Hall PTR, Upper Saddle River, New Jersey, 1997, [6]	Field Manual (FM) 100-1, “The Army”, Headquarters, Department of the Army, Washington, DC, June 14, 1994, 4-1.[7]	Harvey, E.P., “Simulation System Verification and Validation: The Program Management Perspective”, presented to I/ITSEC Winter Conference, 1998.[8]	Joint Simulation System (JSIMS) Verification, Validation and Accreditation (VV&A) Plan, version. 1.0.1 Coordination Draft 14 May 99, 7-7, 7-12.Author BiographyMICHAEL WHITE is a systems engineer with BMH Associates, Inc.  He holds a B.S. in Professional Aeronautics and an MBA in Aviation. He provides subject matter tactical and technical expertise for the design and V&V of cross-domain interactions between Ground and Maritime Synthetic Forces.