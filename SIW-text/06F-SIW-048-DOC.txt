MATREX Data Collection and Analysis:Linking Simulation Results to Military Analyst RequirementsMichael FogusBenton BorumRaytheon - Virtual Technology Corporation5510 Cherokee AvenueSuite 350Alexandria, VA  22312 HYPERLINK "mailto:mfogus@virtc.com" mfogus@virtc.com HYPERLINK "mailto:hborum@virtc.com" hborum@virtc.comDavid ProchnowThe MITRE Corporation7515 Colshire DriveMcLean, VA  22102 HYPERLINK "mailto:prochnow@mitre.org" prochnow@mitre.orgKuan PennScience Applications International Corporation5971 Kingstowne Village ParkwaySuite 410Kingstowne, VA  22315 HYPERLINK "mailto:kpenn@objectsciences.com" kpenn@objectsciences.comKeywords:MATREX, FOM, DCA, data collection, data analysis, AARDVARCAbstract: The U.S. Army’s Future Combat Systems (FCS) program represents its primary initiative to reduce capability gaps it has assessed against projected future threats. Developing the FCS will be instrumental to the Army transforming into its Future Force, and will also contribute to the ongoing war on terrorism by providing FCS technologies to deployed forces as those technologies become available.Due to the cost and time advantages available from simulation based experiments, computer simulation will be a key component of the US Army’s transformation. However, this also presents a challenge in that simulations generate large quantities of data that are difficult to analyze efficiently.  As a contributor to the Army’s transformation process, the Modeling Architecture for Technology, Research, and Experimentation (MATREX) program has an initiative to reduce the data mining and translation time associated with simulation experiments and exercises. The MATREX initiative, which has been named Data Collection and Analysis (DCA), includes a component, or federate, within a High Level Architecture (HLA) run time environment that facilitates the generation of decision enabling data based on U.S. Army doctrine including vocabulary and terminology utilized by the military community. The goal of the DCA effort is to provide for data to be processed, based on adaptable rules, in near-real time.  With DCA, raw data captured during simulation execution will be reduced and transformed into content and format that is immediately suitable for analysis by military subject matter experts.During times of peace, change is usually slow and deliberate. In wartime change must occur as quickly as possible to support deployed combat forces. To respond to the challenges of the Global War on Terrorism, the US Army has accelerated its transformation.  To respond to the Army’s accelerated transformation, the MATREX program is accelerating and providing enhanced agility to the simulation-analysis-decision cycle through its DCA initiative.1. IntroductionModeling and simulation benefits military applications in multiple domains, including training, research analysis and acquisition decision support.  Moreover, the Army requires that all studies supporting Army R&D, acquisition and training should be performed with state-of-the-art technologies. [1] However, in every domain the simulation generated data is of little value, or even useless, if the data cannot be analyzed effectively by the appropriate subject matter experts and done so on a timely basis.The Army increasingly employs modeling and simulation (M&S) as an integral part of equipment acquisition test and evaluation. M&S efforts are used to reduce time, resources, and risks relative to test and evaluation programs. [2] Further, research and acquisition decision support simulations must utilize the results obtained from repetitive simulation executions employing differing but mutually supporting input parameters. Collectively, Army acquisition related M&S requirements are a formula for the generation of massive quantities of data requiring analysis by subject matter experts from numerous and often unrelated specialties (i.e. equipment maintenance, indirect fire support, medical support, etc.)The After Action Review (AAR) capability is crucial to the Army’s conducting effective training. AAR feedback must compare the actual output of a process with the intended outcome. By focusing on the task's standards and by describing specific observations, leaders and soldiers identify strengths and weaknesses and together decide how to improve their performances. [3] As Smith and Allen state, “feedback is a key aspect of the training process because at this point the person being trained is given information that allows him or her to assess how well their performance is meeting the recognized objectives.  With this information in hand either the person being trained, the trainer, or both can take actions that will sustain or improve performance.” [4] Thus, training support simulation data must be collected during simulation execution and be available in subject matter format immediately after execution.  Furthermore, as Graebener points out in discussing joint experimentation work conducted at the Joint Forces Command (JFCOM), “Today the modeling and simulation (M&S) and operations research communities are faced with ever increasing challenges to meet the demand for creditable and quantifiable results.” [5] This paper discusses the Data Collection and Analysis (DCA) process, as it is applied within the US Army Modeling Architecture for Technology Research and Experimentation (MATREX) program, but with the emphasis on data analysis.1.1 DCA Challenges of Data VolumeThe greatest problem in generating simulation data tailored for subject matter expert use is the sheer amount of data produced by computer simulations.  An example is illustrative. From May through October 2004, the US Army coordinated a Future Combat Systems (FCS) Analysis of Alternatives (AoA) study which included a diverse team from across the Army, Federally Funded Research and Development Centers, industry and the FCS Lead System Integrator (LSI). After conducting over 10,000 wargaming or simulated scenario hours in preparation, the AoA study conducted more than 2,000 additional scenario hours for record. To provide further perspective, one MATREX Blue Brigade versus one Red Division simulation, permitted to run for two hours, produces two to three million data records. Thus, the AoA study produced enormous quantities of simulation data which proved exceptionally valuable, but also time consuming to evaluate. The data mining and translating effort of any FCS like simulation almost always requires more time than preparing for and executing the actual computer simulation. 1.2 DCA Challenge in Complex Simulation EnvironmentsAnother issue, especially with complex distributed systems, is that data collected from multiple sources is frequently difficult to incorporate into a system-level analysis.  For instance, as part of the U.S. Army’s Cross Command Collaborative Effort (3CE), the Distributed Test Event 5 (DTE 5) exercise integrated live, virtual, and constructive simulations from multiple organizations, with one of the objectives being to “generate meaningful, credible, and replicable analytical results from data loggers, analyzers, and other tools.” [6] The DTE 5 effort established a good foundation for future work, but the 3CE Data Analysis Working Group (DAWG) subsequently identified several areas for improvement.  Specifically, they noted that DTE 5 generated massive amounts of disintegrated data, with data existing on multiple servers in multiple formats.  In addition, information sharing was difficult because each organization had difficulty interpreting the raw or processed data from other organizations. Further complications exist when, as in the case with DTE 5 [7], simulation data is captured in multiple architectures, such as High Level Architecture (HLA), Distributed Interactive Simulation (DIS), and Test and Training Enabling Architecture (TENA). Data collection tools exist for each of the above simulation architectures, but there are significant challenges associated with incorporating the collected data into an integrated analysis.  The challenges include issues of format consistency, event sequencing, causality, and resolution differences.1.3 DCA Challenges Understanding Simulation DataThe raw data generated by a simulation system can be very difficult to analyze.  Data loggers capture data directly from the simulation infrastructure, but typically this raw data is of marginal use to an analyst.  Firstly, Federation Object Models (FOMs) – the definition of data exchanged during execution – is typically tailored for runtime interoperability.  While the observation of individual updates or specific events can serve some purposes, the main value of analysis comes from the assemblage of appropriate data into meaningful results.  In many cases, data needs to be aggregated to show the cumulative effect of an item or event under analysis.  Examples are statistics for combat effectiveness, survivability, and message generation.In other cases, a chronological sequence of events must be analyzed.  For instance, if a military analyst is to evaluate the chain of events from a perceived threat through a call for fire during a combat engagement, the identification and ordering of individual events (or interactions in HLA parlance) for that specific engagement must be provided.  In this example, acquisition and engagement data will be represented in different HLA interactions, and the shooter may not be the same as the HLA object that perceived the data, so a complex series of events may need to be traced to make the appropriate associations of acquisition event to engagement event.  As Porter points out, this can only be accomplished with traceability built into the FOM: “Every message/interaction must have a ‘thumb print’ from every workstation or node that has input to the message and every node that receives it.” [8] In addition, the raw data may contain a significant amount of superfluous data, such as redundant updates of identical data that should be filtered out to facilitate analysis. Finally, for analysis of future concepts, such as Network Enabled Battle Command (NEBC), there is a lack of appropriate DCA tools.  As Blechinger et al state, “the ability to measure the contribution of the network to the overall effectiveness of a combat force has proven to be problematic. The suite of analytic tools currently available more than satisfied the need to measure the lethality and survivability of a force; those same tools were found lacking when used to explore the contribution of information as an element of combat power and the ability of a network to provide information to the war-fighter.” [9]2. MATREX Program OverviewThe US Army Modeling Architecture for Technology Research and Experimentation (MATREX) program emerged from a combination of two previously existing efforts, the Joint Virtual Battlespace (JVB) Science Technology Objective (STO) and the Army Virtual Distributed Laboratory for Modeling and Simulation (VDLMS) STO.  (See [10, 11] for background on JVB.) The MATREX Program is sponsored by the US Army’s Research, Development, and Engineering Command (RDECOM) and includes participation by the various Army Research and Development (R&D) centers. MATREX provides a persistent, secure distributed simulation environment that reduces the time and cost of experimentation.  Unlike other DoD simulation environments, MATREX is a reusable simulation environment that enables efficient integration of arbitrary multi-resolution models, represents key characteristics of Network Enabled Battle Command (NEBC), scales to Brigade Combat Team, supports human-in-the-loop interactivity and provides the ability to incorporate real hardware.  MATREX provides the Army with a set of Simulation and Modeling tools for Acquisition, Requirements and Training (SMART) that will support a wide variety of analyses of system designs and operational concepts (see [12] for background on the Army SMART efforts).  MATREX is a simulation environment that includes:A Federation Object Model (FOM) suitable for simulating current and future military forcesSoftware libraries that can be used by developers to employ HLA and exchange MATREX object dataThe OneSAF Testbed Baseline (OTB), and eventually the OneSAF Objective System (OOS), to simulate battlefield entitiesA collection of federates developed by the US Army Research and Development (R&D) centers which facilitate high-fidelity representation of particular functionalities Software tools for simulation input and federation managementAn emerging Data Collection and Analysis (DCA) capability as discussed throughout this document3. MATREX Data Collection and Analysis (DCA)As stated in Section 2 above, the goal of MATREX is to provide a simulation environment useful for the analysis of future force concepts.  Because MATREX is focused on developing an analysis environment, as opposed to being designed for a fixed set of studies, the Data Collection and Analysis (DCA) effort must be able to accommodate a wide array of studies, few if any of which can be anticipated at this point.   This section addresses how MATREX is addressing DCA.3.1 Analysis Object Model AOMCurrently, the DoD modeling and simulation community does not have a common data schema suitable for data analysis.  While all HLA federations must have a Federation Object Model (FOM) to describe how data is exchanged at runtime, there is no object model describing how data can best be represented for subsequent analysis.  Since the FOM includes many ephemeral events, it is not conducive to data analysis.  Inquiries that may seem simple on the surface are very complex using the raw data captured during HLA runtime.   (Refer to Section 1.3 for the example of determining time between acquisition and engagement.)A MATREX goal has been to create a common format that will allow efficient and meaningful analyses of any HLA derived simulation data.  The standardization of an “Analysis Object Model,” or AOM, would eventually result in the production of tools specifically designed to operate on AOM data.  In that case, each simulation would need to maintain software to convert FOM data into AOM data through data transformation and reduction, but the AOM data analysis tools would seldom if ever require changes since they would be operating on standardized AOM data.3.2 Analyst Oriented DataSimulation specialists and software developers are comfortable using Federation Object Models, software languages such as C++ and Java, and any database containing all of the objects and interactions produced within HLA simulation environments. However, subject matter experts involved in one of the countless disciplines associated with designing, equipping and employing a ground combat force are not. Accordingly, MATREX must collect, reduce and transform data into content and format that is subject matter expert friendly. That means reducing massive HLA generated databases to focused databases and data summaries readable with common tools such as MySQL, Microsoft Excel and Microsoft Access.  In addition, MATREX requires that data analysis be performed at both runtime and post-execution. The results of one simulation usually produce the requirement for a follow-on simulation employing modified input parameters. Only runtime and almost immediate post-execution data analysis can optimize such a repetitive simulation experiment cycle.3.3 Previous DCA Efforts and Lessons LearnedPreviously, MATREX used a combination of tools for its Data Collection and Analysis (DCA) effort.  They consisted of hlaResults®  and the Data Collection and Analysis Tool (DCAT), both of which are described below, as well as additional specialized, often single purpose tools that MATREX has explored.3.3.1 hlaResults® hlaResults®  is a commercial product to collect and play back data generated by  a federation.  It can be configured to collect any of the object classes and interaction classes specified in the Federation Object Model (FOM).  It stores collected data in a commercial database format that can be accessed at runtime or at post-execution. [13] As  is obvious, data cannot be analyzed unless it is collected, and hlaResults® served this latter purpose.3.3.2 Previously Developed MATREX Data Collection and Analysis Tool (DCAT)DCAT supported several small efforts within the project.  DCAT accessed raw simulation data captured by hlaResults® using embedded Standard Query Language (SQL) statements.  While this tool was valuable for supporting some specific simulation experimentation efforts, it was not a good long-term solution.  Most query sets were anchored to a specific scenario and often to a given time frame within that scenario.  In addition, as new MOEs were identified, additional database queries had to be added to the DCAT software.  Thus, the DCAT software would require modification by its software developer whenever the FOM, scenario or the MOEs changed.  While very useful for HLA software development and testing, the DCAT approach was clearly untenable for future expansion and cumbersome for change.  As mentioned above, the preferred approach is always to allow a subject matter expert/analyst to configure an analysis without having to involve a software developer.3.3.3 Other Data Collection and Analysis ToolsWhen MATREX executed in the Distributed Test Event 5 (DTE 5), it also employed the Digital Collection, Analysis, and Review System (DCARS), a government owned data collection and analysis program. DCARS collects message traffic and other data directly from target C4I systems.  It accomplishes this by using instrumentation that links directly to the same routers and switchers used to connect the C4I systems to its larger C4I network.   DCARS also unloads data from the Army Battle Command System (ABCS) databases and copies selected files from the Army Tactical Command and Control System (ATCCS) and the Global command and Control System (GCCS).  In addition, DCARS can distribute screen captures from C4I system displays.  DCARS also collects data from simulation networks, and recently DCARS was enhanced to collect simulation data sent using the MATREX Federation Object Model (FOM) [14].  However, DCARS has little in the way of built-in analysis capability for the HLA data it captures.  Also, there is no mechanism for associating the data collected directly from C4I systems with the data received over the simulation infrastructure.MATREX also reviewed existing, non-MATREX DCA tools with general applicability to distributed simulations.  Due to the totally distributed nature of MATREX simulation environments, analysis tools used exclusively for individual simulations were not considered.   Some of the evaluated tools showed promise for supporting the MATREX DCA requirements but only after major modifications. Universal in the shortcomings of existing distributed simulation data analysis tools was the requirement for the analyst to be intimately familiar with the Federation Object Model (FOM). This is the opposite of the MATREX goal to develop a DCA capability that allows an analyst to perform meaningful analyses without being an expert in the simulation’s software design details.3.4 The New Way ForwardMATREX is now taking a two-pronged approach in addressing its DCA requirements.  First, MATREX is developing prototype software to determine if a flexible approach can be used for taking raw data (i.e. data collected directly from the simulation infrastructure traffic) and transforming it into analyst friendly data according to rules specified in configuration files.  Secondly, MATREX continues to explore non-MATREX DCA efforts to be leveraged onto that of MATREX.4. AARDVARCThe MATREX approach to alleviating the difficulties in the Army’s data analysis initiative takes the form of the After Action Review Data Vicissitude using Adaptive Rule Collection application (AARDVARC).  The core design goal of AARDVARC is flexibility.  Due to the past results of MATREX DCA efforts, AARDVARC is designed to provide flexibility in its input Object Models (OM), its raw data sources and also in its output products.4.1 AARDVARC RequirementsThe requirements generation phase of AARDVARC development produced a core set of functionalities.  First, the tool must be able to take as input an object model and a raw data source and generate “analyst friendly” data.  The term “analyst friendly refers to a set of data geared toward answering the Army’s standard Measures of Effectiveness (MOE).  (The evolution of the content and schema of this data set is beyond the scope of this paper.)  Second, the tool must be able to take data sources of differing formats.  Third, AARDVARC must operate independently of specific scenario parameters.  Finally, the tool must operate in both runtime and post-runtime modes.  The remainder of this document describes how current design and development efforts are working toward fulfilling these requirements for release in future MATREX versions.  4.2 AARDVARC Concepts of UseThe chain of execution for AARDVARC within the MATREX environment is the following:1) Given the MATREX FOM, the user describes through adaptive rules how the FOM’s data elements are mapped to elements comprising the Analyst Database.  The adaptive rules are designed to be object model agnostic, so one could use any FOM to design a transformation process that generates an Analyst Database.  The Graphical User Interface (GUI) used for defining AARDVARC transformation rules is shown in Figure 1.Figure 1. Establishing Adaptive Rules2) After establishing transformation rules, the user then either connects to a running MATREX federation or to an hlaResults® database containing the raw data from a previous federation execution.  AARDVARC is designed to connect to MySQL and Access databases or to an HLA federation.3) AARDVARC then collects and transforms data from either the live simulation or the raw data source into the Analyst Database.  (See Figure 2 for the GUI displayed during this process.) It then outputs the Analyst Database in either MySQL or Access formats.  Figure 2. Data Transformation in AARDVARC4) Finally, the user can then build a set of his/her own queries against the Analyst database with the AARDVARC Data Analysis Tool or use some of the views and reports distributed with AARDVARC. Figure 3 shows the AARDVARC Report Generator GUI, and Figure 4 displays examples of the products of this process. Figure 3. AARDVARC Report Generator SHAPE  \* MERGEFORMAT Figure 4. Sample Data Products4.3 Potential Applications of AARDVARCThe main goal met by AARDVARC is providing the Analyst database.  The primary benefit for using this product is that one can create a set of views based on AARDVARC, rather than against disparate raw data sources.  Therefore, switching between raw data sources supported by AARDVARC out of the box requires no software developer involvement.  Changing to other raw data sources requires only the creation of a new description of its composition, which can be fed into AARDVARC at execution time.  This provides flexibility in data sources, while providing a stable source of information pertaining to Army Measures of Effectiveness (MOE) and Measures of Performance (MOP).  As a side benefit, one can use AARDVARC to identify simulation shortfalls by observing how the raw data generated during simulation execution fails to fulfill the creation of an Analyst Database directly relating to TRADOC’s standard MOEs.   4.4 Technical Description of AARDVARCFigure 5 gives an overview of the AARDVARC capabilities.  As the figure shows, AARDVARC is designed to link to multiple raw data sources including hlaResults®, DCARS, HLA runtime output, and others.  Currently, development efforts have focused on hlaResults® and it is expected to be the only available data source on AARDVARC’s initial release.Figure 5. AARDVARC OverviewDuring the process of developing the maximum flexibility with regard to the inputs and outputs of AARDVARC, the MATREX DCA team identified a number of choke points in the data collection and analysis process:  1) The first limiting factor in achieving a solid data collection and analysis effort lies in the FOM.  That is, collecting information based on FOM elements is only as stable as the FOM itself.  While there is very little to be said about changing FOMs, MATREX hopes to alleviate as much pain associated with FOM changes as possible.  Through the use of adaptive rules, AARDVARC is shielded from many data type conversion issues.  Even when a FOM changes in its entirety the adaptive rule methodology will require only a single point of change in the execution configuration in order to maintain data consistency in the Analyst Database and its associated views (assuming of course that the new FOM contains an equivalent set of elements as the old).  2) The next limiting factor that MATREX identified was that of the simulation execution’s raw data.  As was discovered in all previous efforts, raw data formats tended to be arranged in esoteric manners.  Additionally, creating queries and views against such raw data collections requires changes whenever FOM elements change or the raw data source changes, e.g. hlaResults® to DCARS.  AARDVARC attempts to soften such issues by removing resident knowledge of the raw data source construction and placing it in external descriptor files which are then used to initialize AARDVARC at execution time.  These descriptor files are used to build a raw data source specification internal to the application and agnostic to the specific implementation. This permits the linking of adaptive rules to a common data infrastructure.  3) Consistency in data views proved also to be  an important but problematic aspect of the MATREX DCA effort.  It is difficult to maintain such consistency when changes in the raw data elements cascade through changes to the views themselves.  In order to eliminate the issues of inconsistent views, it was decided that the AARDVARC application must generate the Analyst Database, on which all views would be written.  This of course does not preclude the changing of the Analyst Database; in fact AARDVARC maximizes flexibility in the Analyst Database’s generation.  However, it does provide stability in views by making it a matter of configuration and/or revision control.  That is, suppose that a view is written against an Analyst Database X generated from a hlaResults® raw data source using FOM Z.  In order to ensure that those views always remain consistent AARDVARC would be configured with:Adaptive Ruleset ZhlaResults® descriptor fileAnalyst Database specification XIn other words, the specific generation rules for generating the Analyst Database are external to AARDVARC in the form of specification files.  5. AARDVARC Adaptive RulesThis section discusses Adaptive Rules and how they are used by AARDVARC.  This flexible rule-based approach is integral to achieving MATREX DCA objectives.  5.1 Adaptive Rule Abstraction An Adaptive Rule as used in AARDVARC provides an abstraction level above the relationship between a FOM and its raw data representation.  In implementation, the Adaptive Rule set is an XML file; however a Rule Editor graphical user interface hides this from the user.5.2 Four Components of Adaptive RulesAARDVARC’s Adaptive Rules consists of four components/entries: Entities, Attributes, Constraints, and Action Types.  1) Entities represent object/interaction class names that the user desires to be in the analyst database.2) Attributes are user selected data fields associated with the object/interaction class defined in Entities. 3) Constraints are user-defined rules used to filter data based on attribute values.  4) Action types define how the entity data is organized, they have three enumerated types: Single, Union, and Join.Single type - Single data entities; a simple way to achieve the data reduction goal.Union type - Allows Entities to have more than one object/interaction class, and all classes are inherited from the same FOM super-class.  Data from all classes will be combined during the transformation process as facilitated by the FOM class structure layout. Join type - Provides a way to combine different object/interaction class data based on either the common attributes or attributes shared by classes. Join operations can also be performed to join with existing user-defined rules. 5.3 Internal ComponentsFigure 6 represents the basic architecture of AARDVAC Data Collection and the data flow among its principle components.   This depiction, as well as the following subsections, will aid in understanding how the initial rule set is used to ultimately generate the data analysis products.Figure 6. AARDVARC Architecture5.3.1 AARDVARC Rule MapThe Rule Map is an internal data structure that holds the rule data, and when the user adds a new rule the Data Validation verifies the data against the FOM before the data is persisted into the Rule Map.5.3.2 AARDVARC Simulation Entity Model The Simulation Entity Model (SEM) holds abstraction data objects that are created based on the rules.  The DB Mapping Table on the other hand contains database physical data mapping information.  Together the information from the two data structures is used by the Transformer to generate a Query List; a data format that is understood by the database management system.5.3.3 AARDVARC Data Analysis ComponentsFigure 7 shows the major components of AARDVARC Data Analysis.  The DB Connector links to the Analyst database and the DB Tree Creator presents database tables to the user for selection.  The Query Builder filters Meta Data of user selected tables based on the graph type (Graphs Generator), and only suitable table columns are presented to the user for query creation.  The user is thus shielded from query syntax and data type conversion issues.  The query results are then sent to the Graphs Generator or the Report Generator for plotting or reporting, respectively.    Figure 7.  AARDVARC Data Analysis5.3.4 Database SourcesAs mentioned previously, AARDVARC is designed to abstract the processes required to query raw data sources using external descriptor files.  These descriptor files essentially describe how a database maps FOM elements into database tables.  Like the Adaptable Rule descriptor files, the raw data descriptor files are simply XML documents.  Currently, all development is geared toward working with hlaResults® databases, so it is conceivable that the data descriptor specifications will evolve as new raw data sources are introduced.  6. Future EnhancementsCurrently, the MATREX DCA efforts through the AARDVARC application are rooted within an HLA environment.  However, future expansion into other middleware environments is expected to occur through the leveraging of the MATREX Middleware Independence Capability (MMIC) initiative.  The general idea is to modify the Adaptable Rule behavior such that instead of matching against FOM elements, it would match against Protocore PCOM elements [15].  This presumably would provide middleware independence, although the precise mechanics behind such an implementation have not been fully explored.  In addition, AARDVARC is slated to support using DCARS raw data collections to generate its Analyst Database.  This would be accomplished by creating a DCARS database descriptor file.  However, DCARS must be explored further in order to generate such a file.Finally, the evolution of the Analyst Database is an ongoing effort that will require collaboration with Army analysts and the broader MATREX and simulation communities.7. ConclusionThe Data Collection and Analysis (DCA) aspect of distributed simulation environments has many challenges that have not yet been addressed by the modeling and simulation community at large.  While the DCA efforts of specific simulation systems have been successful and appropriate for specific studies, there is still a dearth of standards for performing data analysis in distributed simulation environments.  In fact, the experience of some distributed simulation executions, especially those using multiple architectures, have underscored just how serious this challenge is. The key to any DCA approach is to link the simulation results to the analyst requirements.  An effective approach would allow data examinations to be configured quickly and effectively based on original and follow-up questions posed by the analyst.MATREX is assessing, through its AARDVARC application, whether a flexible, rule-based approach is feasible for supporting a wide range of analyses of disparate data in an efficient manner.  As a contributor to the Army’s transformation process, the MATREX AARDVARC initiative aims to reduce the data mining and translation time associated with simulation experiments and exercises, as well as to foster the philosophy of defining analyst requirements pre-execution. AARDVARC is a component of an HLA run time environment that generates “decision enabling” data based on U.S. Army doctrine to include vocabulary and terminology utilized by the military community. AARDVARC data will be generated on at least a near-real time basis.  Initial AARDVARC development objectives are:1) Be independent from the scenario and simulation environments but require raw data in a format that is readable by hlaResults®. 2) Support analysis both at runtime and at post execution.3) Transform data from the hlaResults® (raw) format to a MATREX developed Analyst Object Model (AOM) format that will be familiar to any Military Analyst. The AOM will use the vocabulary of the Army’s Universal Task List (AUTL) instead of the MATREX simulation environment’s Federation Object Model (FOM). 4) The AARDVARC GUI will provide a limited selection of “hot wash” reports based upon US Army documented measures of performance (MOPs) with titles such as:Blue and Red Force StructuresBlue Fire EngagementsBlue Killer versus Red Victim Tables (KV Tables)Battle Damage Assessments (BDAs)Sensors to Fire Missions ListsDuring times of peace, change is usually slow and deliberate. In wartime change must occur as quickly as possible to support deployed combat forces. To respond to the challenges of the Global War on Terrorism, the US Army has accelerated its transformation.  To respond to the Army’s accelerated transformation; MATREX is accelerating and providing agility to the simulation-analysis-decision cycle through its AARDVARC initiative.  At this point, it is too early to gauge the success of this initiative, although early returns have been promising.  If this approach continues to show promise, then AARDVARC will continue to be enhanced, or at a minimum, serve as a prototype for developing requirements for new or existing DCA applications.8. AcronymsAARAfter Action ReviewAARDVARCAfter Acton Review Data Vicissitude using Adaptable Rules CollectionABCSArmy Battle Command SystemAoAAnalysis of AlternativesAOMAnalyst Object ModelATTCSArmy Tactical Command and Control SystemAUTLArmy Universal Task ListBDABattle Damage AssessmentDCAData Collection and AnalysisDCARSDigital Collection, Analysis, and Review SystemFCSFuture Combat SystemsFOMFederation Object ModelGCCSGlobal Command and Control SystemGUIGraphical User InterfaceHLAHigh Level ArchitectureJVBJoint Virtual BattlespaceMATREXModeling Architecture for Technology, Research, and ExperimentationMMICMATREX Middleware Independent CapabilityMOEMeasure of EffectivenessMOPMeasure of PerformanceNEBCNetwork Enabled Battle CommandOneSAFOne Semi-Automated ForcesOOSOneSAF Objective SystemOTBOneSAF Testbed BaselinePCOMProto-Core Object ModelRDECOMResearch, Development, and Engineering CommandRTIRunTime InfrastructureSEMSimulation Entity ModelSMARTSimulation and Modeling for Acquisition, Requirements and TrainingSMESubject Matter ExpertSTOScience and Technology ObjectiveXMLExtensible Markup Language9. References[1]	US Army Regulation 70-1, Army Acquisition Policy, 31 December 2003[2]	Ibid.[3]	US Army Training Circular 25-20, A Leader's Guide to After-Action Reviews[4]	Smith, R., Allen, G., “After Action Review in Military Training Simulations”, Proceedings of the 26th Winter Simulation Conference, 1994.[5]	Graebener, R., Rafuse, G., Miller, R., Yao, K., “The Road to Successful Joint Experimentation Starts at the Data collection Trail”, InterService/Industry Training, Simulation, and Education Conference (I/ISEC) 2003.[6]	Liebert, R., O’Connor, M., “Distributed Test Event 5 in Support of the Army Cross Command Collaborative Effort and Future Combat Systems Integrated Test and Evaluation”, Spring 2006 Simulation Interoperability Workshop, April 2006, Huntsville, AL.[7]	O’Connor, M., DiCola, J., Sorroche, J., Lane, J., Lewis, D., Norman, R., “A Mixed Architecture for Joint Testing”, Spring 2006 Simulation Interoperability Workshop, April 2006, Huntsville, AL.[8]	Porter, M., Herman, M., Severin, F., “Process Control and VV&A of Distributed Simulations at Runtime for Network Centric Implementations”, Spring 2005 Simulation Interoperability Workshop, April 2005, San Diego, CA.[9]	Blechinger, P., Gorski, B., Griffen, R., Johnston, K., Joles, J., Kinn, S., “The State of the Art in FCS C4ISR Network Analysis,” Technical Paper prepared for TRADOC Futures Center and Program Manager, Unit of Action, 4 March 2005.[10]	Briggs, R., “Joint Virtual Battlespace (JVB) Framework and Architecture Explained”, Fall 2002 Simulation Interoperability Workshop, September 2002, Orlando, FL.[11]	Harkrider, S., “JVB Federation Design”, Fall 2002 Simulation Interoperability Workshop, September 2002.[12]	US Army Research, Development, and Engineering Command. National Capital Region Experimentation Portal, RDECOM Magazine, November, 2003.[13]	http://www.virtc.com/hlaresults-full.jsp[14]	Cabezas, R., Congrove, J., Lambie, M., Mayer, H., Nash, K., Reid, R., “Software Version Description (SVD) for the Electronic Proving Ground (EPG) Digital Collection, Analysis, and Review System – Edition 2 (DCARS-II) Version Release 1.5.p1,” Electronic Proving Ground, Fort Huachuca, AZ, May 2005.[15]	Grim, P., Snively, K., “ProtoCore: A Transport Independent Solution for Simulation Interoperability”, Fall 2006 Simulation Interoperability Workshop, 06F-SIW-093.Author BiographiesMICHAEL FOGUS is a software engineer at Raytheon-Virtual Technology Corporation working in the distributed simulations arena.  Currently he is the lead software developer for the MATREX C3Grid software and also the lead designer for AARDVARC.  Mr. Fogus received a B.A. in Computer Science from St. Mary's College of Maryland in 1999, and a M.S. in Computer Science from Johns Hopkins University in 2005.BENTON “HOWARD” BORUM is a simulation specialist at Raytheon-Virtual Technology Corporation.  He is currently working on the MATREX program focusing on MATREX federate integration and testing.  Mr. Borum served 30 years in the US Army as an Infantryman and Aviator, completing command and staff assignments from platoon through brigade levels. He holds a B.A. in European History from the Detroit Institute of Technology and an MBA in Finance and Accounting from the University of Puget Sound.DAVID PROCHNOW is a Principal Software Engineer in the Modeling and Simulation Engineering department at the MITRE Corporation.  Currently, he leads the simulation initialization activities of the MATREX program.  Mr. Prochnow has also served as the technical lead on several simulation programs at MITRE.  While at BDM International and Control Data Systems, he developed software for various corps-level and theater-level wargames.  Mr. Prochnow received a B.S. in Computer Science from the University of Virginia in 1983.KUAN PENN is a Software Engineer at SAIC.  He is currently the primary software developer for AARDVARC.  Mr. Penn had worked various positions in algorithm development, software engineering and database administration at Lockheed Martin and IBM.  Mr. Penn received a B.S. in Computer Science from University of Maryland in 1982 and a M.S. in Computer Science from Johns Hopkins University in 1989.PAGE  2 PAGE 2PAGE  2PAGE  11		