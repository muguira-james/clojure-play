Cluster Computing in Large Scale SimulationLarry MellonDavid ItkinScience Applications International Corporation1100 N. Glebe Rd. Suite 1100Arlington, VA 22201(703) 907-2553/2560mellon@jade.saic.com, itkin@jade.saic.comKeywords:cluster computing; low latency; large scale; data distributionABSTRACT: Simulations tend to be distributed for one of two major factors: the size and performance requirements of the simulation precludes execution within a single computer, or key users/components of the simulation are located at physically separate facilities.  It is generally recognized that distributed execution entails a considerable degree of overhead to effectively link the distributed components of a simulation and that said overhead tends to exhibit poor scaling behaviour.  This paper postulates that the limited scalability of distributed simulations is primarily caused by the high cost of communication within a LAN/WAN environment and the high latency in accessing the global state information required to efficiently distribute data and manage time.  An approach is described where users of a simulation remain distributed, but the majority of model components are executed within a tightly-coupled cluster computing environment.  Such an environment provides for lower communication costs and very low latency between simulation hosts.  With an accurate view of current global state information made possible by low latency communication, key infrastructure algorithms may be made significantly more efficient.  This in turn allows for increased system scalability.IntroductionA number of existing and proposed simulation systems are distributed in nature.  A large number of successful training exercises have been conducted using ALSP and DIS and other distributed simulation techniques.  However, upper bounds on the size, complexity and performance levels of distributed simulation have been shown to exist.  New infrastructure with improved scalability has been successfully prototyped under programs such as JPSD and STOW, and a version of the RTI is under development which encompasses and expands the scalability concepts explored within those programs [1].  While it is expected that the RTI will provide sufficient scalability for the majority of federations, the limitations of distributed execution cannot be fully overcome.  Thus federations with extreme performance or size requirements may require further improvements in system scalability to achieve their goals. A set of potential improvements to system scalability may exist within the domain of cluster computing.  Clusters have successfully replaced supercomputers in certain use conditions as a lower cost option with better upgrade potential. The clustering concept may be extended into tightly-coupled cluster computing, where the computers within a cluster are connected via high bandwidth, low latency transport mechanisms.  Such fabrics are becoming increasingly available and industry standardization efforts along these lines exist, such as the Virtual Interface Architecture [2] and related efforts.  The primary effects of executing within such an environment are the lower costs of transmitting or receiving a message, and the low latencies involved in accessing information on other hosts.  From these effects, it may be possible to devote significantly less compute resources to the system infrastructure thus freeing resources for modeling activities.  Increased scalability is possible from increased accuracy of data distribution – itself a potential gain from low latency access to global addressing information.  Secondary effects are: simplified administration issues such as workstation / network configuration, initialization files, software version control, upgrades; and greater accuracy within some federations due to lower latency between models.  The DARPA program Advanced Simulation Technology Thrust (ASTT) is sponsoring research into such clustering optimizations with the goal of supplying experimental data on potential system scalability techniques to JSIMS and similar simulation systems.While the potential gains from cluster computing in simulation are large, it is recognized that many simulations are distributed for reasons beyond increased compute requirements: users and controllers of the models are often geographically separated.  This does not necessarily preclude the use of clustered computing.  A shift in the definition of distributed simulation is indicated: allow the users of a simulation to be distributed, but centralize the bulk of the modeling computation within a resource-efficient cluster environment.  Standard latency-hiding techniques may be used to increase the effective coupling of users to models.  Note that human time perception limitations may also be effectively exploited at this connection point.The following sections expand on the potential advantages of cluster computing and potential problems that may be encountered. A small set of basic distributed simulation problem areas are defined which may expect gains from cluster computing and the key assumptions which are used in this analysis are outlined. A brief summary of available clustering technology is given and commented on with respect to suitability within this problem domain.  An abstract architecture is defined to support experimentation with major infrastructure algorithms in the context of cluster computing.  Finally, algorithms and experiments under development via DARPA’s ASTT program are described.Cluster BackgroundOne of the motivating factors in examining cluster computing is the high cost of fully distributed simulations done to date.  Exercises such as STOW required considerable manpower in simply configuring the system, access to tailored network hardware, and advanced prototype algorithms in the simulation infrastructure.  Successes were required from multiple R&D efforts and vendor tailoring to allow STOW’s achieved scale.  Examples include Bi-level multicast, LAN edge devices able to support thousands of multicast groups, OS modifications, and WAN stability at the STOW traffic rates and characteristics.  To a lesser extent, the JPSD system also required new infrastructure algorithms, but the centralized nature of JPSD allowed for simpler exercise configuration and data distribution algorithms [3].  One potential lesson learned is the high cost of tailoring network technology to meet the needs of distributed simulation. Another problem in distributed simulation is the high cost of synchronizing distributed host actions across a WAN.  The cost of communication and associated latency (and the variability of said latency) greatly affect distributed algorithms used within both the infrastructure and the models.  Algorithms utilizing dynamic information resident on many hosts are particularly susceptible to this problem. And finally, the issue of multi-level security exists for some proposed federations.  Proposed solutions usually involve some form of security gateway which must validate or downgrade information before it may leave the source site.  Such gateways quickly become data flow bottlenecks in fully distributed simulation where shared state is maintained by all models.  This paper suggests that the complexities of fully distributed simulation are sufficient to prevent low-cost, effective scalability in the currently limited network environment.  By switching to a clustered computing approach, many of the basic problems encountered by attempting to fully distribute a large-scale simulation may be avoided.  Proposed Cluster ArchitectureAs shown in Figure 1, a cluster architecture consists of workstations linked with low-latency network devices.  Controllers and users of models are linked into the simulation via software agents which obtain and pre-process data on behalf of their clients. Agents and clients are connected via standard WAN point to point mechanisms and may employ latency-hiding techniques between each other. As described below, the majority of data transmissions are expected to occur within the low-latency cluster environment.Key AssumptionsIn generating the basic distributed simulation via cluster computing concept, two basic assumptions are made regarding data flows and data rates between distributed simulation components.  Such assumptions are kept very general in our analysis, as they will vary greatly across federations.  First, we assume the bulk of data exchanges within a large simulation are between model components of the simulation.  A far lesser volume of communication tends to exist between model controllers (or users) and models.  This assumption is based in part on the number of model components within a large simulation and the relatively smaller number of human controllers, and the fact that models interact tens to hundreds of times per second, whereas human interactions with the models simply cannot occur at that rate.  As backing data, an informal survey of ModSAF users was conducted to establish rough user to model and model to user data rates, and compared against early STOW projections of model to model interaction rates. The following generalized assumptions are then used: a screen refresh rate of twice per second, a entity state update of once per second, and a command from the controller to the model of once per 5 minutes.  Given these generalized numbers, it is clear that for a large entity-count simulation with few controllers, the majority of data exchanges will be inter-model.  However, it is also clear that some federations will fall outside these bounds – a user controlling a model via realtime joy-stick operations changes the (simplistic) equation.  An additional motivator is the human perception range.  Given that humans cannot react to information changes which occur at a rate of several times per second, it seems logical to minimize latency between fast-reacting model components and allow larger latency links to the slower-reacting humans in the loop.Optimizations possible within a cluster Three basic areas of executing a distributed simulation may be improved via clustered computing.  First, the mechanics of running an exercise are considerably simpler.  From a system perspective, ensuring correct versions of the OS, configuration files and models within a cluster is easier than an exercise at multiple sites under multiple system administration strategies.  It is also possible to upgrade the hardware incrementally, as opposed to many parallel processors where the entire system must be upgraded at once.  Also note that upgrades are from off-the-shelf computers, thus allowing the system to continually include the latest and fastest of the rapidly improving PC and workstation markets.  Upgrades may also include asymmetrical memory sizes at each processor.  This allows better tailoring of resources to models.  Second, the models themselves benefit from a tightly coupled environment, in that lower latency between model components allows more accurate modeling for some federations.  Third, the simulation infrastructure of the system gains many advantages from tightly-coupled clustering.  These infrastructure improvements form the basic research areas of this ASTT effort and are outlined later in this paper.Secondary optimizations available via clustering are the simpler security implementation options that exist when the majority of the models are within a protected site and minimal data flows exist out to distributed users.  Further, the rate of such model to user traffic is expected to be lower than model to model traffic and correspondingly lowers the load on the security gateway.Clustering hardware optionsA number of options now exist in the low-latency clustered computing field, where low-latency is defined as one to ten microseconds for an application to application data transfer.  First, technologies such as SCRAMnet have long been a critical part of realtime control systems.  Second, a number of similar hardware devices have been transitioned from the parallel processing hardware community to the PC and workstation markets.  Point to point devices now exist, as do cards implementing the IEEE distributed shared memory protocol (SCI).  Third, a number of research groups have successfully demonstrated direct application access of standard networking cards, thus bypassing the cost of going through the OS layers and associated data copies.  A sample of such clustering technologies is given in Table 1, drawn from literature surveys and experiments [11].  Finally, industry efforts exist to standardize a proposed interface for low-latency cluster communication, the VI architecture, as well as the related System-Area Network (SAN) concept.  While initial ASTT experiments have encountered a degree of immaturity in the implementation of some devices, the projected low latency is provided.  Given the industry backing of tightly-coupled cluster computing, the future of this technology looks promising.NameTypeLatencyBandwidthATMFast switched network20us155Mbit/sSCRAMNET (Systran)Distributed Shared Memory250-800ns16.7 MB/sMyrinet (Myricom)Fast switched network7.2us1.28Gbit/sU-NetSoftware solution29 us90 Mbit/sHigh Performance Parallel Interface (HIPPI)Fast switched network160ns800Mbit/sScalable Coherent Interface (SCI)Distributed Shared Memory2.7us1Gbit/sTable  SEQ Table \* ARABIC 1 Sample clustering techniquesSimulation-specific clustering problemsWhile clustering is expected to reduce the problems encountered from fully distributed simulation, issues unique to cluster execution are likely to arise.  One line of investigation within this ASTT program is simply to establish that clustering technology is capable of supporting the data transport characteristics of distributed simulation. A known area of concern is the style of communication supported by clustering hardware.  The majority of low-latency cluster work is based on point to point traffic: single transmit, multiple recipient support is not a focus area.  Simulation research to date has primarily explored the use of IP multicast as both a single transmit, multiple recipient interface and as a data segmentation scheme.  How effectively clustering technology supports traffic patterns where many receivers may exist for a given packet will be an important result. A second line of investigation is to ensure that coupling remote controllers and secondary models into the cluster will meet performance and reaction-time requirements for the systems as a whole.As an attempt to avoid problems in general, a goal of this program is to cast the distributed simulation problem into a more mainstream use of networking technology.  We thus hope to avoid many of the hurdles encountered by previous systems, instead leveraging off the commercial world’s direction in networking.  Examples include staying within the IP multicast limits of commercial hardware, and using static links across the WAN to a small set of distributed users.Infrastructure OptimizationsWhile ASTT overall is concerned with many aspects of distributed simulation, this ASTT program is infrastructure-centric: only cluster optimizations at the infrastructure layer of a simulation system are addressed.  This focus is captured in this project’s experimentation architecture, where other components of a distributed simulation system are abstracted into simple producers and consumers of data linked by several infrastructure components.The basic hypothesis addressed by this cluster infrastructure research may be stated as follows: tightly-coupled cluster computing provides an execution environment which allows low-latency access to global data.  More accurate infrastructure algorithms may be constructed with the availability of accurate global data.  System overheads are lowered by the actions of more accurate algorithms reducing the number of network accesses and by the lower cost of network accesses within a cluster.  Clustering in general will allow larger, more accurate simulations to be executed.  Simulation systems as a whole may still be considered distributed, as only the majority of models need be centralized within the cluster. Model controllers and some models may exist remote from the cluster.Data distributionDistributed systems must perform two tasks for all potential data transmissions: addressing and routing.  Addressing requires the system to determine what hosts, if any, require a given data packet.  Routing requires the system to determine the lowest cost mechanism to get a given packet from its source to its destination(s).  These two infrastructure elements are key to system scalability and are expected to map well to a cluster computer environment.  Addressing of data The Global Addressing Knowledge problem exists in all distributed simulations.  The problem may be summarized as follows:Some amount of the overall simulation state is shared in nature.  Changes to shared state by one entity must be communicated to all relevant entities.Not all shared state updates are required by all entities, thus scalable solutions require that state updates must be addressed, i.e. the subset of simulation entities which require that particular state update must be identified.A simulation host acts as the agent for N local entities, where each local entity may change some portion of the overall simulation state and in turn requires access to some subset of the overall simulation state.  From a distributed system perspective then, the shared state actions of local entities may be unified into a host-level view, where a host may be considered a producer and/or consumer of simulation shared state.The set of producers and the set of consumers associated with any given portion of shared state will vary over time.The cost of accurately mapping producers to consumers increases as the number of producers and consumers increases.The high latency and communication costs in a system distributed via a WAN inhibit obtaining accurate data on the current data requirements of consumers and data availabilities of producers.  This inevitably leads to inaccuracies in the addressing solution, which, unable to determine the minimum set of transmissions required, must err on the side of caution and distribute as much data as could possibly be required to each host.  Thus, between the inherent inaccuracies of a dynamic solution to a problem based on distributed (global) knowledge, the high cost of obtaining global knowledge in a WAN environment and the accuracy loss from high latency communication, WAN addressing schemes will generally result in a communication load in excess of the theoretical minimum load. As stated in our hypothesis, the low latency access to global data within a tightly-coupled cluster environment may improve the accuracy of data addressing within the system.Routing techniquesSingle transmit, multiple recipient network technologies such as IP multicast and ATM point to multi-point have been proposed as mitigating techniques for the large volume of network traffic and the CPU load per host of generating multiple copies of the same message.  Similar schemes exist within the clustering world, although to a lesser extent.  Described in [4], three basic classes of routing may be done with such techniques.The routing problem may be summarized as follows:Deliver each packet at a minimum cost to the system overall. System cost is composed of transmission costs, network bandwidth consumed, latency incurred, background overhead in managing resources, and receiver costs.For each data packet, three things may be established: the source of the data, the destination of the data, and the value of the data.Each of those three may be used to establish the routing approach.Also in [4], an analysis concludes that the best routing technique will vary based (primarily) on the number of single transmit, multiple recipient resources available, the load balance of the system, and the number of hosts involved.  Other factors to consider include cost of switching resources, number of false data receptions, and number of transmissions required to ensure complete routing coverage.Time managementTime management is another area which may be optimized within a cluster. This topic is addressed by another component of ASTT, to be described separately.Experimental ApproachTo determine an effective layer of infrastructure to experiment at, a sample set of distributed simulation systems was analyzed and common functionality was extracted. Systems analyzed include the RTI 2.0, the JSIMS CI architecture, and Tempo / Thema [5].  An abstracted architecture was created to encompass both existing approaches and proposed approaches, capturing the basic distributed simulation functional areas in a simple form. By performing experiments at the common abstraction level, a reasoned argument may be made that the cluster computing results obtained within ASTT are applicable to the set of source distributed simulation systems.Such an experimentation architecture (shown in Figure 2) serves a number of purposes. The primary purpose is to set the terms of reference and the scope within which individual experiments will be defined, conducted and analyzed.  Next, by introducing abstractions which describe in general portions of a distributed simulation, it allows static assumptions to be made regarding areas of the infrastructure which this program does not address. This simplifies the analysis procedure and provides a simple context in which to analyze the effects under test. For example, predictive contracts are believed to have a significant effect on large scale simulations.  However, we do not experiment with differing predictive contract schemes and thus abstract their effect into a simple reduced data rate between hosts.  Finally, the experimentation architecture is to identify similarities with other distributed computing problems.  By casting the abstractions of distributed simulation into similar patterns as other distributed systems, we can more readily utilize research and techniques from those fields.Key abstractionsThe primary abstractions used within the experimental architecture are described below.  Where possible, terminology has been drawn from existing sources.  Terms from the DMSO RTI 2.0 and Thema internal architectures are used extensively.Communications within a clusterOf the various low-latency hardware systems under consideration, a number of different communication styles are supported.  For example, SCI and SCRAMnet provide forms of shared memory across distributed hosts, Myrinet supports only point to point traffic, and ATM provides for point to point or point to multi-point traffic.  For the purposes of this architecture, the term communication fabric (drawn from the networking community, and in particular, ATM) is used to encompass the broad set of physical communication devices.To allow accurate comparison across differing communication fabrics, a single use abstraction is proposed.  A channel abstraction will be implemented in the most efficient way possible for a given fabric and the same test suites will be run across all fabrics. Channels are multiple-transmitter, multiple-receiver objects. Note that the number of channels which may be efficiently supported will vary across fabrics, as will the cost of joining or leaving a channel.  Further, a channel implementation may need to restrict the number of transmitters or receivers on a given channel to make more efficient use of the communication fabric. Higher layers of the architecture must be able to determine said costs and potentially restrict themselves to an efficient channel usage scheme for a given fabric.  This technique allows for experimentation with existing but untested theories outlined in [4] such as destination-based routing will be more efficient than value-based routing over ATM.  Further note that the architecture must prevent such tailoring from affecting how client applications are built – only the efficiency of the infrastructure should be affected. Addressing of dataSome mechanism must exist to label each state update such that it may be determined which other entities require this data.  A number of techniques have been used to accomplish this task: the JPSD predicate mechanism [3], routing spaces within the RTI, and categories within Thema [5] among others.  While each technique listed above presents a substantially different interface to the user, internally all three produce an abstract tag based on the  user-presented information.  This tag is used to generically group like elements for efficient bundling onto limited channel resources, and/or as the mechanism to establish what subset of the simulation shared state is required at each host.To support experimenting with a varied set of addressing schemes and to avoid the complications of including the effects of differing user interfaces in our experiments, abstract tags are used within this research. For the purposes of our analysis, we assume some other component of the system has assigned a tag to any given state update, and that remote simulation hosts state what data they require also in terms of abstract tags.  Tags are then mapped to channels using an algorithm that attempts to maximize similarities in the tags assigned to a channel.  If an efficient mapping is achieved, simulation hosts joining channel X to obtain tag Y data will receive a minimal number of non-relevant state updates.  Central to such algorithms is the ability to analyze current tag production and consumption patterns.  Note that such tag usage information is resident at each host in the system, and thus is a global knowledge problem.  The architectural component responsible for mapping tags to channels is thus referred to as the Global Addressing Knowledge (GAK) component, as drawn from the RTI 2.0 internal design.  Note that while the desired abstraction and generality is gained, application-level knowledge is lost which potentially could be used to further optimize GAK-level algorithms.Remote controllersThe experimentation architecture provides for remote controller connectivity via a local agent construct.  An agent is attached to the cluster on behalf of a remote controller.  Agents interact with clustered models strictly as another abstract entity producing and consuming data.   Using the entity interface, an agent subscribes to a subset of the simulation shared state.  From that subset, an application-specific view is created.  A view is a transformation of the simulation state into what is viewable at the remote controller site.  It may be a simple aggregation of platform-level data into platoon-level data, the same data presented at a lower resolution, or a pixelized representation of the simulation state required by the remote controller’s display unit.From that view (local to the cluster), the agent may use application-specific knowledge regarding the remote controller to transmit changes to the view. Further, predictive contracts may be constructed to effectively hide the latency between agent updates to the view and displays of the view to the controller.  Also note the applicability of remote controller agents to other remote component problems.  This basic agent construct is also used in the RTI 2.0 design to allow local clustering of hosts for hierarchical system scaling techniques. Within this architecture, multiple clusters and remote models would handled in the same fashion.Key componentsGAK: The GAK is responsible for an efficient mapping of tagged data to the set of available channels.  Efficiency is determined in a large part by what the underlying hardware will efficiently support and is also affected by the cost of determining said mapping. Network factors which must be considered: raw costs of a transmission, number of channels effectively supportable by the hardware, cost of joining or leaving a channel, unnecessary receptions, and unnecessary transmissions.Note that the tag assignment algorithm may be described as a producer/consumer mapping problem, and is believed to be NP-complete.  Further, the set of producers and consumers is dynamic, thus a good mapping will change over time.  Combined, these two factors will result in GAK algorithms which only approximate a perfect mapping.  Without this loosening, the GAK can consume cycles and bandwidth well in excess of target goals, and may also never 'catch up' to the best current mapping.Channel: Channels are the communication abstraction for distributed hosts.  Channels may have 0..N producers and 0..N consumers.  Channels may restrict the number of producers and/or consumers to best match a given hardware system.  They may also bundle data for extra efficiency.Channels present a single transmit, multiple recipient API to clients, which is then implemented in whatever the hardware efficiently supports.  Due to hardware restrictions, there may also be a limit on the number of available channels.Other componentsFor the purposes of functional isolation, a number of other components are described in this architecture.  Functions such as predictive contracts, load balancing, variable resolution data and similar techniques are expected to be performed within any large scale distributed simulation.  These functions are abstracted into a single layer of the architecture, Data Transmission Optimizations [6]. These DTOs are not research tasks under this program, thus their effects are abstracted into a simple reduction in the number of required data transmissions.Interest Management is another abstracted component.  Some portion of the system overall is responsible for translating data requirements and data availability from application-specific terms into infrastructure-neutral terms suitable for inter-host network communications.  In the JIMS CI, this is done by the MMF.  In the RTI 1.3 and 2.0, this is done as a combination of application and RTI functionality.  For the purposes of our architecture, we assume that some other component has done the above conversion and all data updates are tagged with an application-neutral identifier.The combined effects of interest management and DTOs are contained within the Inter-Host Data Manager.  This component represents all local host operations involved in managing the remote sharing of simulation state.  It generates tagged state changes ready for distribution and provides tag production / consumption data to the GAK.  Test inputs for GAK experiments are in terms of this component and may be extracted from actual data or artificially generated to test GAK behaviour under specific conditions.Research AreasTo test the cluster computer hypothesis stated earlier, three major factors must be demonstrated.  First, that clustering fabrics may effectively support the data exchange characteristics of distributed simulations.  Second, that it is possible to effectively link a remote controller to a model executing in a clustered environment.  Third, that GAK algorithms built to exploit low latency access to distributed addressing information will reduce system overhead in distributing data.  Full definitions of these factors and initial results are to be presented in a following paper.  A summary of ASTT’s current research into these factors is given below.Channel analysis The dominance of point to point communication in commercial clustering technology requires analysis of the technologies in terms of simulation data exchange characteristics, which tend more to point to multipoint communication.  A set of simple tests will be conducted to establish the number of channels a given hardware solution may efficiently support, and the number of publishers and subscribers per channel.  These tests will be executed across a sample set of clustering hardware solutions.  The results will be channeled into the GAK design process.Remote controllers The linkage between agents and remote controllers is application-specific, and as such, impossible to prove in the general case.  A series of tests will be conducted to establish levels of effectiveness in hiding latency for a sample application class.GAK implementations A set of GAK algorithms optimized for cluster execution is under construction in ASTT. GAK experiments are drawn from addressing schemes proposed in [4] and from tag mapping approaches from research fields outside of distributed simulation.  Promising approaches have been found in various online matching problems [7].  A GAK analysis testbed and sample GAK algorithms are currently under construction.  Metrics and GAK test data descriptions are given below.GAK Metrics and Testing DataExperiments will be conducted that compare the effectiveness of GAK algorithms running in a cluster environment.  This will provide a relative measure of effectiveness for each of the GAK algorithms.  In order to understand the effectiveness of the GAK algorithms in absolute terms, a best case and worst case GAK will also be measured.  The best case GAK will have unlimited communication channels in which to map tags to.  The worst case GAK will have only one communication channel, making it equivalent to a broadcast solution.  For each experiment scenario, the GAK algorithm under test will be measured against the performance of both the best and worst case GAK baselines for the same test inputs.  A number of GAK-specific metrics will be used to capture implementation-level comparison data.  In addition, a simple system-level metric will be used: the reduction of system overhead incurred for data distribution.  This encompasses computational resources consumed by the GAK at each host, the number of data packets sent and received, and the number of inter-GAK messages. Subscription and publication test data needs to be generated in order to test the GAK algorithms. The dynamic subscription and publication behavior of the test data can be generated in one of several ways.  Either real-world data can be obtained and analyzed [8] or the data can be generated by characterizing the time-varying distribution of subscriptions and publications [9].   By using measured data, it is easier to demonstrate the validity of the results.  By using stochastic data, more diverse application behavior can be generated.For the initial experiments we have chosen a middle ground. We will generate the data using a simulation of entities that have parameterized kinematic and sensor models.  These entities will roam over a sectorized battlefield, and subscribe and publish data to cells in the sectorized grid.   By using simulated entities, it should be possible to generate subscription and publication behavior that closely matches real-world use and is flexible enough to adequately test the GAK algorithms.Analysis tools will be built that provide insight into how effectively each GAK algorithm mapped tags to communication channels.  The tools will provide both static and dynamic views of the GAK effectiveness.  In addition, the tools will use standard off-line grouping analysis algorithms [10] to provide a measure of the inherent grouping exhibited by the test data.SummaryTightly-coupled cluster computing appears to support an efficient simulation infrastructure.  Scalability improvements are possible within the infrastructure, and WAN communication become a minor portion of the overall system requiring no special tuning for execution.  An experimentation approach was given in which the scope of the ongoing ASTT research effort is defined and relationships to other distributed simulation techniques are established.  Experiments are underway to provide performance comparisons between data distribution algorithms within a cluster and against theoretical maximums.AcknowledgmentsA number of the abstractions used in the experimentation architecture are extended from work begun under a DMSO RTI 2.0 design contract.  Steve Bachinsky, Glen Tarbox, Richard Fujimoto et al were invaluable in analyzing the basic GAK construct and potential implementation options which in turn are based on work with Ed Powell.  Darrin West contributed to the clustering concept.  All work was funded via DARPA, with guidance from Dell Lunceford, PM.References[1] 	Bachinsky, S., et al. RTI 2.0 Architecture.  Proceedings of the 1998 Spring SIW Workshop.[2] 	The VI Architecture. http://www.intel.com/procs/servers/isv/vi/[3] Powell, E., et al. Shared State Coherency Using Interest Expression in a Distributed Interactive Simulation Application.  Simulation Multiconference, 1996. [4] 	Powell, E. The Use Of Multicast and Interest Management in DIS and HLA Applications. Proceedings of the 15th DIS Workshop. [5] 	West, D., Itkin, D., Ramsey, J., Ng, H. Event Distribution And State Sharing  In The Thema Parallel Discrete Event Simulation Modeling Framework. Proceedings of the Object Oriented Simulation Conference, San Diego, January 1998.[6] 	Mellon, L. Intelligent Addressing and Routing of Data Via the HLA RTI Filter Constructs. Proceedings of the 15th DIS Workshop. [7] Albers, S.  Competitive Online Algorithms. Optima, 54:1-8, 1997. Feature article in the newsletter of the Mathematical Programming Society.[8] Billharz, T., Cain, J., Farrey-Goudreau, E., Fieg, D., Batsell, S. Performaance and Resource Cost Comparisons for CBT and PIM Multicast Routing Protocols for DIS Environments.  Proceedings of Infocom 1996, IEEE, March, 1996.[9] M. Jeff Danahoo, Ken Calvert and Ellen W. Zequra. Center Selection and Migration for Wide-area Multicast Routing. Journal of High Speed Networks. Volume 6, No. 2, 1997.[10] Anderberg, M. R. Cluster Analysis for Applications. Academic press, New York and London, 1973.[11] Lepler, J, Zhou, D.  Cluster Hardware Performance Survey.  ASTT internal report, 1997.Author BiographiesLarry Mellon is a senior computer scientist and branch manager with Science Applications International Corporation (SAIC).  He received his B.Sc. degree from the University of Calgary and has worked in the area of parallel and distributed simulation infrastructure for over ten years. His research interests include parallel simulation and distributed systems.David Itkin is a Senior Engineer at Science Applications International Corporation. David Itkin has been active in the DoD simulation community for 9 years. EMBED Word.Picture.8  Figure  SEQ Figure \* ARABIC 1: Basic cluster architectureFigure 2: Experimental Architecture ComponentsFigure  SEQ Figure \* ARABIC 2: Experimental Architecture Components