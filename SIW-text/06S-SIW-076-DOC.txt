Interaction of Human Bahaviour Models Jan Tegn√©rQi HuangSaab Systems ABNettov√§gen 6SE 17588 J√§rf√§lla, Sweden+46 8 580 840 00Botond PakucsCentre for Speech TechnologySchool of Computer Science and Communication KTH -Royal Institute of TechnologyLindstedtsv√§gen 24SE-100 44 Stockholm HYPERLINK "mailto:jan.tegner@saabsystems.se" jan.tegner@saabsystems.se,  HYPERLINK "mailto:qi.huang@saabsystems.se" qi.huang@saabsystems.se,  HYPERLINK "mailto:botte@kth.se" botte@kth.se  Keywords:Behaviour Model, Simulation, Interaction, Communication, Speech.ABSTRACT: The field of human behaviour simulation is vast and includes intelligent behaviour of platforms as well as humanoids capable of a spoken dialogue. Implementations of human behaviour range from complex models trying to replicate perception processes, biological and cognitive functions, emotions, memory etc to simple rule based algorithms. Since the different approaches to human behaviour simulation is so different, there has been little standardization efforts in this area.  A first approach to start standardization in the field of human behaviour could be to look at a human being as a black box and try to define the communication interfaces, i.e. the verbal and non-verbal communication. The interface could then be divided in three main areas: speech, visual appearance and body motion including gestures and mimic.The speech area could be compared to the simulation of radio communication in DIS and HLA RPR-FOM. In the same way as the characteristics of a radio transmitter is defined in a Transmitter PDU, we need to define the characteristics of a voice, such as whether it¬¥s a male or female voice, the pitch and speed of the voice, accents, emotional characteristics, having a cold etc. For these purposes there are a number more or less standardised protocols for voice rating, for example GRBAS (Ishiki et al, 1969) or SVEA (Hammarberg, 2000) developed for slightly different purposes by the speech therapist community. The content of the speech could be transmitted in a similar way as the Signal PDU. There are currently several standards developed for speech based man-machine communication such as VoiceXML, SALT. These standards include both speech recognition grammar and speech synthesis specifications.In computer games visual appearance is defined using Open Flight files or similar standards. However, if two human CGF¬¥s need to interact, an ontology and representation of clothing is needed in a more linguistic way. Using this representation a visualization could map this data onto an applicable Open Flight representations. Regarding body movement, there are a number of different standards. As an example, the MPEG-4 standard defines 64 points in a human face which may be altered. The representation probably needs to be at a higher level, for instance representing different emotions. Finally, the different modalities, speech, visual appearance and body motion have to be studied and described in relation to each other. Several attempts have been made for describing and standardizing human multimodal interaction. For instance, Bernsen (1997) has proposed a taxonomy for output modalities. Martin et al (2004) presented a long list of projects, initiatives and organisations that have addressed the issue of multimodal communication.This paper studies different applicable standards, which may be used to define ontology and representation for human interaction and how they could be mapped onto a Human Interaction FOM or BOMs. By doing this, interaction between different human behaviour models and humans will be simplified.1. Introduction1.1 Background The M&S community has historically put a lot of focus on infrastructure, synthetic environments, platform models and communication simulation. , but during the last years simulation of human behaviour has become an important topic.  Several training system today is built upon commercial gaming platforms, such as training systems based on Americas Army. Shoot ‚Äòem up games, platform simulators and strategy games was the first generation of commercial platforms that were reused for training purposes. The current trend is to add a higher degree of interaction between Human Behaviour Representation (HBR) Models and humans, where the player has a ‚Äúconversation‚Äù with synthetic players. One example is the Swedish game Foreign Ground which was designed for training of troops on peace support missions. While games like Americas Army focus on ‚Äúhard‚Äù skills like gunnery, Foreign Ground focus on soft issues like decision making. The soldiers may have a conversation with people they meet to gather information from local citizens using dialogue menus. Even though menu based conversations is common in this type of games, speech recognition and speech synthesis may be used, especially for conversations with limited syntax.Human behaviour simulation may be divided in two sub areas:intelligent behaviour of platformssimulation of human beingsWhile intelligent behaviour for platforms mainly focuses on decision making, simulation of humanoids has to deal with all aspects of human beings which require an interdisciplinary community. Today research in Human Behaviour Representation (HBR) tends to be ad-hoc and even though there are lots of knowledge in the area it is hard to combine the results from different research groups since there is a lack of standardization. A first approach to start standardization within the HBR area could be to look at the human being as a black box and focus on defining the interfaces of the human being. Human interaction could be divided into verbal and non-verbal communication, where verbal communication denotes only the acoustics produced by a human being while the non-verbal communication denotes the visual representation of a human being including physical appearance, body movement, mimics, gestures, clothing etc.This paper studies both interaction between HBR models as well as interaction between HBR models and human beings. The focus for HBR to HBR interaction is mainly to provide a HLA FOM for representation of all aspects of human interaction., while the focus for interaction between HBR models and  humans is to map the HLA FOM to formats used by visualization, speech recognition and speech synthesis components.A representation for human interaction may be specified at different abstraction levels. As an example could the expression of emotion in a face be described according to the MPEG-4 standards with 64 points in the face. However, a representation at such low representation level makes it hard for the other HBR models to interpret what kind of emotion the points are supposed to represent. This is very difficult, especially when those points also may be affected by visualization of lip movements synchronized with speech. In this case we need visualization at a higher level. It is important to select an interface that both supports a practical exchange between HBR models on one hand and still supports 3D visualization, speech synthesis and speech recognition. The purpose of this paper is to discuss applicability of different existing standards related to human interaction and to provide the foundation for a standardization effort within this area.2.  Interaction between HBR ModelsThe purpose of a HLA FOM for interaction between HBR models is to provide a representation for the general attributes of a human being that may affect another HBR models perception and decision-making process. It is, of course, impossible to define all aspects which may be needed in any possible situations. Thus, it is necessary to be able to extend the FOM with application specific data in the same way you may do with the RPR-FOM.The picture below describes how a Human Interaction FOM, could enable interoperability between HBR models with different fidelity, complexity and design.  In the example below, the Unified Architecture for Behaviour model proposed by Silberman & Bharathy  [6] interacts with a less complex HBR model.Figure 2.1 Interaction between HBR models2.1 Visual appearance2.1.1 Physical appearanceVisualization components are obvious users of information on visual appearance, but also HBR models may need this kind of information, since the reaction of a HBR model may vary when interacting  adults versus children, men versus women, white people versus coloured people.Several approaches to classification of humans has been proposed, such as the typological, populational and clinal models [8], but none of these models work properly since there are no distinct biological groupings of people. The best approach is probably to make classifications for individual physical characteristics, such as skin tone, hair colour, shape of  face and body etc. It is important to have a limited number of characteristics, since the focus is on information carrying cues that affect human or non player characters (NPCs) decision process.  The importance of  different characteristics may vary depending on the situation. Seing a white person in a small village in Africa would  be an important cue, while it would be of less importance if he was located in a west European city.2.1.2 EmotionsDuring the recent years a new research field emerged, affective computing, as sub-field of Human-Computer Interaction (HCI). The main purpose is to enhance computers with the ability to recognize, model, and understand human emotion, to appropriately communicate emotion, and to respond to it effectively. This research field emerge from findings in the fields of cognitive science, psychology, neuroscience, medicine, psychophysiology, sociology.For the purpose of simulations it is desired to include emotions and enable affective computing. As an example, when simulating peacekeeping missions it would be advantageous to be able to simulate human emotions and affective reactions.  The emotions of  a human being is reflected using different body features, such as body pose, facial expressions, voice characteristics etc. It is impossible to represent all possible emotions, but since even humans have problems in recognizing more subtle nuances of emotions a fairly rough classification of emotions might be sufficient. As a starting point could be used the basic emotions proposed by Ekman (anger, disgust, fear, joy, sadness, surprise) [9] or Plutchik (Acceptance, anger, anticipation, disgust, joy, fear, sadness, surprise) [10].2.1.3 ClothingSince clothing also provide information cues about a person and, thus, affect the decision making of other human and non human players there is a need of a representation for clothes as well. HBR models could react differently when approaching a person in civilian clothes compared to military, hospital or police uniforms. There are few classifications of clothing available. The Australian government has a rough classification [13] used for registration of patents, trade marks, design etc, but that classification would need a further break down to be suitable for simulation purposes.2.2 Body movementClassification of body movement and spatial location is required to be able to exchange what kind of action a HBR model is performing, but it is also essential to known the spatial location of the body to determine whether it collides with walls, other humans, furniture etc.There are three types of muscle around 650 muscles in a human body: skeletal, smooth and cardiac. All of these muscles can stretch and contract, but they perform very different functions [15].Skeletal muscle gives the body its shape. They are attached to your skeleton by strong, springy tendons or are directly connected to rough patches of bone. Skeletal muscles are under voluntary control, which means you consciously control what they do. Just about all body movement, from walking to nodding your head, is caused by skeletal muscle contraction. Your skeletal muscles function almost continuously to maintain your posture, making one tiny adjustment after another to keep your body upright. Skeletal muscle is also important for holding your bones in the correct position and prevents your joints from dislocating. Some skeletal muscles in your face are directly attached to your skin. The slightest contraction of one of these muscles changes your facial expression.Smooth muscle is found in the walls of hollow organs like your intestines and stomach. They work automatically without you being aware of them. Unlike other types of muscle, cardiac muscle only exists in your heart. It never gets tired. It works automatically and constantly without ever pausing to rest. Therefore the body movement is actually controlled by the body skeleton's movement. When we simply divide the body into six components: head, left and right arms, body, left and right legs, the body movement can thus be represented by three coordinate systems. The first is the base reference referring to a global coordination system. The second is the body centre (usually the gravitation centre) referring to the base reference . The last is any body component referring to the body centre. Movement itself means that it has a position, speed and acceleration. The reference movement can be described by six degrees of freedom (DOF), i.e. x, y, z, roll, paw, pitch. We can create more references when a detailed description of a body movement, e.g. a palm referring to the arm centre. It however also creates a very difficult modelling of body movement when we try to model all skeletons and theirs joint movement. Unfortunately we haven't seen such a simulation that can detail model the body movement, which requires high calculation capability and high solutions.Fortunately a lot of simulation engines already have the representation of the body movement, such as:  Move forwardMove backwardStep leftStep rightLean leftLean rightRoll leftRoll rightRunWalkSprintJumpThus, exchanging body movements at this level of abstraction may be sufficient, but we also need a more spatial representation to determine collisions etc. 2.3	GesturesGestures is a difficult area, since the gestures used and the meaning of the gestures largely depends on the context. Some gestures may involve the whole body, for instance gestures used at the runway onboard hangar ships, while other only involves the hands (and indirectly the arms), such as sign language. Most research regarding classification of gestures are related to robotic gesture recognition.  Vassilis Athitsos and Stan Sclaroff at Boston University has defined 26 basic shapes of the hand which is used for automatic recognition [5].  Fig 2.1.2.1 Basic hand shapes defined by Athistos/Sclaroff at Boston University.However, since a gesture often involves a combination of several different poses and movements it is difficult to use a static representation of  hand shapes. As an example, the game Americas Army [12] includes a set of eight hand signals for combat communication described in table below.CommandDescriptionStopRaise fist to head level.Move outExtend arm overhead and swing from shoulder.ReadyHold the fist out with thumb up and nod.NegativeExtend the arm parallel to ground, hand open,  and move the arm to the body, in a throat-cutting actionDouble timeRaise the fist to the shoulder; thrust the fist upwards to the full extent of the arm and back to the shoulder level; do this rapidly several times.Look this wayExtend the arm toward the desired direction.AffirmativeExtend the arm and fist, with thumb pointing up.Get downExtend the arm at a 45-degree angle from the side, above the horizontal, palm down and then lower the arm to the side.Table 2.1.1 Hand signals used  in Americas ArmyWhen working with gesture recognition the exact shapes and location of the hands, arms etc is important, but in a distributed simulation a representation with that level of abstraction would imply that the observing HBR model would need advanced gesture recognition functions to interpret the gesture. For simulation purposes it is more appropriate to use a classification at a higher level of abstraction, where we transfer the meaning of the gesture, such as the eight hand signals used in Americas Army. The most frequent usage of  gestures is the sign languages used within the community of people with hearing and speak disorders. The American Sign Language (ASL) is the third most used language in the US. The sign languages contains a large classification of  gestures, but they are only useful in a limited domain. Since the set of gestures used differs largely between different domains it would be better to design different BOM¬¥s for each domain. A HBR model could of course be implemented to misinterpret a gesture anyway, for instance when one person makes ‚Äúthumb up‚Äù to another person, since this is a positive gesture in western countries, while it is an abusive gesture in Arabic countries. Voice communicationVoice communication between HBR models could be compared with simulation of  radio communication in DIS and RPR-FOM [20].  In the same way as a Tranmitter PDU is used to describe the characteristics of a radio transmitter, voice characteristics could be described. A HBR model could then determine using the voice characteristics, whether a person sounds angry, have a cold etc The actual speech could be transferred as text in a equivalent to the Signal PDU.  The HBR Model then need a dialogue functionality to be able to manage the conversation.  The technology required for enabling human-machine dialogue functionality and different related standards are discussed in details in the next section.For HBR modelling and simulation purposes, the description and simulation of human voice characteristics can also be of major importance. In advanced simulations it is desired to be able to describe and model the human verbal communication as natural as possible.  For that reason, we need to define the characteristics of a voice, such as whether it‚Äôs a male or female voice, speaker's age, the pitch and speed of the voice, accents, emotional characteristics and even physical characteristics such as being tired or having a cold. As far as we know there are no standardized solutions for describing all these aspects of voice characteristics. On the other hand, for similar purposes there are a number more or less standardised protocols for voice rating, for example GRBAS (Ishiki et al, 1969) or SVEA (Hammarberg, 2000) developed for slightly different purposes by the speech therapist community.3. Human Interaction with HBR ModelsInteraction between humans and HBR models can be compared to Human-Computer Interaction, a very active  research field. Depending on the used input and output technologies, the characteristics of the application and simulation parameters, users' interaction with HBR models can be from straight forward to very complex. A possible approach is to classify the input and output modalities.  Such an attempt to classify interaction modalities has been made by Bernsen (1997). He has proposed a taxonomy for input and output modalities. For the rest of the paper we will however only distinguish between verbal and visual interaction modes and focus on these major interaction modes.When humans interact with HBR models it is necessary to handle speech recognition, speech synthesis and visualization of the synthetic environment, platforms and other HMB models.3.1 Visualization of humans in game enginesIn game engines, such as Delta3D, visualization of a human being is created by building a skeleton, where each bone of the skeleton (except the first) is connected    to a parent bone. A mesh around the skeleton defines the shape of the body and finally a texture is added to the shape.Figure 3.1 Human interaction with HBR models3.2 Visual appearanceThe visualization components will probably not be able to take advantage of all available data on visual appearance provided by the proposed FOM. The visualization components typically relies on visualization models defined in Open Flight format or equivalent. Thus, the visualization component needs to do a mapping between the visual appearance data and available visualization models.  3.3 Body MovementWhen visualizing the body position &movement of a HBR model different levels of fidelity may be used depending on distance between viewer and the position of the simulated human. At large distance only basic information, such as whether the person is standing, crouching or laying, large arm movements etc is necessary to visualize. At a normal conversation distance (1 meter for west Europeans)  it is necessary to visualize gestures, facial expressions depending on emotions and speech etc. Most game engines use an animation to visualize a person who runs, crawls or walks etc, instead of  making a real time visualization of the body location.  This becomes obvious for instance when running into a corner and the HBR model still looks as if he is running even though he doesn‚Äôt move forward.3.4 Speech synthesisFor simulating human verbal communication we need to simulate both human speech understanding and production. In this section we give a brief description of the available techniques used for text-to-speech synthesis.The use of ‚Äúcaned utterances‚Äù is probably the most straight forward solution used in different applications for providing human speech output. Simple pre-recorded human speech is used for communicating messages. The drawback of the solution is that the output has to be known in advance and only a limiteds number of messages can be handled. However, for some specific simulations this technique could be satisfactory. A more elaborate solution is based on unit selection. Unit selection systems are employing large speech databases of speech segments of varying length for generating target speech output at runtime.  Diphone synthesis is a similar solution where databases of minimal speech segment containing all the diphones (sound-to-sound transitions) occurring in a given language are used for automatically generating speech output at runtime.In formant synthesis, the output of synthesized speech is created using an acoustic model of the speech waves.  Even if the output of the formant based systems sound robotic and artificial, the generated speech can be very reliably intelligible, even at very high speeds, avoiding the acoustic glitches that can often plague concatenative systems.Articulatory synthesis has been a synthesis method mostly of academic interest until recently. It is based on computational models of the human vocal tract and the articulation processes occurring there. These models are very promising but few of them are currently sufficiently advanced or computationally efficient to be used in commercial speech synthesis systems.Currently, the W3C consortium carries out the most of the standardisation work on synthesis components. The SSML specification [16] defines a mark-up language with support for different voice characteristics such as gender, age, volume, pitch, and emphasis. Beside synthesised speech even pre-recorded speech and music is supported.  At the W3C, there is a related effort also on developing pronunciation lexicon specifications (PLS).  On the other hand, several different commercial solutions are available for a number of languages and integrating these products in HAL based simulating platforms should be not more difficult as integrating gaming platforms.  3.4 Voice characteristicsThe above discussed speech technologies are concerned mainly with the handling of the content of the verbal communication. However, for HBR modelling and simulation purposes, the description and simulation of human voice characteristics can also be of major importance. In advanced simulations it is desired to be able to describe and model the human verbal communication as natural as possible.  For that reason, we need to define the characteristics of a voice, such as whether it¬¥s a male or female voice, speaker's age,  the pitch and speed of the voice, accents, emotional characteristics and even physical characteristics such as being tired or having a cold. As far as we know there are no standardized solutions for describing all these aspects of voice characteristics. On the other hand, for similar purposes there are a number more or less standardised protocols for voice rating, for example GRBAS (Ishiki et al, 1969) or SVEA (Hammarberg, 2000) developed for slightly different purposes by the speech therapist community. 3.6 Speech recognitionAutomatic speech recognition (ASR) technologies enable computers to interpret human speech and facilitate alternative interaction methods with computers. From the perspective of the simulation community and for HBR purposes, ASR is a promising technology for enabling spoken human communication with synthesised characters.ASR systems may be classified among a number of different dimensions:Amount of training requiredDimension of the vocabulary usedSpeaker dependent / independent systemsContinuous speech / discrete wordsRobustness to background noisesContext of speech recognition (digits, names, commands or free speech)Adaptation to different channel characteristics (e.g., cellular telephony, land line telephony etc.)Currently, there are several ASR solutions available on the market. However, there are some resources freely available such as the SPHINX toolkit [18] at the Carnegie Mellon University. These solutions could be easily adopted and integrated in different simulation and HBR solutions.The standardisation work focusing on developing ASR standards is mostly done at the W3C consortium. The Speech Recognition Grammar Specification [19] (SRGS) for instance defines syntax for representing grammars used in ASR.Depending on the specific application requirements different type of ASR can be applied in different simulation contexts. For instance, simulation of speech communication with an armoured vehicle driver in combat situation could be realized whit a discrete word, small vocabulary based ASR system. However while simulating peace keeping situations it is more likely that a large vocabulary continuous speech ASR system would be required. If complex human-computer interactions have to be simulated than the use of basic speech technology components has to be completed with language understanding support. To support complex and dynamic speech based human-computer communication Spoken Dialogue Systems (SDS) has to be employed. ASR systems, text to speech synthesis components, natural language understanding and interaction managers are the main components of spoken dialogue systems. Currently there is an growing interest in commercial applications of SDS. Noticeably, there are several successful telephony applications of spoken dialogue systems. One of the major standardisation efforts related to spoken dialogue systems focuses accordingly on telephony applications. The Voice Extensible Markup Language (VoiceXML) [17] is developed for creating speech-based dialogues for telephony applications.  3.7 Synchronization of speech, gestures and facial expressionIn a conversation situation it is important that the visualization of lip movements and gestures during a conversation need to be synchronized with the voice synthesis. The combination of cues gives the viewer additional information. As an example, a person saying ‚ÄúI‚Äôll kill you‚Äù will be interpreted differently if accompanied with an angry face and a waving fist than it would if the person would be smiling and with his arms crossed over his chest. Multimodal interaction is the research field focusing on developing solutions for synchronizing speech, gestures, and facial expressions. A combination of different modalities has been proved to enhance the human-computer interaction. Hence, it would be advantageous to support multimodal communication even in HBR models and related simulation platforms. 4. SummaryA standardization of human interaction is a huge task which would require a multi disciplinary community.  However, the increasing use of gaming platforms and the need to incorporate human beings in distributed simulations will require standards to provide interoperability between different HBR models.The human interaction standard need to be designed to simplify the HBR to HBR interaction, but also to support visualization and speech technologies used for HBR to Human interaction.The authors of this paper recommend SISO to start a Human Interaction Study Group to further investigate this field of interest with the aim to create standards for human interaction in distributed simulations.5. References[1]	Bernsen, N. O.: Defining a Taxonomy of Output Modalities from an HCI Perspective. Computer Standards and Interfaces , Special Double Issue, Vol. 18, No 6-7 (December 1997) Pages:537-553[2]	Hammarberg B (2000) Voice research and clinical needs. Folia Phoniatr. Logop. 52:93-102[3]	Ishiki N, Okamura H, Tanabe M, Morimoto M (1969). Differential diagnosis of hoarseness. Folia Phoniatrica 21: 9-19[4]	J.-C. Martin, E. Den Os, P. Kuhnlein, L. Boves, P. Paggio, and R. Catizone (2004) Editors of the proceedings of the "Workshop "Multimodal Corpora: Models Of Human Behaviour For The Specification And Evaluation Of Multimodal Input And Output Interfaces" In Association with the 4th International Conference On Language Resources And Evaluation (LREC2004), Centro Cultural de Belem, LISBON, Portugal, 25th May 2004.[5]   Vassilis Athitsos, Stan Sclaroff (2002). An Apperance-Based Framework for 3D Hand Shape Classification and Camera Viewpoint Estimation. http://cs-people.bu.edu/athitsos/publications/athitsos_fg2002.pdf.[6]	Dr. Barry Silverman, Gnana Bharathy (2005) Modeling the Personality & Cognition of Leaders (05-BRIMS-31)[7]	Jonas Beskow, Mikael Nordenberg. Data/driven Synthesis of Expressive Visual Speech using an MPEG/4 Talking Head. http://www.speech.kth.se/ctt/publications/papers05/is05expressive_final.pdf[8]	] Modern Human Variation ‚Äì An introduction to Contemporary Human Biological Diversity. http://anthro.palomar.edu/vary/[9]	P. Ekman. (1993) Facial expression of emotion. American Psychologist, 48, 384-392[10]	Plutchik, R. (1980). A general psycho-evolutionary theory of emotion. In R. Plutchik & H. Kellerman (Eds.), Emotion: Theory, research, and experience: Vol. 1. Theories of emotion (pp. 3-33). New York: Academic.[11]	Delta 3D ‚Äì Open Source Gaming & Simulation Engine, http://www.delta3d.org[12]	America¬¥s Army Training Manual, http://www.americasarmy.com/downloads/manuals.php[13]	 Australian Government Classification of Articles and Haberdashery. http://pericles.ipaustralia.gov.au/adds2/help/classification/classes/02_list.htm[14]	Igor S. Pandzic, MPEG- 4 Face and Body animation. http://wwwalt.ldv.ei.tum.de/conferences/digital_behaviour/DigitalBehaviour_FBA.pdf[15]	BBC Science & Nature ‚Äì Human Body & Mind. http://www.bbc.co.uk/science/humanbody/body[16]	W3C Speech Synthesis Markup Language (SSML). http://www.w3.org/TR/speech-synthesis/[17]	W3C Voice Extensible Markup Language (Voice-XML) 2.0. http://www.w3.org/TR/voicexml20/ [18]	The CMU Sphinx Group Open Source Speech Recognition Engines. http://cmusphinx.sourceforge.net/html/cmusphinx.php[19]	W3C Speech Recognition Grammar Specification (SRGS) 1.0. http://www.w3.org/TR/speech-grammar/[20]	SISO Real-Time Platform Reference FOM v2draft17, http://www.sisostds.orgAuthor Biographies JAN TEGN√âR is Technical Manager at the Training & Simulation department at Saab Systems in Sweden. He currently works with C4ISR to Simulation integration issues including simulation of tactical data links and Network Enabled Capabilities.. Previously he has been involved in the development of several C4ISR systems for the Naval and Air Defence domain and simulators for Air Defence systems. During the last years he has mainly been working with R&D in the field of Modelling & Simulation. Jan Tegn√©r is elected member of SISO Conference Committee and SISO Board of Directors. QI HUANG is a senior scientist at the Training and Simulation Department of Saab Systems. He is currently working as project manager for the UAV Sensor to Engagement (UAV-S2E) project. He holds a PhD in Robot Control System from Royal Institute of Technology, May 2001. His research interests include robotics, artificial intelligence and decision-making support for C4ISR systems. He has previously worked for ABB Robotics and Transman Robotics.BOTOND PAKUCS is Research Engineer at the Centre for Speech Technology at KTH, the Royal Institute of Technology in Stockholm, Sweden. His current work involves research and development of spoken dialogue systems with special focus on mobile applications and Human-Computer Interaction.ng