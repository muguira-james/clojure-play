Interoperability Policy RoadmapPaul W. SuttonInteroperability ManagerAdvanced Concepts and Technology (PD 13 TSG)Space and Naval Warfare Systems Command (SPAWAR)4301 Pacific HighwaySan Diego, CA 92110-3127 (619)553-9632psutton@spawar.navy.mil Keywords:  C4I System, DII COE, HLA, Interoperability, JTA, ModelABSTRACT:  The labyrinth of interoperability policy can present many obstacles to unwary, unsuspecting C4I (Command, Control, Communications, Computers, Intelligence) or M&S (Modeling and Simulation) system developers and users.  This paper provides a roadmap to help weary developers and users find their way through this complex, confusing, and often frustrating maze of policies.IntroductionThe purpose of this paper is to provide a roadmap of interoperability policy for C4I and M&S system developers and users.  First, however, it is necessary to define the concept interoperability.  Interoperability has been previously defined as:“The ability of the systems, units, or forces to provide services to and accept services from other systems, units, or forces, and to use the services so exchanged to enable them to operate effectively together.  The conditions achieved among communications-electronics systems or items of communications-electronics equipment when information or services can be exchanged directly and satisfactorily between them and/or their users.”  [1]“The ability of a model or simulation to provide services to and accept services from other models and simulations, and to use the services so exchanged to enable them to operate effectively together.”  [2]One can infer then, that there are at least two different perspectives of interoperability, the systems perspective and the simulation perspective.  Put another way:“Truth is that which serves the interests of a people.  Two groups of people locked in combat cannot be expected to have the same truth.”-- Albert B. Cleage, Jr.Is interoperability a problem?  One of the first and most poignant examples is taken from Operation Urgent Fury, the battle of Grenada, in 1983:“MG Trobaugh, Commanding General of the 82nd Airborne Division, was a very frustrated commander throughout the effort, not only at communications, but at the entire C2 structure.   From the outset of the effort the 82nd Airborne was slow to respond to commands from the overall task force commander, Admiral Metcalf, headquartered on the ship Guam.  At one period MG Trobaugh landed on the island and tried to effect coordination with Admiral Metcalf, anchored less than ten miles away.  Problems ensued due to radio, COMSEC, and frequency incompatibility that did not allow the land-based commander to talk to the overall command headquarters, something that proper J6 planning and coordination should have prevented. MG Trobaugh could see the ship anchored only a few miles away but could only talk intermittently via a satellite link.”  [3]O.K., interoperability was a problem, but is it still a problem?  Some very recent disturbing findings [4] indicate that while some progress has been made, very serious C4I interoperability problems remain unsolved:“First, future naval coalitions involving the U.S. will be stratified into groups of nations, based on C4I interoperability, with gaps between groups.  Not all nations will be able to take on all missions.We found that interoperability is likely to be poor for the Joint Planning Network, in part because of the allied navies’ shortage of SATCOM bandwidth relative to the U.S. Navy and the proliferation of both U.S. and allied systems.  Interoperability of sensor-to-shooter networks is also expected to be poor, because of the U.S. near-exclusive hold on most systems of this type.The Navy, however, does not have a coherent gameplan to promote combined C4I interoperability or to specify how different initiatives should be prioritized.  Nor does it have any developed doctrine and procedures on using advanced information technology specifically for working with allies”So interoperability is a problem, but what does policy have to do with it?  Why is interoperability policy important?  Interoperability policy defines the what, who, why, and where of interoperability programs.  It defines what actions will be taken, who is responsible for them, why they must be taken, and where they’re to be performed.  Interoperability plans and procedures add the how and when, in that they define processes for performing the actions and provide schedules for their completion.  Some people have the mistaken notion that interoperability policy doesn’t matter, that it doesn’t have any effect on the actual achievement of interoperability.  Instead, these individuals believe that only technology factors have an effect on achieved interoperability.  Nothing could be further from the truth, as evidenced by this very recent policy statement:  “Interoperability will be a key determinant in the acquisition of new weapons and a greater set of interests will be brought into the decision making process. There will also be a single focal point to coordinate all  interoperability activities. With the start of the new fiscal year on 1 October, the DoD acquisition officials will begin considering interoperability as a "key performance parameter" in the development of individual weapon systems and military equipment. For the first time equipment will be fielded in part to achieve commonality with the equipment of other US military services or allied capabilities.‘Weapons will be judged by their ability to 'plug and fight',’ said Vitali Garber, the DoD's director of interoperability ­ a position recently created by DoD acquisition chief Jacques Gansler as a central authority for interoperability.” [5]Similarly, M&S project funds can be withheld, or permission to play with other simulations denied, because a simulation is not HLA(High Level Architecture)-compliant – things that are required by DoD (Department of Defense) interoperability policy[6].  The Policy ConundrumNow that we’ve established that interoperability is a problem, that it’s important, and that interoperability policy is important, we can turn our attention to the next question, “Is interoperability policy a problem?”  At least one high official within DoD issued a recent statement that supports the view that it is a problem:  “The ‘architecture and management [of the Defense Information Infrastructure] have failed to provide the security, interoperability and economy originally envisioned,’ Arthur Money, DOD's chief information officer, told a joint hearing of the Procurement and Research and Development Subcommittees of the House Armed Services Committee.”  [7]The labyrinth of interoperability policy can present many obstacles to unwary, unsuspecting C4I (Command, Control, Communications, Computers, Intelligence) or M&S (Modeling and Simulation) system developers and users.  The purpose of this paper is provide a roadmap to help weary developers and users find their way through this complex, confusing, and often frustrating maze of policies.  It is hoped that this paper will help the reader to better understand interoperability policy by condensing, comparing, contrasting, and presenting the basic elements of  interoperability policy in a form that helps bring order, meaning and understanding out of chaos.The problem of analyzing interoperability policy can best be understood by first viewing it within the context of a comprehensive interoperability program, as shown in Figure 2.0 [8]. 3.  MethodTwo basic approaches were taken to analyzing interoperability policy, automated information retrieval & text analysis methods, and manual, semi-automated techniques such as the construction of information matrices with spreadsheets.Automated Text Analysis & Information Retrieval – SMARTThe technique developed by Dr. Gerard Salton for automated information retrieval and text analysis is called the vector-space model [9], which is similar to an n-space dot product of a vector representing a query and a vector representing a document. The elements of each vector are words, word stems, and word pairs found in the documents. The result of the n-space dot product is a scalar value between zero and one, with zero being a document pair having no common terms and one being a document pair having all (100 percent) common terms.  A 1.0 scalar value is always achieved when a document is compared to itself.Automated interoperability policy document information retrieval and text analysis was performed with a freely available and extremely mature software tool called System for the Mechanical Analysis and Retrieval of Text (SMART), which implements the vector-space model[10].Manual Analysis MethodsFive different manual analysis methods were used to analyze interoperability policy.   Each of these methods used intermediate results obtained from the automated text analysis & information retrieval tools that were used in the interoperability policy study.First, a “Top-10 List” of interoperability policy documents was drawn up from those policy documents that had high relevance scores computed by SMART with a list of interoperability policy concept terms, which itself was made by eliminating very rare (low frequency) and very common (high frequency) interoperability policy concept terms.  Some judgement was also used to decide just what or wasn’t a policy document – a policy document had to belong to one of three categories, policy directives, standards mandates, and implementing memoranda.Second, interoperability policy statements and element were manually extracted from each of the documents in the “Top-10 List” by common text outlining and summarization techniques.Third, an Interoperability Policy Roadmap was constructed by manually examining each policy document to determine its precedence and dependence upon related policy documents.  Directives at higher levels of the organization typically spawn related, implementing directives at lower levels; with more process details being added at successively lower levels.Fourth, an Interoperability Policy Element Document Presence Matrix was constructed to relate all interoperability policy elements to each interoperability policy document in the “Top-10 List.”Last, a C4I and M&S Systems Interoperability Policy Elements Comparison Matrix was constructed by sorting and placing interoperability policy elements side-by-side for both the C4I and M&S domains.  This matrix made it possible to visually identify the completeness and consistency of interoperability policy in both the C4I and M&S domains.Analysis and DiscussionThe analysis of interoperability policies in both the C4I and M&S domains is discussed here.  Two kinds of analysis results are presented and discussed, Automated Text Analysis & Information Retrieval, and manual analysis.Automated Text Analysis & Information Retrieval ResultsAnalysis results for Document-to-Document Relevance, Document-to-Interoperability Policy Concept Term List Relevance, Policy Concept Term List Discriminatory Value, and the Document Interoperability Policy Concept Term Presence Matrix are discussed in the following sections. Document-to-Document RelevanceIn Figure 4.1.1 we can see that the following C4I interoperability policy documents are strongly related to one another (document-to-document relevance between 0.6 and 0.8), with the bolded policy directives being strongly related  to more than one other directive[11, p. D-1]: DoDD 4630.5, C3I InteroperabilityDoDI 4630.8, C3I Interoperability ProceduresCJCSI 6212.01A, C4I InteroperabilityIn the M&S domain, however, we see that the top-level M&S interoperability policy directive, DoDD 5000.59, M&S Management, is strongly related to the following policy directives from both domains: DoDI 5000.61, M&S VV&ADoDI 4630.8, C3I Interoperability ProceduresInterestingly, the directive DoDI 4630.8, C3I Interoperability Procedures is strongly related  to policy directives in both domains.  It seems to act as a kind of  link between  the two domains.It is also interesting to note that the top Navy M&S interoperability directive, SECNAVINST 5200.38, DON M&S Program, isn’t strongly related to any other policy document.Document-to-Interoperability Policy Concept Term List RelevanceTable 4.1.2 lists the SMART-computed relevance scores for the “Top-10” interoperability policy documents in descending order[11, pp. 3-1 to 3-3].  In this case, each document was compared to a specific list of interoperability policy concept terms rather than the more general list of interoperability terms that were used to compute document-to-document relevance.Policy Concept Term List Discriminatory ValueOne of the issues that surfaced during the interoperability policy study was the method of selecting concept term lists.  SMART provides one method, which consists of selecting a lower and upper bound to the frequency of concept term occurrence within the document database.   This is only the starting point however.As the interoperability policy study’s authors caution:“In summary, it is important to understand that meaningful text analysis using dot product spaces requires an iterative process to develop a high-quality query.  Terms on the detailed term presence tables were present in many documents.  If those terms have poor discriminatory value (i.e., appearing in documents known to have dissimilar contents), then they must be removed from the queries.The fact that the queries themselves were markedly similar indicated that the queries required additional refinement. Discriminatory value is improved if the queries are as orthogonal as possible.”  [11, pp. 2-14,15]An example of the relative discriminatory values of four different concept term lists is shown in Figure 4.1.3.4.2	Manual Analysis Results DiscussionAnalysis results are discussed in the following sections:  (1) interoperability policy statement and element selection; (2) policy document precedence anddependence; (3) C4I and M&S Systems Interoperability Policy Elements Comparison; (4) policy consistency; (5) policy completeness; and (6) policy effectiveness and efficiency. 4.2.1	Interoperability Policy Statement and Element SelectionPolicy elements were manually extracted from policy statements found in interoperability documents that were highly relevant to the Interoperability Policy Concept Term List.  Twenty candidate interoperability policy documents were originally selected for analysis from the interoperability document database, which includes almost seventy different documents.  The author further reduced this list of twenty documents to a “top-ten” list (see Table 4.1.2) by further selecting only those documents that could be unambiguously identified as being policy-related (a directive, standards mandate, or implementing memorandum).4.2.2	Interoperability Policy Roadmap – Policy Document Precedence and DependenceReviewed interoperability policy documents receive their authority and mandate from higher management and command authority levels or echelons.  This hierarchy of authority is diagrammed for Navy M&S Program Managers in Figure 4.2.2-1, which depicts the interoperability policy document precedence and dependence relationships existing within the list of interoperability documents having the highest relevance to interoperability policy concept terms.  In most cases, documents with later publication dates point to documents written earlier at higher organizational levels or echelons.   Subordinate commands frequently use identical, similar,  or  related names.   For example, the Navy Modeling and Simulation Master Plan points to the DoD Modeling and Master Simulation Plan and succeeds it in its date of publication.  Figure 4.2.2-2 provides a similar roadmap for Navy C4I system program managers; it deletes references to M&S interoperability policy documents. It also adds some specific Navy C4I interoperability  policy references that are uniquely C4I, but were not analyzed or included in the original interoperability policy document database.4.2.3	C4I and M&S Systems Interoperability Policy Element MappingThe author independently extracted interoperability policy elements from each interoperability policy document in the “Top-10 List” (see Table 4.1.2).  These policy elements are primarily concerned with prescribed policy actions or the “what” of interoperability policy; they include very few references to the “who, why, or where.”  The policy elements extracted by the author are also not the same as those extracted by the interoperability study authors [11], but they are based upon them.  Finally, interoperability policy elements are condensed to an abbreviated form of short phrases or statements that can be easily displayed and sorted in a matrix.4.2.3.1	C4I Systems Interoperability Policy Element MappingC4I  systems interoperability policy elements are mapped to interoperability documents in the “Top-10 List,” as shown in Figure 4.2.3.1.  Related C4I systems interoperability policy elements are grouped together and alternately shaded in gray or white to distinguish one conceptual group from another. Figure 4.2.3.1 also shows that with one exception, all the C4I systems interoperability policy elements are confined to six C4I systems policy documents (CJCSI 6212.01A, C4I Interoperability; DoDI 4630.8, C3I Interoperability Procedures; DoDR 5000.2-R, MDAP & MAIS Procedures; DoDD 4630.5, C3I Interoperability; JTA Version 2.0; USD(A&T) Memo JTA Ver. 2.0).  Virtually  none of  them come from M&S policy documents.  This is what was expected since C4I policy documents deal with issues that are common to both C4I and M&S,  while M&S policy documents are largely confined to M&S issues.4.2.3.2	M&S Systems Interoperability Policy Element MappingM&S systems interoperability policy elements are mapped to interoperability documents in the “Top-10 List,” as shown in Figure 4.2.3.2.  Related M&S systems interoperability policy elements are also grouped together and alternately shaded in gray or white to distinguish one conceptual group from another. A closer inspection of Figure 4.2.3.2 also reveals that M&S interoperability policy elements are largely confined to four M&S policy documents (DoDI 5000.61, M&S VV&A; DoDD 5000.59, M&S Management; SECAVINST 5200.38, DON M&S Program; and USD(A&T) HLA Memo).  Virtually  none of  them come from the four C4I policy documents.  This time, however, more than half of the M&S interoperability policy elements come from JTA-related interoperability policy documents [12] rather than M&S interoperability policy documents – this was not expected.  Comparison of C4I and M&S Interoperability PolicyOnce both C4I and M&S interoperability policy elements have been mapped to their respective policy documents it is then relatively simple to compare C4I interoperability policy elements with M&S interoperability policy elements for completeness and consistency.   Table 4.2.4 provides a side-by-side comparison of both C4I and M&S interoperability policy elements that have been sorted, collated, and shaded  to clearly show:  (1)  which C4I interoperability policy elements have no counterpart  elements in M&S interoperability policy documents;  (2)  which M&S interoperability policy elements have no counterpart  elements in C4I  interoperability policy documents; and (3) which interoperability policy elements are common to both C4I and M&S interoperability policy.CompletenessCompleteness of interoperability policy will be analyzed and discussed in the context of Table 4.2.4 and other policy analysis work performed in the interoperability policy study [11].  Table 4.2.4 suggests four categories of interoperability policy completeness that should be discussed, interoperability policy elements that are: (1) in C4I, but not in M&S; (2) in M&S, but not in C4I; (3) in both C4I and M&S; and (4) not in C4I or M&S.Interoperability Policy Elements in C4I, not in M&SThe following types of interoperability policy elements are found in C4I interoperability policy documents, but not in M&S policy documents:CertificationWhy “certification” is used in the C4I domain, while “accreditation “ is used in the M&S domain, is not completely clear since the two terms seem to be used interchangeably. Certification, for example, is defined as “The process by which DOD systems with C4I capabilities is evaluated for satisfaction of requirements for interoperability, compatibility, and integration. “  [13, p. A-1], while accreditation is alternately defined as “The process by which a C4I system is evaluated for meeting security requirements to maintain the security of both the information and the information systems.”  in the C4I domain [13, p. A-1] and “The official certification that a model, simulation, or federation of models and simulations is acceptable for use for a specific purpose.” [14, p.8]. The distinction is further confounded by the meaning previously given to  “Certification” in tactical systems software development, the endorsement by signature of the commanding officer of an operational unit that the certified software had been correctly installed, tested, and found operationally acceptable in that unit’s specific operating environment.DISA (Defense Information Systems Agency) has certification authority for reporting the interoperability of C4I systems, while there is no comparable authority for certifying simulations, or simulation-to-C4I system interfaces.  Certification of the interoperability of M&S systems is not explicitly addressed by DoD, Joint, or Navy interoperability policies.CompatibilityCompatibility is defined in the C4I domain as “The capability of two or more items or components of equipment or material to exist or function in the same system or environment without mutual interference.” [13, p.A-2], but the term is not used in the M&S domain, and there is no good reason why it shouldn’t be used there too. A C4I system or a simulation might be “compatible” with some other system, but not necessarily “interoperable” with it; and the distinction  is both meaningful and important.  DoctrineSince both C4I systems and operational simulations need to faithfully represent the tactical doctrines upon which they’re based, this policy element should also be included in the M&S domain.  This is especially true since doctrine is part of the JTA “operational’ architecture, which is applicable to both C4I systems and simulations.IntegrationIntegration is defined in the C4I domain as “The arrangement of systems in an architecture so that they function together in an efficient and logical way.”  [13, p.A-3]  Once again, the meaning of integration, is entirely different than the meaning of “interoperability” or “compatibility.”  C4I systems and simulations may be successfully integrated with one another without necessarily being compatible or interoperable.  Since M&S systems are integrated in at least three contexts, as information systems themselves, as components of C4I and weapon systems, and as C4I/simulation interfaces, one should expect their integration to be an important part of M&S interoperability policy – but it isn’t.Interface StandardsInterfaces are explicitly addressed in C4I interoperability policy documents, but not in M&S interoperability policy documents. Omission of interface standards from M&S interoperability policy is an oversight, which may prove increasingly costly as simulations are increasingly interfaced with C4I systems.Interoperability Problem ReportingC4I interoperability policy requires that incompatibility, integration, and interoperability problems be reported, while M&S interoperability policy does not.  Since these same problems also occur between simulations and simulation-to-C4I system interfaces, they should also be systematically reported, recorded, and tracked.  How else can effective actions be planned for their resolution?Interoperability RequirementsThe only reference to interoperability requirements in M&S interoperability policy documents is in the DoD M&S Master Plan [15].  M&S interoperability policy is noticeably silent on the management, validation, and identification of interoperability requirements.  The lack of early identification of interoperability requirements could seriously hamper interoperability when models, simulations, C4I systems, and other automated systems need to interoperate.  C4I interoperability policy, on the other hand, provides significant coverage of interoperability requirements. Explicit references to maintenance of interoperability requirements are found in C4I interoperability policy documents.Interoperability Testing, OT&E, T&EC4I system interoperability policy includes no less than six explicit policy elements that pertain to interoperability testing: (1)  interoperability requirements testing; (2) Joint C3I Interoperability Database with MNSs and ORDs; (3) Joint tests verify interoperability in Joint operations; (4) OT&E plans include compatibility and interoperability test objectives; (5) TEMPS include compatibility and interoperability test objectives; and (6) Test and Evaluation (T&E) in all acquisition phases to verify interoperability.  M&S interoperability policies don’t provide any guidance on interoperability testing, even though simulations interoperate with one another and with C4I systems.  This is another glaring omission because it is virtually impossible to verify simulation interoperability without a similar emphasis on simulation interoperability testing, reporting, and tracking.Mapping, Charting, Geodesy Data Standards and SpecificationsC4I interoperability policy requires mapping, charting, and geodesy data standards to support interoperability.  Since simulations also use representations of their environment, such as maps and terrain databases, there is no reason why this policy element should not also apply to the M&S domain.  Mission Need Statement (MNS) and Operational Requirements Document (ORD)C4I interoperability policy requires that the C4I system MNS and ORD comply with interoperability policy.  It also requires the ORD to include: (1) interoperability requirements; (2) communications, protocols, and standards; (3) integration of the system into its architecture; (4) Joint use considerations; and (5) procedural and technical interfaces.  Since a simulation may be a Major Automated Information System (MAIS) itself or a significant part of an MDAP (Major Defense Acquisition Program) or another MAIS, there is no reason why these policy elements should not also be extended to the M&S domain.Interoperability WaiversC4I interoperability policy explicitly defines the process for obtaining a waiver for certification of C4I system compatibility, integration, and interoperability.  There are no comparable policy elements for M&S systems.  The Navy, however, does have specific criteria for granting waivers for HLA compliance, but it is not written down in a policy directive.  Clearly, the policy for granting waivers should also be explicitly stated  for the M&S domain.Interoperability Policy Elements in M&S, not in C4IThe following types of interoperability policy elements are found in M&S interoperability policy documents, but not in C4I  policy documents:AccreditationWhy “accreditation “ is used in the M&S domain, while “certification” is used in the C4I domain, is not completely clear since the two terms seem to be used interchangeably.  The use of these two terms should be standardized and distinctions made in both the C4I and M&S domains.Common Databases and  ToolsM&S interoperability policy explicitly prescribes common databases and tools for model and simulations, while C4I interoperability policy indirectly prescribes them by requiring conformance to the JTA and DII COE.  As newer C4I systems increasingly utilize object-oriented software development techniques, one would expect this commonality to extend across both domains.Data Interchange Standards and Protocols EstablishmentThis M&S interoperability policy element is essentially the same as two separate, common (C4I and M&S) policy elements that require data interchange standards and open systems architecture protocols.  No policy adjustments are needed.Data Validation, Verification, and Certification (VV&C)Since data verification is not just confined to M&S systems, there is no reason why this policy element should not be extended to the C4I domain.FederationsThese M&S policy elements govern the performance of member federates and are applicable to models and simulations.  When most C4I system software applications are developed with object-oriented technology then it might make sense to extend the application of these policy elements to the C4I domain.High Level Architecture (HLA)M&S interoperability policy elements governing the use of HLA are peculiar to models and simulations.  They have no counterpart in C4I systems, but increasing use of object-oriented software development may eventually result in a similar architecture being adopted for real time C4I systems, and similar policy elements would then be needed.Internet Standard and Protocol EstablishmentThis M&S interoperability policy element is essentially the same as other, similar C4I policy elements, and no policy adjustments are required.  Internet standards and protocols are obviously required for both domains, and in some cases both simulations and C4I systems share the same internet.No-pay / No-play Deadlines  for Architecture ConformanceThis M&S interoperability policy element was specifically introduced to enforce HLA-compliance of simulations.  Its equivalent in the C4I domain might be introducing similar deadlines for DII COE conformance, but better results would not necessarily be guaranteed by setting new deadlines.Object Model Data DictionaryM&S interoperability policy elements governing the use of object model data dictionaries are peculiar to models and simulations.  They have no counterpart in C4I systems, but increasing use of object-oriented software development may eventually result in a similar application being adopted for real time C4I systems, and similar policy elements would then be needed.Object Model Template Data Interchange Format (OMTDIF)M&S interoperability policy elements related to the Object Model Template Data Interchange Format (OMTDIF) are peculiar to models and simulations.  They have no counterpart in C4I systems, but increasing use of object-oriented software development may eventually result in a similar format being adopted for real time C4I systems.Risk managementRisk management in Navy simulations that interface with “live” systems, and in some cases, “virtual” simulations, is inferred by SECNAVINST 5200.38 [16], which requires close monitoring and higher visibility of such simulations.  This policy element, however,  cannot be found in any other M&S or C4I interoperability policy directive.  These kinds of interfaces could potentially result in situations where: (1) a real pilot is presented with both real and synthetic targets, but can’t differentiate one from the other; (2) a real air raid occurs while a ship’s C4I system is receiving synthetic targets during an exercise; and (3) a real missile launch occurs while a ballistic missile defense system is being exercised with synthetic missiles.  Such dangers strongly suggest that both C4I and M&S interoperability policy should completely define and emphasize specific requirements for risk mitigation in such situations.Standard Simulator Data Base Interchange Format (SIF)M&S interoperability policy elements related to the Standard Simulator Data Base Interchange Format (SIF) are peculiar to models and simulations.  They have no counterpart in C4I systems, but increasing use of object-oriented software development may eventually result in a similar format being adopted for real time C4I systems.Synthetic Environment Data Representation Interchange Format (SEDRIS)M&S interoperability policy elements related to the Synthetic Environment Data Representation Interchange Format (SEDRIS) are peculiar to models and simulations.  They have no counterpart in C4I systems, but increasing use of object-oriented software development may eventually result in a similar format being adopted for real time C4I systems.Interoperability Policy Elements in both C4I and M&STable 4.2.4 also show which types of interoperability policy elements are found in both C4I and M&S interoperability policy documents by shading those policy elements that are common to both domains.Interoperability policy elements that are common to both the C4I and M&S domains should not present any special problems to program managers or system users.  Instead, they tend to raise questions as to why more interoperability policy elements that are unique to either the C4I or M&S domain are not common to both of them.Interoperability Policy Elements not in C4I or M&SWhat may by more important  to any  thorough analysis of  interoperability policy elements is not revealed by Table 4.2.4 – that is the types of interoperability policy elements that are missing entirely from interoperability policy directives in both domains.   Some that are brought to mind by Figure 2.1 include technology integration, formal interoperability theory and modeling, and interoperability performance measurement and standards.Technology IntegrationNothing can more quickly render interoperability policy elements obsolete than major new advances in information technology.  In software development for example, “componentware” or software through components” promises significant breakthroughs in interoperability, but  none of the interoperability policy directives mention it.  Similarly, today’s interoperability policy elements do no reflect recent improvements in data transfer rates and latency made possible by recent advances in data communications technology.  Some examples include: (1) the use of lasers for intra-satellite data transfer rates of 1 GBPS; (2) cooled lasers that produce faster, single-wavelength optical signals on existing fiber-optic cables at speeds up to 10 trillion BPS; and (3)  routers that use only mirrors to direct network traffic sixteen times faster than electrical switches do.  When interoperability policy, such as the JTA, mandating the use of older, outdated information technology standards which don’t reflect current technology advances, it can result in the program manager and “C4I warrior” being saddled with information systems and an infrastructure that provide worse performance than would otherwise be made possible by updated standards.The omission of  Simulation Based Acquisition (SBA) from interoperability policy documents is equally distressing considering its special emphasis in recent acquisition reform measures.  This topic is discussed extensively in the Navy Modeling and Simulation Master Plan, but is not specifically mentioned in any of the interoperability policy documents that were analyzed.Formal Interoperability Theory & ModelingA formal, unified theory of interoperability performance and related, explanatory models are needed to underpin interoperability policy making in both the C4I and M&S domains.   Without this formal basis, common performance measures and metrics cannot be used to measure and compare interoperability performance in either domain; and the effects of changes in interoperability policy can never be known or understood.Interoperability Performance Measurement & StandardsInteroperability performance must be made measurable by adopting metrics which are common to both domains, and which validate a formal, testable theory of interoperability.  Only then will it be possible to write interoperability performance standards that can be used by policy makers to specify what levels of interoperability performance are actually achievable.  Current interoperability policies are conspicuously mute on the subject of what level of interoperability performance can or should be achieved.  Without quantified performance goals and common measures, it is impossible to set realistic goals or know  whether they’ve been accomplished.Consistency“The only completely consistent people are the dead.”   -- Aldous HuxleyOne view of consistency is concerned with faithful and accurate interpretation and implementation of policy at successively lower organizational levels.  A lower-level organization must issue specific policies that faithfully and accurately implement the broader policy statements of a higher command authority. Reviewed policy documents receive their authority and mandate from higher management and command authorities, as shown in the document hierarchies depicted in Figures 4.2.2-1 and 4.2.2-2.Another view of consistency deals with the technical attributes and details used to describe policy elements in each domain.  These inconsistencies are described in greater detail below in Section 5.2.4.2.5	Interoperability Policy Effectiveness and EfficiencyThe effectiveness and efficiency of interoperability policy cannot be determined because no validated theory or models of interoperability performance currently exist that explain how interoperability works and what causes interoperability performance to vary.  There are, therefore, no associated standard measures or metrics of interoperability performance that can be used to gage whether changes in interoperability policy cause any perceptible changes in interoperability performance.There are some measures and metrics of interoperability in the C4I domain, but they are largely confined to message formats and tactical data link standards and specifications.  They do not provide a single, quantified, validated measure of interoperability performance that can be used in either domain.ConclusionsThe following conclusions may be drawn from the above discussion and explanations.Interoperability Policy CompletenessInteroperability Policy Elements in C4I, not in M&SAs can be seen in Table 4.2.4, the following types of interoperability policy elements are found in C4I interoperability policy documents, but not in M&S policy documents:  Certification and re-certification; Compatibility; Doctrine, Integration, Interface standards; Interoperability problem reporting; Interoperability requirements; Interoperability testing, OT&E, T&E; Mapping, charting, geodesy data standards and specifications; Mission Need Statement (MNS) and Operational Requirements Document (ORD); and Interoperability waivers.Interoperability Policy Elements in M&S, not in C4IAs can be seen in Table 4.2.4, types of interoperability policy elements found in M&S interoperability policy documents, but not in C4I  policy documents, include: Accreditation; Common databases and  tools; Data interchange standards and protocols establishment; Data VV&C; Federations; High Level Architecture (HLA); Internet standard and protocol establishment; No-pay / No-play deadlines  for architecture conformance; Object Model Data Dictionary; Object Model Template Data Interchange Format (OMTDIF); Risk management (SECNAVINST 5200.38 only); Standard Simulator Data Base Interchange Format (SIF); and Synthetic Environment Data Representation Interchange Format (SEDRIS).Interoperability Policy Elements in both C4I and M&STable 4.2.4 also show which types of interoperability policy elements are found in both C4I and M&S interoperability policy documents by shading those policy elements that are common to both domains:  Data interchange standards for application data sharing; Defense Information Infrastructure Common Operating Environment (DII COE); Human-computer interface standards; Information modeling, processing, systems security, and transfer standards; Internet 5-layer network model; Internet standards and protocols; Interoperable with Joint and combined C4I systems and operations; Joint Technical Architecture (JTA); Open systems architecture standards and protocols; Seamless, transparent open systems infrastructure; Standard data elements exchanged by C4I systems and M&S applications; and Systems design and integration rules for technical architecture.Interoperability Policy Elements not in C4I or M&SInteroperability policy elements that are missing from both domains include technology integration, formal interoperability theory and modeling, and interoperability performance measurement and standards.  System acquisition and Simulation Based Acquisition (SBA) interoperability policy elements are also conspicuously absent from both C4I and M&S interoperability policy documents.Interoperability Policy ConsistencyInteroperability policy documents were found to be generally consistent with one another from one organizational level to the next, with the following exceptions.Policy FocusC4I system and M&S policy documents are markedly different when it comes to their focus on interoperability.  Interoperability is a major focus of C4I policy documents as evidenced by their higher correlation with interoperability policy concept terms in the interoperability policy document database [11, p. 7-1].  Interoperability is not a major focus of M&S policy documents as evidenced by their lower correlation with interoperability policy concept terms in the interoperability policy document database [11, p. 7-1].RequirementsC4I interoperability policy ensures that C4I system interoperability requirements: (1) are identified and assessed during system requirements (MNS, ORD) validation; (2) are documented; (3) meet standards; (4) are tested; (5) are validated; (6) are addressed throughout the system’s life cycle; (7) are a major consideration for MDAP/MAIS start and each milestone decision; (8) are consistent with C4I system architecture, integration requirements, interface standards, and procedures; and (9) conform to interoperability policy.  M&S policy does not provide explicit guidance on interoperability requirements. The only reference to interoperability requirements in M&S policy documents is in the DoD M&S Master Plan [15].  This oversight could result in the failure of simulation/C4I system interfaces to provide successful interoperation of simulations and C4I systems.StandardsC4I interoperability policy provides message format standards, tactical datalink standards, tactical database standards, and internet standards.  M&S policy provides architecture standards and internet standards. Navy M&S policy [16] is inconsistent in its reference to the DIS (Distributed Interactive Simulation) standard and technology, instead of HLA (High Level Architecture).  Neither domain provides an interoperability performance standard. Interoperability performance standards are needed by policy makers to specify what levels of interoperability performance are actually achievable. Without quantified performance goals and common measures, it is impossible to set realistic goals or know  whether they’ve been accomplished.InterfacesInterfaces are explicitly addressed in reviewed C4I interoperability policy documents, which require approved, validated interface standards [11].  Interfaces are not explicitly addressed in M&S policy documents.  It is impossible to assess the effectiveness or efficiency of simulation/C4I system interfaces without such interface standards.Validated TheoryA formal, unified theory of interoperability performance and related, explanatory models are needed to underpin interoperability policy making in both the C4I and M&S domains.   A formal theory of interoperability and related explanatory models are not now available in either the C4I or M&S domains.   It is impossible to write a meaningful interoperability performance standard without a sound theoretical basis. Performance Metrics & MeasurementQuantified interoperability performance measures and metrics are not available in either the C4I or M&S domains.  It is, therefore, impossible to measure and compare interoperability performance in either domain; and the effects of changes to interoperability policy can never be known or understood.TestingC4I interoperability policy requires: (1) Test and Evaluation (T&E) in all acquisition phases to verify interoperability; (2) Test and Evaluation Master Plans (TEMPs) and Operational Test and Evaluation (OT&E) plans which include compatibility and interoperability test objectives; (3) interoperability requirements tests that verify interoperability in Joint and combined operations; and (4) DISA maintenance of a database of C4I system interoperability requirements and test results.  M&S policy does not include comparable, explicit interoperability test requirements for the interoperability of models and simulations.  It is, therefore, impossible to assess the effectiveness or efficiency of simulation/C4I system interfaces without similar, comprehensive testing policy elements.Accreditation and CertificationDISA has certification authority [13] for reporting the interoperability of C4ISR systems [11, p. 5-2]. There is no certification authority for the interoperability of simulation/C4I system interfaces.  Certification of interoperability of C4I systems is explicitly addressed by C4I interoperability policy, but the interoperability of M&S systems is not explicitly addressed by DoD, Joint, or Navy policy.  component level of policy document. M&S HLA compliance is addressed by M&S policy, but it does not guarantee interoperability of one simulation with another, or with a C4I system through a simulation/C4I system interface. Processes and ProceduresC4I interoperability policy defines detailed, explicit processes and procedures for achieving C4I system interoperability, but M&S policy does not.  Both the C4I and M&S communities have clearly defined organizational mandates for carrying out policy, but processes and procedures are clearer and more precise in the C4I domain than they are in the M&S domain. Risk ManagementSimulation interfaces with live (real C4I systems) simulations or virtual simulations could result in situations where: (1) a real pilot is presented with both real and synthetic targets, but can’t differentiate one from the other; (2) a real air raid occurs while a ship’s C4I system is receiving synthetic targets during an exercise; and (3) a real missile launch occurs while a ballistic missile defense system is being exercised with synthetic missiles.  C4I or M&S interoperability policy does not completely define or emphasize specific requirements for risk mitigation in such situations.RecommendationsThe following recommendations are made to improve both C4I system and M&S interoperability policy completeness, consistency, effectiveness, and efficiency.Interoperability Policy AlignmentInteroperability policy elements in the C4I and M&S domains should be aligned with one another to improve policy completeness and consistency.Interoperability CompletenessIt is recommended that interoperability policy elements in the C4I domain that are not also in the M&S domain (see Table 4.2.4) be added to the M&S domain as detailed in Sections 4 and 5 above.   Similarly, it is recommended that interoperability policy elements in the M&S domain that are not also in the C4I domain be added to the C4I domain.Interoperability ConsistencyIt is recommended that interoperability policies be amended to make interoperability policy elements in the C4I and M&S domains consistent with one another as prescribed in Section 5.2 above.Interoperability Policy Effectiveness and EfficiencyIt is recommended that interoperability performance metrics be defined that can be used to measure interoperability in both the C4I and M&S domains.  It is also recommended that these metrics be based upon a validated, tested theory of interoperability and associated explanatory models.It is also recommended that the business process for achieving interoperability be formally modeled to provide a baseline for process improvement in both domains.  It is further recommended that the effectiveness and efficiency of this process be measured and tracked by monitoring changes in interoperability performances that result from process improvements or policy changes in both domains.Interoperability Program EstablishmentIt is strongly recommended that Joint, DoD, and DoD component policy makers in both domains adopt a common, coordinated interoperability program similar to the one depicted by Figure 2.0.  Such a program will provide a validated, tested theory of interoperability and associated, explanatory models, which in turn will provide a rational basis for interoperability performance metrics and measurement.  Metrics and measurement will then make it possible to write a meaningful interoperability performance standard that can then be used as a guide to establish, quantitative interoperability performance standards, which in turn can be used to rewrite interoperability policy to include realistic interoperability goals that are actually achievable.AcknowledgementsThe author wishes to acknowledge the contributions of Dr. Vitali Garber, DoD Director of Interoperability, CAPT Steve Chapman, Director of NAVMSMO, Mr. Jim Weatherly, TSG Director, Mr. Dave Merritt, PD 13 Program Director, and Dr. Albert Legaspi, Asst. TSG Director.  The interoperability program and this work would not be possible without their keen insights and special efforts.References[1]	Chairman of the Joint Chiefs of Staff (CJCS) Instruction 6212.01A, Compatibility, Interoperability, and Integration of Command, Control, Communications, Computers, and Intelligence Systems, Enclosure A, 30 June 1995, p. A-3. [2]	Department of Defense Directive (DoDD) 5000.59, DoD Modeling and Simulation (M&S) Management, Enclosure 2, 4 January 1994, p.1. [3]	Fasulo, LTC R.K., Joint Communications Interoperability, Carlisle Barracks, PA: U.S. Army War College, 29 March 1996, pp. 2-3. [4]	Odell, R.R., Morley, P., Gause, K., Ruiz-Ramon, F., Toward a U.S. Navy Strategy for C4I Interoperability with Allies, Alexandria, VA: Center for Naval Analyses, April 1999, pp. 3-4. [5]	Bender, B., “DoD Expands Pursuit Of Interoperability,” Jane’s Defence Weekly 29 September 1999.  [6]	Under Secretary of Defense (Acquisition & Technology) Memorandum, DoD High Level Architecture (HLA) for Simulations,  Washington, D.C.: USD(A&T), 10 September 1996. [7]	Verton, D., “DOD revamping massive information architecture,” Federal Computer Week, 25 February 1999. [8]	Sutton, P., “Interoperability: A New Paradigm,” in Computational Intelligence for Modeling, Control & Automation: Neural Networks & Advanced Control Strategies, M. Mohammadian, Ed., Amsterdam: IOS Press, 1999,  pp.  351-361. [9]	Salton, G., Wong, A., Yang, C.S., “A Vector Space Model for Automatic Indexing”, Communications of the ACM, 18, 613-620. [10]	Salton, G., McGill, M.J., Introduction to Modern Information Retrieval, New York: McGraw Hill, 1983. [11]	Science Applications International Corporation (SAIC), Technical Report, Modeling and Simulation Interoperability Study: DoD, Joint, and Navy Policies, San Diego: SAIC, 1 October 1999. [12]	Defense Information Systems Agency (DISA), JointTechnical Architecture (JTA), Version 2.0 (1st draft), Washington, D.C.: 31 October 1997, p. 1-5. [13]	Chairman of the Joint Chiefs of Staff Instruction, Compatibility, Interoperability, and Integration of Command, Control, Communications, Computers, and Intelligence Systems, CJCSI 6212.01, Washington D.C.: CJCS, 30 June 1995. [14]	Department of Defense Instruction, DoD Modeling and Simulation (M&S) Verification, Validation, and Accreditation (VV&A), DODINST 5000.61, Washington, D.C.: DoD, 29 April 1996. [15]	Department of Defense, Modeling and Simulation (M&S) Master Plan, DoD 5000.59-P, Alexandria, VA: Defense Modeling and Simulation Office (DMSO), October 1995. [16]	Department of the Navy Instruction, Department of the Navy Modeling and Simulation Program, SECNAVINST 5200.38, Washington, D.C.: Secretary of the Navy (SECNAV), 10 October 1994.Author BiographyPaul Sutton is the Navy Interoperability Manager in the Technical Support Group (TSG) of SPAWAR PD 13, which provides direct support to the Navy Modeling & Simulation Management Office (NAVMSMO), OPNAV N6M.  He holds a Bachelor of Science degree in General Engineering from the U.S. Naval Academy, and a Master of Science degree in Management from San Diego State University.   He has more than thirty years experience in distributed, interactive modeling and simulation, advanced networking engineering, systems engineering, software engineering, and project management. He has previously taught both undergraduate and graduate courses at U.S. International University, San Diego State University, the University of San Diego, and the University of La Verne. PAGE  17PAGE  5QualitativeProcess ModelRefineTest, Evaluate &Design, Develop,PerformanceInteroperabilityand InterfaceInfrastructure,Simulation,System,Test C4IHandbookInteroperabilityWrite PMInstructionInteroperabilityWrite NavyPlansPolicy &RefineStrategyRefineModelProcessRefineDevelopPolicy & Plans& TechnologyQuantitativeInteroperabilityDescriptive &Interoperability ResultsObserve & MonitorPlansPolicy &ImplementStandardPerformanceInteroperabilityWriteTheoryInteroperabilityRefineDevelop &ModelsInteroperabilityPrescriptiveAdvancesInteroperabilityEvaluateDataCollect InteroperabilityFigure 2.0  Interoperability Program Architecture EMBED Word.Document.8 \s  EMBED Word.Document.8 \s  EMBED Word.Document.8 \s  EMBED Word.Document.8 \s  EMBED Word.Document.8 \s  EMBED Word.Document.8 \s  EMBED Word.Document.8 \s  EMBED Word.Document.8 \s  EMBED Word.Document.8 \s  EMBED Word.Document.8 \s  EMBED Word.Document.8 \s  EMBED Word.Document.8 \s  EMBED Word.Document.8 \s  EMBED Word.Document.8 \s  EMBED Word.Document.8 \s  EMBED Word.Document.8 \s  EMBED Word.Document.8 \s  EMBED Word.Document.8 \s  EMBED Word.Document.8 \s  EMBED Word.Document.8 \s  EMBED Word.Document.8 \s  EMBED Word.Document.8 \s  EMBED Word.Document.8 \s Table 4.1.2 Document-to-Interoperability Policy Concept Term List Relevance – The “Top-10” ListFigure 4.1.3  Relative Discriminatory Values of Four Concept Term ListsFigure 4.2.2-1  M&S Interoperability Policy RoadmapFigure 4.2.2-2  C4I Interoperability Policy Roadmap