Acceptability Criteria: How to Define Measures and Criteria for Accrediting SimulationsSimone YoungbloodDefense Modeling & Simulation Office1901 N. Beauregard Street, Suite 380AAlexandria, VA 22311703-824-3436smyoung@dmso.milBob SenkoConsultant41940 N.W. Covey LaneBanks, OR 97106503-324-0607 HYPERLINK "mailto:rsenko@dmso.mil" rsenko@dmso.milSelecting VV&A measures is a critical step in the validation and accreditation planning process. These measures define what is essential to the accreditation assessment and subsequent decision and what information needs to be collected to answer the measures. Clearly stated measures help focus the accreditation effort and support determination of resources needed. Accreditation measures are normally developed from an analysis of M&S requirements and are based on the results of the operational risk assessment.What are MeasuresThere are three commonly used types of measures: measures of merit (MOMs) (aka mission level measures (MLMs)) relate the effects of a concept or system to the mission that the concept or system supports; measures of effectiveness (MOEs) are used to measure a system’s effectiveness in the accomplishment of a task; and measures of performance (MOPs) are used as a quantitative or qualitative measure of system or system component capabilities or characteristics. All measures should be derived from requirements. In order to be useful for VV&A analyses, all measures should defined in terms of specifically how they will be supported by data from V&V and Accreditation activities.Measures of Merit measure concept or system capabilities in terms of the effects of these capabilities on the overall mission of which the concept or system is a part. They measure mission or simulation attributes that define the overall objectives that the simulation must be capable of accomplishing. An example of an attribute for an aircraft flight simulator would be realism, and examples of measures for realism might be visual fidelity, control feel, switch placement and functionality, etc. If a MOM is defined in terms that do not permit direct evaluation (specific Acceptability Criteria), it should be supported by one or more MOEs or MOPs that can be evaluated.Measures of Effectiveness measure simulation capabilities in terms of task accomplishment or simulated system attributes. Most simulations involve capabilities that can be related directly to operational capabilities in terms of engagement or battle outcome, e.g., Force Exchange Ratio, Hard Target Kills per Aircraft Sortie. For those that cannot be directly related to operational capabilities, the developed measures should be expressed in terms of concept or system attributes. For example, measures for a Command and Control (C2) simulation might be expressed in terms of capacity, consistency, timeliness, accessibility, completeness, accuracy, transportability, and security. The measures should then be developed to levels of specificity such that the C2 system can be assessed for its fitness for purpose. MOE evaluation criteria (Acceptability Criteria) should be quantitative if at all possible.Measures of Performance are quantitative or qualitative measures of a simulation’s capabilities or characteristics. They are used when it is difficult to directly assess an MOE or to establish quantitative criteria. MOPs can be related to some numerical scale, for example, a communication system simulation throughput. Qualitative MOPs are categorical measures of performance that refer to the presence or absence of specified characteristics. Usually subjective measurement techniques are used to address qualitative MOPs. Qualitative MOPs are based on simulation characteristics that are defined by the needs of the user or meet user-defined system performance requirements.Criteria. Associated with each measure are criteria. Criteria represent a level of performance against which simulation characteristics and capabilities are compared. Specifically, for VV&A, these are called Acceptability Criteria - criteria that a particular model, simulation or simulation federation must meet to be accredited for a specific purpose. Acceptability criteria are tied directly to M&S requirements and should specify measurable performance in the User, Simulation, and Problem Domains. Acceptability criteria are associated with all the above measures.Criteria are frequently expressed as thresholds – the minimum capability needed for the simulation to be fit for the intended purpose as defined and agreed to by the User. Criteria should be unambiguous and assessable, whether stated qualitatively or quantitatively. Often, these criteria are based on a predetermined standard or referent. More discussion of the referent can be found in the elaboration on Fidelity in the on-line VV&A Recommended Practices Guide (RPG), http://www.msiac.dmso.mil/vva/default.htm.  But in it’s simplest form, the Referent is a codified body of knowledge about a thing being simulated”.   How are Measures Identified?The VV&A team should develop measures and criteria for each simulation requirement or objective in order to focus V&V and accreditation activities. For every simulation, there should be well-defined mission requirements and simulation objectives. The measures provide a way of relating the results of V&V and accreditation activities to the simulation’s ability to solve the problem or resolve the objectives (meet requirements). The trace from mission requirements and objectives to simulation verification and validation measures is derived by analysis.Problem Definition, and Risk AssessmentAn accurate and complete Problem Statement is necessary in order to fully develop criteria by which the fitness of the simulation can be evaluated. A tutorial on Problem Analysis is addressed in a separate special topic paper in the On-line RPG. Given that the Problem Statement is well understood, the next step is to define M&S Requirements. These requirements should completely and unambiguously elucidate the capabilities needed by the simulation based on the problem, intended use, and planned implementation. More details on Requirements definition and development can be found in the on-line RPG in that special topic paper. The next step is to complete the Operational Risk Assessment. This assessment provides a basis for planning for the accreditation effort by defining critical requirements on which the VV&A effort should concentrate. More details on Operational Risk Assessments can be found in another special topic paper in the on-line RPG.Requirements AnalysisOnce the M&S Requirements have been identified, specific objectives should be derived for the VV&A effort. The objectives should be worded in terms that are simple and relevant. Decomposition of the objectives into sub-objectives may be necessary so that all can be clearly defined in terms of associated measures and V&V and accreditation tasks that must be accomplished. The ideal situation would be one objective that defines one measure that defines one task. This situation seldom occurs. The principle of simplicity and direct traceability of objectives to sub-objectives, to measures, to tasks should be used as a guide in the full development of objectives.Failure to solidify the Problem Statement, Requirements, and Risk Assessment at the very start of planning for the VV&A effort will impact all subsequent tasks and activities the V&V and Accreditation Agents undertake.These steps are more than fundamental, they are essential. The VV&A effort will not attain meaningful results if the problem and requirements are subject to continual change or revision. This is not to say that M&S Requirements cannot be changed, but changes should occur only when absolutely necessary. The User is the final approval authority for any changes. The Dendritic Analysis StructureDecomposition of missions into functional processes and finally into system attributes is a common analytical approach, sometimes referred to as a dendritic due to the dendritic or tree-like form of the resulting analytic structure. Unfortunately, this approach usually results in a complex analytical structure that defines many more compositional elements than a reasonable VV&A program could measure. The problem can be simplified by considering only the critical elements of the mission or objective function. Inherently, this means that the elements of the problem are not equally weighted and the analyst has to conduct sensitivity analyses, early trade-off studies, or apply expert judgment to select (or deselect) the critical factors that will be measured and consequently drive the VV&A assessment.The dendritic process is a structured process for identifying all elements or actions necessary to address or resolve simulation objectives. The process provides the underlying structure for the VV&A analysis approach and methodology. This structure becomes the basis of the other planning tasks, i.e., selection of V&V events and methods and identification of resource needs. More detail on V&V and accreditation planning can be found in the Core Documents on V&V Agent and Accreditation Agent in the on-line RPG.A key component to the process is linkage: all data collected using the various VV&A activities is required and has a place in satisfying the objectives. This linkage assures that only needed data is planned for and that resources will not be wasted by planning unnecessary activities or collecting unnecessary data. An incomplete dendritic structure may not yield enough information to adequately address the objectives. Worse yet, an incomplete structure may result in false conclusions drawn from incomplete sets of information.Time and resources will most likely preclude addressing every aspect of the concept or problem. Therefore, the approach should include sensitivity analysis and risk assessments of the key issues to identify those specific areas that show the greatest potential for impact on the accreditation decision. In addition, expert judgment is a practical necessity. The judgment of Subject Matter Experts (SMEs) may be useful in developing or confirming the completeness of an analytical structure. This may allow the scope of early trade-off studies and sensitivity analyses to be minimized or focused on areas of greatest uncertainty. Judgment should be recognized as fundamental to the analytical process, and the rationale and impacts of analytical judgments should be consciously addressed.The structured approach should formalize the process to identify all objectives the simulation is to address. One approach to defining simulation objectives is the establishment of a hierarchy in which the objectives (requirements) are at the highest level. These, in turn, are supported by sub-objectives and then measures. Most examples from Department of Defense (DoD) publications show a structure that develops objectives from the problem that the simulation is to solve, decomposes the objectives to a level that they can be addressed by measurable quantities, develops those measures, defines pass-fail criteria, and identifies supporting empirical data for comparison. The discipline of using this analytical hierarchy (dendritic) type of architecture may appear to thwart creativity, but the structure it provides to the VV&A design process should help to illuminate areas that are ambiguous or uncertain and where creative thought is needed.The thrust of this breakdown is to subdivide simulation objectives into more explicit measures or sub-measures that can be assessed against information collected during VV&A events. Measures may be formed at different levels depending on the objective. That is, one objective may have a sub-objective that leads immediately to quantifiable measures, while another objective may require additional subdivisions before measures are evident. Figure 1 provides an example set of levels for a structured approach to developing measures for a flight simulator.To implement this approach the analyst breaks the objective down into smaller and smaller segments by asking, "what do I need to know to answer this question?" Each level of the structure is given a title, i.e., requirement, objective, measure, or empirical data. However, there is no set number of levels.This structured approach fosters discipline, accountability, and visibility. The resulting analytic structure preserves the organization and logic of why specific empirical data are required and, therefore, serves as an audit trail. The analyst’s decisions in establishing each level help to focus the planning in a specific direction. The V&V and Accreditation Agents must understand and consider how these decisions will impact the VV&A effort. The analytic structure will identify the need for collection of specific data during VV&A test activities and for analyses to be performed in a particular manner. Thus each decision made in building the analytic structure may have major resource considerations.Performing this structured analytic process may appear to be simple and easily accomplished because it seems to imply "just continue to analyze and breakdown the problem into measurable parts." In fact, construction is often inductive, not deductive, and requires expert knowledge and often substantial creativity.Establishing Acceptability CriteriaThe Accreditation Agent normally identifies acceptability criteria and then has them approved by the User. These are the most important facts the V&V and Accreditation Agents work with. They set the “pass/fail” data points for the entire VV&A effort.M&S Requirements established in early program documentation form the basis for Acceptability Criteria. Each criterion must be related to a documented M&S requirement, such as a threshold value. All others should be derived by expert analysis of the requirements documents and discussions with the User and other SMEs. Criteria should be quantitative whenever practical, but may be supplemented by qualitative inputs based on VV&A team expertise. Ensuring that values are related to simulation requirements is the single most important consideration when identifying and establishing acceptability criteria.ConclusionProperly developed measures, based on requirements analysis and operational risk assessment, can provide a firm basis for making the accreditation assessment. A structured approach, such as the dendritic, provides a rigorous methodology for developing measures and the criteria used with them. Acceptability Criteria are then identified based on M&S requirements and provide the “pass/fail” points for the accreditation evaluation.More information on developing verification, validation, and accreditation plans can be found in the V&V and Accreditation Agent Core documents.ReferencesDoD 5000.59-M, Department of Defense Modeling and Simulation Glossary, January 1998.Joint Test and Evaluation Program, Joint Feasibility Study (JFS) Handbook, Office of the UnderSecretary of Defense for Acquisition, Technology, and Logistics/Director for Test, Systems Engineering and Evaluation (OUSD(AT&L)/DTSE&E), 1996.Joint Crisis Action Test and Evaluation Program, “Program Test Design”, UnderSecretary of Defense for Acquisition and Technology/Director for Test and Evaluation (USD(A&T) DT&E), JCATE 93-1, 12 October 1993.Air Force Test and Evaluation Center (AFOTEC) Manual 99-101, “Test and Evaluation Business Management Process Manual”, 20 January 2000.Commander, Operational Test and Evaluation Force Instruction (COMOPTEVFORINST) 3960.1H, “Operational Test Director’s Guide”, 13 December 1995.U.S. Army Test and Evaluation Command (ATEC) Regulation 73-1, “System Test and Evaluation Policy”, 10 December 1999. Air Force Material Command (AFMC) “Detailed Test Plan Process Action Team Final Report”, August 1993.  FILENAME 20909_SIW_MOM																			07/03/02