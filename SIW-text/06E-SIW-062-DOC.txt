Systems Engineering in JDEP Test Harness DevelopmentJackson LudwigNATO ALTBMD POOude Waalsdorperweg 612501 CD The HaugeNetherlands+30 (0)70 374 xxxxjackson.ludwig@tmd.nato.in Robert BollingThe MITRE Corporation7515 Colshire DriveMc Lean, VA 22102+1 703-983-6471rbolling@mitre.orgKeywords:HLA, JDEP, automation, distributed simulation, VV&A ABSTRACT: Experience in the Joint Distributed Engineering Plant (JDEP) showed that existing processes for setting up simulations and analyzing data were inadequate and tools were needed to prepare, execute, capture and analyze data in distributed simulations.   During verification and validation of the infrastructure build (IBuild) for the Single Integrated Air Picture (SIAP) program, automated tools were developed to assist testing.  These tools reduced the time and manpower needed to conduct tests, increased accuracy and, reduced the number of errors while enhancing confidence in the test results.  They allowed easy repeatability of tests and facilitated the execution of large sets of test excursions.  These prototype test tools have helped spurred the development of better commercial applications for similar purpose.1. IntroductionWhile technologies such as High Level Architecture (HLA) and faster computers have speeded simulation execution, simulation setup and data analysis activities represent the majority of the time spent during system testing.  Gains in productivity and increased accuracy and confidence in test results can be obtained by improving automation in the setup, control, and analysis phases of simulation-enabled testing.  This paper presents lessons learned during the multiyear development of a simulation environment by the Joint Distributed Engineering Plant (JDEP) in support of the Joint SIAP (Single Integrated Air Picture) Systems Engineering Organization (JSSEO).  Section 1 introduces the JDEP program and objectives for the JDEP-JSSEO simulation development effort.  Section 2 describes the development and testing cycle.  Section 3 discusses the execution of verification and validation efforts, describes support tools developed and highlights ongoing technology needs.  Finally, conclusions from the JDEP development effort are presented in Section 4.1.1 The Joint Distributed Engineering Plant (JDEP)The JDEP program is an initiative sponsored by the Joint Interoperability & Test Command (JITC) to improve the interoperability of newly acquired systems [1] by injecting interoperability testing into programs early in their development.  To accomplish this, JDEP is seeking to develop a generic interoperability test harness.  The idea behind the test harness is to create a set of reusable models and tools, relying on the HLA standard, that when combined form a simulated environment that provides realistic stimulus to a system under test throughout its development cycle [2].  Figure 1 presents a notional representation of the JDEP generic test harness federation.  The system under test (SUT) is the focus of this federation and is fed inputs by the stimulator models.  Stimulator models represent any systems on which the system under test depends.  Examples include sensors and communications systems.  Each stimulator model is fed by a scenario script driver that provides ground truth and environmental information.  Controller and viewer applications provide a user interface to the test harness and manage the configuration and execution of simulations.  Simulation data is collected for analysis using offline tools. SHAPE  \* MERGEFORMAT Figure 1.1  The JDEP Generic Test HarnessWith the test harness, interoperability is enhanced by finding and fixing potential problems while the system is still in early development instead of waiting until the traditional interoperability testing period prior to system fielding [3].  System development also benefits from having access to a realistic simulated environment for developmental testing.  Further, an all-software test environment removes the burden of scheduling hardware-in-the-loop (HWIL) lab time and provides the opportunity for faster turn around and more what-if analyses.1.2 JDEP Infrastructure Build (IBuild)To support the development and interoperability testing of JSSEO‚Äôs software, the Integrated Architecture Behavior Model (IABM), the JDEP team developed the JDEP Infrastructure Build (IBuild) [4].  The IBuild is an instantiation of the JDEP generic test harness, specific to the needs of IABM testing.  To enable testing, several models were developed to stimulate the IABM.  Each of the models developed for use in the IBuild was built to be as configurable as possible to allow for wide reusability.The composition of the IBuild federation was driven by needs of the IABM and evolved to keep pace with IABM functionality.  The IBuild federation contains models representing generic radar, identification friend or foe (IFF) and electronic support measures (ESM) sensors.  These models provide the air surveillance data for the IABM.  All of the sensors have options and configurable parameters that enable them to emulate the functionality of a variety of systems.  A high-fidelity model of the TPS-59 radar and associated IFF were also developed for use in the IBuild.  The IBuild also contains a generic and high-fidelity WSN-7 navigation model to provide perceived position information to the IABM.  Likewise, a generic and a high-fidelity model of the Link-16 (MIL-STD-6016B) tactical datalink [5] provide data exchange between IABMs.  The final model in the IBuild federation is the Mission Computer Program (MCP) model.  The MCP is a first-order representation of the IABM that is sufficient to exercise the interfaces of the IABM and the other IBuild models.  It is also used for testing of the IBuild test harness itself and serves as a lightweight IABM (the SUT) as well as provide the full IABM with emulated peers during testing.The IBuild federation also contains several supporting tools.  Ground truth in the IBuild is distributed using the Common Reference Scenario Driver (CRSD).  Commercial-off-the-shelf products hlaControl and hlaResults are used to support federation control and data collection.  Several other tools were specifically developed by Virtual Technologies Corporation and by MITRE to aid test setup, control, and analysis and are discussed in the following sections.2. IBuild Development & Testing ProcessThe development of the IBuild started shortly after the start of development on the IABM in 2003.  With the exception of ADSIM (the Link-16 model), CRSD, hlaResults, and hlaControl, all models and tools making up the IBuild were new developments.  This reliance on newly developed models to create the test harness meant that the IBuild had to evolve to keep pace with IABM development spirals.  To be able to support testing of IABM spiral releases, each IBuild version itself needed to undergo verification.The challenge in the IBuild development was determining what functionality would be required to support the IABM testing and when it would be required.  By creating highly configurable models, the IBuild provided the IABM with a wide range of inputs and timings without requiring excessive rework of the models.  However, since the capabilities implemented in any given IABM spiral were not known well in advance of the spiral, the IBuild development times were compressed greatly, usually to less than three months.The verification phase for IBuild releases was compressed by the IBuild development cycle and by the IABM verification and validation deadline.  This resulted in the compression of the verification of IBuild models and the federation as a whole into a roughly one-month period of time.  This compression of the verification period drove the need for increased automation in test setup, execution, and analysis.  3. Automation Development across SpiralsDevelopment of automated support tools for the IBuild occurred along with the development of the IBuild models themselves.  Following each verification test cycle, the JDEP team took advantage of the time between IBuild developments to improve the available support tools capitalizing on their experiences from testing.3.1 Initial BuildWhen the initial version of the IBuild was released for verification testing, no tools were available to support test setup, execution, or post-run analysis.  Immediately, it became clear that manually starting and stopping from the command-line applications spread across approximately ten computers was unsustainable.  Batch files were created to speed the initialization of federation runs and this made test execution considerably faster and more reliable.Although faster, the batch file method was still tedious.  Models were still configured manually and configuration management of setup files relied upon the diligence of test operators.  Post-run results were then analyzed by hand to check for correctness.  Setup, execution and analysis of a single test could take one to two people half a day, especially if debugging was required to identify sources of error to the developer.  Given the limited time available to complete verification testing, only a few dozen data points (out of up to a few hundred megabytes) were actually checked to verify correct model operation.  This process was tedious and time consuming, not rigorous, and distracted from other aspects of the development process.  The need for better support tools was clear.3.2 Second BuildThe MITRE test team developed two tools, the Module Oriented Application Launcher (MOAL) and the Module Oriented Analysis Spreadsheet (MOAS).  The next rounds of IBuild verification testing saw the addition of these tools.  MOAL is a general purpose application launching and configuration management tool developed for JDEP.  It allows simulations to be configured and executed across multiple computers from a single workstation.  It stores the configuration files associated with each run and allows saved configurations to be recalled instantly.  This represented a major improvement in productivity.  Configurations could be changed and simulations initialized in a few minutes instead of tens of minutes.  This enabled the total number of verification runs to be increased from only a handful to dozens.  Furthermore, MOAL is a generic application, ‚Äî its ability to control programs or federations extended across multiple operating systems and was independent of the IBuild itself.  This flexibility has enabled MOAL to be used on other projects, such as MITRE‚Äôs federation supporting the US Border Patrol.While MOAL made simulation execution much less labor intensive, it did nothing to address the analysis burden.  To improve the efficiency of analysis, an application called MOAS was developed as an interim measure.  MOAS was a spreadsheet-based analysis tool created especially for IBuild testing.  It was linked to the hlaResults database via ODBC and contained numerous sheets with all the calculations necessary to verify the operation of IBuild models and perform fundemental analysis of overall federation performance.  It allowed results to be checked by entering timestamps of interest and relevant model configuration info.  Using the collected data and truth data, results could be compared directly with their expectations.  With MOAS, accuracy and speed were increased several fold, enabling several dozen data points to be checked for each model.  But, the process was still not fully automated.  More importantly, MOAS was created specifically to support the JDEP IBuild and, as such, has limited reuse potential.  Even with the improvements MOAS provided, the need for a more thorough and reusable solution was apparent.3.2 Third and Subsequent BuildsFor the third and subsequent rounds of IBuild verification testing, the Automated Test Tool (ATT) and hlaEval tools developed by VTC were used.  ATT replaced MOAL and provides a single application for model configuration, execution and management of configuration and results files.  Furthermore, it contained auto-documentation capabilities and linked directly into the analysis tool.  The tool provided an incremental improvement over the MOAL by adding the capability for batch operation and native support for XML-based configuration files.  This meant that batches of runs could be configured, analyzed and portions of the documentation written without operator intervention during runtime.  While it was developed in support of IBuild, it can support any federation that uses XML configuration files.  Its use enabled true regression testing to be carried out on new model releases and increased the number of simulation runs conducted from dozens to hundreds.HlaEval is a Java-based analysis tool that can execute queries on a variety of databases and then perform user-defined operations on the data.  With it, an analyst can create detailed analysis scripts specifying exactly which data elements to examine and what operations to perform on them.  Thus, all data collected during verification testing can be evaluated, boosting the comprehensiveness of the testing.  Speed is also increased over the previous, spreadsheet-based analysis tool because all entries in a database can be checked at once instead of just individual data elements.  Use of hlaEval enabled all the data collected in the database to be examined, errors flagged, and deviations from the expected values to be calculated in the same amount of time previously used to verify only a small subset of the data.  This allowed the results of IBuild verification testing to have statistical merit instead of relying on mostly anecdotal proofs of correct operation.  Also, unlike MOAL, hlaEval is generic and is capable of supporting of future federations.4. ConclusionThe IBuild experience has shown the importance of having a strong suite of supporting tools to aid test setup, execution, and results analysis.  Manual methods such as batch files and spreadsheets are not sufficient to support large numbers of runs and rigorous analysis.  Additionally, they are typically designed and built for a specific federation and series of use cases, thus limiting their effective reuse.  Using tools such as MOAL, ATT, and hlaEval has greatly improved execution and analysis in the JDEP IBuild and they have proven flexible enough for efficient use in other federations.  Although substantial gains have been made by use of these tools, their development was a diversion from the primary purpose of the testing effort.  While the applications discussed in this paper are available (at least to internal and government users), an ongoing need exists for more widely available and user friendly test automation tools5. AcknowledgementsThe authors would like to thank The MITRE Corporation for providing the opportunity to present this work and to the Joint Interoperability Test Command for funding the effort that made this work possible.6. References[ 1] Dahmann, J., Crisp, M., ‚ÄúJoint DistributedEngineering Plant ‚Äì Next GenerationInfrastructure for Network Centric SystemsEngineering and Test‚Äù, ITEA Workshop,September 2002.[ 2] Dahmann, J., Clarke, R., ‚ÄúStandards-basedFramework for Systems of Systems:Federations: JDEP Experience to Date‚Äù,03S-SIW-047, Spring SIW, March-April2003.[ 3] Dahmann, J., et. Al, ‚ÄúCreating SystemIntegration Simulation Environments:Common Design Principles, A Component-Based Reference FOM and a System-of-Systems Federation Development Process‚Äù,European SIW, June 2003.[4]	Eiserman, G., Furness, Z., Ludwig, J., Seidel, D., and Talbot, J.  ‚ÄúImplementation of C4ISR in the Joint Distributed Engineering Plant (JDEP)‚Äù, Fall SIW, 2003.[ 5] MIL-STD-6016B, Department of DefenseInterface Standard Tactical Data Link(TDL) 16 Message Standard, August 2002.8. Author BiographiesJACKSON LUDWIG is a Principal Integration & Test Engineer with the NATO Active Layered Theatre Missile Defence Programme Office, The Hague, The Netherlands.  He is currently working to develop a simulation test bed for interoperability testing.  Previously, he worked as a Senior Modeling and Simulation Engineer at the MITRE Corporation, McLean, Virginia.  There he worked on the Joint Defense Engineering Plant conducting integration, verification and validation testing for the JDEP IBuild test harness.  He holds a B.S. in Electrical Engineering from Rensselaer Polytechnic Institute and an M.S. in Systems Engineering from George Mason University.ROBERT BOLLING is a Lead Modeling and Simulation Engineer for the MITRE Corporation in McLean, Virginia.  For the past few years he has worked on the Joint Defense Engineering Plant test harness development project.  He is a retired Air Force Experimental Test Pilot and has a B.S. and M.E. in Electrical Engineering and M.S. in Aeronautical Science.scriptviewercontrolleranalyzergeneratorarchivearchiverscriptdistributorstimulatorstimulatorstimulatorstimulatorSUT