IntroductionIn order to provide OneSAF Objective System (OOS) users with access to additional analysis and review capabilities, the government formed an After Action Review Interface team consisting of engineers from Northrop Grumman Information Technology, Science Applications International Corporation, and engineers from Acusoft and Tapestry contributing through SAIC.  This team was tasked to develop an Analysis Data Model and associated interfaces to allow existing and future AAR systems and components access to OneSAF data for analysis and review.The AARI team undertook a system-engineering task to harvest and merge analysis data from a wide range of AAR systems and from the analysis user community into a common structure according to a pre-defined schema. The task included the creation of a data model to adequately support and automate the analysis process. The output of this system-engineering task evolved into a data description that we now call the Analysis Data Model, or ADM.The developers’ goal for the ADM is for it to evolve into an artifact with broad Army value. The plan for the ADM is to continue capturing the data needs of the analysis user community across virtual, live, and constructive Army training simulations, by harvesting the analysis products from the applicable simulations and the analysis user communities and merging this data into the ADM. The ADM is intended to become an authoritative data source supporting data requirements definition for analysis and review across virtual, constructive and live training exercises and interoperability among the Army’s AAR systems and other forms of analysis applications. Another goal is to support interfacing existing AAR systems to simulations (such as OneSAF) that produce data defined in the ADM that are analyzed by those AAR system(s).  This paper introduces the ADM and provides an overview of the ADM’s initial development and implementation. It describes the application of the ADM in the OneSAF system; examines its potential usage in live training on the Common Training Instrumentation Architecture (CTIA) program; and discusses benefits of the ADM to the Modeling and Simulation community for Army training. Finally, it provides the current ADM status, along with intended future enhancements, recommendations and conclusions.Overview of the ADMThe ADM, in abstract, is a collection of information describing data available to and desired by the analysis community. Hence, the ADM as developed for OneSAF is a collection of information describing data to be generated by the OneSAF Objective System models for use in analysis. This analysis can take on a multitude of forms, from after-action review to verification and validation—the ADM supports them all.  For all analysis users, the data are described, collected and accessed the same way.  The need for such a versatile data specification mechanism required a great deal of care in the analysis and development processes that led to the ADM.  In this section, we describe the ADM’s development path, implementation and structure.ADM Development and Current StatusThe core of the ADM development process entailed the harvesting of analysis data from various sources, and documented that data in accordance with a pre-defined schema.  The sources from which data were harvested included both a wide range of legacy AAR tools and Subject Matter Experts in the OneSAF user community.  In particular, data from the following sources were examined and incorporated:Advanced Concepts and Requirements (ACR) data requirementsArmy Materiel Systems Analysis Activity (AMSAA) data requirements for verification and validation of physical modelsARCHER AAR SystemAVCATT AARBBS AARCCTT DAR and CCTT AAR improvement study enhancementsJANUS AARPowerSTRIPES AAR SystemResearch, Development and Acquisition (RDA) data requirementsStandard Army After Action Review System (STAARS) data requirementsTraining, Exercise, and Military Operations (TEMO) analysis data requirementsVision XXI AAR SystemWARSIM AAR software requirements and C4I data (partial)There is, unsurprisingly, a high degree of overlap in the data needs of these sources.The development of the ADM took place in six major steps:Collection: Each of the developers of covered AAR tools were contacted and asked to provide information on the types of analysis products and data requirements that their particular tool requires from the legacy simulations that they currently analyze.  The user community SMEs from each of the OneSAF domains were also asked for their analysis data requirements. This information was collected and placed into data documents individual to each source.Unification: The information from these separate data models was concatenated into a unified data model.  Since the different legacy tools and user domain data had a high degree of overlap in their data requirements, the unified data model was processed to eliminate redundant information. This work was performed in the Dynamic Object Oriented Requirement System (DOORS) analysis tool. Definition: The data were evaluated and a schema for documenting the data in the ADM was defined. The ADM schema is defined in XML and describes the information retained for each entry in the ADM. The schema is described later in more detail.Integration: The AARI team then processed the data from the unified data model, converted it to the ADM schema format and recorded it in the analysis tool. Review: The ADM data underwent a peer review to ensure that they satisfied format and schema requirements, and that the information being collected satisfied the data requirements of the various users of the ADM.Iteration: A second wave of legacy tools and domain user data requirements were analyzed and added to the ADM, necessitating another round of of clarification and redundancy elimination.  This version of the ADM went through another peer review; after the defects uncovered by this review were resolved, it became the initial OneSAF ADM.  This version represents the ADM’s current state.Version 1.0 of the ADM was delivered to STRICOM on October 7, 2002. The ADM was further updated on December 17, 2002, adding AMSAA V&V data for the OneSAF physical models and WARSIM C4I message data applicable to OneSAF.  There are currently 736 data entries and 153 organic objects in the ADM; 25% of these data entries are for the AMSAA V&V data. ADM StructureIn this section we describe the overall ADM schema and the ADM’s logical data model, then we walk through the above-mentioned ADM components and give the definitions of their attributes.  We also give examples of the content of each component.ADM SchemaThe ADM schema defines the structure of the ADM.  Given the need to represent the ADM as an XML document, the ADM schema is naturally a W3C XML Schema compliant document.  From the W3C Schema web reference, “XML Schemas express shared vocabularies and allow machines to carry out rules made by people. They provide a means for defining the structure, content and semantics of XML documents.” [2].  Using this approach for the representation of the schema, OOS is able to easily generate and reason over both the ADM and the ADM Schema. The ADM Schema defines three top-level element types (shown in  REF _Ref441724115 \h Figure 1).  Of these three, two support the definition of data (orgData and synData) while the third supports the top-down analysis to generate the data. Figure  SEQ Figure \* ARABIC 1: ADM Schema Top LevelThe orgData element describes the data model for OOS organic data elements.  Organic data entries are data directly generated by OOS.  The synData element describes the data model for OOS synthesized data.  Synthesized data entries are data that is useful for analysis, usually based on legacy system analysis, and can be synthesized from either organic or other synthesized data.  In short, synthesized data is useful for analysis but is not produced by the runtime OOS software, but generated by analysis tools.  The placeholder element describes an entry that supports the top-down analysis effort by deferring the definition of a data type until the design of the generating code.  For example, a system engineer may define that a fighting position description may need to be captured for analysis, but cannot predict what specific elements will describe the environmental object.  The placeholder allows the system engineer to define as much as possible, but to tag the type as undetermined.  Later, software developers complete the description of the placeholder entry to a proper orgData or synData entry in the ADM.ADM Logical Data Model The logical data model of the ADM describes the format in which a ADM data element is in turn described.  Each data element in the ADM is either a synthesized or an organic data element.  Synthesized elements are descriptions of data needs that cannot be directly collected from the simulation; the more common organic data elements either can be directly collected or are aggregations of elements which can.Each organic data element contains a name, description, and type information.  Where appropriate, it also specifies the source from which the data can be collected, the legacy system the data element is derived from, and the application domain served by the data element.Organic data elements may contain a list of references to other organic data elements, to allow for aggregation; finally, they include a persistence flag to specify whether the information they describe is stateful or stateless.Synthesized data elements contain much the same information, except, of course, for source.  They may also contain information on how to derive them from organic data elements.Aggregation is the primary means through which the ADM supports analysis and review.  To give only two examples, data elements are grouped with identifiers to form a coherent picture of a unit or entity’s state at a given time, and to describe every facet of a damage event.As a part of this aggregation, the ADM supports cause and effect analysis.  Simulation events and interactions described in the ADM are grouped with identifiers of their causative simulation events and interactions.  For example, the above-mentioned damage event would contain the identifier of the detonation event that caused the entity to record damage, and the detonation event would contain the identifier of a firing event, and the firing event would in turn contain the identifier of a firing mission, and so on.  This implies a rather powerful data analysis mechanism.Cause and Effect AnalysisModel developers determine the data provided by most simulation systems.  As a result, the data are generally limited to the public parameters that model developers find interesting—i.e., the parameters that models pass to other models as they interact.  Hence, excellent state data is generally available for review, since such data is required for the simulation proper; but model developers need not be concerned with cause and effect in their models, beyond making sure that the proper effects follow the proper causes.  Let us say that we want to know what firing mission resulted in an entity’s kill status changing.  A model does not need to know which firing mission eventually resulted in a detonation event to react to that event; it simply needs to know the location, radius and power of the detonation.Hence, review data does not support cause and effect, not through negligence on the part of the model developers but simply through a natural lack of familiarity with, or focus on, the data required for review.  There is no way to reliably extrapolate this data from state logs: returning to our example, it is quite plausible for detonations from two separate firing missions to occur near the same point near the same time, and if our entity took damage in that area and around that time, quite difficult to determine which detonation caused the damage, let alone which firing mission eventually resulted in the detonation.Hence, the simulation itself must supply cause-and-effect data.  However, requiring model developers to research the data required for review is emphatically not the answer: model developers have more than enough to do without a requirement to be AAR analysts on top of their existing jobs.  The problem needs to be addressed at the front end of design, by telling the model developers what data they need to make available.This action is, again, not much of a problem for state data, because the models require most of the interesting data to begin with, making the problem vacuous.  In the simultaneously subtler and more critical case of cause-and-effect data, however, the problem shows itself clearly.The ADM clears up this issue neatly by explicitly linking cause and effect.  In any event recorded in the ADM repository, there is an explicit reference to the causative event or events.  This reference gives enormous power to analysis of events and actions recorded in an ADM repository: the threads of causality that “tell the story” of an exercise can be tracked through the simulation’s entire run, not by guesswork analysis but by hard links provided by the simulation itself.ADM ImplementationThe data captured during the ADM’s development process are currently stored in the DOORS analysis tool, in five modules:ADM Functionalities: Information about the reports that drive the data requirementsADM Synthesized: Information for synthesized dataADM Organic: Information about organic objectsADM Entries: Information about organic attributesADM Categories: Keywords for categorizing other module entriesThe relationship between the five DOORS modules is depicted in  REF _Ref441724083 \h Figure 2. EMBED PowerPoint.Slide.8  Figure  SEQ Figure \* ARABIC 2: ADM Schema in DOORSFunctionalities ContentThe Functionalities component contains the analysis products that drive the data requirements for the ADM. It describes the uses of analysis products and the data elements required to support them.  It is organized by analysis user domain (e.g. TEMO, V&V) and by data source (e.g., Archer, CCTT).The products in each user domain are further organized to reflect that domain’s analysis needs.  For example, the TEMO domain analysis products are organized by Battlefield Operating System (BOS)—Intelligence, Fire Support, Maneuver, et cetera. This organization allows for filtering on analysis products by domain, by source, by lower-level classifications such as BOS, and by specific product. Each analysis product is mapped to ADM organic data and ADM synthesized data entries containing the data needed for that product to be analyzed. This mapping provides requirements traceability for the data in the ADM.An example of Functionalities content is shown in Figure 3.Organic Data ContentAn organic data element describes a data record directly generated by the OOS and made available for data collection.  Each organic data element has a unique name and description, and the following attributes:Scope: Identifies the data element as "Published" or "Internal".  Persistence: Stateful or stateless.  A stateful element describes data that is tracked over time.  A stateless element describes data that is tied to an instantaneous event.Source: The OOS software component that is the source for this type of information.Reference: The ADM functionality that contributed this data element.Domain: The user domains that use this data.Collectable: Whether or not the element is individually collectable.  An element that is not individually collectable is typically a placeholder for an ADM complex data type.An example of Organic Data content is shown in Figure 4, along with an example of Data Entries content.Synthesized Data ContentA synthesized data element describes desired information that is not directly obtainable from the simulation models.  Each synthesized data element has a unique name and description, and the following attributes:Derivation: The procedure used to derive the data element from organic data elements, if such a procedure has been devised.Domain: The user domains that use this data.Reference: The ADM functionality that contributed this data element.Category: The general category into which this element falls. An example of Synthesized Data content is shown in Figure 5.Data Entries ContentA data entry is an individual variable related to the objects simulated by the OOS.  Each data entry has a unique name and description, along with the following attributes.Data Type: The type of the data entry, primitive or otherwise.  The data type may be a primitive such as integer or float, a flat OOS type, or a reference to an ADM complex data type.Minimum/Maximum: The minimum and maximum possible value for numeric entries.Units: The unit of measure in which the data entry is represented.  The ADM units of measurement are based on the International System of Units (SI) from the National Institute of Standards and Technology [3]. Terminal: Indicates whether or not the data entry is terminal. A terminal data entry is either of a primitive data type (e.g., integer or float) or of a flat OOS type (e.g., Location).  A non-terminal data entry generally refers to an ADM complex data type.MinOccurs/MaxOccurs: The minimum and maximum number of occurrences of this data element in the context of a data object.An example of Data Entries content is shown in Figure 4, along with an example of Organic Data content.OneSAF Usage of the ADMThe OOS holds two views of the ADM: top-down and bottom-up.  Each view supports a different use of the data contained in the ADM.In the top-down view, system engineers are performing legacy and Operational Requirements Document (ORD) analysis to identify expected data elements that support analysis.  The top-down ADM is referred to as the Reference ADM.  The Reference ADM supports OOS’ requirements based engineering approach by enumerating analysis requirement expectations of what data will be generated by OOS.  Model and infrastructure developers use this information to derive requirements and ensure that all necessary analysis data is captured.In the bottom-up view, the ADM is generated from the OOS software by examining the software baseline to extract the data elements the software is actually generating.  The bottom-up ADM is referred to as the Instance ADM.  The need for the Instance ADM was indicated by two characteristics of OOS.First, OOS will be released in stages, much like ModSAF; therefore, not all data will be described in the first set of releases.  The Reference ADM is the analysis of all legacy systems and forecasted OOS needs. Therefore, the Instance ADM is needed to represent the Organic Data that has been realized in the current stage of OOS development.Second, since OOS is a composable system, one would never expect all data identified in the reference ADM to be relevant in a single given composition of OOS.  Therefore, the Instance ADM is needed to identify only the data generated by this specific instance of OOS.  This reasoning was the origin of the term “Instance ADM”.The data described by the ADM can be divided into published and private data.  Published data represent the Runtime Data Model (RDM) used to describe publicly available data in OOS; this is analogous to the FOM in the HLA jargon.   Private data are data used or generated by the software but not supported by the publication or distribution system.Given the data driven nature of OOS and the fact that the ADM describes pertinent OOS data, there are many Instance ADM clients.  Chief among them is the OOS software itself: any data that can be collected by the system is described by an Organic Data entry, and the data collection system is driven by the definition of these Organic Data elements in the ADM.  More specifically, the clients of the ADM presently include the data collector, the data collection specification system, the run-time database, the GUI generation system, the federation management (HLA interoperability) system and the scenario generation system.Potential for Usage in Live TrainingAlthough the ADM analysis and development focus on the constructive and virtual simulation domain, it has potential application to the live domain as well. With respect to the purpose and scope of the ADM components, the users of the data do not necessarily know—and probably should not know—the source of the data they are analyzing. In the abstract, all three simulation domains can be treated as data sources for analysts. Although focused on different aspects of training, all types of training simulation are conducted for the purpose of collecting data that can be analyzed to assess the trainee’s readiness state.First consider individual and crew training.  Both live simulation and virtual simulators provide a means of conducting training against tank crew tasks. Each method has its own inherent set of advantages and disadvantages (e.g., the realism of live training versus the cost effectiveness of virtual simulators). However, assessing individual or crew capabilities against a specific task requires that the same data be collected. Similarly, large force-on-force exercises at maneuver combat training centers (MCTC) can be used to train some of the same tasks as a constructive simulation. Again, the data required to assess the commander and staff against doctrine-based tasks and standards are the same.Finally, during most large MCTC exercises, a brigade sized training unit will consist of a combination of two battalions in the box and one represented in a constructive simulation. Assessment of the data collected against the three battalions should be independent of the representation of the unit: the analyst should focus on the collected data, and not the means by which it was collected.Some aspects of the ADM do not apply to the live domain—event-based constructive simulations lend themselves well to a priori cause and effect threading, while the nature of live training is limited to deductive threading after the events have occurred.  However, the capabilities designed into the ADM do provide a consistent methodology and associated data set to assess task-based training irrespective of the type of simulation used to conduct the training. Additionally, the ADM offers a means of visualizing a live exercise by using a constructive simulation to play back data collected during the live exercise.Benefits to the M&S CommunityThere are many potential benefits from adoption of the ADM.  We summarize a few of them here.Data Analysis: The ADM contains the analysis data requirements for many of the existing training and simulation systems. Therefore, future combat systems development, including live training systems, can leverage the data already defined in the ADM and add more data requirements (such as the data required for live training) to support the Objective Force’s system requirements. The ADM also supports analysis to determine whether the existing AAR systems can completely or partially satisfy the Objective Force’s system AAR requirements.Data Content: The ADM identifies the specific data collection requirements for analysis and review.Interest Management: The ADM identifies data collection requirements for the specific analyses being performed, allowing data collection to be based on a system’s composition or on specific analysis requirements.Interaction Identification: The ADM helps support “annotated” high-resolution simulation playback through the recording of interactions of interest. Cause and effect analysis: The ADM includes causal data—elements that link events with other events and with persistent objects, forming threads that trace interactions throughout the course of the simulation.Interoperability: Because it represents a comprehensive analysis of the data requirements of many of the Army’s existing AAR systems, the ADM has the potential to provide interoperability between these systems.Standardization: The ADM, because it provides a standardized list of analysis data, allows independent development of new analysis and review systems.HLA Compliance: The ADM supports HLA interoperability needs, such as FOM requirements, for analysis federates.  Data Collection Efficiency: The knowledge represented in the ADM allows data collection needs to be better limited. For example, an ADM-enabled AAR data collection system could conceivably ignore far more HLA/RTI packets than current data collection systems, leading to lower network, processor and database loads.Conclusions and RecommendationsAlthough the ADM effort was not a research project, a major finding was that a significant amount of the data analyzed is common among the existing AAR systems. As the data merging process matured, the addition of new data decreased significantly. As the clearest example, the merging of BBS AAR did not require the addition of new data to the ADM at all. Another finding was that the existing AAR systems data did not contain the all of the data structures necessary to support cause and effect analysis.We conclude that with interface modifications, the existing AAR systems analyzed will be able to interoperate with the OneSAF system. Some data conversions and modifications to enumerated lookup tables will be necessary because there are differences in data representation among these systems. These differences are documented in the individual system data models. The connection modes for connecting external AAR systems to the OneSAF system to access the data defined in the ADM are described in the OneSAF Product Line Architecture Specification [1].We recommend that the ADM continue to evolve to include analysis data from other sources as well as data from live training programs such as Common Training Instrumentation Architecture and Future Combat Systems.  We further recommend that the ADM evolve to support interoperability among the Army’s AAR systems, and become an authoritative data source to support both the development of future analysis and review systems and the reuse of existing ones.Future WorkThe evolution of the ADM will be a challenge. Two barriers stand in the way of the completion of the ADM’s evolution: a lack of funding to finish work and the lack of a community focused on data engineering for analysis and review systems.  These barriers can be overcome by establishing and funding a focus group of analysis and review specialists to expand the ADM into a comprehensive definition of data needs for analysis and review systems.  This group would produce a single, authoritative, canonical description of the data needs of the analysis and review community.  ADM Enhancements PlanThe pan for maturing the ADM for broad Army value is divided into five areas:  (1) ADM schema standardization and logical model refinements. (2) Data population to expand the ADM to include Live Training, JSIMS, and C4I messages. (3) Standardization of ADM compliant datasets produced by the simulation (4) Standardizing access to ADM-compliant datasets and, (5) Conversion of the DOORS ADM format to XML. The five areas are described in more detain below.  The schedules for these enhancements are unknown at this time. It is estimated that 2 fulltime engineers over a one year prior will be required to complete the these tasks and produce an ADM for distribution to the M&S community. ADM Schema Standardization and Logical model Refinements.This task will analyze and refine the ADM schema and logical data model. The current schema is based on OneSAF requirements. Live Training programs may require modifications to the schema. The ADM synthesized data derivation methods needs to be completed and the logical data model refined to better support effect and cause analysis.ADM Data PopulationThis task will make the ADM more comprehensive by adding analysis data (if not already there) for Live Training simulations and other simulations such as JSIMS, and JCATS to include observer controller inputs, and message data from the Army Battle Command Systems (ABCS), FBCB2, and Joint Service-GCCS.Standardization of ADM Compliant DatasetsThis task will standardize the physical structure for the data produced by the simulation. Existing OneSAF components and Data Interchange Formats (DIF) will be examined for this effort. Standardizing Access to ADM Compliant DatasetsThis task will specify how existing AAR systems, and analysis applications in the future, can access ADM-compliant datasets to retrieve data for analysis and review—this description currently exists only for OneSAF specific use. This task will examine interfaces such as ODBC, JDBC and XML schemas.Conversion of the ADM Doors Format to XMLThis task will convert the ADM format currently implemented in DOORS to XML format. The XML format will better support schema definition, ADM distributions, and data collection and analysis software development.More support for cause and effect analysisThe ADM, as it is, covers a great many simulation events and covers them well.  However, its cause-and-effect analysis could benefit from improvement in two areas.  First, it simply needs to cover more simulation events; a comprehensive study deriving events and interactions by examining simulation behavior and entity models would be a first step toward dealing with this problem.Second, the ADM’s means of linking interactions needs to be further elaborated.  The ADM’s cause-and-effect system is, we believe, fundamentally sufficient: a chain of uniquely specified interactions, missions and other events is robust enough to handle most reasonable cases in terms of cause-and-effect.  However, a comprehensive analysis of which interactions, missions and events can link to which other interactions, missions and events is necessary to close the circle from the model developers’ point of view.As the ADM stands at present, it is clear to the model developers that cause must be linked to effect in general—but it is unclear what causes and what effects must be linked.  We have already decreased the amount of guesswork model developers must go through, but the goal of the ADM is not to decrease the guesswork by degrees or even factors; it is to decrease it exponentially, to approach eliminating this guesswork entirely.  For the ADM to be a truly complete work, this analysis must take place.References[1]	OneSAF Product Line Architecture Specification.  Information at URL:  HYPERLINK "http://www.onesaf.org" http://www.onesaf.org[2]	W3 Consortium.  W3C XML Schema.  URL:  HYPERLINK "http://www.w3.org/XML/Schema" http://www.w3.org/XML/Schema [3]	United States Department of Commerce/National Institute of Standards and Technology.  NIST Reference on Constants, Units and Uncertainty.  URL:  HYPERLINK "http://physics.nist.gov/cuu/index.html" http://physics.nist.gov/cuu/index.htmlAuthor BiographiesDr. WESLEY MILKS has worked for Lockheed Martin Information System as a system engineer/architect since 1999. Dr. Milks is currently assigned as the Chief Architect for the Common Training Instrumentation Architecture program. Prior to joining LMIS, Dr. Milks worked for the Army for over 10 years, with the majority of the time spent at the US Army's Simulation, Training, and Instrumentation Command. Dr. Milks has extensive experience across the three simulation domains (live, virtual, and constructive) in addition to expertise using models and simulations in support of system acquisition, design, and evaluation. While working for STRICOM, Dr. Milks was selected as the Standards Category Coordinator for System Design and Architecture by the Army Modeling and Simulation Office. Dr. Milks holds a Bachelor of Science degree in Mechanical Engineering from Ohio Northern University, a Master of Science in Industrial Engineering from Texas A&M, and a Doctor of Philosophy in Industrial Engineering from the University of Central Florida.MATTHEW B. GERBER is a Research Scientist at the University of Central Florida’s Institute for Simulation and Training in Orlando.  Mr. Gerber earned his B.S. and M.S. degrees in Computer Science from UCF, and is currently pursuing a Ph.D. in Computer Science there.  His research interests include simulation interfacing and interoperability, formalization of architectures and I/O systems, computer security and forensics, and computer-generated forces.WAYNE A. LINDO is a Senior System and Software Engineer for Northrop Grumman Information Technology. Since 1995, Mr. Lindo has worked on the development of the OneSAF, WARSIM and JSIMS simulation programs in the areas of Life Cycle Applications and has extensive experience in Data Collection and After Action Review systems.  Between 1986 and 1995 Mr. Lindo worked on the development of the Northrop Grumman flight simulation system and has participated in SISO since 1989. Mr. Lindo holds a Bachelor of Science Degree in Electrical Technology from The City College of New York, Master of Science Degree in Computer Science, from Pratt Institute, Brooklyn, New York, and a Graduate Certificate in Telecommunications, from Azusa Pacific University, Azusa California.BOB BURCH is a Chief Scientist for SAIC Orlando.  Mr. Burch is presently the Software Architect and Architectural Modeler for the OneSAF Objective System Architecture and Integration Task Order.  Prior to that Mr. Burch was the Chief Scientist for Close Combat Tactical Trainer Semi-Automated Forces (CCTT SAF).  Mr. Burch has been involved in modeling and simulation since 1982 with an emphasis on crew cabin simulation.  Mr. Burch holds a Bachelor of Science degree from the University of Central Florida.Organic Entry NameCollectableData EntriesData Entry Description Entry TypeMinOccurMaxOccurExerciseYesOverlaysSet of overlays used in the exerciseOverlay_Entry1999999Termination_TimeDate and time the Sim/Exercise/Site/Run endsTime_TypeStart_TimeDate and time the Simulation startedTime_TypeMax_WorkstationsThe maximum number of workstationsIntegerCheckpointSet of checkpoint records produced in the exerciseCheckpointRestart1999999Ovelay_EntryYesSymbolsSet of symbols in the overlaySymbol_Entry1999999Exercise_IdentifierUnique system identifier for the exercise or scenario being run.Exercise_Identifier_TypeNameThe name of the side, force, entity, item, engineering object or workstation.StringIdentifierUnique system identifierIdentifier_TypeSymbol_EntryNoSymbol_Code_IdCode that identifies the symbol to be drawn as represented in 2525B.StringSymbol_ModifiersSet of text modifiers to be drawn on the overlayString120Anchor_PointsSet of anchor points that describe a symbolAnchor_Point_Entry210Exercise_Identifier_typeNoRun_NumberSequential numbering of the run for the same Simulation/Exercise.StringSimulation_IdThe unique identification number for the simulationIdentifier_TypeFigure 4: Organic Data/Data Entries ExampleIDADM synthesized elementsDerivationReferenceDomainCategorySYN4864 Loss_Exchange_RatioThe ratio of enemy kills to friendly killsTotal number of enemy kills divided by the total number of friendly killsPersonnel Casualties: ArcherPwrSTRIPESSTAARSWARSIMBBSCCTTSynthesizedFigure 5: Synthesized Data ExampleIDADM Analysis functionalitiesSourceadm organicadm synthesizedUSE342.1.1.5 Personnel CasualtiesUsed to determine the relative effectiveness of hostile and friendly fires.a. Number of Friendly personnel killedb. Number of Enemy personnel killedc. Number of Friendly personnel woundedb. Number of Enemy personnel wounded.Filters: Per side, force, Echelon, unit, crew and entityADM Synthesis dataLoss Exchange Ratio - The ratio of enemy kills to friendly killsForce Exchange Ratio - The ratio of the percentage of enemy kills against the percentage of friendly kills Personnel casualty summary - the total personnel lost per platoon-level (and higher) unitArcherPwrSTRIPESSTAARSWARSIMBBSCCTTORG57: DamageORG29: SideORG30: ForceORG8: EntityORG10: PersonnelTypeORG6: UnitLoss_Exchange_RatioForce_Exchange_RatioPersonnel Casualty SummaryFigure 3: Analysis Functionalities ExampleAn Analysis Data Model for Modeling and SimulationWayne A. LindoCuspy Engineering Solutions4538 Saddleworth CircleOrlando, FL 32826(407) 275-6989 HYPERLINK "mailto:wlindo@iag.net" wlindo@iag.netMatthew B. GerberInstitute for Simulation and TrainingUniversity of Central Florida3280 Progress DriveOrlando, Florida 32826407-882-1341 HYPERLINK "mailto:mgerber@ist.ucf.edu" mgerber@ist.ucf.eduBob BurchScience Applications International Corporation12901 Science DriveOrlando, Florida 32826321-235-7678 HYPERLINK "mailto:burchr@saic.com" burchr@saic.comWesley A. Milks, Ph.D.Lockheed Martin Information Systems12506 Lake Underhill Rd., MP-847Orlando, FL 32825321-235-7506 HYPERLINK "mailto:Wesley.milks@lmco.com" Wesley.milks@lmco.comABSTRACT: The primary reason that a user chooses to conduct a training exercise or analysis experiment using a simulation is to generate data that is either too expensive or too dangerous to produce using real people, real equipment, and real bullets. While it is critical to develop models that have sufficient credibility to allow users to have confidence in the results, the real payoff for modern training systems is the tight coupling between the data generated by the simulation models and a rich set of analysis tools that can transform the collected data into meaningful information. This information can then be used to support an acquisition decision, assist in the evolution of doctrine, or provide objective evidence associated with a unit’s training status. The development of most simulation training systems does not give enough attention to data analysis and review: while it is generally considered, it should in fact be considered a critical driving factor. Training system developers do not necessarily ignore analysis requirements, but they usually address it at the end stages of the development cycle often after the majority of the project’s funding has been committed to model development. This imbalance in priorities results in a series of problems, including limited data availability, stove-piped analysis systems, and a lack of support for cause-and-effect analysis of events. The OneSAF program has taken a step towards addressing data analysis during the early stages of the development process by developing the Analysis Data Model, or ADM. The ADM represents the harvesting of analysis data from a wide variety of existing AAR systems and the analysis community into a common data model to support analysis and review. The ADM is also being considered for use within the Common Training Instrumentation Architecture (CTIA) program. This paper will describe the development and implementation of the ADM as well as examine its possible applications and evolution.Keywords:Analysis Data Model (ADM), One Semi-Automated Forces (OneSAF), OneSAF Objective System (OOS).