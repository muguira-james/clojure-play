Remote Agent Technology: An Approach to Monitoring and Testing Distributed Simulation SystemsConway A.Bolt, IIIDavid L. Fisher, Ph.D.Northrop Grumman Information TechnologyDefense Enterprise Solutions12000 Research Parkway, Suite 132Orlando, FL 32826-3211boltco@northropgrumman.comKeywords:Simulation, Agent, Testing, Distributed Military Training, Network, Protocol Data UnitABSTRACT: Distributed simulations over a WAN are inherently difficult to test. While there exists a host of commercially available tools to monitor network health such as connectivity and latency, there are few tool that address the issues unique to distributed simulation. Data integrity issues, such as testing for valid PDUs and HLA message packets, testing filtering, and validating protocol transformations at remote simulation sites, need to be performed. Fair Fight testing also needs to be addressed to ensure that the synthetic environment existing at each of the remote simulation sites does not vary significantly enough to affect the outcome of an event.This paper addresses these issues by discussing how a tool set called the Distributed Test Management Tool (DTMT) is being developed to perform monitoring and testing at remote simulation centers. DTMT utilizes a set of software agents located at each remote site to perform specific tests, mimic simulations, execute data reduction and transmit data or analysis results back to a control station in real-time or in a post-execution logging mode. The goal of this system is to greatly reduce the effort that ensures the success of the distributed training event.1. IntroductionDistributed simulations over a WAN are inherently difficult to test. The distance separating sites exacerbates the problem. The fact that a remote site cannot be accessed directly by the test team adds an additional layer of complexity. It is not unusual for the test of a distributed simulation network to require the coordination of individuals at remote sites, a process which is time consuming and expensive. Commercially available tools that are designed to perform network monitoring such as connectivity and latency analysis are readily available; however, these tools generally look at the network as a stream of datagrams.  While this level of analysis is sufficient for some tests, it does not give any insight into the content of the data. Such insight is crucial to the analysis of the simulation as a system.2. Statement of ProblemCommercially available network analysis tools perform a variety of functions including, but not limited to, Bandwidth Utilization. Bandwidth Utilization shows a network or switches‚Äô current bandwidth or throughput and is useful in determining whether a given network or segment is running at or near capacity. This can be combined with network latency analysis so that , as network traffic increases, a network engineer can identify and resolve issues using these two metrics. This analysis helps determine the network hot spots and likely causes of latency issues due to insufficient bandwidth. A crucial question when troubleshooting a networked system is whether the data is being transmitted and received correctly. As previously discussed, commercially available tools can determine whether datagrams are being correctly received; however, we need to look at the DIS PDUs or HLA Messages within the datagrams to determine what effect the data is having on the simulation system. An error as mundane as an incorrect exercise id has the ability to prevent a simulation from running and a malformed PDU has the potential to cause a simulation to crash. Another issue to troubleshoot is whether data addressing is working properly. By data addressing we mean whether all the data intended for a simulation host arrives at the host and whether data not intended for a simulation host is filtered from the simulation host.  The former is critical to the function of the simulation, however the latter can be equally important. If all data is broadcast to all entities and the entity count is sufficiently high as in the case of a Joint STARS entity sensing a ground battle, a WAN may become saturated. Further issues arise when a simulation event contains simulations running various protocols simultaneously. Simulation protocol translators or gateways must be utilized to translate between the simulation event standard protocol and the protocol of simulators not natively operating in that protocol. Thus the requirement for testing the transformation from one protocol to another is imposed on various remote sites.One of the most crucial and complex issues for any distributed simulation event is the verification of a ‚ÄúFair Fight‚Äù. This verification goes beyond the synchronization of the representations of the synthetic natural environments. In a synchronized environment, all of the entities are located in the same space and time in each participating simulation. Differences factors such as simulation fidelity and terrain models may have an adverse effect on event outcome. An entity getting shot down by a weapon fired through a mountain that does not exist to another entity at a remote site or the relative positions of an aircraft and a missile differing sufficiently in two simulations as to cause a hit in one and a miss in the other are but two examples of   ‚ÄúFair Fight‚Äù issues and cannot be resolved by simple network analysis.3. Using Agents to Assist in Testing Distributed SimulationsThe DTMT solution to the problem of distributed simulation testing is to utilize remote agent technology in conjunction with a central controller and display module for viewing the test results. An agent is a semi-autonomous, collaborative, software entity. They are designed to allow software systems to delegate tasks and undertake roles in an intelligent manner. In the context of DTMT, agents are the software entities that reside on remote simulation systems and have the ability to determine and execute needed actions, seek and incorporate relevant information and transmit this information for human display or analysis. These entities essentially serve as proxies for the individuals who would otherwise need to be at the remote sites participating in the testing process. Agents have been used for a relatively long time managing vast amounts of information on wide area networks such as the internet where they are sometimes referred to as web-crawlers and spiders. The mission of distributed simulation is to provide realistic mission training to combat forces utilizing advanced mission trainers networked across a LAN or WAN. At each site, we envision the requirement for at least one computer to act as a test computer. The test computer shares the same network segment as the simulation computers at the site and has the responsibility for executing the agent software and storing of any log data generated by the agents. The lead site will also need to allocate a computer to run the DTMT control software that directs the execution of all tests and displays the results for analysis.  In this architecture, the gateway, depicted in figure 1, represents a router, a switch and any protocol transformation device such as a DIS to HLA gateway. EMBED PowerPoint.Show.8  Figure  SEQ Figure \* ARABIC 1: DTMT ArchitectureThe DTMT is broken down into two major components: The test management agents and the controller. The role of the agents is to perform remote testing, monitor simulation traffic and mimic the function of a MTC. The role of the controller is to load the agents on the appropriate test computers, initialize the agents with specific data, collect test data and display the results. The controller uses FTP to transfer agents to the target test computer. This is done at the start of each test as a mean of configuration control. By transferring the agent to the remote test site at the start of each test, the test team can ensure that the correct version of the agent software is running at the remote site. When the agent has been transferred to the remote site, the controller starts the agent‚Äôs execution. Once running, the agent first ‚Äúlooks‚Äù on the local computer for local system and network information, then contacts the controller on a listening port. The controller responds with a unique communication port that is used for all subsequent network communication. At this point the agent starts performing its programmed test procedure or monitoring function. The function of the agent can be completely autonomous, for example monitoring simulation traffic and logging results either locally or over the network making decisions based upon the detected simulation behavior, or setup simply as a proxy under the direct control of the control station.The advantage of using an agent based system to perform remote testing is that agents can be developed to perform specific tests or monitoring functions, correlate data, communicate with other agents and report the results back to the controller or human operator. Once a framework is developed that provides for basic functioning of the agent: initialization, communication with the controller and exception handling, the focus of the agent development is on the logic specific to the tests that need to be conducted.  EMBED PowerPoint.Show.8  Figure  SEQ Figure \* ARABIC 2: Agent ArchitectureOur current implementation does not provide for inter-agent communication. At present, if a test needs to be coordinated between two or more agents, that coordination is currently managed through the controller. Our design; however, does not preclude this implementation. One implementation would be to initialize with the controller port number in addition to the IP address and port number of the agent with which it is attempting to establish direct communication. This implementation, while not completely autonomous from the agent‚Äôs perspective, is favored over direct discovery of other agents because direct discovery techniques generally rely on multicasting or other unreliable delivery protocols, which tend to undermine the test effort. An implementation involving direct inter-agent communication could have benefits in a variety of situations: allowing the agent to spawn additional agents as the need arises, offloading of the coordination responsibility to the remote agents reducing the processing load on the controller and potentially reducing network bandwidth requirements. 3.1 Agent ImplementationThis section discusses the agents that have been developed and utilized for DTMT to date.Distributed ping and Latency MonitoringThese two tests are closely related and used primarily to test the health of the test suite and the simulation network. Distributed Ping verifies that all test computers are ‚Äúalive‚Äù and all communication paths are available. In this test, each agent on each test computer pings its counterparts on other test computers. Each agent sends back to the controller a summary of statistics that the controller displays for the test team. Computers that are down or paths that are unreachable are readily identified. Latency Monitoring can be broken down into two categories: Network latency and gateway latency. Network latency can easily be measured using distributed ping. Distributed ping measures the transit time between network layers in the sending and receiving computers; it does not measure the propagation time through application software such as the gateway nodes. Gateway latency is determined by sending data packets (PDUs or HLA messages depending on the implementation) between the simulation sites and measuring the transit time through the application software. It includes the network latency. Therefore gateway latency represents a more accurate measure of the length of time a packet of simulation data will take to transit the system.PDU AnalysisAs the name suggests, this test is geared specifically towards DIS simulation analysis. The PDU Analysis agent monitors all DIS traffic and logs statistics related to the PDU traffic observed. This tool is useful in verifying that all expected PDU traffic is coming to the MTC under test and the sources of data. Unlike a logger which simply logs the PDU traffic to a storage device, the PDU Analysis agent collects the PDU data, counts the number by type, determines the rate, and effectively reduces the data such that the results can be sent to the controller for real time analysis. Because the data is reduced to summary form, the transmittal of the data in real time does not consume an appreciable percentage of the simulation bandwidth. In addition to generating statistical data the PDU analysis agent can be set-up to associate PDUs or look for a specific PDU. For example, upon detecting a detonate PDU, the analyzer can attempt to associate entity state PDUs with damage that correspond to the detonation event. The advantage here is that the test team can remotely verify that the models are reacting appropriately to the event. In this configuration, the agent(s) can either be set up to listen to ‚Äúlive‚Äù simulation data, or a logger file. In the case of the latter, one or more agents can be set up as log file player agents, while other agents play the role of analyzers. The benefit to using log files is that it should be a fairly straightforward process to set up an expected results file for each of the PDU Analysis agents. Thus the actual data received at the site can be compared with the data in the expected results file and discrepancies uncovered. EMBED PowerPoint.Show.8  Figure  SEQ Figure \* ARABIC 3:  PDU Analysis Agent  ReflectorThe reflector is a simple yet powerful agent that allows us to inject data into the simulation at a remote location as if that data were originating from the simulation site while in fact the data is being generated locally to the test team. The advantage of this can be illustrated by example. If we wanted to perform fair fight testing between two remote sites, we would need as a bare minimum, each site running a modeling simulator controlling computer generated forces. We would then need to coordinate the results obtained at each remote site for analysis. As the number of sites grows, so do the number of model generators, the number of licenses required and the complexity associated with the coordination of the results from the remote sites. With the use of reflector agents we can achieve the same results and greatly reduce the complexity of the computational system at the remote sites. Essentially the reflector agent is nothing more than a small application that receives data on a back-channel port and retransmits the same data on the simulation port. This function allows the model generators to be local to the test team and under their direct supervision, and the output directed to the sites under test at remote locations. The data can be collected and analyzed locally.   EMBED PowerPoint.Show.8  Figure  SEQ Figure \* ARABIC 4: Reflector Agent4. DTMT Structure and FunctionalityAt the present time, the basic DTMT command monitor is written in Java.  There is a basic and versatile framework for the DTMT architecture.  This allows for easy implementation of new test modules and module-like structures.  This architecture is composed of subdivided GUI frames with components sections for alarms, controls, agent selection, system initialization and the display of data.  Other non-GUI framework includes the automated management, registration and assigned ports through TCP sockets of test agents.We have developed an abstract module class that all test modules extend.  This permits the implementation of core, universal and necessary functionality without knowledge of the specific type of test module(s) that are implemented.   When DTMT is initially started, part of this initialization process is to look in a specifically defined directory and instantiate all classes in that directory that extend module and store them for use.  A drop down list is automatically generated with the instantiated test modules name for selection.  Because of this architecture, the addition of a new test module is simple; all that is necessary to implement a new test module (which must extend the abstract module class) is to include the new test modules compiled code in the specified directory.The registration and initialization process of agents by DTMT is also completely automated.  A test agent registers on a listening port of the DTMT.  The agent is first validated to determine if it is the correct agent for that test module.  Once the agent has been validated and registered, it is assigned its own TCP communications port to communicate with the DTMT.  Upon establishment of communication on the new port, initialization information is sent to the agent.  This initialization information is derived from initialization files at the DTMT.  Once initialized, the agents then wait for the start command to be issued from the DTMT.  The agent also responds to pause, re-start (re-initializes), continue and stop commands.  The stop command will stop execution of the agent. EMBED PowerPoint.Show.8  Figure 5. DTMT GUIOnce the start command is issued, the remote test agents will start their test functionality.  The remote test agent will generally perform analysis on raw data it receives and then periodically transmits the results from the data analysis to the DTMT.  The DTMT then parses the data sent by the test agent.  At this point, a number of possible functions may be performed depending on the specifics of that test module.  The data may receive further analysis, may be stored, may be compared to thresholds for alarms and/or may be displayed. There is a potential for considerable data as there will in general be multiple test running on multiple sites.  Normally only the data from one test agent is displayed at a time.  The selection of a particular site and test for that site is accomplished through a simple drop down list of test modules generated as described earlier and a JTree for selection of the specific site. The JTree is also dynamically generated as agents register with the DTMT.6. Future DevelopmentFuture development of this system will undoubtedly include the development of more sophisticated tests and the test agents to support them. Additionally, we would like to place effort put into developing and integrating an agent communication language (ACL). With such a protocol, DTMT controllers and agents would be able to share data and services with non-DTMT agents. The result would be the enhancement of the functionality and versatility of the system when agents and their respective controllers were not limited to communicating solely with each other and can take advantage of functionality provided by 3rd party agents. Author BiographiesCONWAY A. BOLT, III is a Software Technical Lead for Northrop Grumman Information Technology, Orlando FL and has been developing distributed simulation systems for nearly a decade. He holds a Master‚Äôs degree in Computer Science from The Johns Hopkins University.DR.  DAVID L. FISHER is a software developer for Northrop Grumman Information Technology, Orlando FL.  Since receiving his doctorate in Physics from the University of Texas at Austin in 1995, he has worked in the areas of theoretical plasma physics, the detection of explosive materials, millimeter wave sensor technologies and applications, and in the development of an Internet business.  He is presently developing several software tools.  David has 17 professional publications and 2 patents (1 pending).