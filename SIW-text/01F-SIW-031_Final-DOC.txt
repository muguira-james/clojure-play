Using a Night Vision Simulation Modeling Tool for the Development of 3 Dimensional Terrain Databases on PC PlatformsPaul Dumanoir, James Grosse, and Brian ComerU.S. Army Simulation, Training,And Instrumentation CommandOrlando, FL 32826407-384-3926, 407-384-3928, 407-384-3685Andrew Tosh, Jesse LiuAcuSoftAcuSoft, Inc.Orlando, FL 32826407-658-9888Russ Moulton, Brian RadermanJRM Technologies, Inc.540-368-8106Russ MoultonJRM Enterprises, Inc.540-785-4585Keywords:Night Vision Goggles, FLIR, 3 Dimensional Terrain Databases, Virtual Environments, and OpenFlightOpenFlight®ABSTRACT: The ability to train the Armed Forces in the future and to perform mission planning & course of action analysis is becoming more dependent on the use of 3 Dimensional (3D) Terrain Database models of urban areas.  These 3D Terrain Databases are used in desktop & and laptop workstations through fully immersive virtual reality simulators to render the target area.  A key area, that is underdeveloped, is the realistic representation of the 3D Terrain Database using Night Vision Goggles (NVG) or Forward Looking Infrared (FLIR) devices on low cost PC platforms.  Currently, the simulation of NVGs and FLIR consists of either crude representationsrepresentation on PCs, usually by turning the environment green, or through the use of very expensive super computers.  To enhance the night training capability on the low cost PC platform, a tool has been developed that allows the accurate renderings of urban areas in 3D terrain databases using these night vision devices.Two simulation areas were developed as part of this tool, the SENSOR Model and the SENSOR Effects t simulations.  The SENSOR Model Simulation allows the user to enhance their SEDRIS and OpenFlightOpenFlight® terrain databases by modeling the different physical properties of varying materials within the database to reflect a NVG or FLIR representation.  The SENSOR Effect Simulation will allow the user to vary the conditions under which the night vision database is used (e.g.i.e. varying lighting conditions, differing noise levels in the NVGs, shadowing, etc.)  With the use of these simulation tools, accurate rendering of NVG andor FLIR 3D Terrain Databases can be accomplished in a short period of time on a low cost PC platform.[General Comment: Since the abstract refers to two main parts of the development of the tool – SENSOR Model, and  SENSOR effects simulations , the body of the paper, ie.e section 3 –5 should clearly describe those parts. – Is SENSOR effects simulation refering to the Sensor Editor?]IntroductionThe visual system is one of the most critical parts of a human-in-the-loop simulation. Developing higher fidelity, lower cost sensors and night vision simulations is one of the major objectives of the US Army STRICOM’s Individual Combatant (IC) Simulation Science & Technology Objective (STO). The IC STO year 2000 R&D was focused on establishing a baseline Night Vision Goggle (NVG) simulation using low-cost PC Image Generator (IG) platforms to study the fidelity requirements for the IC simulation and training. This paper describes 1) the NVG simulation approach, 2) the fidelity requirements of IC NVG simulation, and 3)  the year 2000 accomplishments in the area of NVG simulation database modeling on low-cost PC platform.Traditionally, sensor simulations use expensive hardware as an add-on to existing Image Generation (IG) systems. This add-on hardware is typically designed for specific proprietary IG architectures and databases. It is highly desirable to develop a software-based, high fidelity, physic-based sensor simulation solution. The software-based solution should support standard databases, such as SEDRIS, and use standard off-the-shelf graphics hardware. AcuSoftAcuSoft has performed research and development of an advanced visual system to support the requirements of the IC STO. In the year 2000, the development focused on integrating our  our existing sensor simulations with STRICOM’s Individual Virtual Environment Technologies (IVET) test bed systems and on developing developing tools to enable IC simulator users to fine-tune the sensor images through easy-to-use tools. Potential follow-on efforts may be focused on further advancing the fidelity of the overall IC visual system.Figure 1 illustrates the software architecture of a typical  SENSOR-capable PC IG visual system.The Sensor Model Simulation models the physical phenomenology of the spectral band thermal emission and reflection of the synthetic environment.,  Tand the Sensor Effect Simulation models the Electro-Optics characteristics. Figure 1.  Typical Visual System Software ArchitectureFigure 1.  Typical Visual System Software ArchitectureNVG Simulation OverviewPhysically realistic synthesis of FLIR and NVG (or Near IR) imagery requires intensive phenomenology calculations of the spectral band thermal emission and reflection from scene elements in the database.  These calculations predict the heat conduction, convection, and radiation exchange between scene elements and the environment, as well as the bi-directional reflectance from the scene elements to the sensor. These calculations predict the heat conduction, convection, and radiation exchange between scene elements and the environment. Balancing these is requirements is the need for imagery to be presented to a display in a timely fashion, often in real time. In order to support these conflicting requirements, some means of overcoming the gap between real time and high fidelity must be achieved. Over the past several years, the US Army Night Vision and Electronic Sensors Directorate (NVESD) has been developing a real-time forward looking infrared (FLIR) sensor simulation known as Paint the Night (PTN). As part of this development, NVESD has explored schemes for optimizing signature models and for mapping model radiometric output into parameters compatible with OpenGL’s real-time rendering architectures. The benefit of the NVESD approach is that high quality FLIR and NVG simulations can use inexpensive OpenGL graphics hardware [1].  NVGs typically consist of an input optic assembly, an image intensifier assembly, and a display optic assembly.  Figure 2 illustrates the basic operation of the primary component, the image intensifier assembly. An optics assembly provides a fixed depth of focus photon flux on the photo-cathode (cathode), which converts photons to proportional energy level photo-electrons (e-).  These photo-electrons are emitted across a fixed gap to a micro-channel plate (MCP) where they are captured into a specific channel.  As the photo-electron collides with the channel walls, an applied voltage (Vmcp) The NVGs typically consist of an input optic assembly, an image intensifier assembly, and a display optic assembly.  Figure 1 illustrates the basic operation of the primary component, the image intensifier assembly. An optics assembly provides a fixed depth of focus photon flux on the photo-cathode (cathode), which converts photons to proportional energy level photo-electrons (e-).  These photo-electrons are emitted across a fixed gap to a micro-channel plate (MCP) where they are captured into a specific channel.  As the photo-electron collides with the channel walls, an applied voltage (Vmcp) across the channel induces the emission of many more  --- sometimes many thousands more -- electrons.  This amplified electron stream is then captured by a phosphor screen where they are converted to visible light projected to the user’s eye. Figure 2  Block Diagram of Image Intensifier AssemblyAs in the simulation of any sensor, there are a multitude of key factors that must be characterized properly.  IBut, in the case of NVG image intensifiers, there are a number of characteristics that make their real-time simulation difficult in practice.  First, intensifier assemblies can respond to extremely wide intra-scene dynamic range variations typically encountered in tactical scenes.  A car headlight can be as bright as 50,000 foot-lamberts (FL), while night sky measurements can be as dark as 10-8 FL and less.  As Figure 2 illustrates, Automatic Brightness Control (ABC) and Bright Source Protection (BSP) circuits allow the intensifier to accommodate these input swings, butand  they produce a non-linear behavior that is characteristic to the NVG experience as revealed in Figure 3a.  In addition, the micro-channel plate (MCP) component gives rise to a phenomenon known as “haloing” which is difficult to simulate. Figure 3a3  Output/Input Gain Characteristic of Image Intensifier AssemblyFor the purposes of discussion, there are 4 general areas that must be addressed for credible NVG sensor simulation:1. Image Intensifier Dynamic Range 2. Intensifier (Photo-cathode) Spectral Response 3. Other NVG/Intensifier Effects:  fixed depth of focus, MTF, “Haloing” and Noise4. Correlation between Visible/Near-IR Spectral Signatures Synthesis (1), (2) and (3) are typically addressed by a combination of software and special digital signal processing (DSP) hardware accelerators. Figure 3a. above illustrates the dynamic range behavior that needs to be captured for addressing (1).   Figure 3b shows the 3rd Gen Image Intensifier photo-cathode spectral response that must be faithfully simulated in (2).  And Figure 3c. shows a representative blur kernel for an NVG system that addresses area (3).For the purposes of this discussion, these challenges can be categorized into 5 general areas:1. Image Intensifier Dynamic Range 2. Intensifier (Photo-cathode) Spectral Response 3. Other NVG/Intensifier Effects:  fixed depth of focus, MTF, “Haloing” and Noise4. Input Signal Dynamic Range Characterization5. Correlation between Visible/Near-IR Spectral Signatures Synthesis Figure 3b.  3rd Gen Image Intensifier Photo-cathode spectral response.Figure 3c.  3rd Gen NVG blur kernelArea (4) is a simulation database modeling issue.The category (1), (2) and (3) are typically simulated by a combination software and special digital signal processing (DSP) hardware accelerators. The category (5) is a simulation database modeling issue.[General Comment: Besides the technical description provided above, this section should probably also include a section describing effects the NVG simulation supports – i.e. Gain, Time of day, Air Temp, Moon Phase, Visibility, Humidity, Automatic Gain Ctrl]3.   NVG Simulation and  the IVET Test Bed[General Comment: there is reference to a fidelity reqts “study” in several places through-out this paper but no detials about that study itself– suggest we delete “study” reference, or expand what the study was about in the paragraph below or somewhere else.]In order to demonstartedemonstrate the AcusoftAcuSoft low-cost, PC-based, advanced visual system concept, the AcusoftAcuSoft NVG simulation tool was integrated with one of the IC Simulators in the IVET test bed.  This target IC simulator for this integration/demonstration was the Real Guy simulator system developed by Veridian.  In addition, the MckennaMcKenna MOUT visual database in OpenFlightOpenFlight® format was chosen as the target visual DB for this effort. 3.1  IC Fidelity Requirements & the Sensor Model.In order to establish a baseline NVG simulation to support the IC simulation fidelity requirements study, we adopted the sensor simulation software product from JRM Technologies, Inc., the SigSim.  SigSim is based on the NVESD PTN sensor simulation model, with many improvements and features. The SigSim provides a credible NVG  sensor model simulation. Particularly Iin the case of Night Vision Goggles (NVG), where the signature phenomenon is entirely reflectance, SigSim adopted a detailed Bi-directional Reflectance Distribution FunctionBi-Directional Reflectance Function (BRDF)***(COMMENT – shouldn’t the acronym be BDRF ???)*** ) reflectance modeling, which is important for R&D. C A complete, high-resolution reflectance modeling is the key to realistic probabilities of identification as well.   The SigSim uses the Hapke, Sandford-Robertson and Beard-Maxwell BRDF models.   In order to drive the SigSim BRDF signature synthesis models, appropriate scene material propertiesy databases are required.   The following is a list ofs those material properties required by the SigSim simulation software:Altitude of the material system in metersBoundary condition (bc1) at the top surface of the material, exposed to the sensorBoundary condition (bc2) at the hidden bottom surface of the material exposed to the sensorTemperature of bc1Air/gas velocity of bc1Temperature of bc2Air/gas velocity of bc2Solar absorptivityLong-wave (3-inf um) emissivity8-12 um emissivity3-5 um emissivityNear-IR reflectivityRED-.4-.7 um diffuse reflectivityGREEN-.4-.7 um diffuse reflectivityBLUE-.4-.7 um diffuse reflectivityPercent of diffuse component of reflectivityPercent of specular component of reflectivitySpecial coefficient of convection flag Thermal conductivitySpecific heatDensityDiurnal depthShininess factor for spectral reflectionRecovery factor for aerodynamic skin heatingStandford number for aerodynamic skin heatingShadow flag to indicate non-solar-loadedIn theory, if all the properties of a material are accurately modeled, the SigSim sensor model will produce the accurate intensity under a given atmosphere and lighting condition. Therefore, an accurately modeled database with all the correct material property is essential for the required fidelity of the NVG simulation.  3.2  AcuoSensor APISensor API.The target IVET test bed IC Simulator for this effort, is based on Quantum3D’s OpenGVSOpenGVS® scene graph. OpenGVSOpenGVS® is a widely adopted scene graph technology that is supported on multiple platforms. The OpenGVSOpenGVS® currently supports Multigen’s OpenFlightOpenFlight® database format. An Out-the-Window (OTW) visual database in OpenFlightOpenFlight® format, McKenna MOUT, was the visual DB used by the IVET test bed simulator.   In order to seamlessly integrate the SigSim® sensor simulation with the OpenGVSOpenGVS®-based IVET test bed simulator, AcusoftAcuSoft developed an application-level, scene-graph-independent sensor simulation API, the AcuSENSORAcuSensor API. We recognized that there are many different scene graph software packages used by different PC-based simulation.  The AcuSENSORAcuSensor is designed to make it easy to integrate the highly-detailed, physic-based sensor models with simulation applications written with different scene-graph. We have successfully integrated the AcuSENSORAcuSensor API with OpenGVSOpenGVS®, SGI’s Performer®, and VISKIT, all popular commercial-off-the-shelf (COTS) scene-graph. Figure 4 illustrates the concept of operation of the integrated system.The AcuSENSORAcuSensor API provides an easy-to-use application interface to setup the senor attributes, atmospheric conditions, lighting conditions and loading of the senor attribute table. The AcuSENSORAcuSensor API layer also takes care of the real-time update of the sensor-related parameters.3.3 Prepare Database For Sensor Simulation  3.3 Sensor Effects.AcSThe original McKenna MOUT database chosen for this effort has no required attributes for sensor simulation.  AcusoftAcuSoft develop a database tool, the Sensor Editor, to enhance existing McKenna MOUT visual database with required sensor simulation material properties.  The Sensor Editor provided an easy way to convert the existing, non-sensor databases into sensor-ready databaseones, and it provided the flexibility that is essential to support the NVG simulation fidelity study. We will discuss the Sensor Editor in detail in the following section.Figure 4 illustrates the concept of operation of the integrated system. Figure 4.  Integrated IVET Test Bed Sensor SimulationThe Sensor Editor As mentioned earlier, having an accurate physics-based sensor model simulation is only one half of the solution for achieving high fidelity sensor simulation. An accurately modeled database is equally important. Unfortunately, database modeling is an expensive process, especially when the number of attributes that are required by the physics-based sensor simulation is very large. Collecting authoritative data can be a daunting task.  BIn the past, based on our experience, it is very rare that a simulator user, or a Subject Matter Expert (SME), is was completely satisfied with the visual performance when evaluating a visual or sensor simulation. Database fine-tuning is almost always required at the last minute.  Unfortunately, the past generation IG architectures typically requireds a pre-compiled run-time database., Therefore, the turnaround time for database tuning can could be hours or even days.In order to overcome these challenges, we have developed an easy-to-use tool, the  Sensor Editor. The Sensor Editor is a PC  based tool and runs on Windows-based PC.  The Figure 5 is a picture of the Sensor Editor. Figure 5.  The Sensor EditorThe Sensor Editor provides a GUI for the user to select any database element, and edit the related sensor attributes. The integrated SigSim sensor simulation software gives the user feedback in real-time. The tool also allows the user to select database elements based on feature code or visual attributes in order to perform batch processing of large databases. Since SigSim’s  sensor model is based on OpenGL’s lighting model, the Sensor Editor can support any PC that supports OpenGL graphics cards. We have successfully converted the McKenna MOUT database using the Sensor Editor. At the final integration and testing phase of the project, Mr. Andrew Tosh, the author of the Sensor Editor software, walked through the actual McKenna MOUT site wearing a NVG, with a laptop PC in hand,  and fine-tuned the database in real-time with real-world reference. As a result, the SME were satisfied with the NVG sensor simulation  integrated of with the IVET test bed simulator.  The 4.1 Side- by--Side Viewer.  To further improve the efficiency of the database conversion process, we integrated the Sensor Editor with AcuSoftAcuSoft’s SEDRIS Side-by-Side Viewer tool. The Figure 6 is a picture of Side-by-Side Viewer withdisplaying the  sensor view and Out-the-Window view simultaneously. The Side-by-Side Vviewer provides the unique capability to visualize, compare and correlate multiple databases simultaneously. Using the Side-by-Side Vviewer as part of the database conversion and fine-tuning process, the user getsives the users the instantaneous visual feedback on the effects of sensor simulation. This capability could be very useful when evaluating different effects of different sensor models when comparing the effectiveness of a particular sensor performance attribute parameter.In addition, the Side-by-Side Viewer supports the SEDRIS Data Reference Model (DRM) and Environmental Data Coding Specification (EDCS).  It is feasible to provide a data mapping interface between the sensor simulation material attributes and the SEDRIS EDCS, and establish an authoritative reference sensor attributes database for EDCS defined standard material types. With the reference material attributes as part of SEDRIS EDCS, it will greatly facilitate the database re-use in the future. [General Comment. This paragraph seems to belong on Future Work section.] It was also our ultimate goal to integrate the full sensor effects simulation as part of the sensor editor so the user can prepare the NVG simulation database on the PC with sensor editor. The NVGs have very characteristic Poisson-distributed noises which are signal-dependent, originating from noise sources in the photo-cathode, MCP, and phosphor screen display. In year 2000, we implemented a very rudimentary noise simulation using an accumulation buffer only as a proof-of-principle demonstration. Although high fidelity sensor effect simulation is computationally demanding, on the regular PC without a special DSP processor, the frame rate will not be suitable for human-in-the-loop training. However, it is quite feasible for database modeling and verification at lower frame rate. Therefore, we believe it is desirable to integrate a complete sensor effects simulation in the future, so the user can perform complete sensor database modeling on an inexpensive PC.5. SummaryHigh fidelity sensor simulation requires an accurately modeled database. We have presented a methodology to adopt existing visual databases for high-fidelity NVG and other sensor simulations on thea PC platform. Using The Sensor Editor on an inexpensive PC with OpenGL graphics accelerator provides an inexpensive and effective way to accomplish this complicated task. 6.  Future Work.Our ultimate goal is to integrate the full sensor effects simulation as part of the sensor editor so the user can prepare the NVG simulation database on the PC with the sensor editor. The NVGs have characteristic Poisson-distributed noise that is signal-dependent, originating from noise sources in the photo-cathode, MCP, and phosphor screen display. In year 2000, we implemented a very rudimentary noise simulation using an accumulation buffer as a proof-of-principle demonstration. High fidelity sensor effect simulation is computationally demanding, on a PC without a special DSP processor, the frame rate will not be suitable for human-in-the-loop training. However, it is quite feasible for database modeling and verification at lower frame rates. Therefore, we believe it is desirable to integrate a complete sensor effects simulation in the future, so the user can perform complete sensor database modeling on an inexpensive PC. This complete sensor effect simulation capability can be integrated to real-time PC Image Generator as well. Potential users of this R&D results can including database developers who has the needs to convert existing database, and developers of PC visual system that requires a [General Comment. This section should talk about where do we go from here – how do you build on what has been done in FY2000, What can be done to improve the Acusoft advance visual simulation, what potential users can benefit from this R&D and subsequent product.]7.  AcknowledgmentThe US Army Simulation, Training, and Instrumentation Command (STRICOM) supported the work presented in this paper. The authors want to thank Mr. Pleban and Ms. Slater from the Amy Research Institute, and all the Army personnel who how provided valuable feedback during the development and evaluation phases of this effort.   Special thanks goes to Charlie Jones from VVeridian for his support of in the IC simulator integration effort. 8.  References [General Comment. If there’s no references delete this section.][1]. Jacobs, Moulton, Liu, et al., “Optimized mapping of radiometric quantities into OpenGL”, SPIE Proceedings of SPIE Modeling, Simulation, and Visualization for Real and Virtual Environments, Vol. 3694, p. 173-182, SPIE Press, Bellingham, Washington, July 1999.[2]. Wolfe and Zissis , The Infrared Handbook, The Infrared Information Analysis Center, ERIM, Ann Arbor, Michigan, 1993.[3]. Accetta and Shumaker, The Infrared and Electro-Optical Systems Systems Handbook, Vol. 1, 23 and 5, SPIE Optical Engineering Press, Bellingham, Washington, 1996.[4]. Incropera and DeWitt, Fundamentals of Heat and Mass Transfer, John Wiley & Sons, New York, 1996.[5]. Watt and Watt, Advanced Animation and Rendering Techniques, ACM Press, New York, 1992.[6]. Woo, Neider, Davis and Shreiner, OpenGL Programming Guide, 3rd Edition, Addison Wesley, Reading Mass., 1999.[7]. Stefanik, Night Vision and Electronics Sensors Directorate internal publications on Image Intensifiers, 1973.[8]. Hapke, Theory of Reflectance of Reflectance and Emittance Spectroscopy, Cambridge University Press, New York, NY, 1993.[1].[2].Author BiographiesPAUL DUMANOIR is the lead principle investigator for mixed virtual/live technologies at the U.S. Army STRICOM.  Mr. Dumanoir leads the Individual Warfighter Embedded Training Science & Technology Objective (STO).  Prior to his involvement with individual warfighter simulations, he worked as software and systems engineer on the various M&S programs.  His current interests include CGFs, Human-In-The-Loop (HITL) networked simulators, and embedded training applications.  He earned his B.S. in Electrical Engineering from the University of South Alabama in 1987 and his M.S. in Computer Systems from the University of Central Florida in 1991.JAMES GROSSE is the Lead Principle Investigator for the Individual Combatant (IC) Simulation Programs at the US Army’s STRICOM.  Mr. Grosse works on the DoD Defense Technology Objective (DTO) for the IC and Small Unit Operations Simulation and the IC Science and Technology Objective (STO).  Prior to his involvement with IC Simulations, he worked as the lead Engineer on various M&S programs involving instrumented ICs inside a MOUT site.  He earned his BS in Electrical Engineering from Drexel University in 1988.BRIAN COMER is the Systems Engineer for Individual Combatant (IC) Simulation Programs at the U.S. Army STRICOM.  Mr. Comer works on the DoD Defense Technology Objective (DTO) for IC and Small Unit Operations Simulation, and the IC Science and Technology Objective (STO).  He earned his Bachelor of Science degree in Computer Engineering from the University of Central Florida in 2000.ANDREW TOSH isJESSE LIU is president of AcuSoft, Inc. He received his Bachelor of Science degree from the University of Taiwan and his MSEE from the University of Florida.  He has more than 16 years experience in the design and implementation of simulation software systems.isRUSS MOULTON is President of JRM Technologies, and a recognized expert in sensor simulation systems, EO/IR phenomenology, and real-time computer graphics.  He graduated summa cum laude from the US Naval Academy in 1984 with a BS in Physics, and received his MSEE in Digital Signal Processing from the University of Pennsylvania Moore School of Electrical Engineering in 1989.BRIAN RADERMAN is senior software engineer with JRM Technologies, specializing in real-time computer graphics applications for sensor simulation.  He graduated summa cum laude from the University of Maryland in 1989 with a BS in Computer Science.  He is currently developing both real-time multi- and non-real-time hyper-spectral simulation applications.   RUSS MOULTON isOTWDatabaseSensor EditorSensor TableAcuSensor APIIVET TestBedSimulationApplication(OpenGVS)SigSimFigure 4.  Integrated IVET Test Bed Sensor SimulationFigure 6.   The Side-by-Side Viewer