Federation Decomposition: HLA MetricsAnn F. PollackJohn P. BakerThe Johns Hopkins University Applied Physics Laboratory11100 Johns Hopkins RoadLaurel, MD 20723-6099240-228-5887, 240-228-4895ann.pollack@jhuapl.edu, bakerjp1@jhuapl.edu Keywords:HLA, software metrics, decomposition, legacy simulationsABSTRACT: In creating a High Level Architecture (HLA) Federation Object Model (FOM), the question arises, "How far should an entity be decomposed within an HLA federation?"  As one decomposes a federation into successively more and smaller federates, modularity and flexibility increase, but efficiency decreases.  Objective measures are required to explore these tradeoffs. This paper develops a set of HLA-specific metrics for use in FOM development.  Metrics are developed for application to individual federates, specific interactions, and the overall federation.  The specific case of legacy simulations presents a unique opportunity to apply code-based metrics to current implementations to influence the HLA design process.  In an example, cyclomatic complexity analysis of an existing simulation identifies the most complex algorithms thereby guiding federate decomposition.1. IntroductionIn recent years, modeling and simulation has undergone a revolution, away from large monolithic simulations to distributed, object-oriented architectures.  This revolution, fueled in part by the introduction of the Department of Defense standard High Level Architecture (HLA), has increased system modularity, flexibility, and maintainability.  This modularity is generally applied to complete real world entities.  For example, in a ballistic missile HLA federation, federates could consist of an incoming threat missile, an interceptor, and a launch ship.  To perform high fidelity modeling, however, these large scale objects must be decomposed into subsystems that are typically comprised of multiple algorithms, each of which can significantly impact overall system performance.  Modifying a monolithic simulation to test various subsystem designs can be difficult.  However, a distributed, modular simulation with a large number of independent entities incurs a communications overhead that may degrade run-time efficiency, limiting the number of scenarios or Monte Carlo runs that can be processed in a timely manner.  Balancing these competing factors requires a methodology for determining an appropriate level of decomposition.Within an HLA context, the degree of decomposition is represented by the number of federates in the federation.  A federation with minimal decomposition will have reduced overhead, while a highly decomposed federation allows more flexibility and permits parallel processing.  Different applications will lend themselves to different levels of decomposition.  Determining the correct level can have a major impact on the utility and efficiency of the resulting federation.  This paper explores metrics for use with HLA and concentrates on methods for determining optimum federation decomposition.Existing software metrics provide a starting point; however, they fail to capture and exploit aspects of the HLA design process.  Just as the object-oriented paradigm resulted in new metrics suites [1], the HLA paradigm requires new approaches.  Section 2 discusses metric theory, examines some classic existing measures, and looks at metrics for the HLA environment.  Effective metrics require both theory and application for scientific validity [2].  Section 3 describes efforts underway to employ, evaluate, and refine the HLA metrics described in Section 2.  This is not an attempt at formal validation which would require multiple data sets and a variety of different problem domains.  Rather it is a preliminary proof of concept to explore utility and applicability to an ongoing practical project.  It is also a testbed for identifying potential refinements to these metrics.  While still in progress, preliminary conclusions are presented in Section 4.  2. MetricsTo be most effective, metrics must be clearly motivated and possess what has been termed “face validity,” that is they must appear logical to those who are asked to apply them [3].  Without this face validity, metrics efforts tend to suffer from poor data collection by skeptical participants.  This paper seeks to define metrics for use with HLA, particularly as a measure to determine an appropriate level of decomposition.  Since the Federation Object Model (FOM) defines the number of federates, effective decomposition metrics must be applied during the design phase.  Other metrics efforts may be intended to provide lessons learned for future efforts or for a corporate capability maturity assessment as defined by the US Software Engineering Institute [4].  These type of metrics can be applied later in the development process.  Influencing individual project development through metric application, however, requires that the metrics be applied as early as possible.  The motivation is to define a metric or set of metrics for use in determining simulation decomposition.Given a clear motivation for developing a set of metrics, it is necessary to understand how to construct a mathematically rigorous metric with applicability across problem domains.  Section 2.1 explores metric theory.  Section 2.2 presents a survey of some classic metrics as a starting point to examine how they might be applicable to the HLA decomposition application.  HLA-specific metrics are proposed in Section 2.3 with special attention to the important HLA issue of legacy simulations.2.1 Metric TheoryMetrics can be classified into two varieties, internal and external [3].  Internal metrics can be measured directly and objectively from a design or source code (e.g. Lines of Code (LOC)).  External metrics (e.g. Maintainability, Reliability) are more subjective.  In general, one can measure internal metrics, but one desires external metrics.  In a manner of speaking, internal metrics can be considered a data set from which we wish to estimate external metrics.  Metrics can also be divided into predictive and analytical measures.  Predictive metrics attempt to estimate external metrics at an early stage in the software cycle, while analytical metrics offer tools to examine a completed software product for subsequent improvement.  This paper deals primarily with predictive internal metrics.For an internal metric to be considered “good” it should possess the following properties [3]:Reliability – The metric should be repeatable and exhibit the same value regardless of the measurer.Sensitivity – The metric should reflect changes in the design when those changes lie in that metric’s degree of freedomOrthogonality – The metric should be orthogonal to other metrics such that a change to unrelated design areas does not affect it.  This maximizes the information content of the combined metric set.  Most design changes will affect more than one metric, but the possibility should exist for independent movement of metrics.  Otherwise, the metric set should be reduced to a minimally sufficient set.Consistency – For example, a complexity metric, cplx(), of algorithm A should be less than or equal to the complexity metric of another algorithm B, if A is included in B.  Notationally, this can be represented as cplx(B) ( cplx(A), if A ( B.Design Details – Given two implementations, P and Q,  of the same function, a metric m(P) does not necessarily equal m(Q).  The design details of the individual implementations can change the metric, even though both implementations perform the same function [1].Quantification – The metric should map to a real number so that comparisons can be performed easily.Early Availability – Predictive metrics must be available early in the software life-cycle in order to have a meaningful impact on system design.Computational Ease – The metrics should be easily calculated from design documentation.Many other desirable internal metric properties have been suggested, but many appear too rigid in their requirements or too vague in their definition.  Also, some are special cases of the general guidelines listed here.External metric properties are defined by the International Standard Organization in ISO-9126 [5].  This standard suggests six quality characteristics which may serve as external metrics:  functionality, reliability, usability, efficiency, maintainability, and portability.  Unfortunately, this standard has seen little use in the metrics community due to the difficulty of mapping these characteristics to quantifiable internal metrics. Internal and external metrics will vary from project to project.  Basili and Rombach [6] suggest a top-level first approach known as Goal-Question-Metric.  The GQM approach first requires that the metric users define a goal, which in turn guides them to questions that specific metrics can answer.2.2 Existing MetricsMany metrics currently exist for a wide variety of languages, design methodologies, and application areas.  A few of the most cited examples of metrics are summarized below to provide a historical perspective to the HLA Metrics proposed to guide the FOM generation process.  These metrics include Lines of Code, Function Points, McCabe’s Cyclomatic Complexity, and various Object Oriented Metrics.2.2.1 Lines of CodeThe earliest measure of software size and sometimes complexity was the Lines of Code (LOC) metric.  While the LOC metric does provide a sense of the overall size of a software effort, there is still debate as to how to compute a LOC value.  Are comments included?  Macro Definitions?  Is it language specific?  In addition, once the definition is agreed upon, the LOC metric provides little insight into more subtle measures of complexity and interdependencies.   This makes it difficult to establish a clear relationship between LOC and higher-level, external metrics.  However, since it is relatively objective and easy to calculate (once defined), it enjoys wide-spread acceptance as an indicator of program size and, unfortunately,  complexity.2.2.2 Function PointsA function point analysis ([7], [8], [9]) is performed by counting the external inputs, outputs, inquiries, and interface files in addition to internal logic files in accordance with explicit counting rules.  Subjective ratings are assigned to each type of function point to assess its complexity.  The function point methodology emphasizes the data flow from an application to the external world or user.  For interactive business applications and World Wide Web servers, this captures the bulk of the functionality.  However, engineering applications that perform complex tasks entirely internally are more difficult to characterize in this fashion [10].Function points can be applied to a software design early in its life-cycle as it can make use of requirements documentation  to estimate the number external and inputs, outputs, inquiries, and data files that may be necessary.  This makes it a possible choice for a predictive metric.  A strong correlation has been noted between the number of function points and LOC [11] such that one can be used to estimate the other.2.2.3 Cyclomatic ComplexityCyclomatic Complexity [12] is a graph-theoretic measure of algorithm complexity that depends on decision points within the algorithm.  The method focuses on computing the cyclomatic number V(G) of a graph G with n vertices, e edges, and p connected components byV(G) = e – n + 2p.From graph theory, it can be shown that the cyclomatic number is equal to the maximum number of linearly independent circuits of a strongly connected graph, G.  To represent an algorithm by a graph we assume a unique entry point and a unique exit point, and assume that the exit is reachable from all points within the algorithm.  We then connect the exit point to the entry point by a “phantom” connection.  If all points are reachable from the entry point, this phantom connection makes the graph strongly connected, i.e. all points are reachable from any point.  In this representation, graph nodes are segments of sequential code and edges are decision branches from one code segment to another.As noted in [12], V(G) can be simplified in structured code to the number of individual predicates (or conditions) plus one in an algorithm.  Structured code places the restriction that one cannot branch into or out of a loop or decision.  One must respect the unique entry and exit node conditions for all decision and loop structures.Unfortunately, this metric requires the source code (or detailed algorithm description).  This defeats the purpose of a metric to be used for predictive uses.  While the complexity density may be a useful analytical or maintenance productivity predictor, it requires too much detail for use in the initial design stages.  It may, however, be useful in analyzing legacy code.2.2.4 Object Oriented MetricsObject oriented metrics concentrate on the complexity and relationships between objects.  A benefit of object oriented techniques is that it allows a great deal of system design to be performed before coding.  Defining objects by their associated member data and functions provides sufficient information to compute a number of informative metrics in the design stage.  This allows feedback from metric evaluations to designers early in the software life-cycle, when adjustments to the design are much less costly to implement.  Also, traditional metrics for procedural code are not always appropriate for object-oriented designs.Some of the most cited object oriented metrics are enhancements by Chidamber and Kemerer [1] to previous metrics.   They proposed a suite of six measures to evaluate object oriented design and implementation quality as listed below.Weighted Methods per Class (WMC)Either a count of methods per class (unit weighting) or the sum of the McCabe cyclomatic complexities of each method in a class.Depth of Inheritance Tree (DIT)This is the length of the longest path from a class to the root of its inheritance tree.  Note that multiple inheritance can lead to multiple paths; the longest is chosen as the metric.  This can also be averaged over all classes with the same root class.  A class with a large DIT inherits many variables and methods from super-classes, requiring more effort to understand and predict behavior.Number of Children (NOC)This metric indicates the number of unique derived classes from a given super-class.  More sub-classes indicate more reuse, but more opportunity for abusing the inheritance mechanism.Coupling Between Object Classes (CBO)CBO is a count of the non-inheritance relationships from one class to other classes.  More coupling implies less modularity and more effort required to predict the effects of change or to trace anomalous behavior.Lack of Cohesion of Methods (LCOM)This metric attempts to reflect the unity of purpose of all member functions of the class.  It is computed by finding the number of disjoint sets of member variables used by separate member functions.  A high LCOM value indicates that several separate algorithms are being computed.  Such separate algorithms may be more appropriately located in separate objects.Response for a Class (RFC)This is a count of the number of methods (both inside and outside the class) invoked by the reception of a message to the class.  A high RFC value indicates a complex object.2.3 HLA Metrics Given the existing metrics described in Section 2.2, the challenges are to chose, modify, and create metrics for use in a HLA environment.  To evaluate candidate HLA metrics, the criteria for metrics must be stated clearly.  Section 2.1 describes desirable characteristics of metrics, most of which can be directly applied to HLA metrics.  Thus, HLA internal metrics should be reliable, consistent, sensitive, available early, orthogonal, quantifiable, and easy to compute. Internal metrics should give rise to external metrics, which are useful, specific, normalizable, and able to provide feedback toward a better solution.To accommodate the early-availability characteristic, an HLA metric should be obtainable from the Federation Object Model (FOM), which is one of the first design documents of an HLA simulation.  This will allow evaluation of the FOM in light of the metrics and allow changes to be made to improve the simulation before too much effort is spent on a non-optimal design.  The reliability and consistency characteristics indicate metrics that are objectively calculated, rather than subjectively judged.  The sensitivity criteria implies that the metrics should reflect small as well as large changes to a FOM. Orthogonality prevents having two metrics which behave in the same manner for all changes to a FOM.Our metric suite is separated into three areas: Federate, Attribute and Interaction, and Federation metrics.  This separation allows the metrics to illuminate issues within each type of HLA construct. Federation metrics are composite metrics used to optimize a “balanced” federation.Federate MetricsFederate metrics attempt to characterize a federate in terms of its reliance on other federates and on its internal uniformity of purpose.Coupling Between Federates (CBF)CBF is a count of the interactions and attribute reflections to other federates to and from a given federate.  This data is available from the FOM in the form of publication and subscription lists of attributes and interactions.  A federate that has a relatively large CBF has more dependence than other federates on its neighbors. Maintenance of federates with high CBF is more difficult since many other federates depend on its data.  It is also more susceptible to changes to external federate data.Federate Lack of Cohesion (FLC)FLC is the number of unique objects within a single federate.  Multiple objects instantiated from a single federate can be of the same or disparate types. A single federate may be best for implementing objects of the same type, especially if a large number of simple, objects are created.  However, a federate that instantiates more than one object type increases the likelihood that changes to one object type will cause inadvertent changes to another object type. Maximizing cohesion within a federate allows cleaner designs with fewer extraneous concepts.  This also fosters modularity and flexibility, which facilitates “plug-and-play” federations. Object Lack of Cohesion (OLC)OLC is a count of the discrete subsets of attributes and interactions reflected or sent as a result of an event, where an event is defined as a time advance, receipt of an interaction, or reflection of an attribute. OLC attempts to measure the number of separate functions performed by a single object. The underlying assumption is that an object with concise functional duties should have highly dependent inputs and outputs.  Disjoint input and output sets indicate independent functions that can be separated.  This metric requires more design information than is present in the FOM, but such information should be available to FOM designers. For example,  REF _Ref454684862 \h Figure 1 shows an object having three attributes A, B, and C.  It subscribes to interaction IAB and publishes interaction IOUT.  When interaction IAB is received, attributes A and B are updated and interaction IOUT is sent in response.  Attribute C, however, is only updated and reflected upon the occurrence of a time advance.  The response subset of {A, B, IAB, IOUT} is disjoint with {C}. Figure  SEQ Figure \* ARABIC 1 Object Lack of Cohesion ExampleFigure  SEQ Figure \* ARABIC 2 Separated Cohesive ObjectsEven if knowledge of C is required to compute A, B, or IOUT, the object may still be a good candidate for decomposition if C is published.   REF _Ref454692109 \h Figure 2 shows the result if the single object is decomposed into two separate, more cohesive objects.  Since C had already been published, the OLC = 2 subsets.  In general, a value of OLC > 1 indicates a possible candidate for decomposition. The designer relies on his or her knowledge of the required object flexibility.  If it is likely that one of the decomposed objects will be changed more frequently than the other, decomposition may reduce future adaptation effort and maintenance.  However, an OLC of unity indicates that decomposition carries a penalty in additional communication costs that may not be balanced by increased encapsulation benefits.Federate Balance (FB)FB is a composite metric that allows a trade-off between the competing costs of communications, represented by Coupling Between Federates (CBF), and flexibility and maintainability, represented by Federate and Object Lack of Cohesion (FLC and OLC). EMBED Equation.3  Minimizing FB for a given federate is the objective of the designer.  To minimize FB, a federate would consist of a single (FLC = 1), cohesive (OCL1 = 0) object with a minimum of inputs and outputs.  Multiple objects are penalized by FLC while non-cohesive objects are further penalized by OLC.  To avoid having too finely granulated federates, the CBF tends to draw related functions together by penalizing communication between federates.Note that an arithmetic sum of metrics rests on the assumption that the units of the elements are comparable.  CBF is in communication units, FLC is in object counts, and OLC is a combination of object counts differenced with communication units.  Therefore, there is an implied assumption of a one-for-one valuation of communication units with object counts.  Also, differing sizes of individual attributes and interactions are not accounted for in this metric.Attribute and Interaction MetricsA published attribute or interaction affects a federation in proportion to how many federates subscribe to it, how many publish it, how large it is, and how often it is sent.  In general, one wished to measure the average communication cost associated with a given attribute or interaction.  This will allow identification of the most costly communications and identify potential bottlenecks in the FOM.  These metrics assume a specific communication or a single federate.  Total, multi-federate communication costs will be computed in the federation metrics.Number of Subscribers (NOS)NOS is a predicted count of instantiated federates which will subscribe to a given attribute or interaction.  Since the RTI must send attribute reflections and interactions to each federate separately, the communication costs are linear with the NOS.  Also, a communication with a large NOS would be difficult to alter or remove, since all recipients would have to be modified to adapt to the change.Number of Publishers (NOP)NOP is a predicted count of the number of instantiated federates that generate a given attribute reflection or interaction.  Since each federate must send its communication through the RTI, the communication cost is linear with NOP.  As with NOS, a communication with a large NOP is difficult to change, since a large number of federates would need to be adapted to any alteration.Size (SZ)The size of the communication is a sum of the data being transferred and the RTI overhead.  This will vary from one RTI implementation to another.Average Transfer Rate (ATRm,f)	The ATR is the average rate of attribute update or interaction send rate for a given communication, m, from a given federate, f.  Federation-level metrics will account for different federate output rates.  Note that if a federate does not send a given communication, the ATR for that combination will be zero.  This becomes important when computing total communications costs across a federation.Federation MetricsWhile the federate and attribute metrics aid in the design and tracking of individual elements of a federation, overall composite metrics are required to attempt a global optimization.  Combinations of the element-level metrics are computed to reflect federation-level design measurements, which may be useful in aiding design decisions concerning federate sizing and communication methodologies.Expected Federate Cardinality (EFCf)EFCf is the expected number of federates of a given type, f, over all planned federation instantiations.  This metric allows a designer to see which federate types may dominate performance due to the sheer number of federates to be supported and the communications to and from these federates.Total Expected Federate Cardinality (TEFC)TECF is the sum of Expected Federate Cardinalities over all federate types.  This provides a measure of the total expected size of the federation.  Note that specific scenarios may have Total Federate Cardinalities much greater than the expected value.  However, the expected value should be of more use than a maximum value since the majority of federation executions will, by definition, fall closer to the expected value than an extremum.  However, if an unusually large federation scenario is envisioned as required, the maximum cardinality should be considered in design decisions. EMBED Equation.3  Total Expected Federation Bandwidth (TEFB)The TEFB metric attempts to predict the average volume of message traffic over all envisioned scenarios required for a federation. EMBED Equation.3  Message Percent Bandwidth (MPBm)MPBm is the percentage of federation bandwidth generated by a specific attribute or interaction, m.  It is useful for identifying the communications that dominate the performance of the federation. EMBED Equation.3  Average Lack of Cohesion (ALC)The ALC averages the Federate Lack of Cohesion metric across the federation.  Since the ideal value for each FLC is one, an average of much greater than one may indicate finer federate granularity is advisable.  Since it is independent of federation size, this metric may provide a metric for inter-federation comparisons. EMBED Equation.3  Normalized Federation Bandwidth (NFB)The NFB is the expected average bandwidth per federate.  Higher values indicate possible communication bottlenecks, especially if the federation is expected to grow.  A designer can look to a rank-ordered list of MPBs to concentrate efforts to reduce the bandwidth. EMBED Equation.3   Total Federation Balance (TFB)TFB is an attempt to provide an overall optimization metric for a federation.  This metric can be used as a cost or objective function to be minimized.  Both the ALC and NFB are normalized metrics which are independent of federation size.  The ALC penalizes monolithic designs while the NFB penalizes excessive communications among federates.  The constant, K, converts the NFB (in bytes/second) into the units of the ALC (communication and object counts).  A reasonable value for this constant would be the inverse of the average communication size. EMBED Equation.3  While minimizing this metric obviously cannot represent the goal for all federation designers, it may provide guidance when seemingly arbitrary choices must be made.  Achieving a joint minimum of the opposing ALC and NFB metrics balances the federation structure between the extremes of monolithic and communication-intensive designs.2.3.4 HLA and Legacy SimulationsWith the advent of HLA, there has been strong interest in converting existing legacy simulations.  Many of these simulations represent years of development and were designed as stand-alone applications.  HLA conversion provides a means to synergize multiple simulations for a more comprehensive and realistic model of complete systems.  Legacy simulations present both unique challenges and opportunities.  Metrics for conversion need to reflect these particular characteristics.When considering converting a legacy simulation to HLA, the first step is to define goals.  If the only motivation is to be able to claim HLA-compliance, then the resulting FOM will be significantly different than if the goal is to be able to integrate with related simulations or even to build toward a future vision. Once goals are established, the next question usually is what will it take to reach these goals.  This is where metrics need to play a role.  There are several potential metrics unique to the legacy simulation arena.  For example, if one of the goals were to continue to have a stand-alone version of the simulation in addition to an HLA version, a useful metric would be to measure the robustness of the HLA code to underlying simulation change.  The design should attempt to minimize the effort to keep two different versions synchronized.  A related metric would measure the amount of change required in performing the HLA conversion.  By limiting change, it will be easier to keep versions coordinated and there will be less cost and less opportunity to introduce faults in the code.  Of course, minimizing change can also limit flexibility and functionality increases possible with HLA.  HLA conversion is an opportunity to enhance a legacy simulation, perhaps by replacing a low fidelity component with a higher fidelity representation via an alternate HLA federate.  Measuring the potential for increased functionality would be another metric of interest.  These metrics would measure characteristics peculiar to legacy code.Legacy simulations also present a unique opportunity to apply more detailed metrics during the design phase.  For most applications, the metrics suite is somewhat limited during design because the code is not available.  In general, code-based metrics are used as a point of reference for the next product or too evaluate a developer team’s capability maturity.  In the case of legacy simulation, however, code-based metrics can be applied to the existing legacy code for application to the design of the FOM.  It offers the best of both worlds since a large portion of the code already exists while the project is still in the design phase.  Thus, metrics which usually have little chance to influence the design phase can be applied during the HLA conversion of a legacy simulation.The pure FOM metrics do not capture the underlying complexity of each federate’s objects and algorithms.  This requires some measure of complexity of the implementation that is not available from the FOM.  In a legacy conversion, however, the following metrics may be computed to aid in the FOM design process.  Some of these metrics are object-oriented in nature.  These metrics, of course, apply only to object-oriented legacy simulations.Legacy Federate Lack of Cohesion (LFLC)LFLC is computed by finding the number of disjoint sets of member variables used by separate member functions within a federate.  This is essentially the object-oriented Lack of Cohesion metric for an object-oriented legacy simulation applied to each member object brought into a given federate.  If the legacy simulation contains highly cohesive objects, the transition to HLA can be accomplished straightforwardly by mapping objects to federates.  Of course, some supporting object classes will be used by multiple federates.  In a highly cohesive object-oriented design, the conversion should be a process of assigning objects to federates. If, however, the legacy simulation exhibits a lack of cohesion in some objects, it may be necessary to decompose the non-cohesive objects to achieve the modularity desired in the HLA federation.  This metric can be averaged across a federation to provide a federation-wide, size-independent Lack of Cohesion metric.Legacy Percentage Federate Cyclomatic Complexity (LPFCCf)LFCC is computed by summing the McCabe cyclomatic complexities of all functions within a given federate, f, and dividing by the total federation cyclomatic complexity.  This will provide a measure of the complexity of each federate in relation to the whole. EMBED Equation.3  Legacy Federation Entropy Ratio (LFER)Entropy is a measure of a distribution.  Normally, this is a probability distribution, but in this case we apply the entropy measure to a distribution of complexity across a federation consisting of N federates (N > 1).  A high entropy ratio (near one) indicates that the federation complexity is evenly distributed.  A low entropy ratio indicates that the bulk of the complexity is contained by relatively few federates.  EMBED Equation.3  The underlying motivation for the LFER metric is the desire to avoid concentrating the majority of a federation’s complexity into a small portion of federates.  Maintaining a high LFER will result in an evenly distributed federation, which increases modularity.  Since complex code is relatively likely to be modified, encapsulating high probability of change code in separate federates reduces the likelihood of modification ripple effects as well as increasing the flexibility to alter parts of the simulation transparently.3. ApplicationThe original motivation for a quantitative measure of federate decomposition arose from the Theater Ballistic Missile (TBM) efforts underway at the Johns Hopkins University Applied Physics Laboratory (JHU/APL).  JHU/APL has developed multiple Theater Ballistic Missile (TBM) simulations of various fidelity levels.  Some of these simulations have been in existence for many years and use procedurally oriented Fortran code.  Others are designed as modular object oriented C++ code.  Each simulation models a different element of the TBM domain.  Bringing these simulations together via the HLA paradigm should produce synergistic effects for overall system analysis.Initial focus has been on HLA conversion of the Ballistic missile Localization And Selection Tool (BLAST), a comprehensive ballistic missile simulation environment providing a high-fidelity representation of interceptor sensing and information processing.  Because it is a comprehensive environment, it contains placeholders for all of the system components.  With careful FOM design, these currently low-fidelity BLAST components can be upgraded when the high-fidelity individual simulations achieve HLA compliance.  BLAST also offers the advantage of being an object oriented C++ simulation making it a good candidate to use in a proof of concept for HLA metrics.A high level FOM was developed and implemented without application of any metrics to serve as a baseline.  Applying lessons learned from previous HLA experience [14], tools were used to expedite this process.  The Defense Modeling and Simulation Office (DMSO) Object Model Development Tool (OMDT) was used for FOM development. REF _Ref454776924 \h Figure 3 shows the simple baseline FOM definition.  Five federate classes are defined: Threat, Missile, Ship, Celestial, and Scenario Manager.  The scenario manager starts the federation and coordinates the other federates.  Most data is passed via interactions rather than attribute reflection.  Six Interactions are defined.   IR_Look requests infrared signature data that is returned via IR_Irradiance.  Similarly, Radar_Transmission is a data query for a Radar_Return interaction.  The Handover_Dataset and Track_Data pass data from the Ship radar to the Missile IR seeker.  Not shown in the diagram, a state attribute containing position, velocity, and orientation information is published by all federates but the Scenario Manager.  Figure  SEQ Figure \* ARABIC 3 Baseline FOM3.1 Application of HLA MetricsThe initial FOM for the application was created by ad hoc encapsulation of physically separate entities such as missiles, threat objects, ships, and celestial bodies.  The HLA metrics of Section 2.3 were then computed to guide further FOM development.Table  SEQ Table \* ARABIC 1 Initial FOM HLA MetricsFederateCBFFLCObject TypesOLCFBThreat54RV425ACM4Booster4Debris4Missile71Missile412Ship51Ship39Celestial32Satellite411StarPlanet2Scenario Manager11Scenario Manager13The metrics imply that there is much room for further decomposition in the federation.3.2 Application of Legacy MetricsAfter implementation of the baseline FOM, a number of metrics tools were applied to the BLAST legacy simulation to provide insight into FOM development.  Understand for C++ by Scientific Toolworks Inc. computes lines of code metrics and 3 different versions of cyclomatic complexity.  The analyzed version of the BLAST simulation, without including the numerous matrix and utility functions, consists of 64 classes, defines 934 functions, and has 34,854 lines.  REF _Ref454780501 \h Figure 4 shows the Legacy Federate Percentage Cyclomatic Complexity (LFPCC) metric for each federate.  The Missile federate dominates this metric with 64% of the cyclomatic complexity.  The corresponding Legacy Federate Entropy Ratio (LFER) computes to 0.65, significantly below unity indicating a concentration of complexity in a subset of federates (in this case the Missile federate).  Calculations show that if the 64% complexity of the Missile federate were separated into two equal portions of 32% each, the LFER would rise to 0.83, a significantly more even distribution of complexity in the federation.  Therefore, the next iteration of the FOM will focus on decomposing the Missile federate into roughly equal-complexity sub-federates.  In addition, the ship federate may be broken into at least two sub-federates.Figure  SEQ Figure \* ARABIC 4 Legacy Federate Percentage Cyclomatic Complexity4. ConclusionsDesign decisions regarding the level of decomposition can have a major impact on the eventual utility and efficiency of a FOM and the resulting federations.  This paper develops HLA-specific software metrics for use in setting the level of decomposition. The identified metrics provide a quantitative measure for achieving a balanced federation in the effort to optimize the sometimes competing goals of utility and efficiency. Applying these metrics to our specific legacy simulation and a baseline FOM identified federations which clearly needed further decomposition.Looking beyond the specific goal of determining an appropriate level of decomposition, what this paper highlights is the potential benefit of HLA-specific metrics.  Just as the object oriented paradigm resulted in new metrics suites, HLA-specific metrics can take advantage of unique aspects of the HLA development cycle.  Many HLA efforts can also leverage off of legacy simulations.  Detailed metrics can be applied to this existing code, providing an opportunity to learn from the existing simulation in laying out the FOM and designing the HLA simulation.  This is a unique opportunity to gain insight into the underlying structure and complexity and incorporate that intelligence into the HLA design.  Future efforts should perform a more comprehensive examination of which existing metrics applied to legacy simulations provide the most insight into the FOM development process.  Also, research is needed to identify and validate a minimal orthogonal set of HLA metrics.5. References [1]	S. R. Chidamber and C. F. Kemerer: “A Metrics Suite for Object Oriented Design”  IEEE Transactions on Software Engineering, Vol. 20, No. 6, pp. 476-493, June 1994.[2]	B. Henderson-Sellers: Object-Oriented Metrics Measures of Complexity, Prentice Hall, New Jersey 1996.[3]	W. A. Mehrens and I. J. Lehmann: Measurement and Evaluation in Education and Psychology, Holt, Rinehart, and Winston, New York 1978.[4]	W. S. Humphrey: Managing the Software Process, Addison-Wesley, Reading, MA 1989.[5]	International Standards Organization, “Information Technology – Software Product Evaluation – Quality Characteristics and Guidelines for Their Use,” ISO-9126:1991(E), December 1991. [6]	V.R. Basili and H.D. Rombach, “The TAME Project: Towards Improvement-Oriented Software Environments,” IEEE Trans Software Eng,  June 1988, pp. 758-773.[7]	A. J. Albrecht and J. E. Gaffney, “Software function, source lines of code, and development effort prediction: A software science validation,” IEEE Transactions on Software Engineering, Vol. SE-9, Nov. 1983, pp. 639-648.[8]	R. Rask, P. Laamanen, K. Lyytinen, “Simulation and Comparison of Albrecht’s Function Point and DeMarco’s Function Bang Metrics in a CASE Environment,” IEEE Transactions On Software Engineering, Vol. 19, No. 7, July 1993, pp. 661-671.[9]	J. Bansiya, “A Hierarchical Model For Quality Assessment Of Object-Oriented Designs,” Ph.D. dissertation, University of Alabama Huntsville, AL, 1992.  HYPERLINK http://www.cs.uah.edu/~jbansiya/phd/index.htm http://www.cs.uah.edu/~jbansiya/phd/index.htm[9]	J. Bansiya, et.al., “A Class Cohesion Metric For Object-Oriented Designs,” Journal of Object Oriented Programming,  January 1999, pp. 47-52. [10]	J.E. Matson, B.E. Barrett, and J.M. Mellichamp, “Software Development Cost Estimation Using Function Points,” IEEE Trans Software Eng, Vol. 20, No. 4, April 1994, pp. 275-287.[11]	C. Jones, “Backfiring: Converting lines of code to function points,” Software, Vol. 14, No. 2, November 1995, pp. 87-88.[12]	T. J. McCabe, “A Complexity Measure,” IEEE Trans Software Eng Vol. SE-2, No. 4, December 1976, pp. 309-320.[13]	G. K. Gill and C. F. Kemerer, “Cyclomatic Complexity Density and Software Maintenance Productivity,” IEEE Trans Software Eng, Vol. 17, No. 12, December 1991, pp. 1284-1288.[14]	J. P. Baker, W. E. Bowen, and M. A. Harris: “Lessons Learned from Human-in-the-Loop HLA Implementation”  Proceedings 19th Interservice/Industry Training, Simulation and Education Conference, December 1997.Author BiographiesAnn Pollack is a Senior Engineer at The Johns Hopkins University Applied Physics Laboratory, Laurel, MD.  She is the lead designer and integrator of the Ballistic missile Localization and Selection Tool (BLAST).  Ms Pollack obtained her BES and MSE from the Johns Hopkins University (JHU) in 1988 in electrical engineering and computer science.  She subsequently earned an MS in electrical engineering in 1991 also from JHU.John Baker is a Senior Engineer at The Johns Hopkins University Applied Physics Laboratory, Laurel, MD.  He holds BSEE and BSCS degrees (1990) from Washington University in St. Louis and a MSEE from JHU (1995).  Currently, he is involved with sensor modeling and simulation efforts for Theater Ballistic Missile Defense projects.