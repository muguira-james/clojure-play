Real Time Composable SE Analysis, enAble.R3® Richard HepplewhiteQinetiQSt. Andrews RoadMalvern, Worcestershire.WR14 3PSUKRhepplewhite@qinetiq.comKeywords:Real Time, Analysis, Verification & Validation, DIS, HLA, data repository.ABSTRACT: This paper describes our “Real Time Composable Analysis tool”, enAble.R3®, for recording, reviewing and analysing simulation data. The principle objectives for the tool are to provide a common approach to all analysis needs across the range of synthetic environment domains and applications, and a generic data repository to promote data sharing and reuse. The tool uses a structured modular approach, which enables users to load any set of modules they require. The core application provides the necessary services to control the modules and provide a structured communications system between the modules. There are no inherent restrictions on the modules’ functionality, hence they could include simulations, C2 functions, planning aids, and so on. The repository based on an XML database, acts as a repository for any kind of data to be stored using predefined schemas. This removes the boundaries of data proprietary and access issues, greatly enhancing the scope of data re-use. This paper will discuss the development of enAble.R3® and the experience of its application to simulation problems. The project has been supported by the UK MoD Directorate of Analysis, Experimentation and Simulation with funding from Package 3A of the MoD Research Programme.Introduction‘Real Time SE Composable Analysis’ is a three year research project, funded by UK MoD, that aims to demonstrate the concept of a composable analysis tool that can be rapidly configured to adapt to new synthetic environments or analysis requirements. This project builds on the experience and ideas generated during the development of Proteus/Themis [ ref_Proteus 1]. Proteus is an analysis tool that was primarily aimed at verification of DIS based synthetic environments but which is now deployed at the UK Combined Arms Tactical Trainer sites as an After Action Review (AAR) tool. The much more flexible tool, enAble.R3, aims to fulfil more general user requirements across the community thus providing a consistent approach to analysis across SE domains and applications.BackgroundEffective analysis of SE activity is fundamental to gaining insight or answering specific questions. However, conducting this analysis efficiently and managing the output is problematic, for instance:SEs already produce large quantities of complex data, and the increasing use of federated SEs will generate even more data;SE data needs to be analysed in a timely manner to generate useful information; this analysis data needs to be stored, managed and retrieved in a consistent format (across domains and types of SE, e.g. land/air or virtual/live).Analysis needs are frequently user or application specific – for example, quick analysis in real time for training purposes, after the event for detailed analysis, post hoc analysis for identification of longer term trends or detailed capability comparisons. Currently many systems use “stove-piped” or proprietary applications, which are largely incompatible and address different aspects of the problem domain. The net effects include:difficulty in sharing analysis data between different systems, leading to difficulty in performing like for like comparisons across diverse SEs, particularly SE federations;additional time and expense in conducting trend analysis and capability comparisons;decreased effectiveness in training, system development or concept evaluation.The real time composable analysis project has demonstrated a real time analysis capability that addresses these issues, how the analysis and training of existing SEs can provide can be enhanced. This inevitably leads to reduced costs associated with supporting a large number of incompatible analysis/review tools. ObjectiveThe primary goals of this project are to demonstrate an analysis tool that:Performs analysis either during or after an event (simulation);Can be applied to different domains and types of SE  (e.g. land/air/sea, training/OA/ experimentation/V&V, virtual/live/ constructive);Store relevant information in a generic database repository;Provide an easy mechanism for creating new analysis algorithms, and data visualisation.To meet these objectives, a tool will be developed comprising three elements. The first is an application that will carry out data logging, analysis and visualisation. This will be developed to be “plug and play”, to allow the application to be constructed according to the user’s particular requirements. The second element is a database to be used as a repository for SE contextual information and analysis data. The third element will explore methods that potentially improve the ease by which data analysis can be approached. This tool will result in a unified approach that can be used for storing almost any data relating to SE based systems including analysis algorithms, analysis data, SE configuration information and user notes.User requirementsAlmost every SE application requires an analysis capability of some form or other. The particular function and needs vary according to the particular application and use of the SE. This requirement is equally present during SE development as part of an objective Test and Acceptance or VV&A activity. Therefore, a generic analysis tool must be capable of providing the functionality required to support this range of activities. Table 2.1 summarises the principal application areas, uses and key requirements.Application areaUsesRequirements/needsTrainingReal time exercise monitoringQuickly identify issues for debriefsIdentify training issues which need immediate corrective actionMonitor the exercise to prompt the controllers for manual inputAfter action reviewReplay previously recorded vignettes for illustration/examplesReplay elements of the exercise for reviewProvide objective evidence of performanceTake home packRequirements currently being definedEnd of season analysis/lessons learntSufficient contextual information requiredTrend analysis for training performanceOA/experimen-tation & acquisition supportObjective data gatheringExperimentation data to feed OA studiesStandalone studiesArchiving and retrieving dataStoring contextual information.PsychologicalMeasure human variabilityVerification and validationSE VerificationEncoding specification dataComparing performance against set standardsRecording performance dataTable 2.1. The principal application areas, uses and key requirements.Modes of operationAn analysis application could operate in one of three modes.Real Time – the analysis tool is collecting data, visualisation and analysing it as the scenario is executing.On-Line/Post Event – analysis is conducted immediately at the end of the execution, while the application is still connected to the SE.Offline – analysis conducted some time after the exercise has been conducted, possibly at a different location.Each of the user requirements in the table above has different requirements for operating mode. The specific impact of each user requirement on the operating mode is not covered in this paper. However, it is clear the operating mode has specific implications on the tools architecture.Integration IssuesWhen integrating an analysis system into an existing or new SE application a number of issues must be born in mind.Availability of Data. An analysis tool can only analyse the information to which it has access. For DIS based systems, often only information contained within the DIS protocols can be captured and used for analysis, effectively making the internal workings of the simulation models ‘black boxes’. Whilst this might not be so significant for training, it could limit the analysis required during experimentation,  Operational Analysis (OA) and verification and validation (V&V). The Higher Level Architecture, HLA, poses an even greater challenge for analysis for two reasons. Firstly, the information published by a model may be even more restricted than DIS. Secondly, unlike DIS, only updates are published, making random access into the log data difficult to accomplish. For systems based on proprietary data formats the availability of information depends entirely upon the provider. Thus when building any simulation, the developers should have a clear understanding of what the required analysis will be so that the appropriate information can be built into the simulation infrastructure. Attempting to do this post event could be very difficult and costly. The European Co-operation for the Long-term in Defence project, known as Euclid, has defined the Synthetic Environment Development and Exploitation Process (SEDEP)[ ref_SEDEP 2].  The SEDEP requires that an SE be designed to meet specific analysis requirements that are defined during the earliest developmental stages. Relevance of data. There could be situations in which two SEs expose data that would support the same analysis, but the nature or context of that analysis is not relevant. For instance, analysis to assess trainee performance could equally be applied to Computer Generated Forces, but it has no relevance in a training environment. This is an issue for the user to resolve.Comparative data. Storing analysis information in a central repository will allow comparative studies to be made. For instance, tracking trainee skill performance as they migrate from one training establishment to the next, or using analysis from experimentation to feed OA studies. However, it is vital that the source of this information is known to ensure that the information is comparable. For example, gunnery skills cannot be compared between a part task trainer to a high level tactics trainer. This is also an issue for the user to resolve. Requirements SummaryThe following summarises the capabilities that should be included in a ‘Real Time Composable SE Analysis Tool’.  No significance should be attached to the order in which they are presented because their relative importance will depend on the uses to which the tool is put.Training, experimentation and OA. The tool should be capable of simultaneously supporting training, experimentation and operational analysis requirements. Similar products are required for both training and experimentation although each has specific requirements that change depending on the objectives for the exercise or experiment.  OA requirements vary from being similar to those for experimentation and training to specific requirements for populating the large combat models with raw data from human-in-the-loop models/simulations.Combinations of live, virtual and constructive. The tool should be capable of analysing SE exercises in constructive, virtual and live environments, both individually and independently, and collectively, when used in combinations in the same exercise. Combinations of different types of synthetic environment have traditionally relied on all the data required for analysis being available in one or other of the SE’s. If an exercise were to be constructed using real and virtual components it would be assumed that all data relating to the live elements would be present in the SE.  Although this is still the case with most situations, valuable information is often lost in the conversion.All services. The tool should be capable of supporting all services both on their specific SE’s and when used together in joint exercises. More and more operations and hence training and experimentation requirements are emphasizing joint capabilities, linking the equipments of the respective services. Increasingly synthetic environment analysis must be capable of joint outputs combining and integrating the results of joint missions.Replay and quantitative analysis. The tool should be capable of providing both replay facilities and quantitative analysis. Replay facilities are generally required in addition to quantitative analysis with the ability to stop the replay and interrogate the data at that point.  In addition, live updating of analysed output whilst the replay is progressing would also be of use.Real-time capability. The tool should be capable of providing real-time analysis as a SE based exercise progresses. Real-time capability is required for the real-time monitoring of synthetic environment based exercises in order to trigger responses to situations and also for observers to monitor and provide pointers for post exercise debriefs, and to make adjustments to exercise play.Multi-site/multicast. The tool must be capable of analysing data from multi-site and multicast exercises. Multi-site exercises over WANs can lead to incomplete datasets at several locations.  This could occur for several reasons including security and network capacity.  Correlating and combining the logs from several sites is necessary to provide a complete log, enabling analysis of the complete dataset. Under certain circumstances, the data excluded from a site may be unnecessary for complete analysis, this is, however, unusual. Product varies with level of command. The tool must be able to cater for analysis output for different levels of command. Higher level commanders will not require the same output as a vehicle or system commander.  Vehicle commanders require information about the individual performance of their system and about individual engagements whereas higher level commanders generally require aggregated data. Long term archiving. The tool must aid the archiving of data, both in recording and in recall of the data. Data archives are required for a number of activities, crossing the training, experimentation and operational analysis bounds.  The training requirement is for data to support regular training trends analyses such as the annual observations from training report compiled by the Land Warfare Centre. The key issue with long term archiving is storing the context in which the data has been collected and some form of statement of what it can validly be used for. For OA, data is required to create baselines for experimentation and models.  It effectively defines the standard, against which, new techniques, doctrine, equipment etc can be tested.HLA FOM. The tool must provide assistance when building HLA based SE’s in the design of the FOM relating to the requirement for analysis of exercises. The latest version of the FEDEP and the SEDEP require that analysis considerations must be taken in to account at the design phase of an SE and the simulation engineered to support the required analysis. The tool should support this process by enabling the definition of analysis algorithms in a flowchart style format, as required by the SEDEP.Core applicationIn order to support a range of different applications and uses, enAble.R3 has been designed around a highly modular architecture. At the heart of the application is the ‘core’. The user provides the core a configuration defining the required functionality and the type of system or interfaces to be used. For instance, specifying a DIS based logger will load a particular module set while specifying an HLA based logger will load a different versions of particular modules. Using a configuration file the core finds modules that fulfils the specification, loads these modules, and then starts the initialisation and execution processes. Therefore, the capability of the application can very easily be changed or enhanced simply be swapping or adding new modules. This is conceptualised in Figure 3.1. EMBED PowerPoint.Slide.8  Figure 3.1. Composable architecture. Independent modules are loaded and used by a core application. Subject to a few unavoidable exceptions, the absence of a particular module will not prevent others from working. It is also possible to carry out the same analysis on different SEs by changing only those modules that are specific to each.The main element is the core, which serves two functions.Module management: finding, loading, controlling the execution of, and shutting down the modules.Providing the gateway for modules to communicate with each other. Internal application communications use a “run time XML”, a simplified XML specifically developed for this application, to improve run time performance.Basing module interfaces on textual descriptions (i.e. XML) removes specific machine or coding dependencies. Therefore, new modules can be implemented without necessarily requiring any compile-level dependency on other modules. However, for performance reasons, some modules may wish to form direct links with others. For instance, the Plan View Display (PVD) Module may wish to link to Terrain Module to improve the efficiency of terrain look-ups: converting all the features into a XML representation would be very time consuming. Therefore, the PVD may wish to have a direct handle to the Terrain Module. However, the particular terrain module used may vary between executions. Therefore, a generic interface must be deployed to avoid direct dependency between the PVD and a specific terrain module. Two possible approaches could be adopted:Define a set of objects that represent terrain features which can be passed through the internal communications. This is illustrated in figure 3.2;Use an interface object. Taking advantage of object hierarchies, a parent terrain class defines a standard set of functions that external modules may wish to access, then the terrain module inherits this object and provides the specific implementations. Thus no matter which version of the terrain module is loaded external modules always see an identical interface. This also requires the definition of standard objects to represent the terrain features.  This is illustrated in figure 3.3. EMBED PowerPoint.Slide.8  Figure 3.2. Pass objects that define terrain feature through the core. EMBED PowerPoint.Slide.8  Figure 3.3. Direct module communications used where high rates of data transfer must be supported.The former is certain to be too slow to support frequent access to large amounts of data, as would occur when refreshing entities on the PVD. For such high data rate communications direct communications, as described in the latter approach, will be required. Design discipline is required when implementing direct inter-module communications as it would be easy to introduce accidentally unwanted dependencies.A third mechanism has been used for communicating between modules based on a call back approach. This allows module A to register to receive events automatically from module B. In this case, module A sends a command to module B requesting to register a function call into a callback array (C function address or C++ object member function).The enAble.R3 architecture therefore comprises of three main elements:Configuration information describing the requirement elements;A core application responsible for module management and communications;Standalone modules, which provide the actual functionality.Currently, enAble.R3 can interface to modules created as DLLs, for ease of implementing a demonstration system. However, the framework can easily be extended to include other mechanisms such as sockets, CORBA, COM, DCOM, SOAP, and so on. DatabaseRequirementsThis component of the project is to investigate the design and implementation of a data storage module for storing all necessary information relating to analysis carried out in an SE. The module should be designed for use with any SE, and automatically cope with changing data requirements.There are two parts of the module to consider. The first is to decide what data needs to be stored and how to capture that data from the SE. The second part is to design the repository so that it satisfies the following requirements:All necessary information needs to be stored for future reference, such that the conditions can be re-created, and understood in the context they were generated;The information stored should be self-contained so that the data is re-usable;Not all the information has to be stored internally in the repository – in some cases, particularly for large data log files, it is not practical to keep the data “online”, therefore it must be acceptable to record only the location of the data;The repository must be able to cope with a wide range of data with varying formats;Related information in the repository must link together. This is necessary to ensure that the data is accessible and re-usable;However the data is stored it must be done seamlessly by the application: the process must be automated as far as possible.Repository solutionIt was decided that the data to be stored in the repository should be in XML format [ ref_XMLStandard 3] because XML is extensible, platform independent, and easy to use. The repository solution must therefore be able to store XML data. There are two options for achieving this: the data can be kept in a relational database or in a native XML database. When deciding which type of database to use in the project, the main consideration was the requirement that the repository must be able to cope with sets of data with different formats. This is called semi-structured data. In relational databases, the data is stored in rows in structured tables. Data has to be ‘mapped’ into these tables when it is stored and a separate table is required for each different data format. Hence, relational databases are not well suited to store semi-structured data. Native XML databases are specifically designed to store XML documents intact, so the data does not need to be manipulated in any way when it is archived. Related documents can also be grouped together and partitioned from other groups using collections. Native XML databases are aware of XML structures and formats, allowing them to access an element at any level in the document hierarchy. This gives them powerful searching capabilities. It is for these reasons that a native XML database was chosen as the repository solution. For this project a free database solution was selected. The principal free native XML databases are eXist[ ref_eXist 4] and Xindice[ ref_Xindice 5]. For this project eXist was chosen because (at the time) it offered more access options than Xindice.Implementation overview       The database for this project has been designed to use collections to group together documents with a similar format; this is used to separate types of information such as analysis data, user information, contextual data and so on. This makes searching the database quicker and more efficient, since only the relevant collections need to be searched. There are several possible extensions to the current database design. Analysis modules could be stored in their own collection of the database. Once a new module has been created it could be registered with the database and stored. When the analysis tool starts up it could interrogate the database and return a list of available analysis modules. At any time during analysis the user can select to view the analysis results. An XML document containing the results is processed by the analysis tool and the results presented to the user in a suitable form (graph, table, chart, etc). The user can chose to save them to the database simply by clicking a button. The database API takes the XML document, constructs an appropriate filename from the internal data and adds it to the database. Hence, once the user has requested to save the results in the database they are not required to do any more work. Given that users of enAble.R3 will have the opportunity to produce their own analysis it is necessary to provide checking of the XML results documents to ensure they are correctly generated before they are added to the database. Two types of checking are available when processing XML which are both used. Documents should not deviate from the XML standard [ ref_XMLStandard 3], and there is the option to validate documents against a set of rules, to ensure all the necessary data is present and no unexpected elements are present. Currently, the project is using the Microsoft™ XML parser MSXML™ 4.0 [ ref_MSXML 6], although Xerces C++ [ ref_Xerces 7] was originally considered. Schemas [ ref_Schemas 8], which define the data format and structure of XML documents, are used to validate the XML documents. These are written using xmlspy® [ ref_xmlspy 9]. Each collection has a schema associated with it (also stored in the database).Access to the database is via an application programmer’s interface (API) or a web interface. Figure 4.1 illustrates the server software configuration. The components of the web server have been separated to facilitate maintenance; hence it is significantly easier to upgrade just one element. In addition, the functionality of the server can be easily expanded to include additional web applications and services. EMBED PowerPoint.Slide.8  Figure 4.1. Structure of the data repository server software.Web InterfaceThe database repository provides a web server capability, allowing access to the repository data using a standard web browser application. To demonstrate this capability, a set of web pages has been created that enable a user to search the repository for particular information. For instance, search parameters may include: particular kinds of analysis, types of simulation, trainee organisation and so on. The server returns a list of relevant entries, which the user can view. In order to review analysis data, the results display developed for enAble.R3  have been compiled as activeX components, and hence can be included as part of any web page. This enables the results to be displayed in a consistent manner between the tool and web interface.SummaryCurrently a working version of enAble.R3 has been developed and demonstrated recording, replaying and analysing data in real time and post event. enAble.R3  has been interfaced to both DIS and HLA (RPR-FOM) simulations systems, but in principle can capture data from any data source. Replay functionality includes real time visualisation and monitoring, fast forward and rewind, and speed control. Additionally, previous parts of a simulation can be reviewed whilst recording. The simulation can be visualised using a highly configurable plan view display system, to the extent of terrain, simulation objects and overlay drawing are entirely changeable (through the use of modules). An experimental ‘tactical 3D PVD’ has also been demonstrated which has been shown to be of significant benefit to particular applications.  A range of analysis has also been developed in the land and air domain ranging in complexity from simple statistical analysis (e.g. rounds fired), performance characteristics (e.g. aircraft performance) to more sophisticated analysis (e.g. determining possible fratricide events). A number of real time indicators have been developed to provide simple instantaneous feedback to the user of the analysis. For example, during training, these can be used to indicate how well the trainees are performing, what stage of the training exercise has been reached and so on.  Detailed representation of the analysis is similarly available, providing more detail, and these automatically update as analysis progress through the simulation data. Results from analysis have been added to the database, and reviewed through a standard web browser application. The results viewer module of enAble.R3 uses ActiveX controls, hence, these have been re-used when viewing results from the database, using a web browser.Demonstration of enAble.R3 to a number of stakeholders has generated significant interest to use the tool in a range of application areas such as verification and validation, experimentation, and training review (After Action Review).ConclusionsUser requirementsKey military requirements have been identified for an analysis tool that will provide:real time operationquantitative analysis and monitoring both during and after an exercisevisualisation and replay of an SE exercisefacility to merge simulation data from different siteslong term archiving of analysis and contextual dataassistance with designing FOMs in support of analysis on HLA based SEsIt is important to be able to deploy the tool across a variety of SEs for a range of purposes. Flexibility of use in the context of a SE analysis tool can be defined by the requirements to:provide analysis aimed at different levels of commandsupport training, experimentation and operational analysisgenerate analysis relevant to the Army, Air Force and Navyanalyse constructive, live and virtual SEsComposable analysisA working software tool which can be dynamically composed according to user or application requirements has been built and demonstrated working in a number of different environments. This has shown that a generic tool, can be specialised to work with different applications and fulfil particular user requirements. Thus providing the advantages of a common analysis approach, a consistent methodology for representing and storing data, highly effective method of re-using components. A key observation with the architecture developed, is that enAble.R3 is not limited to “analysis” functions, but could include many other functions, such as exercise management, simulation components, and digitisation infrastructure. Providing a framework by which many of the individual SE elements can be brought together facilitates building complex systems in a consistent fashion.Data repositoryA native XML database (eXist 0.8.1) has been chosen as the repository solution for the project and Microsoft™ MSXML™ 4.0 has been chosen as the XML parser. eXist 0.8.1 comes with a servlet server, Tomcat, produced by the Apache Software Foundation,  which has been used to set up a demonstration analysis results web server based on XSP pages. Schema for the analysis results documents have been written using xmlspy™ and the entire process of carrying out analysis, validating results, entering them in the database and retrieving them using a web browser has been demonstrated.  References[ set ref_Proteus seq REFERENCE 1 1 ref_Proteus 1]	Hepplewhite. R., Rigg, G., Morley, R., “Themis: The Objective Assessment of CGF Performance”, Ninth Conference on Computer Generated Forces, pp 133-138, May 2000.[set ref_SEDEP  seq REFERENCE 22 ref_SEDEP 2]	Ford, K., Payronnet, P.,  “The Euclid RTP 11.13 Synthetic Environment Development and Exploitation Process”,  HYPERLINK "http://www.euclid1113.com" http://www.euclid1113.com[ set ref_XMLStandard  seq REFERENCE 3 3 ref_XMLStandard 3]	The XML standard,  HYPERLINK "http://www.w3.org/TR/REC-xml" http://www.w3.org/TR/REC-xml[ set ref_eXist  seq REFERENCE 4 4 ref_eXist 4]	The eXist project home page, http://exist-db.org/[ set ref_Xindice  seq REFERENCE 5 5 ref_Xindice 5]	The Xindice project home page,  HYPERLINK "http://xml.apache.org/xindice/" http://xml.apache.org/xindice/[ set ref_MSXML  seq REFERENCE 6 6 ref_MSXML 6]	The Microsoft™ website,  HYPERLINK "http://www.microsoft.com" http://www.microsoft.com[ set ref_Xerces  seq REFERENCE 7 7 ref_Xerces 7]	The Apache Xerces C++ project home page,  HYPERLINK "http://xml.apache.org/xerces-c/index.html" http://xml.apache.org/xerces-c/index.html[ set ref_Schemas  seq REFERENCE 8 8 ref_Schemas 8]	The standard for Document Definition Markup Language that is used in XML Schemas  HYPERLINK "http://www.w3.org/TR/NOTE-ddml" http://www.w3.org/TR/NOTE-ddml[ set ref_xmlspy  seq REFERENCE 9 9 ref_xmlspy 9]	The xmlspy® home page,  HYPERLINK "http://www.xmlspy.com/" http://www.xmlspy.com/Author BiographiesRICHARD HEPPLEWHITE has a first from Oxford in Engineering and Computer Science, and has been working at QinetiQ for the past eleven years. During this time he has worked on a variety of simulation related topics, such as protocols and networks, Artificial Intelligence and Computer Generated Forces, Verification and Validation of a major UK trianing system, After Action Review and trainee performance assessment capabilities. He is currently the technical lead for developing analysis software tools.