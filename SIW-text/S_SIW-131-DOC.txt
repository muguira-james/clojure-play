Developing and Integrating Advanced Human Performance Models With Mission-Level Simulations:  Progress And Lessons LearnedEdward A. Martin, PhDDavid G. HoaglandAFRL/HECIHuman Effectiveness DirectorateAir Force Research Lab937-255-8072, 937-656-7013Edward.Martin@wpafb.af.mil, David.Hoagland@wpafb.af.milBryan E. BrettScience Application International Company4031 Colonel Glenn HighwayBeavercreek, OH 45431937-431-4390BrettB@saic.comNils D. LaVineMicro Analysis & Design, Inc.4900 Pearl East Circle, Suite 201EBoulder, CO 80301(303) 442-6947nlavine@maad.comKeywords:Decision-support, System analysis, System engineering, Man-machine interfaces, Human operator modelingABSTRACT:  The Combat Automation Requirements Testbed (CART) Program, within the Air Force Research Laboratory’s Human Effectiveness Directorate, is developing the tools and process that will enable the representation of goal-oriented human performance in High Level Architecture (HLA) compliant constructive simulations.  CART will provide a human performance modeling capability that supports:  (1) representation of information seeking and attention sharing, (2) representation of workload effects on performance, (3) systematic manipulation of key performance attributes (e.g., time, accuracy, task prioritization) by the user, and (4) HLA compatibility.  This paper provides an overview of the CART program, specific technical objectives and tasks, a description of the CART process, and updates on progress and lessons learned in accomplishing the integration of realistic human behavior into Department of Defense-accepted simulation environments using the HLA.1.  Introduction“All weapon systems, including uninhabited vehicles, depend on Human Systems technologies—there are no unmanned systems.”[1]1.1  Problem StatementAnalysts and decision-makers rely heavily on constructive simulations of a system in its intended environment to help translate mission requirements identified by the warfighter into system performance requirements.  Within constructive simulations, sensitivity analyses are conducted on key subsystem attributes by selectively varying attribute levels and measuring the results on mission performance.  In this way, performance levels are identified for key subsystem attributes that yield desired levels of mission performance  thus providing the basis for statements of system requirements.Unfortunately, consideration of the crew interface as part of the system has usually been avoided in these requirements generation efforts.  The acquisition community continues to use expensive man-in-the-loop simulation partly because of their inability to model crew member behavior and human-computer interactions within constructive simulations.  Hence, crew interface requirements are not quantifiably linked to the set of overarching measures of weapon system effectiveness as are the other subsystem-attribute requirements.  Not only can the lack of realistic consideration of the operator on system-level performance requirements lead to crew interface inadequacies, but it can also drive performance and cost unnecessarily because the simulated system did not represent appropriate tactics.  One military analyst recently noted that, “Every single analysis that I have ever seen has suffered from the lack of capturing smart tactics.  Mistakes such as pursuing an attack when the tactic should have been ‘run away’ lead to mission outcomes (aircraft loss) that seem to indicate system deficiencies when in fact the system was misused tactically.”[2]  Analysts and decision-makers need a means to readily model and understand the effects of human performance on total weapon system effectiveness when translating operational requirements into system requirements, and need to be able to visualize these effects at different levels of aggregation.[2]1.2	The Combat Automation Requirements Testbed (CART) Program:  A Brief OverviewTo address the problem outlined above, the Air Force Research Laboratory’s CART Program objectives are to provide a tool that permits users of constructive simulations to readily develop and integrate human models that can appropriately influence exercise results.1.2.1  CART program objectivesThe objectives of the CART Program are to:  (1) advance the state-of-the-art in human modeling using interoperable simulations and practices, such as High-Level Architecture (HLA) [3], (2) demonstrate a robust human modeling architecture that is compatible with current and future Department of Defense (DOD) simulations, (3) link operator performance with mission effectiveness, and (4) provide the capability to trace cause-and-effect relationships during or after simulation runs.1.2.2  CART program toolsThe CART program will extend current constructive modeling and simulation (M&S) testbed capabilities by providing two new tools for generating crew interface requirements.  One is a human performance modeling capability.  With this tool, analysts will be able to create models that simulate activities operators would perform in a system.  Analysts also will be able to parameterize the models to reflect different levels of operator capability.  These human performance models will be integrated with constructive models of a system and interact with the system in the context of a simulated mission.  The second tool provides performance assessment capabilities.  This tool supports generation of measures of operator performance.  Operator measures will be linked to measures of system performance and mission effectiveness.  With this tool, relationships between operator performance and system and mission performance can be visualized and traced.  Levels of operator performance that are required to produce desired mission outcomes can be identified.1.2.3  CART program tasksThe current phase of the CART program consists of six tasks.  Each of these tasks are discussed in greater detail in Hoagland et al. [4] and Brett et al.[5].  The objectives of each task are summarized briefly below.Task 1:  Crew System Requirements Establishment.  Characterize the current acquisition process for DOD acquisition programs, highlighting the nominal extent to which crew interface requirements are analyzed and established.  Essentially, Task 1 was to identify the ‘user’ of CART, the environment and processes in use today, and the most appropriate niche for CART in the acquisition environment.Task  2:  Human Modeling Architecture.  Define a human modeling architecture to be integrated with a selected military simulation.  Develop a Human Performance Model (HPM) and an interface to an engagement-level simulation that characterizes the flow of information to and from the human operator and provides the appropriate human control interactions to the constructive system model.Task 3:  Conduct Trade Studies.  Conduct two trade studies to select the two most appropriate operational contexts from among various predefined military domains of interest.  Identify active acquisition programs within the two different topic areas that are suitable for use in the two different Case Studies to be conducted under Task 5.Task 4:  Real-Time Operational Mission Simulation.  Modify and prepare a mission simulator for test to represent the system and environment for the domains of interest selected in Task 3.  Add the needed data collection capabilities for computing mission-level measures-of-effectiveness (MOEs) down through lower-level human performance measures.Task 5:  Conduct Two Case Studies.  Conduct both constructive and virtual simulation tests for identical mission tasks within the mission contexts selected under Task 3.  Compare the constructive and virtual simulation results to demonstrate the efficacy of the human performance modeling architecture and interface.  This activity will allow the results of human-in-the-loop simulation to be compared to those of a constructive simulation.  This Case Study Task represents the heart of activity in CART.Task 6:  Prepare Testbed Definition.  Develop a CART performance specification that defines an implementable testbed.  The specification will include guidance regarding how to integrate equipment or instrumentation in a simulation, the software architecture description, Runtime Infrastructure (RTI) implementation, and a reconfigurable hardware architecture methodology.2.  CART Program AccomplishmentsProgress and lessons learned are presented in this section for each of the tasks.2.1	Task 1:  Crew System Requirements Establishment.Task 1 has been completed.  Task 1 focused on CART’s application of human performance modeling and its integration with the system acquisition process via today’s Simulation-Based Acquisition (SBA) process.  However, during Task 1 it was determined that CART will be more than simply a human performance modeling technology.  It will also be a process for applying that technology in conjunction with other system modeling and evaluation activities to generate system and subsystem (including crew interface) requirements.  The CART Process derived under Task 1 is discussed further in Section 3.In the process of identifying the current state-of-the-art for constructive simulations and how system requirements are currently derived, Task 1 uncovered a need for three fundamental capabilities.  These are needs for:  (1) techniques for decomposing missions that support linking operator performance metrics to mission-level measures-of-effectiveness, (2) techniques that allow human performance models to interact with constructive simulations, and (3) tools that allow analysts and decision makers to visualize and trace performance metrics from the operator’s level up through mission outcomes.  Conceptual approaches for satisfying these needs are included in the CART process.Task 1 also identified the concept for CART’s role within the DOD acquisition process, how CART-derived requirements would be generated and documented, and how they would be included in the design process.  This is documented in references [2] and [5], and not discussed further here.2.2  Task  2:  Human Modeling ArchitectureTask 2 has identified an architecture that builds upon a tool developed by the U.S. Army Research Laboratory called the Improved Performance Research Integration Tool or IMPRINT.[6]  IMPRINT, together with its CART extensions, will provide a human performance modeling capability that supports:  (1) representation of information seeking and attention sharing, (2) representation of workload effects on performance, (3) systematic manipulation of key performance attributes (e.g., time, accuracy, task prioritization) by the user, and (4) HLA compatibility.  The architecture is discussed further in the following paragraphs.Task 2 also identified concepts for the performance assessment capabilities supporting:  (1) mission decomposition in a way that links operator-performance metrics to mission-level MOEs, and (2) performance assessment and visualization.  These concepts are discussed further in Section 3.2.2.1  CART HPM architectureThe basic architecture identified for integrating human performance models into constructive level simulations is shown in Figure 1.  The human performance-modeling environment will be a hybrid of two approaches to human performance modeling: task network modeling and first principle modeling.Task network modeling will be the core human-performance modeling method.  Task network modeling breaks the human performances of interest into a series of tasks, each characterized in terms of performance time, accuracy, and probability.  Tasks are linked together into networks that represent sequences and paths performance can take.  Within CART, the IMPRINT tool will be used to provide baseline task network modeling capabilities, and extended to provide representation of the goal-oriented nature of human performance and to communicate with external models via the HLA.[2][4][5]  The CART architecture internally supports representation of human information seeking, along with a goal-prioritization structure.  These architectural requirements are derived from two fundamental assumptions.  The first is that operator success is achieved by meeting mission performance demands that are levied by factors external to the operator.  If the operator does not meet the demands, mission performance can be degraded.  The second assumption is that, in meeting demands, the operator functions as an information-processor as illustrated in Figure 2.[7]  In order to successfully meet mission demands, the operator must determine which demands are impinging at a point in time, prioritize them if there are multiple demands, and then act to meet those demands.  Perception of demands is an active process in which the operator purposefully seeks specific information about current demands.  Since multiple demands can be active simultaneously, a mechanism is needed to sort among concurrent demands to chose which one(s) get serviced first.  The model assumes that in a given system-mission environment the operator has an internal goal structure that helps him assess and prioritize demands to be met.  These goals are associated with functions that must be performed successfully to accomplish the mission.  In goal-state evaluation, information from the environment provided by the perceptual processes is compared with internally held knowledge of expectations about world states and rules for determining when a goal state becomes active.  When goals become active, attention turns to selecting a course of action for bringing the current state of the world into the desired state.  Course-of-action selection can involve a variety of levels of cognitive processing (e.g., skill-based, rule-based, or knowledge-based reasoning).  It can also involve other perception and action components that are applied to gain additional information needed to select a course of action.  Once a course of action is selected, it is implemented and its effect on the environment is observed.  Course-of-action implementation generally involves motor activity (e.g., manipulate a control or throw a switch).  Observation-of-effect is performed by the perceptual capabilities, which, in turn, feed the goal states and the cycle repeats itself until the desired state is achieved.  The implementation of this modeling approach is discussed in more depth in references [2] and [5].Users will also have the capability to augment the task network models with external first-principle models that provide high-fidelity representations of human performance.  Essentially, tasks in the task network will call first-principle models that represent relevant aspects of task performance.  The first-principle model will then execute, and return the parameters required by the task network model using the communications architecture described below.2.2.2  CART HLA interfaceThe communications link between models will be HLA compliant.  Data will be passed between architecture components using the HLA RTI as illustrated in Figure 1.  The task network model will receive data regarding system and mission status from the constructive system simulation and data about the external world (e.g., missile launches) from the mission environment models.  Actions to be implemented by the system (e.g., maneuver, target designation, weapon launch) will be passed to the constructive simulation by the task network model.  Similarly, the task network model will use the RTI to pass data to first principle models to initiate them and receive the results of first-principle model execution.New HPM Simulation Object Models (SOMs) will be required as CART is applied to human performance modeling in different system and human-performance domains.  For example, the data that a HPM of a tactical fighter pilot needs and produces will be very different from that of an infantryman.  Consequently, different SOMs will be developed for these environments.  We expect that  over time  classes of human SOMs will emerge that are used by communities with common domains of interest, and that these SOMs and associated models will be shared within these communities.CART’s HLA interface will employ the Real-time Platform Reference Federation Object Model or RPR FOM.[8]  While CART modelers will be able to readily access entity state data directly available in the RPR FOM, CART will make extensive use of the FOM’s Simulation Management (SIMAN) Interactions capability.  SIMAN Interactions will be used to pass HPM-unique data (e.g., information displayed on operator interfaces and inputs to operator controls) between the HPM and the system it is controlling.  A graphical user interface will enable a user to define SIMAN Interaction data packets for a given HPM during model development.  The CART RTI middleware will use these SIMAN definitions to conduct the data exchange with the constructive simulation.  Thus, CART RTI middleware can remain constant while the HPM changes or as the CART tool is used to develop new models and integrate with new constructive simulations.The CART team chose to utilize the RPR FOM over creating new FOMs or adopting an Agile FOM Framework (AFF) for the following reasons:  (1) using the SIMAN Interactions, the RPR FOM can quickly and efficiently be adapted to send much of the non-standard data that will quite often be communicated in a CART Federation, (2) with the RPR FOM, CART users do not have to possess the programming skills required to develop and maintain middleware code, (3) the AFF only works if a Federate’s SOM is similar to the FOM of the Federation (for the mapping to work, the SOM must have a corresponding concept within the FOM), (4) the RPR FOM is well thought out, tested, and reliable, (5) the RPR FOM is available now with no development or maintenance expenses, and (6) the RPR FOM will most likely be the FOM of choice for many federates with which a CART user will want to interact.2.3  Task 3:  Conduct Trade StudiesCase studies are to be conducted to provide a basis for comparing mission outcomes with a constructive pilot to those with live pilots in an operationally relevant simulation environment.  Each case study will conduct a series of constructive simulations (using a constructive HPM), and compare the outcomes with the results of virtual simulations (using experienced pilots).  Task 3 conducted a trade study to select an appropriate simulation topic area for the first CART case study.  Selection criteria included human performance complexity, availability of existing system and environmental models, the CART team’s simulation-domain expertise, and program affiliation with SBA initiatives.  It was determined that the Command, Control, Communications and Computer Intelligence, Surveillance and Reconnaissance (C4ISR) domain was clearly the best candidate, and it was decided to use a time-critical-target (TCT) scenario for which some virtual simulation data were available from recent studies.  CART’s first case study will be conducted in the SIMulation & Analysis Facility (SIMAF) at Wright-Patterson Air Force Base this summer.  This case study will involve the integration of a CART HPM with the Simulated Warfare Environment Generator (SWEG) and an aircraft system model currently being used in SIMAF studies.2.4	Task 4:  Real-time Operational Mission SimulationThe required mission decomposition activities (see paragraph 3.1) have been completed.  Mission decomposition was conducted in a way that supported clear linkage of operator performance to mission-level MOEs using concepts developed under Task 2.  For this case study, these concepts were implemented using commercially available products such as VISIO and Microsoft Excel.  Task 6 will capture the lessons learned during the conduct of the two CART case studies as a specification for an implementable testbed toolset.CART engineers are currently working in the SIMAF facility to understand the hardware and software that is in place, and to develop the middleware necessary to allow the HPM to run within the SIMAF’s constructive simulation environment.  In order to assess the impact of varying levels of human performance on the broader system and mission performance, constructive system and mission environment simulations are needed that can respond to changes in human performance.  For our first case study, we found that the constructive mission models being used in SIMAF could not represent important human-system interactions (e.g., maneuvering to defeat a threat, interaction with an auto-replanner, management of shoot lists, or complex sensor employment in target acquisition).  Consequently, we decided to modify and employ the SIMAF Fighter Requirements Evaluation Demonstrator (FRED) for use as our constructive system simulation because it represents the full range of capabilities required by our human performance model.  This aspect of interfacing CART with constructive simulations can present some major challenges, and will typically require some development of facility-specific software since simulation implementation details vary from facility to facility.  There should be a plethora of lessons learned derived from the integration efforts of this case study.  These will be becoming available as the constructive simulation runs begin this summer.2.5  Task 5:  Conduct Two Case StudiesThe CART team is in the process of developing and verifying the CART extensions to IMPRINT and the interface of CART with the SIMAF simulations.  Verification tests have been defined for the extended IMPRINT stand-alone as well as for the total CART system (the latter will demonstrate the capability of a HPM to interact with and control a constructive SIMAF simulation).  Since the extended IMPRINT is not yet available to actually begin the HPM development, the IMPRINT verification tests are being designed to exercise required functionality needed for the integrated system as a way of catching potential problems early enough to avoid schedule impact.2.6  Task 6:  Prepare Testbed DefinitionTask 6 is documentation activity that will not begin until late in the program.3.  The CART ProcessThe process for implementing CART within an acquisition program was developed under Task 1, and is illustrated in Figure 3.  The process derived is consistent with the methodology for developing human behavior representations recommended in Pew and Mavor’s Methodological Issues and Approaches chapter.[9]  Activities within each of Figure 3’s blocks are summarized below.  Additional details can be found in reference [5].  3.1  Mission DecompositionBefore the modeling development begins, a series of decomposition activities must be conducted to identify key operator tasks that need to be represented in the human performance models and to identify how performance on those tasks relates to overall mission outcomes.  These activities include a means-ends decomposition, specification of goal state definition and interaction, information-decision-action analysis, and MOE extension and data collection definition.The means-ends decomposition is structured as an “abstraction hierarchy,” similar to that proposed by Rasmussen et al.[10]  This represents various attributes of the system, from its highlevel purpose down to the physical components required to perform any actions, as illustrated in the left-hand side of Figure 4.  The CART approach incorporates a goaldirected representation of human behavior, therefore the next step involves specification of goal state definition and interaction.  Each goal state identified in the means-end decomposition is given a priority and a brief description and the events that trigger its onset and offset are specified.  Within CART’s implementation of the human performance model, filters will monitor for these events and trigger the appropriate operator goal states when they occur.  Information-decision-action analysis involves the identification of essential information elements, decision logic, and actions for each task.  This is done in conjunction with the mission decomposition,  and relate to the operator’s perceptual, cognitive, and motor activities for the task.  This information helps model developers identify the required inputs to  and outputs from  the human performance model, as well as the decision rules to be implemented.The MOE extension and data collection definition activity consists of developing a specific set of metrics that quantify performance for each function, task, and subtask identified in the means-ends decomposition, and the data collection requirements for the simulation runs to support evaluation of these metrics.  In specifying metrics, the objective is to provide an assessment of the critical attributes or dimensions of performance of a function, task, or subtask expected to influence mission success or failure.  The resulting MOE hierarchical structure maps directly to the hierarchical components of the means-ends decomposition as illustrated in Figure 4, and ensures that the highlevel MOEs that the warfighter has deemed most important can be traced directly to MOEs of the lowest-level task performance.  In Figure 4, MOE extension begins at the generalized-function level.  The general function Acquire Targets is measured by the percentage of targets acquired and the time to acquire the targets.  At the task level, the task Designate / Accept Aimpoint is measured in terms of percent targets designated and time to complete designation.  Finally, at the subtask level, the metrics center on quantifying performance of Slew Cursor to Desired Pixel within the designation task.3.2	Human Performance Model Development and IntegrationOnce key functions, tasks, subtasks and equipment essential to mission performance have been identified by the mission decomposition activities, development of human performance models and integration of those models with the constructive system and mission environment testbed can begin.  Each of these activities is described below.The CART concept employs a task network modeling approach to HPM development.  Using a modified version of the IMPRINT task network-modeling environment, users begin to develop a series of task networks representing the functions, tasks and subtasks identified in the mission decomposition.  Associated with each node of the task network is a set of userdefined parameters in which a number of performance values and ranges can be specified  including the task time, accuracy, and ratings of operator workload associated with the task.  The workload ratings are multidimensional, allowing the user to assign ratings independently across the visual, auditory, cognitive and physical dimensions of workload.  In developing the human performance component of the task network models, users can draw upon a number of sources to assign values to the human performance and workload variables.  First, they can draw upon micro-models resident within IMPRINT.  These internal first-principle models of human performance contain empirically-based measures of human performance for several basic perceptual, cognitive, and motor activities.  For more complex or systemspecific tasks, users can rely upon empirical data in the human performance literature that address the specific task of interest.  Where simulation resources are available, performance data can be obtained from human-in-the-loop simulation.  Finally, when no such data are available, users must rely upon performance estimates from operational subject matter experts.Human performance models generated using the CART approach will not be general models of human behavior applicable to all scenarios and tasks.  Rather, they will be developed to represent human actions and decisions within the specific scenario(s) defined by the acquisition program.  This scenario-specific modeling will allow a maximum level of model detail, resulting in a greater degree of model fidelity for a relatively low level of effort to develop the model.  However, the ease of use of the IMPRINT interface and the relative simplicity of task network modeling mean that CART models developed for one purpose can be adapted readily and extended to support new or changing study requirements  an initial investment in a model can be leveraged extensively as the model is modified and reused.HPM integration efforts will focus on integrating the human performance models to run within the simulation program’s constructive simulation environment.  The CART concept assumes that the program is using the HLA framework.[3]  The majority of the CART integration effort will center on design and development of the federation.  Key integration activities will include:  (1) ensuring that the HPM SOM identifies all data required by the HPM, (2) defining the data packets to be exchanged between the CART middleware and the RTI, and (3) the development of HPM data middleware for the constructive simulation environment to communicate with the RTI.  In the event that the human performance model requires input data that are not available from any other models in the federation, a decision must be made whether to modify one or more of the other models to provide the required data or to modify the human performance model to eliminate the data requirement.  3.3  Constructive Simulation and Data AnalysisUpon completion of the model integration activities and subsequent testing, the federation will be ready to support the acquisition program’s simulation activities.  In preparation for testing, a matrix is developed that outlines all of the different test cases that are to be examined in the simulation trials, specifying the levels of each variable to be manipulated.  Multiple simulation runs are then performed within each particular test case.  The savings in time and cost that can be realized by using constructive human representations instead of live subjects is one significant advantage that this approach offers.  Further, because trials can be run faster and easier with constructive models  with greater variability of initial conditions  greater numbers of trials can be run within each test condition, leading to greater statistical reliability of simulation results.As the simulation trials are run, data collection routines collect all of the required performance measures necessary to calculate the expanded MOEs described earlier.  Explicit and derived MOE data are then compiled for each test condition and formatted for subsequent data analysis.CART’s implementation of a human performance representation within the constructive simulation environment will not change the highlevel goals and approach of the traditional data analysis used in generating requirements.  That is, the analysis will still focus on identifying differences in mission outcomes (reflected in missionlevel MOEs) achieved by the various system concepts being addressed.  The CART approach will simply allow a more detailed data analysis, providing traceability of mission results to particular task performance and crew interface components.  If differences in mission-level outcomes are identified, the analysis then turns to the lower levels of the MOE hierarchy to identify which function, task, and subtask performance measures best explain the differences in mission outcome.For example, the hypothetical analysis shown in Figure 5 found that the use of Radar System B in the weapon system resulted in a greater degree of mission success than Radar System A.  At the general-function level, it was found that varying the radar system had the greatest impact on the target acquisition function.  Turning to the tasks that support the target acquisition function, Designate / Accept Aimpoint performance is seen to vary substantially between the two radar systems.  Finally, the best explanation for designation performance differences is the Cursor Slew Time and Cursor Slew Accuracy.  With this insight of how low-level task performance can significantly impact overall mission outcomes, the analyst is able to suggest focused, performance-based requirements for the crew system interfaces used in performing the key tasks and subtasks.3.4  Requirements DevelopmentBased on the identified linkage between cursor slewing performance during target designation and overall mission outcomes illustrated in Figure 5, a performancebased requirement for aimpoint designation can be specified.  For example:Requirement.  The crew interface for the targeting radar system shall support target designation accuracy to within 1.5 pixels, and target designation times not to exceed 5.0 seconds / target.A requirement such as this specifies a given designation accuracy and time that the crew interface must support in order to achieve the desired level of mission performance, without specifying any particular physical design or detailed characteristics of the system.  Such a requirement, developed relatively early in the acquisition program, would provide a clear metric against which crew interface designers could later evaluate various design options.  This would also serve to limit the search space so that subsequent humanintheloop simulations would be able to focus on the most promising crew interface concepts.4.  Discussion and Conclusion“Models will be used only if they are easy to run and modify to meet the changing needs of the user organization.”[9]As M&S technologies continue to make rapid advancements, their utility and value to the acquisition process continue to increase.  However, there is still room for improvement in both the tools and processes being implemented to support acquisition.  Care must be taken not only to assure that the tools provide a useful product, but that the tools themselves are useable by the target user population  and that the process does not impose undue additional burden on the user.  Development of a suite of tools that permit analysts and decision makers to consider the human during the early constructive simulation-based trade studies  and to understand how performance at the crew-interface level impacts mission outcomes  is a major challenge in itself.  Nonetheless, regardless of how useful such a tool may be, it is doubtful that the tool will be used if it is also not readily usable.An AGARD Working Group was recently chartered to address the issues associated with using and developing human performance models.[11]  This Working Group targeted military and industrial organizations involved in the specification, procurement, design, qualification and certification of military systems where the human contribution impacts mission effectiveness.  They addressed some of the pragmatic issues associated with the fielding of HPMs, and provided guidance regarding HPM usability by system engineers.  Some of the key recommendations put forth by this Human Performance Modeling Working Group include:Ensure human performance data are in a form meaningful to the overall system design process (i.e., don’t just provide workload or Situational Awareness metrics)Work towards tools that provide data relevant to understanding system performanceUse scenarios to evaluate total system performanceDevelop and use a detailed concept of use for the system throughout design to assess fitness for purposeIntegrate models with existing systems engineering tools / modelsUse standard interfaces to facilitate data exchangeMake the system easy to useReduce the burden of data collection by offering default values, etc.Develop libraries of human performance dataStandardize data storage and handling characteristics to allow data exchange between subsystem modelsHuman performance tools should be reliable, robust, easy to use, and supported.Validate models wherever possibleThe CART Tool Suite will be consistent with these recommendations.  The performance assessment capabilities discussed earlier specifically address the first two recommendations.  The development of an architecture that allows the HPM to be interfaced to existing constructive models of the system and environment using standardize interfaces (vis-à-vis stand-alone HPMs that require a local replication of the modeled system and environment) goes a long way towards satisfying recommendations three through seven.  The relatively straight-forward graphic user interface that is provided the CART user, along with the default values and micro-models of human performance that will be included in the CART software address recommendations seven and eight.  The CART Team is working through the Air Force Agency for Modeling and Simulation to address recommendations nine through eleven.  The CART technical approach addresses the twelfth recommendation in that the constructive HPMs will be validated by comparing results to those obtained with live operators in virtual simulations of comparable mission environments.Task-network modeling (TNM) was chosen as the approach for human performance modeling in CART due to its relative simplicity in model design and its ability to explicitly control performance attributes.  The addition of goal-oriented performance capabilities to the IMPRINT TNM tool that is being leveraged and extended by CART provides the ability to represent the operator as a dynamic, adaptive agent that modifies its performance as the situation changes.  This represents a significant advance in capabilities for modeling human behavior.  The CART TNM environment does, however, have significant limitations in terms of the ability to represent complex decision making.  At present these capabilities must be provided by another, external model which performs the decision making activity and then passes the results to the CART TNM environment.  While this arrangement is consistent with the original CART hybrid architecture concept, it adds complexity in the modeling environment.  Ultimately, we would like CART to provide a self-contained, TNM-based human performance modeling capability that can represent the full range of human performance capabilities without the need to rely on augmentation from external simulations.5.  References[1]	Director of Defense Research and Engineering:  Chapter IX “Human Systems” in “Defense Technology Area Plan”  February 1999.[2]	Martin, E. A., B. E. Brett, & D. G. Hoagland:  “Tools for Including Realistic Representations of Operator Performance in DOD Constructive Simulations”  AIAA Modeling and Simulation Technologies Conference and Exhibit, Portland OR.  Paper Number AIAA-99-4027, August 1999.[3]	Defense Modeling & Simulation Office:  “High Level Architecture”  [A Primer on CD-ROM] DMSO, Alexandria, VA 22311-1705, March 1998.[4]	Hoagland, D. G., E. A. Martin, & B. E. Brett:  “The Combat automation Requirements Testbed (CART) Program:  Improving the DoD's Requirements Process Through Inclusion of Realistic Operator Performance”  Simulation Interoperability Workshop Paper Number 99S-SIW-129, Spring 1999.[5]	Brett, B. E., J. A. Doyal, D. A. Malek, E. A. Martin, & D. G. Hoagland:  “The Combat Automation Requirements Testbed (CART) Task 1 Final Report:  Implementation Concepts and an Example”  US Air Force Research Laboratory Technical Report Number AFRL-HE-WP-TR-2000-xxxx, in press.[6]	US Army Research Laboratory, Human Research and Engineering Directorate:  “Improved Performance Research Integration Tool (IMPRINT) User’s Guide Version 4.0”  Aberdeen Proving Ground MD 21005-5425, April 1998.[7]	Hendy, K. C. & P. S. E. Farrell:  “Implementing a Model of Human Information Processing in a Task Network Simulation Environment”  DCIEM Technical Report Number 97-R-71, Defence and Civil Institute of Environmental Medicine, North York, Ontario, Canada, December 1997.[8]	Simulation Interoperability Standards Organization:  “Guidance, Rationale, and Interoperability Modalities for the Real-time Platform Reference Federation Object Model”  SISO-STD-001-1999, Orlando, FL, 14 September 1999.[9]	Pew, R. W. & A. S. Mavor  (Eds.):  “Modeling Human and Organizational Behavior”  National Academy Press, Washington, D.C. 1998.[10]	Rasmussen, J., A. .M. Pejtersen, & L. .P. Goodstein:  “Cognitive Systems Engineering”  John Wiley & Sons, New York, 1994.[11]	AGARD Aerospace Medical Panel:  “A Designer’s Guide to Human Performance Modelling”  AGARDAR356, NATO/AGARD, Neuilly-sur-Seine Cedex, France, December 1998.6.  AuthorsEDWARD A. MARTIN is a biomedical engineer in the Air Force Research Laboratory's Human Effectiveness Directorate where he is currently Technical Director of the Combat Automation Requirements Testbed (CART) Program.  He earned the MS in Electrical Engineering from Syracuse University in 1971, and a PhD in Biomedical Engineering from the Ohio State University in 1985.DAVID G. HOAGLAND received his Bachelor's of Aeronautical and Astronautical Engineering degree from the Ohio State University in 1985, where upon he joined the US Air Force as a civilian engineer at Wright-Patterson AFB.  As a crew systems engineer working in the Aeronautical Systems Division, he was involved in the design, development, testing and acquisition of major weapon systems, namely the F-16 Fighting Falcon, the 767 AWACs, the F-117 Stealth Fighter, and the E-3 JSTARs.  He moved to the Air Force Research Laboratory in 1995 as a plans and programs engineer, and later became the deputy program manager for the Crew-Centered Design Technology (CCDT) Advanced Technology Demonstration 6.3 program.  He has served on numerous unmanned vehicle initiatives in the Air Force including the 1996 SAB UAV Summer Study team and the DARPA/AFRL UCAV program.  He currently serves as Program Manager for Combat Automation Requirements Testbed (CART) Program.BRYAN BRETT received a Bachelor degree in Psychology and a Masters degree in Experimental Psychology, both from the University of Florida.  He currently manages the Crew Systems and Simulation Business Area of SAIC's Aeronautical Systems Division.  He has experience as a senior analyst, principal investigator, and currently serves as Chief Scientist for the CART Program.NILS D. LaVINE received a Bachelor degree in Engineering from the United States Military Academy, West Point, NY.  He received his Masters degree in Civil Engineering from Colorado State University.  He is a Principle Systems Engineer with Micro Analysis & Design, Inc. (MA&D).  He has over five years experience working on and managing projects in area of Distributed Simulations, Computer Generated Forces, and Human Performance Modeling.  He is the MA&D Program Manager for CART.  FRED provides the dynamic, real-time link between human operators and the simulated mission environment for virtual simulations conducted in SIMAF.PAGE  7Figure 5.  Data AnalysisFigure 4.  Example of MOE Extension and Definition of Data Collection Requirements EMBED PowerPoint.Slide.8  Figure 3.  The CART Implementation Process EMBED Word.Picture.8  