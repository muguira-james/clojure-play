Results of the Common Human Behavior RepresentationAnd Interchange System (CHRIS) WorkshopEileen A. Bjorkman, Lt Col, USAFDefense Modeling and Simulation Office1901 N. Beauregard St, Suite 500Alexandria, VA 22311703-998-0660bjorkman@dmso.milPhilip S. Barry, PhDDefense Modeling and Simulation Office1901 N. Beauregard St, Ste 500Alexandria, VA 22311703-998-0660 HYPERLINK mailto:pbarry@dmso.mil pbarry@dmso.mil HYPERLINK mailto:pbarry@dmso.mil barry@dmso.milJohn G. Tyler, PhDPrincipal ScientistThe MITRE Corporation1575 Colshire DriveMcLean, VA 22102-7508703-883-6511jtyler@mitre.orgKeywords:Human Behavior Representation, Human Behavior Modeling, Human Behavior StandardsAbstract.  The Defense Modeling and Simulation Office (DMSO) hosted a two-day workshop in June 2001 to determine if there is a need and sufficient maturity in the human behavior modeling community to develop a common representation and interchange system/specification.  The objective of such a system would be to provide authoritative, fully structured descriptions and interchange mechanisms of human behavior in the military domain that lead to complete, unambiguous representation for military modeling and simulation.  The notional Common Human Behavior Representation and Interchange System (CHRIS) is built around concepts for large-scale modeling and simulation interoperability that have been demonstrated through the Synthetic Environment Data Representation and Interchange Specification (SEDRIS) initiative since 1995.  CHRIS is intended as an extension to SEDRIS and a complementary specification, which will allow for complete human behavior representation (HBR) that facilitates interchange among multiple independent HBR models.  This paper summarizes the CHRIS concept and details the results of the June workshop. 1.  Introduction.    The year is 2014.  NATO is involved in stability and support operations in a remote region of Southwest Asia.  The region is being threatened by both formal military organizations and at least one terrorist cell.  A squad of Italian soldiers captures a member of a terrorist organization during a routine patrol.  The individual is carrying explosives and several small arms and is dressed as a civilian.  After an intensive interrogation lasting two days, NATO planners put together a profile of both the individual and his claimed terrorist group.  This profile is transmitted via secure datalink to intelligence personnel back in the United States.  These intelligence personnel run the profile through an automatic knowledge acquisition process, and produce a computer-readable, standard representation of the terrorist cell 30 minutes after receiving the profile.  This representation is immediately transmitted to the Joint Task Force staff back in theater, where several military officers are standing by to perform a course of action analysis based on this new information.  The military officers run the representation through an analyzer, which tells them the computer model they are currently using to analyze terrorist cells within a stability-and-support operation will not support adequate analysis of a terrorist organization with this particular profile.  The analyzer recommends a better model, which one of the officers selects via an icon and drops into his existing stability-and-support operations simulation.  In the meantime, another officer runs a series of simulations using the original model, so they can compare the results from the two models.  A third officer continues to run the analyzer, looking for any discrepancies in the data they have received. When she finds some missing information, she immediately works with the JTF intelligence officer to make estimates on the missing data.  The updated information is immediately passed to the two officers running the simulations.  The two different simulations run for several hours using a variety of initial conditions and courses of action.  After about 8 hours, the officers analyze the output and decide that the new model is definitely producing more credible results than the older model.  After further analysis, the officers decide on a recommended course of action for dealing with the terrorist cell and head off to brief the JTF commander.  On the way to the briefing, one of the officers remarks, “You know, I remember when I was a lieutenant trying to do an analysis like this.  It would take six months and a half million dollars to get the model updated and by the time we finally did, usually the analysis would be so old it would be irrelevant.  It sure is a good thing we came up with some standard ways of representing this kind of human behavior a few years back – now we can actually do analysis that is relevant to our buddies on the pointy end of the spear!”Although the above vignette may seem a bit far-fetched, it is in fact within our grasp if we make the investments  needed to improve representations of human behavior within the military domain.  However, all the best models and representations in the world will not significantly improve support for analysis, training and acquisition if we can’t come up with an easier and more cost-effective way to update the human behavior representations within our existing models.  Standards-based methods and technologies for representing and exchanging human behavior information will be the key toward promoting reuse and interoperability of human behavior models.1.1  The DMSO Role.According to the Department of Defense (DoD) Modeling and Simulation (M&S) Master Plan (MSMP), the DoD vision for M&S is to, in part, ensure that “… modeling and simulation environments will be constructed from affordable, reusable components operating through an open systems architecture.”  Part of this vision includes objectives to provide authoritative representations of both human and organizational behavior in the military domain.  These objectives further include sub-objectives to provide generic models of human capabilities, limitations, and performance; developing the capability to rapidly generate models of human behavior for specific applications, and to develop verification, validation, and accreditation processes, resource repositories, and configuration control processes for human behavior representations (HBR).  [1]The Defense Modeling and Simulation Office (DMSO) oversees all M&S activities within the DoD.  In 2000, DMSO embarked on a new vector to emphasize support to the warfighter.  The new vision of DMSO is to lead and integrate the DoD’s M&S community and leverage M&S science and technology advances to ensure that the warfighters of today and tomorrow have superior and affordable M&S tools, products and capabilities to support their missions and to give them revolutionary war-winning capabilities. Using the guidelines set forth under the DoD MSMP and the DMSO vision, the DMSO HBR program strives to enhance reuse and interoperability of human behavior and performance models for constructive, virtual and live simulations used by warfighters and those who support them.  As part of this effort, DMSO initiated an examination of the feasibility of developing a common representation and interchange system for human behavior and performance. 1.2  SEDRIS Background.The CHRIS effort is initially conceived following the framework of the successful DMSO Synthetic Environment Data Representation and Interchange Specification (SEDRIS) program, which focused on producing a common representation and interchange system for environmental data.  [2] As its name implies, SEDRIS is composed of two parts: (1) representing environmental data, and (2) interchanging environmental data sets.   SEDRIS has a data representation model, augmented with an environmental data coding specification and spatial reference model that allows users to clearly and unambiguously articulate their environmental data.  Interchange is facilitated through the SEDRIS API, its format, and associated tools and utilities.Although the need for SEDRIS came from the M&S community, the SEDRIS development team realized that the representation would have wider application.  The challenge for SEDRIS was to provide a way to represent environmental data that applied to a wide range of potential users and was not too difficult to implement.  SEDRIS does not attempt to judge data sets or the intended use of any given data, but instead facilitates the description and sharing of environmental data regardless of the viewpoint taken by the data.  For example, consider the representation of a road.  As stated in [2], “Whether it is viewed as a linear feature in one domain, or as a series of polygonal facets in another does not (and should not) change the fact that the representation is about the same ‘thing’. Similarly, a cloud is a cloud, whether it is represented as a collection of moisture content point samples within a geographically large 3D grid, or within a weather map whose features are identified as ‘fronts’ and low- or high-pressure regions.”  If one then considers human behavior and performance as an analogous domain, then an effective human behavior representation and interchange system should provide a similar capability. Any HBR Common Representation and Interchange System would first have to develop a set of comprehensive objectives similar to the SEDRIS objectives.  The SEDRIS objectives are to: Articulate and capture the complete set of data elements and associated relationships needed to fully represent the physical environment. Support the full range of simulation applications (e.g., computer-generated forces, manned, visual, and sensor systems) across all environmental domains (terrain, ocean, atmosphere, and space). Provide a standard interchange mechanism to pre-distribute environmental data (from primary source data providers and existing resource repositories) and promote data base reuse and interoperability among heterogeneous simulations.A similar set of objectives, cast into the domain of human behavior and performance data and representations, would help drive development of a common representation and interchange system for HBR.DMSO initiated the SEDRIS project in 1994 with the goal to overcome the above challenges and accomplish the stated objectives.  From the beginning, the SEDRIS project has involved members from government, industry, and academia to provide an open and participative environment for all interested users.  Because a common representation mechanism will present many of the same challenges in the HBR domain, it may make sense to use the SEDRIS development process as a model for developing an HBR standard as well.2.  CHRIS Workshop Overview.An initial workshop to investigate the feasibility of developing an HBR Common Representation and Interchange System was conducted at DMSO during June 2001.  The workshop took place over 1-1/2 days, and was attended by 27 representatives from government, industry, and academia. 2.1  Structure of the Workshop.The first part of the workshop included presentations from government, industry, and academia representatives to provide different insights and points of view on the need and feasibility for a CHRIS-like system.  These briefings focused primarily on the technical challenges and issues involved in creating such a system.  Representatives of the SEDRIS program then presented a briefing, which focused primarily on the business process model used to develop SEDRIS.  These presentations generated considerable discussion among workshop participants regarding the requirements for a CHRIS-like system and the barriers to developing such a system.  Armed with information from the initial presentations, workshop participants were then divided into three breakout groups to discuss various aspects of the CHRIS approach.  These groups :focused on: (1) Concept of Operations, (2) Notional Data Model and Language, and (3) Notional Architecture.  The Concept of Operations group examined CHRIS from the applications point of view.  How would such a system be useful?  What are some use cases that show how such a system might be used?  The Notional Data Model and Language group and the Notional Architecture group discussed issues associated with developing data models, languages, and architectures for the proposed CHRIS system.  The intent behind these breakout sessions was not to propose a particular solution for CHRIS, but to gain better definition of the issues surrounding development of such a system.After the breakout sessions, participants reassembled as a larger group to present their findings and exchange ideas regarding each sub-area.  A discussion was then held to determine final conclusions and recommendations from the workshop.Participants.The workshop was conducted on a by-invitation-only basis to keep the group to a manageable size.  An effort was made to provide a good cross-section of the community.  Participants from government organizations represented both military operations and intelligence communities with particular focus on M&S research and development.  Representatives from industry and academia were also active in research on human behavior modeling and simulation.  Overall, the participants had a wide variety of backgrounds and perspectives on modeling human behavior.  The experience base ranged from research and development of high fidelity, psychology-based cognitive models, to human performance moderator research, to integrating decision models into existing military models and simulations used for analysis and training.To help orient and guide the three breakout group sessions, he workshop was facilitated by professional facilitators from an outside organizationprofessional facilitators from an outside organization facilitated the workshop. DMSO participation in the discussions was also minimized to ensure that the results would reflect the views of the workshop participants.WORKSHOP RESULTS.The initial presentations and breakout sessions considered a broad range of challenges and issues surrounding the development of some sort of standard for human behavior representation.  This prompted the participants to seriously consider the questions: “What is CHRIS?” “What should CHRIS become?” and “Is CHRIS Feasible?”  Discussion of these fundamental questions, and many others, was animated, highly focused, and succeeded in bringing to light many important issues upon which may hinge the success of this initiative.  After meeting in their breakout sessions, the three groups then provided a series of briefings to the full workshop providing initial recommendations for further work in this area.Challenges and Issues.Several challenges arise when trying to develop new standards for any community.  The biggest of these is to determine when a community is ready for a standard.  Pushing a standard too early before there is industry consensus may fail and stifle innovation.  Often, it is better to start with a small part of a community and expand rather than to try to solve the entire problem at once.Developing standards for human behavior representation is particularly challenging since much of the field is still in its infancy.  There is no general consensus among researchers and model developers on terminology, architectures, algorithms, or techniques.  While there are many approaches to modeling human behavior and performance, much of what needs to be modeled is difficult or impossible to measure using current technology.  This is in sharp contrast to environmental modeling, which has a long history, well-defined physics-based models, and general consensus among the community regarding terminology.To develop a standard for human behavior representation, three fundamental questions must first be answered:Is there a common taxonomy and lexicon that is generally agreed upon by the community?Is there an existence proof of an interchange or conversion format between two dissimilar models of human behavior, performance, and cognition?What kind of data is useful to exchange – information on the cognitive state or physics-level information?  [3]The workshop addressed these questions both in the plenary and breakout sessions.  The consensus was that significant preliminary work remains to be done prior to beginning a large-scale specification effort.  One of the biggest continuing challenges facing the human behavior modeling community is that there is no agreed-upon HBR taxonomy.  During the workshop this became evident as many disagreements arose due to different interpretations of the same words.  The question regarding an existence proof for, or a working implementation of an HBR interchange format, could not be answered at this point. However there is evidence that it can ultimately be answered in the affirmative.  Recent work done by the Air Force Research Laboratory on the model comparison portion of the Agent-Based Model Representation (AMBR) Program indicates that dissimilar models can in fact perform the same function, although to date there has been no attempt to convert directly from one model to another or to convert to an intermediate format for comparison. [4] Likewise, a comprehensive, cohesive body of HBR data/knowledge that could be exchanged does not yet exist, which makes it difficult to answer the third question.In general, there are two approaches to human behavior modeling: reductionist behavior models, and first principle or information processing models.  Reductionist models have little or no underlying psychological basis; instead, they try to represent behavior in a way that “looks good.”  This is the most common method used in military modeling and simulation today.  Reductionist techniques include finite state machines and rule-based systems, both of which are widely used.  First principle models are based on psychological models of how the brain actually works and are generally validated through experimentation.  First-principle models are generally much higher fidelity models than reductionist models and can be used in circumstances where behavior needs to be predicted.  [5]  For example, first  principle models can replicate actual eye movements and visual cognition workload and can be used to determine the best layout of a display or control station used by a human operator.  Both kinds of behavior models have many different approaches based on a variety of theoretical foundations, algorithms, and knowledge acquisition.  Users of both kinds of models also want to vary performance parameters as well, such as the effects of fatigue, emotion, and training.  Any standard representation and interchange system needs to account for these disparate kinds of models and the wide variety of techniques used, even within these two distinct communities.  HBR models differ in many ways, including the level of abstraction and the data that is required to run the model.  Thus, gaining consensus on the kind and form of data required will be very difficult, especially across these two communities.  Reaching consensus within either community may be a more near-term achievable goal.Techniques and approaches for modeling human behavior can be organized into several general areas (these apply to both individual and organizational behavior):Knowledge acquisition regarding the tasks to be performed by the model (e.g., a four-ship of aircraft in air-to-air combat or a hasty defense by an infantry platoon) and representation of the task knowledge (e.g., a series of rules or a set of connected graphs). Algorithms for reasoning and learning (e.g., a finite state machine or a Bayesian network).Anthropometric data and performance of various tasks (e.g., how quickly the eye can transition from inside the cockpit to outside the cockpit, scan a sector for enemy aircraft, and detect an enemy aircraft within visual range).Human performance moderators (e.g., how fatigue affects performance or how emotion affects decision making).Although on the surface these four areas may appear to be very distinct approaches, in fact there is considerable overlap among them when it comes to representing human behavior based on the theoretical and architectural underpinnings of the representation.  For example, some models treat knowledge acquisition as a separate piece, storing and using the knowledge separately from the reasoning process.  In other models, the knowledge is tightly embedded in the reasoning engine and not easily separated.  A third set of models appears to be somewhere in between, with a “core” set of knowledge embedded within the model, but a separate set of task specific knowledge treated separately.  Likewise, performance data and performance moderator functions may be tightly coupled to or embedded in the reasoning process.  Or they may be treated as separate components (e.g., fatigue could be treated by simply increasing the probability that a human entity makes a poor decision or it may be explicitly represented by slowing down the reasoning process and reducing the situation awareness of the entity.  Any HBR standard developed also needs to account for these very different architectures and approaches to modeling human behavior.An obvious solution to the above challenges might be to try to standardize the approaches taken to modeling human behavior in the first place.  However, this would be exceedingly impractical.  Just as there are many ways of modeling a combat aircraft (e.g., a high fidelity model of all components vs a probability of kill against a specific target) based on the use of the model, there must be many different ways of modeling human behavior.  There is simply no “best” way to model human behavior, just as there is no “best” way to model “physics-based” phenomena.Some Possible Solutions.Although human behavior cannot be as precisely represented as environmental data, there are several reasonable possibilities for useful standards within the human behavior modeling and representation community.  These include potential development of: A standard taxonomy for the community to enhance communication., A standard for representing human performance data.A standard process for capturing and representing knowledge about specific tasks.A standard way to measure one modeling technique against another, or to convert from one model to another.The first standard may seem easy to do, but would still be challenging, given the wide variety of communities involved in human behavior research and modeling.  To complete such a task requires the involvement of cognitive psychologists, artificial intelligence researchers, software developers, systems engineers, and combatant subject matter experts, just to name a few.  However, developing a standard taxonomy is clearly needed to help with sharing of information and clear communication, even if such a taxonomy does not directly contribute to developing a set of standards for data and representation.The second potential standard, for representing human performance data, is clearly the most controversial one. Some participants at the workshop felt there was no need to create performance standards, since a large enough body of knowledge does not yet exist that would make such a standardization effort worthwhile.  However, even the participants who felt this way believed that there was enough anthropometric data available to initiate a subset of such a standard, even if there is insufficient data to represent all aspects of human performance.  On the other hand, others argued that now is the time to develop such a standard across all areas, instead of waiting until many dissimilar – and incompatible – data sets are created and the community is forced into the need for a standard.  At this point, any approach to standards needs to remain as flexible as possible to accommodate future data collection and areas of research that may arise years from now.  In addition, since the modeling and simulation community is normally not a collector of human performance data, but merely a user, the drive to develop such a standard really needs to come from the cognitive psychology research community responsible for collecting such performance data.  Developing standard ways of collecting and representing knowledge of human (individual or organizational) tasks appears to be the most promising area for standardizing human behavior representation.  Everyone at the workshop agreed that knowledge acquisition (both fundamental knowledge of how people perform everyday functions and specific knowledge of how to perform a particular task in a combat domain) is the most difficult part of implementing a human behavior model, accounting for 50-80 percent of the effort.  This is consistent with what every system analyst knows -- writing code is easy; capturing requirements and knowledge is the hard part.  However, there are many promising new technologies and techniques that may help to automate the knowledge acquisition process and to encode the knowledge in a standard format that makes it readily accessible.  Some of these technologies/techniques include the Extensible Markup Language (XML), DARPA Agent Markup Language (DAML) [6], Knowledge Interchange Format (KIF) [7], and Conceptual Graph Interchange Format [8]. Among these, XML and KIF already have been formally proposed as standards.  Although none of these may prove to be an immediate or complete solution to the knowledge acquisition problem, the fact that they exist provides some hope for a future solution.  Investment to foster further development of these technologies should be a main feature of standards development in this area.Finally, it is desirable to have a standard way to convert from one kind of model to another, and to be able to measure one model against another.  This is not necessarily to prove the “goodness” of one model versus another, but rather to be able to easily understand the tradeoffs and benefits of using one model over another.  There is currently no easy way to do this.  The DMSO Challenge Problem and the AMBR program mentioned earlier are attempting to develop a common series of testbeds and scenarios that allows models to be compared to each other, but this will only allow models to be compared in a limited set of circumstances [9]. A more general technique for examining the completeness of one model versus another would be useful for determining model appropriateness for a given application. One technique proposed at the workshop was to develop a process that would convert different representations into finite state machine representations that could then be compared  [10].  Although such an ability is probably many years away, the possibility of being able to conduct such a comparison could be very useful to the community.  Using the SEDRIS Business Model [11].The need for SEDRIS was motivated in the mid-1990’s by very high environmental database development and reuse costs across the spectrum of modeling and simulation.  Nearly every new model developed prior to that time had to develop a new database and a new representation of the environment.  Several previous efforts to standardize environmental data had failed, but had produced some good “starting points” for follow-on efforts.By the mid-1990’s it was becoming apparent that a more intensive standardization effort was needed.  In addition to the cost driver, the potential for using disparate networked simulations also increased the need to have a standard way to describe and interchange environmental data.  The initial team selected to work on the problem consisted of only six people with a good cross-section of skills – database expertise, experience with computer graphics and visuals, experience with semi-automated forces and vehicle simulation, and two systems engineers.  This small team worked for several months to build an initial philosophy and plan for further development. The team recognized the need to establish a set of guiding principles early on, then focused on practical solutions built on existing, proven technologies and an iterative/spiral design process.  Many of the technical challenges faced by the environmental community were the same as those now facing the HBR community:  many different domains and applications, the desire to build a standard that does not stifle future innovation, and the need to balance a desire for open exchange with individual developers’ legitimate proprietary concerns.  The really tough problems seem very similar to those now confronting HBR:  (1) getting a total set of requirements, (2) keeping commercial vendors involved but maintaining an open exchange mechanism, (3) many divergent approaches used within the HBR domain, and (4) the lack of an underlying and unifying framework.    The SEDRIS team started small and first developed what they considered to be the required core technology, a data representation model.  They involved the environmental community and key modeling and simulation vendors and data providers.  After this initial development, the core team was gradually expanded, and DMSO took over management of the project.  SEDRIS is now a mature standard supported not only by a continuing core team, but by many SEDRIS “associates” as well.  Becoming a SEDRIS associate is free; SEDRIS associates agree to use SEDRIS in their applications (if any), participate in meetings and e-mail discussions, to provide feedback and promote SEDRIS use.However, there are reasons why the SEDRIS experience cannot be duplicated out of whole cloth in the HBR domain.  One significant difference between the SEDRIS experience in the environmental community and the potential for CHRIS in the current state of the HBR community is that foundation standards for M&S have progressed significantly since 1995.  The High Level Architecture (HLA) has emerged from concept to full-fledged development.  Today, HLA exerts considerable influence over the development of many major simulation systems and the creation and refinement of models across several disciplines.  With HLA-based systems in place or on the horizon, some of the architecture problems that initially confronted the initial SEDRIS effort may not require a similar level of effort to resolve.  While HLA can provide benefits that were not available early-on in SEDRIS, the Architecture breakout group recommended against HLA compliance as a requirement for development of human behavior models.Workshop participants generally supported the suggestion that the business approach followed by the SEDRIS community might apply to the HBR community as well.  Although human behavior data and modeling are not as well researched and defined as in the environmental domain, there are many useful parallels between these two domains that make the SEDRIS approach and lessons learned attractive.  Any attempt at a standard for human behavior needs to involve the whole HBR community and needs to proceed in an iterative or spiral development process, given the state of the art within the community.  The workshop participants generally agreed that chartering a small focus group to further develop initial ideas put forth for a standard for human behavior would be worthwhile.Getting Started.The breakout sessions conducted during the workshop contributed many good ideas for the focus group to use as a starting point.  These ideas are listed here in no particular order of priority:Develop an interchange standard for anthropometric data/models.Develop a standard federation object model for interfacing human behavior models in a federation.  This would not be a “one-size-fits-all” FOM but would be a starting point that one could use in the FOM development process.Develop an HBR ontology.  Start with a well defined sub-area, such as close air support, and develop a standard representation for data needed to support close air support modeling, as well as a set of rules and techniques related to the HBR needed.Develop a standard description of tactical behaviors implemented in a standard language. Develop a data representation model and interchange language.  The short-term effort should be interface-oriented and be concerned with complete agents (i.e., the “looks good” discussed previously). Near-term efforts should emphasize experimentation with proposed standards rather than full-fledged standards development.  Longer-term efforts should be interchange-oriented and focus more on cognitive components of behavior.Conduct a business case analysis to determine if the market for HBR is large enough to support a standard. Conduct further research in many less-understood areas, like the impact of culture and organizations on decision making processes.3.5  Conclusions Given the above challenges, at the conclusion of the workshop, the participants determined that it would be best to charter a small focus group of 6-8 participants that will take the results of this workshop and determine a focused plan of action, similar to the SEDRIS initial core team.  There was much discussion among the workshops as to who should be on this core team and what the mission of the focus group should be.  Since the team won’t be chartered until the new fiscal year begins in October, DMSO participants agreed to put together a suggested focus group charter, along with qualifications desired by team members.  This charter will be circulated among workshop participants on a reflector, and comments will be incorporated to produce a final charter.  Team members will then be selected to match the desired qualifications.  Given the small size of the team, many members will probably have to have multiple qualifications to be considered.The intent is to have the focus group work for about six months to produce an in-depth white paper on the pursuit of HBR standards.  Once this white paper is distributed, DMSO will hosthave a follow-on workshop/conference in about April 2002.  This workshop will be open to all interested participants to maximize the interchange of ideas.  It will have a more extended agenda to allow for more thorough examination of the kind of thorny issues that were raised in this initial CHRIS workshop.  Like this workshop, it will benefit from professional facilitators who can maximize the coherence of information produced out of the discussions.CONCLUSIONAlthough it might appear that trying to develop a standard for human behavior representation is hopeless, we believe that the goal is achievable, although certainly daunting and requiring a long-term commitment.  The CHRIS workshop brought out many of the challenges involved in producing such a standard, but also brought out many good ideas for pursuing some portion of a standard.  The focus group will be chartered to take the output from the workshop and refine it further to produce a starting point and roadmap for establishing a standard for human behavior modeling. [1]  Department of Defense Modeling and Simulation (M&S) Master Plan, Under Secretary of Defense for Acquisition and Technology, Washington, D.C., October 1995.[2]   HYPERLINK http://www.sedris.org http://www.sedris.org[3] Katz, W., “SISO View on Common Human Representation and Interchange System (CHRIS),” briefing at DMSO CHRIS Workshop, 19 June 2001.[4]  Finerman, L.E., D. L. Prochnow, K. A. Gluck, and R. P. Willis, “A Persistent Federation for Human Behavior Representation  Models,” Simulation Interoperability Workshop Paper Number 01S-SIW-058, Spring 2001.[5]  Young, M., “What Do We Mean By Human Behavior Representation?” briefing at DMSO CHRIS Workshop, 19 June 2001.[6] Lacy, L., “Computer Generated Forces Behavior Representation Interchange System,” briefing at DMSO CHRIS Workshop, 19 June 2001.[7] Genesereth, M., Knowledge Interchange Format, draft proposed American National Standard (dpANS)                                                NCITS.T2/98-004.  HYPERLINK http://logic.stanford.edu/kif/dpans.html http://logic.stanford.edu/kif/dpans.html [8]  Skipper, D., “Conceptual Graph Interchange Format,” briefing at DMSO CHRIS Workshop, 19 June 2001.[9]  Bjorkman, E. A. and P. Blemberg, “Review of the Defense Modeling and Simulation Office Human Behavior Program,” Simulation Interoperability Workshop Paper Number 01S-SIW-080, Spring 2001.[10] Gonzalez, A., “Computational Models for Human Behavior Representation Interchange Standard,” briefing at DMSO CHRIS Workshop, 19 June 2001.[11] Mamaghani, F., “SEDRIS” Does It Make Sense as a Model for HBR?” briefing at DMSO CHRIS Workshop, 19 June 2001.BIOGRAPHYLt Col EILEEN A. BJORKMAN, USAF, is the Chief, Concepts Application Division, at the Defense Modeling and Simulation Office (DMSO).  She has held a wide variety of development test and evaluation assignments, and was a tactical fighter analyst at Air Force Studies and Analyses Agency.  At DMSO she is responsible for the Human Behavior Representation program, the Smart Sensor Web program, and DMSO Community Support activities, including the Modeling and Simulation Information Analysis Center, Modeling and Simulation Resource Repository, and M&S Education.PHILIP S.  BARRY, PhD, is the Chief of the Science and Technology Initiatives Division for the Defense Modeling and Simulation Office.  He is also the project manager for the FY01 HBR Challenge Problem.  He has worked numerous software engineering and modeling and simulation projects.  He also teaches system engineering and knowledge based systems at George Mason University in Virginia.JOHN G. TYLER, PhD, is a Principal Scientist in Cognitive Science and Artificial Intelligence at the MITRE Corporation. Dr. Tyler provides support to DMSO on the HBR Challenge Problem.  He also leads systems engineering and architecture support on the development of advanced C2 training systems for the U.S. Army Program Executive Officer for Command, Control and Communications Systems.  Dr. Tyler received the M.S. in Computer Science (’84) and Ph.D. in Instructional Design from the University of Florida (’93).