Data, Functionality, and Collaboration: Identifying and Evaluating Briefing and After-Action-Review CapabilitiesCapt Kristen M. BarreraLt Brenda E. BlueggelDr. Winston BennettCapt Michelle NashAir Force Research LaboratoryWarfighter Research Readiness Division Mesa, AZ, 85212480-988-6561 HYPERLINK "mailto:kristen.barrera@mesa.afmc.af.mil,brenda.blueggel@mesa.afmc.af.mil" kristen.barrera@mesa.afmc.af.mil,brenda.blueggel@mesa.afmc.af.mil,  HYPERLINK "mailto:Winston.bennett@mesa.afmc.af.mil" Winston.bennett@mesa.afmc.af.mil,  HYPERLINK "mailto:michelle.nash@mesa.afmc.af.mil" michelle.nash@mesa.afmc.af.mil  Keywords:Distributed AAR, long haul AAR, After Action Review, Performance Measurement and AAR, Performance Measurement and Distributed TeamABSTRACT:  In the military, a majority of priming for learning occurs in the briefing and a substantial amount of actual learning occurs in the after-action-review (AAR).  Given the potential impact on training effectiveness of the briefing and after action review, it is important to examine what different technical features and functions can enhance learning, data use, and collaboration.  In the USAF Distributed Mission Operations (DMO), differences in individual briefing and AAR styles, data use, and familiarity with equipment functionality within a team can be disruptive and distracting.  Moreover, these differences not only impact the learning and overall effectiveness for a co-located team, but they also have implications for distributed mission planning and collaboration. Team cohesion and flexibility to adapt to new situations can lead to accelerated learning, while ineffective team work can severely degrade the learning environment.  The Air Force Research Laboratory in Mesa, Arizona has developed a variety of integrated methods and tools and created a collaborative testbed focusing on improving data, functionality, instructional tools, and collaborative methods for mission planning, briefing, and AAR in distributed training environments. This paper will review pertinent research and will present preliminary results in the form of lessons learned from a series of evaluations conducted in the Briefing and AAR testbed. We will discuss how results are being used to establish standardized practices, improve the application of tools and technology, and how the testbed is being used to quantify improvements in individual and team training and performance.  1. Introduction to Briefings and After-Action Review (AAR) Concerns for Training EventsMission briefings prime students and warfighters for learning by going over the past training they have received and how they are going to apply this training to the future mission.  The mission briefing ensures all participants are aware of the plan of execution, the rules of engagement, and emergency procedures.  After execution, during the post mission phase, students and instructors review not only what happened during the execution period but analyze why and how it happened.  The Air Force Research Lab (AFRL) has realized that the post mission review is crucial for the student because the majority of the learning seems to take place in the AAR.  It is during this time that the student can see what they did right or wrong, as well as learn how they can either improve upon or correct it.  One of the concerns that presents itself is that of ad hoc team performance.  For training as well as operational purposes, teams are usually put together without much notice and they must come together successfully to accomplish a mission, performing as well as a consistent and cohesive team would.  The team must quickly get through the four steps of team building (forming, storming, norming, performing); a process which can take a lot of time and that some groups never complete.  For the team to perform well, each member must recognize their role in the mission and respect the roles of others.  If a team should happen to stop in the storming step, the team will not establish their individual roles as they will still be bidding for control, confronting each others ideas and perspectives [1].  The solution: finding a way to get the team through the steps and make it quick.  One of the ways this may be accomplished is through achieving team cohesion.  To aid in this team building process, a common brief/debrief toolset can help foster the teaming relationship and identify major items in the brief/debrief.  A common system takes some of the focus off of learning to operate the equipment and allows the team to focus on team collaboration. 1.1 Team Cohesion Distributed Team ResearchThe literature on team cohesion suggests controversial findings in relation to team performance.  Some literature suggests there is a positive relationship between team cohesion and team performance, while other literature suggest there is a negative relationship [2].  According to Mullen and Copper [3], the consensus in literature is that cohesion is positively correlated to performance.  Team cohesion can be fostered easier in face-to-face interactions over time, but when the team operates in a distributed manner, a different approach is required to develop cohesiveness.  Driskell, Radtke, and Salas [4] claim that distributed teams may not be as effective as collocated teams because the lack of face-to-face interaction hinders the various methods the team would otherwise use to communicate.  Distributed groups must find alternate means of conveying physical gestures, voice inflections, and other nonverbal cues that can change the meaning of team communications.  To remedy this, distributed teams can turn to technology. Currently, there is a lot of technology on the commercial market designed to give distributed groups a more face-to-face feel.  For example, current video teleconferencing equipment in combination with conferencing software can provide as close to a face-to-face feel as you can get without being in the same location.  Through this means of communication, teams can compensate for some of the nonverbal cues that would otherwise be lost.  After distributed teams have overcome barriers in communication, team cohesion is that much closer.  In addition to communication within and between teams, the culture of the organization must be able to support a distributed approach.    Literature on distributed teams suggests that for them to be successful, the culture of the organization must adapt and cater to this distinct form of collaboration [5].  Lack of support from the organization will limit the capability of the distributed collaboration, therefore it is essential for military leaders to be aware of the research being conducted and take an active role in implementing lessons learned.  An organization must provide the tools and the flexibility to accommodate distributed teams while making the individuals that make up the team still feel committed and focused toward the organizational goals.  There must also be a balance in how far the organization’s cultural influence will affect the creativity and individual thought of the team [6].  The groups must feel they are an important part of the organization while still able to express creativity.  The best team or team performance comes from a diverse collection of individuals that can bring a wide breadth of experience, knowledge, and ability of the team [7].2. Testbed MethodologyAt the Mesa Research Site (MRS), a division of the Human Effectiveness Directorate (HE), studies on briefing and AAR tools and techniques are conducted by hosting Training Research Weeks in which, two teams of active warfighters are brought in to participate.  Each team consists of two to four F-16 Pilots, one Instructor Pilot, and one or two Weapons Controllers (WC).  During these research weeks, the teams fly benchmark missions on Monday and Friday and either fly a Mission Essential Competency (MEC)-based syllabus or a MEC-focused syllabus, increasing in complexity as the week progresses.  This approach allows researchers to track the individual team performances in a controlled environment.  The teams brought in to participate come from different units across the Air Force and offer a sampling of the different briefing and AAR techniques and approaches employed by the warfighters.  The literature on team collaboration supports the belief that a variety of techniques and the necessity to constantly adapt to new methods is counterproductive and can lead to much frustration among team members [8].  By bringing these diverse teams together to collaborate, a cross flow of tools and techniques is established and teams can then choose the approach to briefing and AAR that suits their mission.  Researchers benefit, in turn, by piecing together the various methods and approaches and providing continuity by documenting interactions in an attempt to establish standardized practices based on empirical research.  Various platforms such as the Air Operations Center (AOC) may also benefit by this objective collection of methods and practices in their integration with pilots and controllers.2.1 Coalition Training WeeksMRS also hosts Coalition Mission Training Research (CMTR) weeks which function in a similar format to the Training Research weeks, previously mentioned.  The main difference being that the teams are distributed and include coalition partners participating in the studies.  In order to compensate for the lack of face-to-face interaction, the teams conduct mission planning, briefing, and AAR using polycoms, conferencing software, and interactive synchronous displays.  Hosting collocated as well as distributed teams allows researchers to test various technologies and methods while having the ability to observe and compare a control group.  During distributed studies, everyone participating takes a series of surveys throughout the week to identify if research objectives are being met.  The participants are asked about every event of the mission each day and are asked to identify the most successful and least successful aspects and why they were or were not successful.  Researchers observe the entire process, from the planning, briefing, execution, to the AAR and take notes on team interaction. The researchers are particularly interested in how did the team used the collaboration tools provided, how well the team communicated, was their communication effective, were any technical problems encountered and how they resolved such problems, and what was successful.  The Coalition counterparts take a range of surveys as well to complete the picture.  The data is then processed and compiled as a report and lessons learned are identified to be used for future exercises and for real world coalition events.    2.2 Brief/Debrief TestbedMRS’ Brief/Debrief Testbed is focusing on improving data, functionality, instructional tools, and collaborative methods for mission planning, briefing, and AAR in distributed training environments.  MRS currently has seven reconfigurable testbed set-ups where Warfighters can test new technologies and participate in studies to identify the best training and rehearsal methods as well as ideal technology system setups.  Two of the seven testbeds are deployable self-contained units capable of being shipped out for use in the operational field.  Each of the testbeds includes hardware as well as software that is either commercial-off-the-shelf or government-off-the-shelf technology that warfighters can easily acquire for their home units. For example, the Distributed Mission Operations Control Station is government-off-the-shelf software developed by MRS engineers to control the simulated missions as well as digitally record and playback the missions.  Warfighters are able to recreate the mission and focus on the critical events during their AAR using synchronized displays and communications.  During a typical Training Research Event, each pilot participant has the ability to view their own displays as well as the displays of their wingmen, the WC screen, and a god’s eye view of the mission.  This capability provides them with “truth data” and allows them to see not only what happened but gives them the tools to easily identity why it happened. The software also allows Warfighters the ability to jump to markers established by instructors during a simulation event, as well as digitally Fast Forward, Rewind, and Pause mission at their convenience.  Smartboard™ overlays are used to visually emphasize key mission events and instruct team members on lessons learned as well as offer the ability of remote interaction. Users can also take still pictures and print them for their records.  At the end of each training week, participants are currently being given the option of taking their recorded missions home with them on a DVD for further review and analysis of their performance.  These options are not available with the actual aircraft equipment.  Most of the jets like the F-16 still record the flight data on 8mm tapes making the debrief difficult to achieve.  A future goal of the MRS team is to allow Warfighters the ability to have all the conveniences of simulated mission for real world missions.  MRS has been able to take the data from 8mm tapes and successfully convert it into a digital recording that can be replayed in the Briefing and AAR testbed.  But the process is very lengthy and not practical.  Some newer aircraft like the F-15 can record mission data in a digital format which can then be replayed in the testbed as soon as the pilot steps from the jet.The development of this system was completed by researchers asking teams for their feedback on how the brief and AAR testbed tools could be improved; researchers then use this information to enhance the testbed systems, often working directly with industry and academia partners.  For example, many of the pilot participants expressed the desire of being able to “draw” on Portable Flight Planning Software (PFPS) files using the SMARTboard™ and the ability to save them into a briefing for their AAR.  MRS and SMART Technologies have been working with Georgia Tech (creators of PFPS) to allow SMART Technologies the ability to “look inside the box” so they can make their PFPS “Smart aware” allowing the two toolsets to interact seamlessly.  The pilots needed a way to review what they had planned to do and their contingency plans.  By working with PFPS and SMART Technologies, MRS is further aiding the planner’s and the instructors’ ability to plan, brief, execution, and debrief a mission.  Empirical studies are also being designed to validate the tools selected for the brief and AAR testbed against mission performance measurements.2.3 Contrasting Platform AARs Current WC training research being conducted on briefing and AAR is looking at addressing the WC community’s need to conduct one-on-one instructor to student reviews of the mission.  Researchers will review and integrate various types of collaborative tools to identify ideal brief and AAR system set-ups for this type of personalized interaction.  This enables researchers to identify the technology strengths and weaknesses of individual and mass AAR systems.  It is important to note that instructors teach students in different ways and thus may require a different set of technology to enhance the learning that takes place.  Nevertheless, there are set guidelines for these teaching processes; however, these guidelines (radar mechanics, setting up and refreshing the scope, etc) are left up to individual preference.  MRS’ Brief/Debrief Testbed allows for a more in-depth AAR, permitting the teams to dig deep to see what they were doing and find out why they were doing it.  Figure 1 shows a student WC working with an Instructor during a mission.   HYPERLINK "https://fsqbge19/EventsPromo/snapshots2006/wic_week/pages/USAFWS_August_2006_09.htm"  INCLUDEPICTURE "https://fsqbge19/EventsPromo/snapshots2006/wic_week/thumbnails/USAFWS_August_2006_09.jpg" \* MERGEFORMATINET Figure 1: WCs at the console during an exerciseDuring individual debriefs, a WC will sit with their instructor and review the playback of the mission.  They watch their scope picture along with the truth picture (the god’s eye view) to see if they are using the correct mechanics and terminology and giving timely and accurate feedback to the pilots.  In addition WCs also sit in and debrief with the pilots for a portion of the AAR.  Pilots can critique the WCs and let them know how they are doing in regards to using the correct terminology, if they are getting the information they need, what they can do better, etc.  A good amount of learning takes place in both WC only and Pilot-WC debrief.  To improve Pilot-WC debriefs, MRS is working on several tools to capture and analyze the communication interaction that is currently not reviewed.  Using latent semantic analysis software, the mission audio can be converted into time-stamped text that can be used in conjunction with communication interference identification software allowing controllers the ability to go over their calls at the time they were made and the pilot’s responses to their calls.  The learning environment is increased between pilots and controllers by providing empirical data to support Pilot-WC interaction. When the Pilot-WC debriefs are done, the WCs get together for a WC only team AAR.  They discuss common errors, what the controllers did well, discuss any goals for the day and highlight areas for the students to brush-up on for the following day.  Instructors will ask the team questions, creating discussions for the students to carry-on after they have left the lab.  For example, framing the radar picture while controlling is often an item of discussion during both individual and team debriefs.  There are guidelines for the timeframe when the controller is supposed to frame the picture; however, each WC will develop their own strategy according to the situation and their personal preference. 3. Lessons Learned Thus FarCollocated briefs and debriefs allow teams to exchange information and gather critical data for mission accomplishment.  Training environments such as DMO Training Research weeks allow a team to establish a learning environment.  Collocated briefs increase the amount of interaction between crewmembers and further enhance the mission planning, brief, and AAR because all crew members are able to ask questions, read each other’s faces, comment and ask questions on items/issues they might not have if they had limited interaction with the rest of the crew.  Working closely with a team, being able to view the individual expressions and personalities enhances the crew interaction and generates a better debrief.  Distributed coalition teams such as Exercise Battle Buzzard (EBB) (Figure 2) further highlighted the need for a common set of brief/debrief tools and standards.  Different countries require different amounts of route study, planning, briefing, etc.  Researchers from both the US and UK were able to highlight the need for a standard/common briefing criteria.  Large files and playback capability is limited especially when the systems being run are not compatible.  Some of the difficulties experienced when working with Coalition partners is making the technology on all sides work together.  File sharing can consume valuable collaboration time as well as cause disruptions to the mission when part of the team gets into their simulators to fly the mission with out last minute changes that didn’t make it across in time.  The distributed participants were not always communicating in the same frame of reference because of the slow rate of file updates which caused a lot of confusion and wasted time getting back on task.  Figure 2: Mission brief during Exercise Battle BuzzardVirtual Flag (VF) is a large-scale DMO exercise hosted by the Distributed Missions Operations Center at Kirtland AFB, NM that allows multiple platforms to link together for a multi-platform training exercise.  From our VF observations, we have been able to identify both positive and negative issues during debriefs.  Distributed locations experience lag times in slides being sent across the network.  The lags are sometimes so dramatic that the teams tend to tune out the debrief altogether.   We have also observed that participants at distributed locations often limit what they say at the briefs/debriefs.  They seem to be more hesitant when speaking to a crowd where they cannot view or hear the reactions to their questions.  Guzzo and Dickson [9] suggest inhibition in distributed groups is lowered because individual status is equaled, however, from our observations, a participant is less likely to ask a question in a distributed briefing or AAR if the speaker’s response is delayed for over 10 seconds due to technology lags.  In some instances, individuals were observed to call the speaker directly after a video teleconference to ask a question they had during the briefing.  While the individual is still satisfying their communication needs, the rest of the team that participated is not privy to the additional information or may be trying to reach the speaker as well.  This lack of open communication can lead to mission degradation or disruption.Exercise Red Skies, a Red Flag spin-up event conducted in March 2005, consisted of US and UK pilots who were deploying to Nellis AFB, NV to participate in a Red Flag exercise the following week.  Red Skies participants were exposed to the Nellis Range and were forced to fly handpicked Red Flag training scenarios, with Red Flag rules, in the simulators before they deployed to Red Flag.  The teams used MRS’s brief/debrief system to identify boundary lines, review rules, objectives, and goals.  The teams were able to familiarize themselves with several points on the Nellis Range, as well as gain confidence in flying missions they would see the following week.  Teams debriefed using the brief/debrief system and were able to highlight strengths and weaknesses, while further familiarizing themselves with Red Flag rules.  MRS followed up with Red Skies participants at the end of the Red Flag event to see if Red Skies had an impact on their Red Flag experience.  MRS found that pilots who participated in Red Skies felt they could have skipped the familiarization day because they were already familiar with Red Flag rules and range boundaries (one of the main reasons for the familiarization day).  This demonstrates that mission rehearsal, be it specific scenarios, or area familiarization flights, can help minimize spin-up time for live fly events.4. Future VectorThe focus for the Briefing and AAR testbed has predominantly concentrated on F-16 pilots and WCs; however, future efforts will be expanded to include other platforms such as the AOC, Joint Tactical Air Controllers, A-10 pilots, as well as many other weapon systems.   To really explore and understand briefing and AARs, we need to investigate multiple weapon system communities, how they brief and what capabilities are required.  A couple of our future studies are described below. 4.1 AOC Community The AOC community has expressed interest in training research weeks.  The AOC team would like to observe the pilot mission planning, briefs, and AARs to help them better identify ways to improve the Air Tasking Order (ATO) generation process.  The team would then be able to collaborate on ideas, find out what did and did not work, and ultimately streamline the ATO process.  The key item here is that AOC team members would need to observe and understand what information, and in what format a pilot and other players need to do their job.  In reality, the AOC will never debrief with the operational platforms, however, a training environment can offer the opportunity and provide tools and suggestions on how to make the process better.  Weapon system interaction will not only benefit the teams that participate, but will in turn better educate the greater weapon system operator as each crew completes the training.  Getting the information to the warfighter in the correct formation and in a language they understand is crucial and practice makes perfect.  Not only would each weapon system operator gain a better understanding of their roles and responsibilities, they will further understand how their decisions and planning affects the others involved.  4.2 AAR StudiesObservations during a coalition exercise as compared to VF, brought up the comparison of synchronous and asynchronous communication and data exchanges and the impacts on team interaction and the quality of the mission planning and briefing process.  During a coalition exercise, video teleconference, conference software, and interactive projectors were used for briefings and AARs to provide a more synchronous environment.  During VF, only video teleconferencing and conferencing software were used which lead to a quasi-synchronous environment.  VF participants seemed to feel more disconnected during planning and AAR than the coalition participants. Although these observations are purely anecdotal, the questions arose:  How does no real-time versus real-time data sharing affect the perceived level of team involvement?  Secondly, what is the indirect impact of level of team involvement on assessed mission performance?  From these ponderings, a study was proposed to provide three levels of team interaction:  (1) conference software (asynchronous), and voice only (synchronous); (2) synchronous video teleconferencing and web-based conference software and interaction (quasi synchronous); (3) Synchronous interactive displays with polycom.  Questionnaires for perceived team interaction are currently being developed to capture data on the team’s involvement in the planning process, degree of confidence in the planning process, and the plan itself.  The questionnaire will also capture the team’s assessment of the success of the plan as developed in each level of interaction and with the different sets of tools.  The hypothesis is that level 3 team interaction and tools (e.g., being able to interact to view and modify data from distributed sites synchronously) will show increases in perceived team involvement, confidence in plan, and confidence in success of plan.  Ultimately these will impact overall mission performance and success.Another study being developed is to identify the key characteristics that influence AARs as learning/instructional tools in collocated environments and in distributed situations.  This study came about with the observation that WCs seem to take AAR a little farther, in terms of the level of detail discussed, than pilots do and some of the levels of detail also depend on the particular instructors.  Some WC instructors allow the trainee to take the lead role and evaluate their performance while the instructor makes sure the student is capturing all the learning points.  This seems to force the trainee to pay closer attention to their mistakes and they are also forced to describe how they can correct them for next time.  By allowing the student to take the lead, the instructor can also identify the knowledge gaps the trainee is unaware of when pointing out their own mistakes.  This is just a different approach that may not work in every situation but certainly adds another method to test.  A difference also exists in collocated environments where AARs are very in-depth vs. in distributed environments where AARs are rushed through.  All these differences appear to impact the value of the AAR as a learning event.  The research questions in this case are:  what are the key characteristics that differentiate AARs in terms of quality and which characteristics are impacted by collocated vs. distributed environments.  In order to answer these questions, two studies will be undertaken.  Study 1 will examine key characteristics and data used in AARs in a University laboratory setting using a commercial gaming environment as a synthetic task environment (STE) in both collocated and distributed contexts.  The participants will be divided into groups that will be collocated or distributed and within these groups half will conduct AARs while the other half will simply have more training time in the game.  Both groups will be compared in a criterion-task related to the objectives of the STE.  Study 2 will attempt to replicate a portion of the results from the University study in the MRS research testbed using operational teams.  One team will get a full AAR vs. a team that will only get to see a linear replay of the scenario.  Both will fly a parallel form of the initial performance task as a criterion-task.  In the study 2 work, both teams will be collocated due to logistical considerations.  Our hypothesis is that AAR groups will fair better than simply training groups and that the collocated AAR groups will fair better than the distributed AAR groups.4.3 Live, Virtual, Constructive OperationsMRS has teamed with Air Education and Training Command (AETC) to lay the framework for live, virtual, constructive (LVC) operations.  LVC operations will not only relieve pressure from current Air Force Smart Ops 21st Century (AFSO 21) flying hour cuts, but it will allow for more beneficial training for both live and virtual players.  Currently, live players (such as an F-16) act as training aids for Air Battle Managers (ABM).  With LVC operations, the Air Force can expand its current training perspective by creating effective training for both live and virtual players.  LVC capability allows for bi-directional training between live and virtual players.  Currently, virtual players can see, target, and engage live players (Figure 3).  However, the live aircraft receive no training value because they only see the virtual simulator as a positional indicator on their display.  LVC operations will increase the value of training to both types of players, in addition to supplementing the reduced live-fly training hours.  A key player in LVC operations is MRS’ state-of-the-art brief/debrief testbed which will allow both live and virtual players to debrief as if they were flying in the same arena (live or virtual).  Figure 3: Live aircraft displayed on virtual screenTypically, the fighter pilot (F-16 for example) has an instructor flying beside them making mental notes in addition to writing a few items down on a knee-board while subjectively observing the trainee.  With LVC technology, the live-flying aircraft can have just as in depth debrief as the virtual players.  MRS’ engineers are able to capture positional data from the aircraft, along with other performance measures, to display the live aircraft as a digital entity from the network.  Because the live aircraft’s data will be collected as if it was a simulator, it allows live-flying aircraft the ability to dig deeper into the debrief and permit the pilots to visually see where and what the aircraft was doing at a certain point in time.  In addition to aircraft data, teams as well as individuals can see the impact their decisions had on the mission as a whole.  MRS expects to use existing technology to enable LVC operations, further enhancing individual and team debriefs, adding a more objective point-of-view to the mission debriefs and learning points.  LVC operations should show an increase in pilot performance with a decrease in the need for actual live-fly hours, especially concerning basic flight maneuvers and certain aspects of mission rehearsal.  5.  ConclusionThrough participating in, as well as hosting, several DMO training events, MRS has learned that students tend to learn the most during AARs.  The use of technology in AARs during DMO events plays a large role through increased data use and collaboration (both within and across teams).  Differences in mission planning, brief/debrief tools, team location, and how quickly team cohesion is established can either enhance or degrade team and individual performance in a learning environment.  We can learn a great deal about team dynamics by studying multiple weapon systems across multiple arenas.  Our work in these areas will allow us to see what each team of weapon system operators needs to be effective in their areas of operations.  MRS can then create a list which could be used in training and as well as have real world implication.  MRS has the potential to help better define training requirements and overall education of different weapon systems by allowing weapon systems that would not normally brief/debrief together the opportunity to observe and see what their specific impact was on the mission.  By developing a common toolset for brief/debrief systems MRS can help to ensure the maximum amount of learning possible.  A common brief/debrief toolset has the potential to increase the amount of learning by decreasing potential technological roadblocks by identifying non-compatible software or hardware.  The toolset has the potential to allow different weapon systems to understand how they interact and fit into the bigger picture, allow for more focused and greater detailed debriefs, as well as allow team members to focus on what was really important during the AAR.MRS’ Brief/Debrief Testbed is focused on improving data, functionality, instructional tools, and collaborative methods for mission planning, briefing, and AAR in distributed training environments.  With this in mind, MRS will continue to research multiple platforms during DMO events (both US and CMTR) in an effort to establish standardized practices, improve the application of tools and technology, as well as use the data to help quantify improvements in individual and team training and performance.   5. References [1]  Tuckman, Bruce: “Developmental Sequence in Small Groups” Psychological Bulletin, Vol. 63, pp. 384-399.[2]  Dion, Kenneth L: “Group Cohesion: from ‘Field of Forces’ to Multidimensional Construct” Group Dynamics: Theory, Research, and Practice, Vol. 4(1), pp. 7-21, 2000. [3] Mullen, Brian, & Cooper, Carolyn: “The Relationship Between Group Cohesiveness and Performance: An Integration” Psychological Bulletin, Vol. 115(2), pp. 210-227, 2004[4] Driskell, James E., Radtke, Paul H., & Salas, Eduardo: “Virtual Teams: Effects of Technological Mediation on Team Performance” Group Dynamics: Theory, Research, and Practice, Vol. 7(4), pp. 297-323, 2003.  [5] Perch, Richard J.: “Termites, group behaviour, and the loss of innovation: Conformity Rules!” Journal of Managerial Psychology, Vol. 16(7), pp. 559-574, 2001.  [6] Chen, Leida, & Nath, Ravi: “Nomadic Culture: Cultural Support for Working Anytime, Anywhere” Information Systems Management Journal, Fall 2005.[7] Knippenberg, Saan van, Drew, Carsten K. W. De, & Homan, Astrid C.: “Work Group Diversity and Group Performance: An Integrative Model and Research Agenda” Journal of Applied Psychology, Vol. 89(6), pp. 1008-1022, 2004.  [8] Pickering, Cynthia, & Wynn, Eleanor: “An Architecture and Business Process Framework for Global Team Collaboration” Intel Technology Journal, Vol. 8(4), pp. 373-382, 2004.  [9] Guzzo, Richard A., & Dickson, Marcus W.: “Teams in Organizations: Recent Research on Performance and Effectiveness” Annual Review of Psychology, Vol. 47, pp. 307-323, 1996.  Author BiographiesKRISTEN BARRERA is a behavioral scientist at the Air Force Research Laboratory, Warfighter Readiness Research Division in Mesa, AZ.  She is the program manager for Brief/Debrief technology research.  She received her B.S. in Psychology from the United States Air Force Academy in 2000 and a M.S.H.S. in Emergency and Disaster Management from Touro University International in 2004.  BRENDA BLUEGGEL is a behavioral scientist at the Air Force Research Laboratory, Warfighter Readiness Research Division in Mesa, AZ.  She is the deputy program manager for Brief/Debrief technology research.  She received her B.S. in Psychology from Angelo State University in 2005.   MICHELLE NASH is a behavioral scientist at the Air Force Research Laboratory, Warfighter Readiness Research Division in Mesa, AZ.  She is the deputy branch chief for the HEAS branch.  She received her B.S. in Psychology from Weber State University.   WINSTON BENNETT JR. PhD is a Senior Research Psychologist and team leader for the training systems technology and performance assessment at the Air Force Research Laboratory, Human Effectiveness Directorate Warfighter Readiness Research Division in Mesa, AZ.  He received his PhD in Industrial/Organizational Psychology from Texas A&M University in 1995. 