Development of an HLA Voice Federate for Individual CombatantsBilly FossRon HoferRobert FranceschiniRichard UrichInge ByrneInstitute for Simulation and TrainingUniversity of Central Florida3280 Progress DriveOrlando, FL  32826-0544407-658-5540,407-658-5576, 407-658-5519 HYPERLINK mailto:bfoss@ist.ucf.eduu bfoss@ist.ucf.edu,  HYPERLINK mailto:rhofer@ist.ucf.edu rhofer@ist.ucf.edu,  HYPERLINK mailto:rfrances@ist.ucf.edu rfrances@ist.ucf.edu Paul DumanoirJames GrosseU.S. Army Simulation, Training, and Instrumentation Command12350 Research ParkwayOrlando, FL  32826-3276407-384-3838, 407-384-3872 HYPERLINK mailto:paul_dumanoir@stricom.army.mil paul_dumanoir@stricom.army.mil, HYPERLINK "mailto:james_grosse@stricom.army.mil"james_grosse@stricom.army.milKeywords: HLA, Voice Recognition, Voice Synthesis, C2, Individual CombatantABSTRACT: Creating synthetic environments for training and evaluating operational concepts for individual combatants involving both live participation and synthetic representation introduces a number of technology challenges. The STRICOM Individual Combatant Science and Technology Objective (STO) program is addressing these challenges through integrating and evaluating key technologies needed to create individual combatant synthetic environments. A voice recognition and synthesis interface between live participants and the augmented synthetic individual combatants represented by semi-automated forces is needed to achieve a natural sense of immersive presence for live participants in the environment.This paper outlines the development of a voice federate for use with the High Level Architecture (HLA). This federate as part of an HLA exercise allows a user to give spoken commands to computer generated individual combatants (virtual ICs) in the simulation exercise. These voice commands are translated to natural language text and then translated to commands that the Semi-Automated Forces (SAF) system can accept. The approach includes investigation of and enhancements to the CommandTalk system and its interface to ModSAF to better support command and control (C2) of individual combatants (ICs). A separate Voice Federate (VF) being developed by the Institute for Simulation and Training (IST) allows immersed soldiers to command and control computer generated ICs. This VF interfaces to Dismounted Infantry Semi-Automated Forces (DISAF) via HLA. This interface reuses some CommandTalk system design and software components. Enhancements for the CommandTalk system are described and lessons learned from the development of an HLA interface to DISAF for voice based on a previous architecture are presented. Development of a Simulation Object Model (SOM) for the VF to transmit voice commands, text, and other C2 data using the HLA are also included.IntroductionCreating synthetic environments for training and evaluating operational concepts for individual combatants involving both live participation and synthetic representation introduce a number of technology challenges. The STRICOM Individual Combatant (IC) Science and Technology Objective (STO) program is addressing these challenges through integrating and evaluating key technologies needed to create individual combatant synthetic environments.  Through development of an Individual Virtual Environment Technologies (IVET) test bed, a number of technology demonstrations are being formulated and evaluated to determine the effectiveness and performance of an integrated environment of live and virtual individual combatants.  A voice recognition and synthesis interface between live participants and the synthetic individual combatants represented by semi-automated forces is needed to achieve a natural sense of immersive presence for live participants in the environment.  This paper describes IST’s Voice Federate project, one of the key IVET components.   It provides project background, goals, approach, and lessons learned so far for developing a reusable High Level Architecture (HLA) Voice Federate (VF).   Background and ObjectivesThe goal of this project is to develop a reusable Voice Federate for use with the HLA that provides capability for live and virtual ICs to interact through spoken voice statements.  This federate, as part of an HLA exercise, allows a user to speak commands to computer generated individual combatants (virtual ICs).  These voice commands are translated to natural language text and then translated to commands that the Semi-Automated Forces (SAF) system can accept.  Correspondingly the VF translates virtual IC command statements to synthetic voice spoken statements understandable by live participants, thus providing for two way voice interaction between live participants and virtual IC entities in an integrated human-simulation environment. Key VF objectives include:recognize and synthesize speech, run on a PC under Windows NT, operate with a US Army IC speech set (with future capability to operate with other speech sets/languages other than English), communicate with DISAF in live IC command and operator interface modes,have an HLA compliant interface (including compatibility of the VF Simulation Object Model (SOM) and Real-time Platform Reference Federation Object Model (RPR FOM) for easy integration into Distributed Interactive Simulation (DIS) legacy environments), use the latest Commercial Off-The-Shelf (COTS) products for voice recognition and synthesis, andreuse existing software, when possible.The current phase of the project focuses on two modes of voice interaction with virtual ICs.  The first mode allows a SAF operator to use voice to control the graphical user interface (GUI) for the SAF.  The second mode allows a small unit leader to use voice to control the virtual ICs with command and control (C2) commands.  Both modes could also be used simultaneously by a small unit leader sitting at the SAF GUI, who could manipulate the SAF GUI and issue orders to the virtual ICs.In the first mode, a SAF operator sits at the SAF GUI and uses voice commands to control aspects of the Plan View Display (PVD) of the scenario.  These commands include “zoom in”, “zoom out”, “center on unit x”, “set scale to one over y”, and “center on x y”.  In the second mode, a small unit leader issues command and control (C2) commands to virtual ICs.  Normally the leader will be in a fully immersive virtual environment, but the same voice commands could be issued from the SAF GUI.  These commands include “move to objective charlie”, “move out”, “halt”, and “hold fire”.  The leader receives voice feedback from the virtual ICs to indicate the orders were received and understood.  This allows the small unit leader to coordinate and control virtual ICs without additional operator intervention.  Figure 1 illustrates the conceptual network layout for the simulation architecture and the corresponding mix of live and virtual ICs in an integrated simulation environment.  The human sitting at the Voice Federate PC is acting as a fire team leader and SAF controller.  When the leader speaks the “move out” command, the Voice Federate translates the speech to commands.  These commands are then sent over the HLA network where the SAF application interprets them and changes the appropriate behavior of the Virtual ICs.Approach Two parallel efforts are underway to provide voice interaction capabilities for the fall 2000 IVET demonstration. One focuses on modifying the CommandTalk system to interface with DISAF using the US Army IC speech set to provide a low risk path to demonstrating voice recognition by reusing much of the CommandTalk system. The other focuses on developing a Voice Federate (VF) that goes beyond CommandTalk by building an HLA compliant federate using the latest COTS products for voice recognition and synthesis while reusing some of the CommandTalk design. These two parallel efforts are being integrated into a single test environment for further research and evaluation.  The main components of this test environment are illustrated in Figure 2.  This figure illustrates that the VF uses the Run Time Infrastructure (RTI) of HLA to communicate with DISAF through an HLA Voice Interface, while the CommandTalk system uses a separate existing interface to DISAF.  Since DISAF is not HLA native, it requires the HLA Voice Interface to DISAF to receive voice commands from other HLA federates.  More detailed descriptions of these two efforts are given in the following sections.Enhanced CommandTalkCommandTalk was originally developed for the Synthetic Theater of War (STOW) exercise.  Its interface to the STOW version of ModSAF supports SAF GUI commands as well as C2 commands to selected entities.  The technical architecture uses the Open Agent Architecture (OAA) as a unifying framework with distributed agents provided for voice recognition, natural language understanding, context interpretation, and communications with ModSAF [1][2][3].  IST extended CommandTalk in two ways.  First, the CommandTalk SAF interface was modified to work with DISAF.  This required changes to support new IC functionality available in the latest version of DISAF, including new versions of suppressive fire and movement.  Second, the CommandTalk vocabulary and grammar were extended to use US Army-appropriate language in speaking with ICs.  A description of this Army IC Speech Set is provided in Section 3.3.HLA Compliant Voice Federate DevelopmentThe Voice Federate (VF) supports spoken orders from immersed squad leaders and commands from SAF operators. It recognizes speech commands and synthesizes spoken responses.  The new voice federate architecture uses COTS speech recognition and synthesis products and introduces an HLA compliant interface to the existing OAA-based connection to DISAF.  Additionally, the design incorporates a grammar supporting the Army IC Speech Set and considers important factors in developing the SOM that will provide for extended voice federate capabilities in the future. Figure 3 illustrates the components of the VF design framework and their connection to DISAF using HLA.    It also shows the use of COTS speech recognition and synthesis packages within the Voice Federate.   This development effort also serves as a basis to evaluate the software usability and to determine areas of improvement.As compared to the CommandTalk system, the VF combines a simplified design with improved accuracy and fidelity.  The use in the VF of commercially available packages eliminates the need for several of the CommandTalk components.  As noted earlier, some CommandTalk components are being reused in the short term to reduce development time.  The use of commercially available products for speech recognition should improve the accuracy of the speech recognized.  This is because the COTS products use newer technology than was available for CommandTalk.  Similarly, the COTS speech synthesis products provide higher fidelity sound.Figure 4 illustrates the screen view of the HLA Voice Federate.  The buttons at the top allow the user to connect to and disconnect from HLA and start and stop the speech recognition and speech synthesis engines.  The checkboxes allow the user to specify which interfaces are connected to each other.  For example, the row labeled “Microphone” indicates that the VF will accept input from the microphone and will output to the screen (in the form of text) and to HLA as both text and a waveform (WAV) file.  The center area indicates voice commands that were accepted and responses that were received from DISAF.  The text box at the bottom allows the user to input commands via the keyboard (rather than using the microphone).The HLA Voice Interface to DISAF provides an HLA interface supporting voice commands for DISAF.  The VF uses this HLA interface to send commands to DISAF.  The interface is limited to the objects and interactions described in the VF SOM. Several existing components developed for the CommandTalk interface to ModSAF are reused or modified.  These applications are based on the OAA and include natural language processing (NLP) and context interpretation (CI).  Figure 5 illustrates the components of the HLA Voice Interface to DISAF by placing them in a dashed box.  The HLA to OAA component, developed by IST, translates HLA spoken text into the OAA equivalent data. It subscribes to commands from the VF via HLA and translates them into the equivalent OAA commands.  The NLP provides natural language interpretation of the recognized speech and translates the text into a representation of the command known as the logical form. IST modified it to support the Army IC Speech Set.  The CI provides context interpretation based on the known objects in DISAF.  IST modified it to understand the new enumerations, tasks, and behaviors in DISAF.  The Open Agent Architecture Facilitator (OAA) provides the connections between these and DISAF.Army IC Speech SetA small speech set suitable for Army IC C2 communication has been developed.  This speech set focuses on C2 commands necessary for squad leaders to command and control their fire teams in an immersive environment.  The speech set uses the CCTT Dismounted Infantry Manned Module (DIMM) commands and the CommandTalk commands as a starting reference. The current version of the Army IC speech set is described in the next two subsections. Two different vocabulary and speech sets are supported by the VF and the Enhanced CommandTalk system.  Small samples of the vocabulary and grammar are provided.  GUI Control CommandsThe GUI control commands allow the SAF operator to manipulate the display and perform simulation management functions using voice instead of the mouse and keyboard.  The following phrases are supported:Zoom InZoom OutCenter on X, YCenter on UNITSet scale to one over XCreate point at X YIC Command and Control CommandsThe immersed squad leader can issue commands and change parameters of these commands using the phrases described below.  The initial implementation supports commands to the entire fire team only.  For example, the squad leader could issue a “move out” command to his alpha fire team and the entire team would respond. Several tables below give a detailed view of the capabilities of the Army IC Speech Set.  The following list demonstrates a few of the supported commands.one bravo spread outBravo fire team increases spacing distance in the Traveling task one alpha move to 6554 8463Alpha fire team creates a Traveling task to move to 6554 8463one alpha fire at 6553 8462Alpha fire team creates a Suppressive Fire task to fire at 6553 8462 one alpha haltAlpha fire team temporarily replaces the current Traveling task with a Halt taskone bravo move to objective deltaBravo fire team creates a Traveling task to move to objective Done bravo fire at objective echoBravo fire team creates Suppressive Fire task to fire at objective EThe squad leader will use the following table to identify the particular fire team.  Currently, DISAF supports three different fire teams (alpha, bravo, and charlie).  DISAF allows these units to be labeled with any valid unit marking including duplicates.  For the purpose of these scenarios, we mark the fire teams with the letter that matches their type (i.e. an alpha team would be labeled with an A).  Due to the method of matching speech commands to fire teams, there will only be one alpha fire team per squad.  Table of Unit Names SpeechDISAFSquadTeamIndivOne Alpha1 A 1AAllOne Alpha One1 A 11A1One Alpha Two1 A 21A2One Alpha Three1 A 21A3One Alpha Four1 A 21A4X Bravo YX B YXBYX Charlie YX C YXCYThe grammar allows the following command structure.<unit identifier> <action> <parameters>Table of ActionsAll actions require a team specifier.SpeechDISAF BehaviorHaltHaltMove outRelease Move On Order ORResume Move BehaviorMove to XTravelingFire at XSuppressive FireCommence fireRelease Fire On Order ORResume Fire BehaviorCease fireEnd Suppressive Fire TaskTable of ParametersAll parameter changes require a team parameter.SpeechChanges in Execution MatrixSpeed upChanges traveling parameterSlow downChanges traveling parameterHold fireSet rules of engagement to tightFire at willSet rules of engagement to freeGet in columnChanges traveling parameterGet in lineChanges traveling parameterGet in fileChanges traveling parameterGet in wedgeChanges traveling parameterSpread outChanges traveling parameterTighten upChanges traveling parameterVF SOM DevelopmentFormulation of the Simulation Object Model (SOM) for the Voice Federate uses an evolutionary approach.  The implementation of the VF for the initial IVET demonstration uses a limited HLA network interface. Other entities in the demonstration are connected using a DIS network interface. Future demonstrations will expand the number of speakers connected through the HLA network interface. Therefore, the aspects of developing the VF SOM discussed in this paper will consider factors necessary to provide a SOM organization that has both flexibility and extensibility to adjust for future IC technology demonstrations.  The conceptual reference model for developing the VF SOM, shown in Figure 6, explicitly identifies a voice interface that can be associated with either live or computer based entities.  Each interface may send and receive voice interactions.  There are two modes of interaction.  One mode (Paths A&B) interchanges spoken voice statements from live participants with computer based entities and returns synthetic voice representation to the live participant of statements received from the computer based entities.  The other mode (Path C) interchanges spoken voice statements directly between two live participants.  As illustrated, a live entity may be either a participant in the exercise or a SAF operator.  For the initial IVET demonstration, the VF SOM represents exchanges between the live and virtual ICs across the HLA network.  The initial VF SOM is patterned after the RPR FOM organization; this provides a general-purpose voice interface that can be used in many federations and is extendible for future IC demonstrations.  The VF implements the voice interface shown in the reference model as an embedded system that can be associated with any entity represented in a federation in the same way the RPR FOM treats designators, emitters and radio transmitter/receiver interactions.  A new subclass, “VoiceInterface,” is included as an extension to the RPR FOM “EmbeddedSystem” subclasses.  The outline of primary VF SOM tables shown in Figure 7 illustrates the initial SOM organization. EntityIdentifier and HostObjectIdentifier attributes are incorporated from the RPR FOM to provide unique identification of entities and hosts to which the VoiceInterface is embedded.  The InterfaceMode attribute is currently included to provide capability for implementing a VF capable of operating in more than one mode (for example, as a SAF Operator or as an immersed soldier).  Two interaction classes, one for interchanging IC voice commands in the form of ASCII text and the other for interchange as binary strings provide the flexibility to implement voice command exchanges in formats most suited for the application.  As additional implementation details develop in preparation for the IVET demonstration, a full set of SOM tables will be completed using the RPR FOM [4] and the emerging IEEE 1516.2 standard [5] as the working frame of reference.  These tables will include how voice statement time tags and transport will be handled by the demonstration VF.   Consideration will also be given to how the VF will respond to pause and resume during an exercise as well as other exercise management functions that are important for controlling the VF interactions.  Future extensions to the VF capability will necessarily require revision of the VF SOM. Lessons LearnedIST recently performed an initial engineering test for the Enhanced CommandTalk system and the HLA Voice Federate.  During the development and preparation for this test, we discovered several issues, which will be discussed in this section.  First we describe the lessons learned from the enhancement of the CommandTalk system.  The primary modifications were to the vocabulary and grammar to support the Army IC Speech Set.  The current CommandTalk system contains an extensive grammar and vocabulary coded in several hundred lines of Prolog.  This code required a special Prolog compiler of which we only have a version that generates executable code for the SGI platform rather than for the PC NT platform.  This Prolog code is closely tied to DISAF by including detailed lists of specific unit types, task frames, and task parameters available in DISAF.  As DISAF is upgraded, these lists also need to be modified.  We therefore found it difficult both to extend the vocabulary and grammar of CommandTalk and to port CommandTalk to new SAF applications or operating systems. The files to change the grammar and vocabulary for the Nuance Speech Recognition component of CommandTalk were not included in the software we received.  So, while we were able to change the commands recognized by the Gemini Natural Language and Context Interpreter components, we could not change the speech phrases recognized by the Nuance speech recognition engine.  Nuance is now a commercial product and the original software is not available.  This limited support makes it impractical to continue development of the CommandTalk system using new environments such as the Windows NT PC platform.  Next, we describe the lessons learned from the development of the HLA Voice Federate.  We started development with the Microsoft Speech Application Programmer’s Interface (API) Software Development Kit, which included simple Speech Recognition and Text-to-Speech engines.  We found the accuracy unacceptable (approximately less than 10%) so we tested three commercial Speech Recognition packages: Dragon Systems NaturallySpeaking [6], IBM ViaVoice [7], and L&H VoiceExpress [8].  Each of these systems required the user to train to their voice and to a sample set of expected phrases before reasonable accuracy could be achieved.  One of our design goals is to provide high accuracy recognition for a limited vocabulary over a wide range of speakers.  As part of this effort, we looked at customizing the vocabulary and grammar to recognize only the limited Army IC Speech Set.  This customization requires the development version of each of these products.  We chose to use Dragon Systems due to the lower initial licensing costs.  Soon we learned that only the Professional edition of NaturallySpeaking supports the custom vocabularies.  NaturallySpeaking natively supports a very large vocabulary.  We developed custom vocabularies in an effort to minimize the recognition of phrases outside the Army IC Speech Set.  We learned that custom vocabularies are added to instead of replacing the existing vocabulary.  This allowed words and phrases to be recognized that are not in the Army IC Speech Set.  The custom dictionaries do, however, change the statistical processing so that the Army IC Speech Set phrases are recognized more often.  We were able to perform the engineering test with good accuracy by training the system on specific users and specific phrases.  However, the accuracy of speaker independent recognition with no training is limited.Summary and ConclusionsThis paper presented an overview of the approach and design for developing a reusable voice input/output federate with an HLA interface for use with live-virtual simulation environments.  The initial implementation of the approach supports the first IVET demonstration in the fall of 2000.  A number of issues will need to be addressed in future versions of the voice federate.  These include how the voice federate will interface with radio models and direct live to live paths as well as how to include interfaces for voice inputs and outputs from multiple speakers in both the live and virtual domains.  Equally important will be issues related to how to allocate and implement functions and capability that are resident in the stand alone component of the voice federate and those integrated in to the SAF interface.  Continued design and implementation of the speech set grammar will consider how interchangeable command speech sets in languages other than English may be incorporated in future versions of the VF.  Extensions to the current work will include VF SOM descriptions that represent spoken voice interchange between two or more live participants.  This will include interfaces with radio models and representation of multiple voice input sources.  These capabilities will enable voice interactions among multiple speakers and listeners within simulated earshot of one another.  Insights gained from the initial demonstration will provide focus for how to approach these and other issues that may arise. AcknowledgmentThe work presented in this paper was supported by the US Army Simulation, Training, and Instrumentation Command under contract N61339-00-K-0006.  That support is gratefully acknowledged.References [1] SRI, Open Agent Architecture Website,  HYPERLINK http://www.ai.sri.com/~oaa http://www.ai.sri.com/~oaa.[2] D. L. Martin, A. J. Cheyer, and D. B. Moran, “The Open Agent Architecture: A framework for building distributed software systems,” Applied Artificial Intelligence: An International Journal. Volume 13, Number 1-2, January-March 1999. pp 91-128.[3] Robert Moore, John Dowding, Harry Bratt, J. Mark Gawron, Yonael Gorfu and Adam Cheyer. (1997) “CommandTalk:  A Spoken-Language Interface for Battlefield Simulations”, in Proceedings of the Fifth Conference on Applied Natural Language Processing, Washington, DC, pp. 1-7, Association for Computational Linguistics.[4] SISO-STD-001. 1-1999:Real-time Platform Reference Federation Object Model.   HYPERLINK "http://www.sisostds.org/" http://www.sisostds.org/[5] IEEE P1516.2/D5 “Draft Standard for Modeling and Simulation (M&S) High Level Architecture (HLA) – Object Model Template (OMT) Specification,  HYPERLINK "http://standards.ieee.org/" http://standards.ieee.org/[6] Dragon Systems NaturallySpeaking Professional Product Overview.  HYPERLINK http://www.dragonsystems.com/products/naturallyspeaking/professional/index.html http://www.dragonsystems.com/products/naturallyspeaking/professional/index.html.[7] IBM Voice Systems, IBM ViaVoice Product Overview.   HYPERLINK http://www-4.ibm.com/software/speech/millennium/professional.html http://www-4.ibm.com/software/speech/millennium/professional.html[8] Lernout & Hauspie, L&H VoiceExpress Pro Overview,  HYPERLINK http://www.lhsl.com/voicexpress/pro http://www.lhsl.com/voicexpress/pro.Author BiographiesBilly Foss is an Associate Research Computer Scientist at the Institute for Simulation and Training.  He is currently researching voice recognition in an HLA environment and virtual object overlay into live scenes.  He also is investigating applications of cognitive modeling in distributed simulation.  Mr. Foss received a B.S. and a M.S. both in computer science from the University of Central Florida.  He is currently pursuing a Ph.D. in Computer Science.  His research interests are in distributed computing, software architecture, and simulation.RONALD HOFER is an Associate Director for the Institute for Simulation and Training at the University of Central Florida. He has extensive background and experience in formulating, directing and conducting research in the area of distributed simulation to achieve new and advanced capabilities with both government and commercial applications. Current research interests forcus on collaborative synthetic environments, distributed learning environments, object based modeling methods and abstraction frameworks, knowledge representation, complex information visualization, simulation system life cycle cost reduction and multi-disciplinary technology integration.  Dr. Hofer holds a Ph.D. degree in Electrical Engineering from Purdue University with minors in mathematics and physics.Robert Franceschini is an Assistant Professor in the UCF School of Electrical Engineering and Computer Science and a Senior Research Computer Scientist at the Institute for Simulation and Training.  He has performed research at IST in distributed simulation, computer generated forces, and multi-resolution simulation.  He has over 30 published papers in those areas and has been awarded research contracts worth over $1.2 million.  Dr. Franceschini received a B.S. in Computer Science from the University of Central Florida in 1992 and a Ph.D. in Computer Science from UCF in 1999.RICHARD URICH is a Research Assistant at IST.   Mr. Urich is pursuing a B.S. in Computer Science at UCF.INGE BYRNE is a Research Assistant at IST.  Ms. Byrne is pursuing a B.S. in Computer Science at UCF.PAUL DUMANOIR is the lead principle investigator for Individual Combatant (IC) simulations at the U.S. Army STRICOM.  Mr. Dumanoir leads the DoD Defense Technology Objective (DTO) for IC and Small Unit Operations Simulation, and the IC Science & Technology Objective (STO).  Prior to his involvement with IC simulations, he worked as software and systems engineer on the various M&S programs.  His current interests include IC CGFs and Human-In-The-Loop (HITL) networked simulators.  He earned his B.S. in Electrical Engineering from the University of South Alabama in 1987 and his M.S. in Computer Systems from the University of Central Florida in 1991.JAMES GROSSE works on the Individual Combatant (IC) Simulation Programs at the U.S. Army's STRICOM.  Mr. Grosse works on the DoD Defense Technology Objective (DTO) for IC and Small Unit Operations Simulation, and the IC Science & Technology Objective (STO).  Prior to his involvement with IC Simulations, he worked as the lead Engineer on various M&S programs involving instrumented ICs inside a MOUT site.  He earned his B.S. in Electrical Engineering from Drexel University in 1988.		 EMBED MS_ClipArt_Gallery  HLA Network EMBED MS_ClipArt_Gallery  SAFVirtual ICsVoice FederateFigure 5: HLA Voice Interface to DISAF.Figure 7.  Outline of primary VF SOM tables.Objective CMove OutHuman ICMicrophoneFigure 6.  VF SOM Reference Model. EMBED PowerPoint.Slide.8  CommandTalk (Effort 1)HLA NetworkMicrophoneIST VF(Effort 2)HLA Voice Interface to DISAFDISAFRTIFigure 2. IC Voice Federate test environment.Figure 4.  Screen view of the HLA Voice Federate.Figure 3: Voice Federate design framework.HLA Network (RTI-NG 1.3v3)COTS Speech RecognitionDISAFCOTS Speech SynthesisHLA Voice Interface to DISAFMicrophoneVoice FederateSpeakerObject Class Structure TableEmbeddedSystem (N)VoiceInterface (P/S)Attribute TableObjectAttributeDatatypeUpdate TypeEmbeddedSystemEntityIdentifierEntityIdentiferStructStaticHostObjectIdentifierRTIObjectStructStaticVoiceInterfaceInterfaceModeenumeratedConditionalObject Interaction TableInteraction 1Interaction 2VoiceInterfaceInterchange  (N)VoiceTextInterchange (IR)VoiceBinaryInterchange (IR)Parameter TableInteractionParameterDatatypeVoiceTextInterchangeVoiceStatementLengthunsigned shortVoiceStatementASCII stringVoiceBinaryInterchangeVoiceStatementLengthunsigned shortVoiceStatementBinary stringHLA Voice Interface to DISAFCI liblesafRTIDISAFHLA to OAAOAAVoice FederateMicrophoneHLA NetworkNLP   Virtual ICSAF ControllerFigure 1.  Conceptual illustration of Voice Federate HLA network interface and voice commands in an integrated live-virtual IC environment.