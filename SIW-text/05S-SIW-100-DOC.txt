Challenges in Developing a Performance Measurement System for the Global Virtual Environment.Antoinette M. PortreyLoren B. KeckBrian T. SchreiberLockheed Martin Systems ManagementAir Force Research LaboratoryWarfighter Readiness Research Division6030 South Kent StreetMesa, Arizona 85212-6061 HYPERLINK "mailto:Antoinette.portrey@mesa.afmc.af.mil" Antoinette.portrey@mesa.afmc.af.mil HYPERLINK "mailto:Loren.keck@mesa.afmc.af.mil" Loren.keck@mesa.afmc.af.mil HYPERLINK "mailto:Brian.schreiber@mesa.afmc.af.mil" Brian.schreiber@mesa.afmc.af.milABSTRACT:  The Air Force Research Laboratory, Warfighter Readiness Research Division, is continuously researching technologies to measure and track performance of knowledge and skills from an individual level to the Command and Control (C2) level, within both high fidelity distributed simulation environments and live training environments. This program initially developed a performance measurement system, the Performance Effectiveness Tracking System (PETS), which successfully captured the objective data necessary for Mission Essential Competency (MEC) evaluations, end-user performance feedback, simulator technology developer validation, and for researcher and program manager evaluation of training techniques and technologies. This paper describes the preliminary testing of a new architecturally-structured performance measurement system (PETS²), expanded to encompass the need for higher-level measurement capabilities within multiple diverse virtual environments. This system has been implemented in a Distributed Mission Operations Testbed (four high-fidelity F-16 simulators, one Airborne Warning and Controller System console, and Instructor Operator Station), that can interoperate in  highly distributed, long haul DIS/HLA networks.  This paper will describe the initial application of PETS² within multi-player, multi-platform virtual exercises; discuss the lessons learned, and the challenges faced in producing effective analysis of training results.IntroductionThe dramatic transformation of America’s strategic environment continues its significant impact on today’s Warfighter and its demand for transformations in how we prepare for combat operations. Emphasis remains on shifting from deliberate to adaptive war planning, and from permanent organizations and large hierarchies to smaller, highly-distributed joint and combined forces [1]. A distributed training environment that constitutes global, multinational networks of constructive computer simulations, man-in-the-loop virtual simulators, and live forces at instrumented ranges is key to achieving military performance objectives and meeting current and future Warfighter demands [2]. Distributed joint and coalition simulation developmentAs the necessity of military commitments steadily increases in today’s world, joint and coalition training continues to play a critical role in Warfighter preparation.  Currently, Warfighter forces regularly participate in large scale “live-fly” joint exercises such as Red Flag and Maple Flag to meet training objectives. This type of high-visibility training event, often involving coordination of multinational forces, typically occurs several times a year and helps keep the Warfighter combat ready.The use of equipment, maintenance, and support personnel required to perform these “live-fly” events has stimulated a significant need for simulation based training.  Simulation based training in a Distributed Mission Operations (DMO) environment is currently used to prepare the Warfighter and augment live-fly events by providing the ability to practice or learn skills and techniques that can be validated and sharpened through live training and real world use.  DMO based training fills the need to “train the way we intend to fight,” and has become a critical requirement in Warfighter preparation, enabling the Warfighter to maintain combat readiness through joint and coalition mission rehearsal in operationally realistic environments.Simulation technology today allows Warfighters to participate in a continuous training cycle and maintain a high state of combat readiness by using cost-effective simulation alternatives in conjunction with live-fly operations and training missions. The rapid advancement of networking technologies, the increase in bandwidth capabilities, and the continued improvement of protocol standards/architectures such as Distributed Interactive Simulation (DIS), High Level Architecture (HLA) and the Test and Training ENabling Architecture (TENA) have all contributed to an environment where large-scale, multi-force DMO joint and coalition exercises have become a reality. Current development and integration of live, virtual, and constructive systems for training, mission rehearsal, and operations support has emphasized the need for more complete simulation network interoperability among joint and coalition forces. Specifically, improvements in protocol standardization and acceptance of comprehensive performance measurement systems stimulate interoperability and help to validate the return on investment (ROI) that joint operations provide.  With the change in training scope from traditional to transitional training, including environments such as special operation forces, urban operations, and joint/coalition, measuring human performance gains in these environments is critical if we are to understand our Warfighters’ readiness levels. The Need for a Robust Performance Measurement SystemThe Air Force Research Laboratory in Mesa, AZ (AFRL/Mesa), a scientific organization chartered to implement and/or evaluate methods to improve Warfighter readiness, has been commissioned to do human performance assessment research with respect to United States Air Force Mission Essential Competencies.  One of the primary goals of this research was to investigate the ability to assess Warfighter performance in a DMO environment.  This capability, if done properly, would allow the research division to collect data on any number of diverse studies.That initial line of research resulted in a DIS compatible tool capable of collecting objective data on the DMO assets within the local area network (LAN) site at that division.  As an evaluation tool, the proof-of-concept Performance Effectiveness Tracking System (PETS), see references [3], [4] and [5], emphasized tracking human performance data in an empirical (i.e., scientific) manner for formally evaluating training techniques and technologies.The early successes of this system has allowed AFRL/Mesa to automatically capture and store kill ratios, weapons employment, and other skill-related metrics on over 400 fighter pilots participating in multi-player DMO exercises at AFRL/Mesa’s site since January 2002.  The usefulness of this empirical data facilitated a number of recent human performance DMO studies, see references [6], [7], [8], [9], and [10], and data collaboration and study efforts from both industry and academia. Additional AFRL/Mesa studies are either planned or currently in the data collection stage (e.g., DMO studies on skill decay and transfer).  Due to the a) usefulness of this data for studies, b) an unanticipated piqued warfighter interest in its potential for feedback, and c) expanding DMO exercises outside of LAN environments (e.g., joint and coalition), this measurement capability was then sought by AFRL/Mesa for DMO exercises involving assets outside the LAN—essentially an expanded scope of the original human performance assessment research.  Additionally, this proof-of-concept assessment tool, PETS, was identified by the United States Air Force (USAF) Air Combat Command (ACC) as a Category 1 Advanced Technology Demonstration project potentially capable of being instituted across the USAF for evaluating individual and team skill.  If successful, this system would be standardized and become a primary piece of DMO technology to assess the Air Force’s Mission Essential Competencies (MECs) [11].  These new requirements reduce, but do not eliminate, the emphasis on empirical data.  Rather, these broadened requirements increase the emphasis to collect data on any entity involved in a DMO network and report performance metrics as feedback (i.e., an increased emphasis on a new “observational” measurement capability) to Warfighters and their instructors.  To fulfill these new, larger objectives, a different architecture was required.  Performance Effectiveness Tracking System v1.6The PETS software, developed at AFRL/Mesa in 2000-2001, was designed as a proof-of-concept human performance measurement system that could collect over 80-100 ‘core’ air-to-air and air-to-ground combat performance measures in real-time from a distributed network using the DIS protocol. This original PETS system architecture is actively used to provide a number of previously unavailable objective measures which significantly increase the effectiveness and quality of DMO training and research.  However, due to its origin as a proof-of-concept application, some limitations to its extensibility existed. The original proof-of-concept application was designed to gather empirical data within the LAN DMO at AFRL/Mesa, and therefore it could not be expanded to meet the growing need for virtual joint and coalition exercises. As a result of inherent variability in different Instructor Operator Stations (IOS) and network environments, PETS lacked the flexibility and configurability needed for use at other sites, making any potential distribution of the PETS software difficult, (i.e., would require customized patches or site modifications).  Inevitably, limitations in the original design led to the first PETS system not being able to reach its full potential as a highly distributed application capable of capturing common performance data across DMO installations [11].  In order to collect data on any entity on multiple DMO networks, the architecture and programs must be robust enough to be “shrink-wrapped” or have simple click to install/run capability.Performance Effectiveness Tracking System Squared (PETS²)PETS² beta developed in 2003-2004, was designed to address the flexibility and configurability issues inherent in the original proof-of-concept version [12].  More specifically, we wanted an architecture that went beyond the previously “empirical-only” LAN architecture to a more robust and flexible wide-area “PETS empirical” and “PETS observational” architecture, hence the “squared” nomenclature. PETS² is also driven by the requirements for increased warfighter performance feedback and higher-level, aggregate measurement capabilities. A high-level view of the PETS² design is illustrated in Figure 2.1.Figure 2.1 PETS² ArchitectureThe PETS² project is designed as a modular, multi-threaded application, capable of robustly handling high volumes of network entities [4]. The preliminary version of this system is capable of handling several hundred entities within a DMO environment.  It provides a number of interfaces used to customize real-time informational graphic displays, such as a force demographic summary.  This system also employs a multi-tiered lookup system for additional user supplied internal state data that may be unavailable on the network, thus making it more interoperable to any network by removing the dependency on custom data requests.PETS² development has been extended to the team, package, and force levels. Therefore, the system has the ability to calculate measurements (see Table 2.1) at the team, inter-team (package), and teams-of-teams (force) levels, which can theoretically extend the potential measurement capabilities of PETS² up to, and including, objective data at the Command and Control (C2) level. Since this new architecture follows all entities, PETS² is able to evaluate overall mission performance on teams including both man-in-the-loop and of Computer Generated Forces (CGF), allowing the trainer to assess the entire picture from both the friendly and enemy perspective. PETS² provides a default implementation of team, package, and force objects, and the capability exists to build upon the base implementation as measurements are developed to assess higher-level competencies. OUTCOMEMEASURESPROCESSMEASURESMUNITIONEMPLOYMENTBombers reaching 2nm of targetTime each entity spent within MORWho shot what type of weapon at whomMinimum distance to target bombers achievedTime each entity spent within MARResult (hit, miss, and some types of misses).  Also records distance of misses for missiles.  For bombs, records left/right & long/short error distancesFratricidesTime each entity spent within N-pole2D and 3D range of shotMortalitiesChaff/Flare usageAltitude at pickleEnemy fighters killedMissile Clear Avenue of Fire, measured by anglesF-pole for hits and missesEnemy strikers killedG-load at pickle A-poleMissiles fired that resulted in a killWingman position in relation to leadLoft angle at pickleScenario demographics (e.g., number of threats presented, etc.)How often and for how long flight “steps-on” one another when talking on mikeMach at pickleTable 2.1 Types of DMO objective training effectiveness data currently available in the initial PETS² implementation.PETS² was designed from the ground up to support DIS standard data (currently DIS 2.04) and RPR1.0.  Although this may limit the amount of special (i.e., non-standard) data that a particular simulator emits, it allows PETS² to work with any simulator that conforms to the standards.  However, in order to support more complex measurements, PETS² has an array of user input screens that allow configuration of non-standard data such as weapons load, and various initial entity state conditions.  PETS² is designed as a “horizontally integrated” application that provides as much vertical depth as the protocol and user provided input allow.To help PETS² take advantage of non-standard network data, PETS² has the capability to add custom entity modules that can handle custom information packets “passed through” the network via the DIS or RPR protocol (such as DIS Set Data PDUs, etc.).  These custom entity modules are physically separate from the PETS² core and can be added or removed as the functional need or security requirements permit.  For example, if a particular site has a new, cutting edge, flight simulator operating at a higher classification level, that site can create a custom entity module that is kept and implemented only at their site for collection of special measurements while standard measurements are collected at all other locations on the network.Based on a user-defined rule set, PETS² can be configured to use custom entity modules for specific entities on the network.  For example, this could allow participants in a Joint or Coalition exercise to capture additional data on certain entities on a site-by-site, entity type, or force affiliation basis. Issues Observed during Virtual ExercisesJoint and Coalition exercise environments usually consist of diverse simulation systems and a wide array of internal and external networks. This diversity limits the effectiveness of a performance measurement system in providing information to the units and command staff.  Performance outcomes measured are inversely proportionate to the number of simulation systems and the complexity of the teams within the exercise environment (see Figure 3.1).  SHAPE  \* MERGEFORMAT Figure 3.1. Performance Measurement ComplexityAs previously mentioned, each participant is operating different simulation systems which, in real-world situations, often have different DIS data capabilities, DIS enumerations, and custom (non-standard) PDU information. Some of the main issues that were observed during joint and coalition exercises are described below. These issues play a big part in whether or not a performance measurement system can accurately report outcome measures from such exercises. Participant CapabilitiesInterface & Enumeration StandardsExercise Protocols, or Rules Of Engagement (ROE)Participant CapabilitiesAlthough the protocols are standard, due to real world requirements, each simulation system might be different in the variations of data packets that are emitted or handled. For example, interoperability is frequently adversely affected by missing or incomplete DIS PDUs, incorrect DIS enumerations, or non-standard data such as proprietary voice data implementations, or use of unsupported tactical data links. Better use of network protocols and improved interoperability awareness is needed to present a clear picture for performance measurement of all participants within the exercise.  Most simulations systems output some degree of non-standard or incomplete data during an exercise; while standards exist, installations do not strictly adhere to them.  Non-adherence simply should be resolved across all DMO installations, not just for performance measurement, but also for improved large-scale exercise interoperability.In addition to the “non-standard” problem, simulators also may have inconsistency in models and platforms. An example of inconsistent model that was observed in various coalition operations included inconsistency of flight models such as AIM-9 missile performance; some had unrealistic ranges or maneuvers. This inconsistency can significantly affect simple performance measurements such as shot/kill ratios. Interface and Enumeration StandardsThere needs to be clear standards regarding DIS header information and enumerations.  Accurate identification of platform models is essential to utilizable performance measurement.  Entity marking string usage was inconsistent among the simulation systems adding complexity to entity identification within the performance measurement system.  The complexity significantly increases the time to analyze the data for entity performance and outcome measures.  Distributed Mission Operations Center (DMOC) has instituted an Entity Marking requirement for virtual flag exercises where if an exercise participant is flying a friendly aircraft whose mission appears in the ATO, then the simulation must place an abbreviation on the aircraft call sign in the Entity Marking field of the Entity State PDU [13].  This is one example that can ensure that the correct measures correspond to the correct entity.Inconsistent timeout values are another major issue that can affect outcome measures.   For example, many sites will have different timeout values for air and ground entities.  This common change done to improve the performance of the network can hinder performance measurements by impacting the fidelity and latency of the output.  Due to the timeout differences, single entities could show up as multiple entities within the same exercise confusing the output results.Missing or incomplete network data typically plagues a coalition exercise.  Many simulation developers will opt to only include data that is significant to their own site or purposes, without regard on how this affects their overall visibility in a large scale DMO exercise.  For example, many simulators do not emit shooter/target information for bombs.  This causes a problem when determining performance at time of launch, measuring data during the fly-out, or doing real-time performance comparisons with other simulators on the network. These omissions can cause air-to-ground performance measurements to be incomplete or non-existent. Exercise Protocols or Rules of EngagementProtocol issues such as the use of shields and regeneration rules need be set, agreed upon, and followed consistently to ensure that common measurements such as shot/kill ratios are recorded accurately. It is extremely difficult to assess overall mission performance when the “fair-fight” issue is compromised.  For example, in a recent coalition simulation event a particular site determined that they would use shields on their aircraft in order to maximize time in the simulation event.  Because they were perceived as invincible by both themselves and other players, this caused the pilots and other participants to not respond in a realistic manner, thereby compromising the training quality of the data.  Worse yet, any shots taken and detonated on an entity with shields severely (and negatively) impacts the most important metric—kills.  In order to accurately train and assess performance at the C2 level, all players in an exercise must adhere to common rules that map directly to real world situations.  One simple over-arching rule must govern all simulation exercise protocol—realism.  That is, if it is not possible in war, the console operator(s), mission directors and simulation operators should not be allowed to do it in a simulation (e.g., use shields, reload weapons mid-air, “freeze” or “static” fuel burn, etc.).  If realism is emphasized when performance is calculated for an exercise, the results have much higher credibility and relationship to that of an actual battle—after all, the original impetus behind training in DMO was to translate performance gains into actual battle!  The paradox is that the DMO community continually strives for realism in all technology aspects (e.g., visuals, flight models, missile models, CGFs), but the DMO community appears to largely disregard exercise realism from the IOS.  Of course, occasional uses of shields or other functions may be highly desirable during early stages of training, but those non-realistic scenarios must be the exception, not the norm.Standards from simulation community To effectively study human and team performance within the DMO environment, simulation systems need to ensure that standards are adhered to.  However, each individual simulator provides a certain degree of non-standard simulation data.  This is because simulation environments are typically developed to address a specific training problem, such as weapons, flight, or instrument training.  As mentioned earlier, non-standard data is a problem that will need to be dealt with in order to better meet joint and coalition interoperability objectives.In the process of following current network protocols, the extension of standardized data by using existing and new capabilities of the DIS/HLA standards would be beneficial in establishing a more concise picture of performance for individuals, teams, joint and coalition exercises.  The main purpose of this extension would be to provide additional data needed to perform C2-level Warfighter performance assessments and training, which is a significant reason distributed simulation environments were created in the first place.A significant portion of the extended data would be in the form of “internal” state data, examples of which include, but are not limited to, switch positions, display modes, radar modes, radar tracking lists, weapons load, fuel status, fuel burn rate, throttle position, targeting information for bombs, and others, all of which are not currently part of the DIS standard or base HLA RPR Federation Object Model (FOM).  This type of information is not needed to stimulate the image generators or simulation environment models, but rather to provide a performance analysis system such as PETS² insight into what the simulation operator (pilot, weapons officer, etc.) is doing.  This information helps to map actual performance to Mission Essential Competencies (MECs) and achieve ACC’s vision of assessing skill proficiency across DMO installations.When an entity is placed upon the network, it could send out an initial update of all its properties. During the exercise, any entity exposing additional properties would then be required to send an update when it’s internal state data changes. For example, if an entity is participating in a bombing task, then targeting data would be included in the update. Having internal state data as part of an established protocol-level standard would enable performance measurement systems such as PETS² to effectively gather C2-level data and analyze performance across multiple DMO sites. Currently, standardized measurement tools such as PETS² are limited in collecting performance measures from multiple sites, as the enumerations or data structures used to pass such internal state data is not consistent from site to site[4]. ConclusionFuture military operations will be effects-based, rely on increased Warfighter use of integrated on-demand sensor information, require more responsive time critical targeting, incorporate a growing arsenal of precision weapons, and utilize expanded non-kinetic options, including information operations. To respond quickly to the dynamic challenges of today’s environment, training needs to be flexible, operationally effective, and integrated with real-time, globally distributed mission rehearsal and C4ISR capabilities. To do this, available networks for mission rehearsal, simulation, and just-in-time training must be continuously modernized and utilized; and performance metrics need to be systematic and complete to improve operational effectiveness, both individually and collectively. This global training model emphasizes the necessity for following strict standards such as DIS, HLA (with a common FOM such as RPR) or TENA, and the need to establish new standardized data to provide better performance feedback to the training and operational communities. There are many DMO installations throughout the Modeling and Simulations (M&S) community, and currently most of these sites have no method of objectively assessing the degree or amount of knowledge transfer that takes place when Warfighters train in these virtual environments. With the training community developing performance assessment systems such as PETS² to address joint and coalition training issues, there is an increase need to broadcast internal state information on the simulation network for acquisition and analysis. Providing a dynamic, capabilities-based training for the Warfighter must be a joint effort between the simulation, operational, and training communities if it is to succeed in today’s environment.References HYPERLINK "http://firstsearch.oclc.org.libproxy.gc.maricopa.edu/WebZ/FSQUERY?searchtype=hotauthors:format=BI:numrecs=10:dbname=WorldCat::termh1=United+States.+Office+of+the+Under+Secretary+of+Defense+Personnel+and+Readiness:indexh1=cn%3D:sessionid=sp07sw05-50542-e4cyynpk-tlracw:entitypagenum=4:0:next=html/records.html:bad=error/badsearch.html" United States. Office of the Under Secretary of Defense (Personnel and Readiness). (2004). Department of Defense Training Transformation Implementation Plan Jun 2004. Washington D.C.: Office of the Under Secretary of Defense for Personnel and Readiness. HYPERLINK "http://firstsearch.oclc.org.libproxy.gc.maricopa.edu/WebZ/FSQUERY?searchtype=hotauthors:format=BI:numrecs=10:dbname=WorldCat::termh1=United+States.+Office+of+the+Under+Secretary+of+Defense+Personnel+and+Readiness:indexh1=cn%3D:sessionid=sp07sw05-50542-e4cyynpk-tlracw:entitypagenum=4:0:next=html/records.html:bad=error/badsearch.html" United States. Office of the Under Secretary of Defense (Personnel and Readiness). (2002). Strategic Plan for Transforming DoD Training. Washington D.C.: Office of the Under Secretary of Defense for Personnel and Readiness.Schreiber, B. T., Watz, E., Bennett, W. Jr., & Portrey, A.  (2003). Development of a Distributed Mission Training Automated Performance Tracking System. In Proceedings of the Behavioral Representations in Modeling and Simulation (BRIMS) Conference. Scottsdale, AZ.Watz, E. A., Schreiber, B. T., Keck, L. B., McCall, J. M., & Bennett, W. Jr. (2003).  Performance Measurement Challenges in Distributed Mission Operations Environments.  [Electronic Version] Simulation Technology Newsletter. 03F-SIW-022.Watz, E.A., Keck, L. B., & Schreiber, B.T. (2004). Using PETS software to capture complex objective measurement data from Distributed Mission Operations (DMO) environments. Paper presented at the 2004 Spring Simulation Interoperability Workshop. [Electronic Version] Paper available as  HYPERLINK "http://www.sisostds.org/conference/download.cfm?Phase_ID=2&FileName=04S-SIW-143.doc" 04S-SIW-143.doc.Gehr, S.E., Schreiber, B.T., & Bennett, W. Jr. (2004) Within-Simulator Training Effectiveness Evaluation. In 2004 Interservice/Industry Training, Simulation and Education Conference (I/ITSEC) Proceedings. Orlando, FL: National Security Industrial Association.Schreiber, B. T., Watz, E. A., & Bennett, W. Jr. (2003). Objective Human Performance Measurement in a Distributed Environment:  Tomorrow’s Needs.  In 2003 Interservice/Industry Training, Simulation and Education Conference (I/ITSEC) Proceedings. Orlando, FL: National Security Industrial Association.Stock, W., Schreiber, B.T., Symons, S., Portrey, A., & Bennett, W. (in preparation).  Can Performance of Air Combat Teams in Distributed Mission Operations Training Exercises be Described by Signal Detection Parameters? Manuscript in preparation.Portrey, A., Schreiber, B.T., Stock, W., & Schvaneveldt, R. (in preparation) Pairwise Positional/ Energy Metrics Used as Concise Measures for Air Combat Performance.  Manuscript in preparation.Schreiber, B.T., Bennett, W. Jr., Schurig, M., & Gehr, S.E. (in preparation).  Distributed Mission Operations Within-Simulator Training Effectiveness Baseline Study.  Manuscript in preparation.  Colegrove, C. M. & Alliger, G. M. (2002, April). Mission Essential Competencies:  Defining Combat Mission Readiness in a Novel Way. Paper presented at the NATO RTO Studies. Analysis and Simulation (SAS) Panel Symposium. Brussels, BelgiumSchreiber, B.T. (under review).  Transforming Training:  A Perspective on the Need and Payoffs from Common Standards.  Manuscript submitted to Journal of Military Psychology for publication.Berry, A. (2004). Lessons Learned From Virtual Flag Integration. In 2004 Interservice/Industry Training, Simulation and Education Conference (I/ITSEC) Proceedings. Orlando, FL: National Security Industrial Association.Author BiographiesANTOINETTE M. PORTREY is a Research Engineer with Lockheed Martin Systems Management at the Air Force Research Laboratory, Warfighter Readiness Research Division in Mesa AZ.  She completed her B.S. in Applied Psychology - Human Factors from Arizona State University in 2001.  She is the project lead for the automated performance evaluation tracking system for Distributed Mission Operations.LOREN KECK is a Lead Software Engineer with Lockheed Martin Systems Management at the Air Force Research Laboratory, Warfighter Readiness Research Division in Mesa, Arizona.  He received a B.S. in Computer Science from Arizona State University in 2001, and has several years of experience in the real-time data acquisition and analysis, voice communications, and human operational training fields. He is responsible for the program architecture and implementation of the automated performance effectiveness tracking system (PETS2) for Distributed Mission Operations.BRIAN T. SCHREIBER is a Staff Scientist with Lockheed Martin Systems Management at the Air Force Research Laboratory, Warfighter Readiness Research Division, in Mesa, AZ.  He completed his M.S. in Human Factors Engineering at the University of Illinois at Champaign-Urbana in 1995. He is the conceptual designer and advisor for both PETS 1.X and PETS2.ComplexityTeam competenciesF-15CInter Team competenciesJSTARSF-16 NATOAWACSTornadoGR4EuroFighterHorizontal IntegrationMeasurement Opportunities