The Use of Analysis Requirements in the Development of the ACS Concept Exploration FederationGary K. EisermanVirtual Technology Corporation5400 Shawnee RoadSuite 203Alexandria, VA  22312-2300Keywords:Aerial Common Sensor, Analysis, Federation, HLA, FOMAbstract:  The Aerial Common Sensor (ACS) will be a multi-function system that will provide the Army with a self-deployable airborne reconnaissance, intelligence, surveillance and target acquisition capability for the 21st century.  The ACS program is a simulation based acquisition program now in its initial concept exploration (CE) phase.  During this phase, competing teams are developing models of their system concepts to be evaluated in a common operational level environment that is capable of simulating different threat scenarios and weather conditions.  This paper traces the development process followed by the Aerial Common Sensor Concept Exploration Federation (ACS CE Federation) with an emphasis on the central role played by analysis requirements.  It begins with an overview of the ACS system and the primary measures of effectiveness (MOEs) that the ACS CE Federation was designed to support.  The next section discusses the implications that the MOEs had for federation development and traces the federation design decisions that were made in order to better support MOE evaluation and ensure an unbiased assessment of competing ACS concepts.  This is followed by a section discussing the tools that were developed and used to collect and evaluate federation data in support of the analysis requirements.  The paper concludes with a summary of lessons learned and recommendations for other federation developers intending to follow a similar course.1. ACS Overview1.1  ACS DefinedThe Aerial Common Sensor (ACS) will be a multi-function system that will provide the Army with a self-deployable airborne reconnaissance, intelligence, surveillance and target acquisition capability for the 21st century.  The system will likely consist of one or more aircraft carrying a variety of different types of sensors, including electro-optical (EO), infrared (IR), communications intelligence (COMINT), electronic intelligence (ELINT), synthetic aperture radar (SAR), and moving target inticator (MTI).  The aircraft may perform initial processing on images and intercepted signals before feeding information down to a ground station for further processing and evaluation, or they may pass raw signals directly to the ground.  The ACS program is a simulation based acquisition program now in its initial concept exploration (CE) phase.  During this phase, three competing teams are developing models of their system concepts to be evaluated in a common operational level environment that is capable of simulating different threat scenarios and weather conditions.  The federation that has been developed for use during the CE phase is an HLA based federation.  It is an operational level, as opposed to an engineering level federation, whose purpose is to provide insight into the strengths, weaknesses, and overall effectiveness of the competing system concepts in a variety of missions, under varying environmental conditions.  The federation development team consisted of staff from Science Applications International Corporation (SAIC) and Virtual Technology Corporation (VTC), working under the US Army’s Joint Precision Strike Demonstration Project Office.Federation Design ProcessThis federation was intended to facilitate the development and evaluation of different system concepts.  Because the system concepts were being developed at the same time as the federation, it was impossible to completely specify the federation before the competing contractor teams began developing their operational models.Recognizing this, we took an iterative approach to defining the federation.  The ACS program office began by defining 12 measures of effectiveness that they would use in evaluating system concepts.  Of these, four were eventually deemed appropriate for operational level modeling during the CE phase.  These related to the ability of the system to detect, locate, and identify targets (MOEs 1, 2, and 4) along with measures of the systems data processing capacity (MOE 3).  The remainder of the MOEs would be evaluated through a variety of other means, including cost models and engineering level analysis, or they would be deferred until later in the program.  These initial MOE definitions were used together with a notional ACS system architecture to develop a strawman federation architecture and accompanying federation object model (FOM).  The MOEs and the federation documentation were given to each of the contractor teams, who then provided feedback on MOE definitions and on changes needed to the federation to provide a credible assessment of the capabilities of their proposed systems.   Changes were reviewed by the government and federation development team, with those that were agreed to being incorporated into the next round to MOE definitions and federation design.  This cycle was repeated a number of times.  Between the start of the CE phase and the start of the final federation integration tests, there were six revised versions of the MOE document and more than 20 versions of the FOM.  A federation that is designed to support concept definition and evaluation will have a different focus than one designed for training or for the testing of an existing system.  The next section discusses some of the basic issues considered in designing this federation.  The four CE phase MOEs will be discussed in the subsequent sections.  Each of those section begins with an overview of the MOE.  This is followed by a discussion of implications for federation development, along with any issues raised by the contractors and the federation development team.  The section ends with a discussion of specific features of the federation that were implemented to address the analysis requirements of the MOE.2. General Federation Design 2.1 Design ConsiderationsThe ACS operational models needed a way to report on their performance during the course of a simulation run.  They needed to report both higher level results, referred to in this paper as system level results, as well as lower level results, referred to here as node level results.  For example, the reporting out of when the system would first be able to claim a detection of a new target would be a system level result.  Additional information about which sensors or signal processors had made detections that contributed to the system level report would be node level results.   The evaluators wanted to be able to trace the flow of information through each operational model.  This was important in order to support the MOE that addressed system information processing capacity.  This meant that it was necessary to record the timing and characteristics of all signals that actually impinged on each operational model’s sensors, information about how the signals were processed and passed through the operational model (OM), and finally, what information the OM would be able to report out, and when.  Because this federation was going to be used to evaluate multiple competing system concepts, it was also critical that it provide an unbiased environment that would not favor one concept over another.  Verifying this meant that it would be necessary to record the states of all of the sources of detectable signals, which we referred to as ground truth data, during the course of each simulation run.  2.2 General Design Decisions2.2.1 Nomenclature and Initial Object ClassesA top level picture of the object classes defined for the ACS CE federation is shown in figure 1.  The organization of these classes in the Federation Object Model (FOM) is shown in figure 2.  Two general classes of objects were defined, Platforms and Equipment.  The Platform class was further broken out into ACSnodes, which represented the aircraft and ground stations that made up an ACS system, and Targets, which represented the tanks, trucks, buildings, and other detectable entities in the test environment.The other class of objects, Equipment, represented the hardware carried on Platforms.  The Emitter class was broken out into four subclasses.  Emitters represented equipment that actively emitted RF signals.  This included radio and radar emitters on target platforms, MTI_SAR emitters on ACS nodes, and ACScomm emitters that were used to model communications between ACS nodes.The Sensor class represented the equipment carried on ACS nodes to intercept signals from targets and the emitters they carry.  The CommThroughput class was used to report communications throughput information for the various data links between ACS nodes.  It will be discussed in more detail in the section on MOE #3, below.The SignalProcessor class was added after discussions with contractor teams.  It represented all of the equipment other than sensors within an ACS system that would be used to process intercepted signals.   EMBED PowerPoint.Show.8  Figure 1.  ACS Object ClassesFigure 2.  ACS FOM Object Classes2.2.2 Federation ArchitectureThe single most important design decision made on this program was that the competing contractor teams would be responsible for developing the operational models of their ACS system concepts, rather than trying to develop a single generic ACS system model that would then be tailored via input data and tunable parameters to represent each of the competing concepts.  The federation development team would be responsible for all other aspects of the federation.  We chose to segment the federation in this way for several reasons.  The first was that it would avoid imposing arbitrary limitations on the contractor’s system designs.  We wanted them to be free to develop the best concepts they could, rather than forcing them to designs that could be accommodated by an existing model.  Another reason was that this approach would allow each team to avoid exposing proprietary designs or system concepts.  As long as they satisfied the federation interface requirements defined in the FOM and supporting documents, they were free to implement anything they wanted.  A final reason for drawing the boundaries where we did was that losses due to the propagation of signals through the environment would be an important factor in determining which signals could be detected.  Many existing sensor models have signal propagation effects embedded within them, but the level of fidelity with which these effects are modeled is not always the same.  It is often true that in a direct comparison of a higher fidelity sensor model against a lower fidelity model, the higher fidelity model will appear to be less capable because it takes into account more of the environmental effects that can degrade sensor performance.   Modeling the environment effects on signal propagation on the federation side, and not the contractor side, would ensure that propagation effects were being played equally across all of the teams.Another federation design decision that we made was to break the CE federation into two pieces.  One piece, the “Ground Truth” federation, would simulate the target behaviors for each of the test scenarios.  This would include target movement and the operation of radios and radars on the target platforms.  All of this information would be recorded so that it could be played back during the execution of the second piece, the “Evaluation” federation.  This was done to ensure that the ground truth target set presented to each contractor would be exactly the same.  It also allowed us to generate test data that could be given to the contractors to use during their own in-house development.  An additional benefit was that playing back canned tracks during evaluation runs significantly sped up and simplified the execution of the evaluation federation.The basic structure of the ACS CE federation is shown in figure 3. EMBED PowerPoint.Show.8  Figure 3  ACS CE Federation ArchitectureGround truth data was generated by two federates.  The Eagle model was used to simulate aggregated unit behavior.  This was then fed to a second federate, the Eagle Disaggregator Model (EDM), which generated individual target behaviors.  The ground truth data was recorded for playback using the hlaResultsTM collection tool.During evaluation runs, hlaResultsTM was used to play the recorded data into the evaluation federation. A second instance of hlaResultsTM was used in the evaluation federation to collect data from the execution for post run analysis. On the other side of the figure, the circle marked “ACS OM” represents the contractors’ operational models.  These implemented their ACS sensors, signal processors, communications links, and the aircraft and ground stations that carried them.  In practice, the ACS OM federates could be implemented as single or multiple federates within the federation, depending upon how the contractors chose to implement their models.Acting as intermediaries between ground truth and the OMs are two classes of propagation federates, one to handle RF signals and one to handle EO/IR signals.  Each of these federates would monitor the mode, position, and orientation of the associated class of ACS sensors along with the status of all of the ground truth targets.  Whenever the propagation federate determined that a signal from a target would impinge upon one of the ACS sensors that it was responsible for, it would compute the propagation losses based upon environmental conditions and current geometries, and then create a signal interaction for the ACS OM to process.  In order to improve execution speed, we implemented multiple copies of the RF and EO/IR federates, each of which was responsible for a subset of the ground truth targets.The evaluation federation was monitored and controlled via the hlaControlTM federate.  This was used to enable a systematic and repeatable start-up process for the federation, to prevent simulation time from advancing until all of the federates had joined, and to monitor the status of the federates and the traffic across the federation during each federation run.2.2.3 Other General Design DecisionsTwo classes of interactions were defined to report system and node level results in support of the detect, locate, and identify MOEs.  Whenever an OM generated one of these interactions, it assigned it a unique report ID.Whenever one of the propagation federates generated a signal interaction, it also generated a cross reference interaction.  This cross reference could be used to connect the ground truth state of the target and the receiving ACS sensor and its platform at the time that the interaction was generated. Every signal interaction created by a propagation federate was assigned a unique ID.  This ID was used to trace the processing of signals through the ACS operational models.3. Measures of Effectiveness3.1 MOE #1 – Capability of the ACS System to Detect TargetsThe goal of this MOE was to provide insight into how well the ACS system could detect important objects on the battlefield (targets of interest) over the course of a mission.  While “capability to detect” may seem to be a relatively straightforward concept, a number of issues were immediately identified.  The first question was what, exactly, constitutes a detection?  Is a target detected as soon as any sensor or signal processor can sense a signal above the background noise level?  What if the signal is subsequently discarded by the system before it reaches a human operator, should it still be counted as having been detected?  What if it does reach an operator, but he or she delays reporting it out or never reports it at all?  This issue was resolved by defining a target as having been detected by the ACS system when information about that target first reaches a point in the ACS system from which it could be reported out.  In other words, it is detected as soon as you can tell someone else that it is detected.  3.1.1 Federation Design to Support System Level ReportingThe implication for federation design was that some means was needed to identify when a target was first detected.  The solution was to have the ACS contractors identify the points in their system concepts that would be reporting to the outside world.  These points were called reporting nodes.  We then defined a system level interaction, called the SimSystemDetect interaction.  The ACS OM was to issue a SimSystemDetect interaction whenever information about a target first reached any reporting node in the ACS system being modeled.  The interaction would contain the ID of the first signal interaction from that target that the system had detected.   We could then use simulation timestamps to mark when the target first appeared in the simulation playback, when the first signal from it actually reached the ACS system, and when the system was first able to declare a detection.The next issue raised was that of relevant versus non-relevant targets.  In general, the ACS system would be expected to look for specific targets or activities, and so the evaluation of system concepts should consider how many relevant targets were detected and not simply count the total numbers of objects detected.  The solution to this was to add an attribute to the ground truth target definition that would indicate if and when a target became important.  As the ground truth test scenarios were being developed, the government identified target activities (troop movement, radars being turned on or off, increased radio traffic, etc) that an ACS system would be tasked to look for.  Each of these activities was assigned a Priority Information Request (PIR) number.  When targets began performing these activities, they were marked with the associated PIR.  Since the ground truth targets were recorded as part of the simulation data, we could correlate the presence or absence of PIR numbers for a target with SimSystemDetect messages on that target to determine if and when relevant targets were being detected by the ACS system.  We could also count the number of PIRs assigned at any time during the simulation to determine the number of relevant targets available for detection at any time during the exercise.  In this way, we could measure the fraction of relevant targets detected by each ACS system over the course of each simulated mission.3.1.2 Node Level Analysis RequirementsIn addition to knowing how well the system as a whole could detect targets, the evaluators also wanted insight into how much the components of an ACS system were contributing to the overall capability.  In other words, they wanted to know which sensors and signal processing equipment were contributing to initial detections.  They also wanted information as to how well the system could maintain its awareness of a target after it had been detected.Examining these secondary requirements raised some additional issues:  How often should each sensor or signal processor in the system report on its activities?   When should it report?  If a signal persists for many seconds and a sensor revisits the signal several times, should that count as a single detection opportunity or several?  If a target produces a signal, but there is not a sensor of the correct type in a position to receive it, should that count as a detection opportunity?   If there is no clear line of sight to a target, should it still be counted as detectable?The resolution to these questions began with the definition of detection opportunity.  First, it was agreed that detection opportunities should be an attribute of the target, and should not depend upon whether or not there were sensors looking at the target.  Next, it was recognized that targets would produce different kinds of signals in different numbers and at different times, so a separate detection opportunity type was identified for each class of signal.  The opportunity types that were defined for the ACS CE federation were EO/IR, COMINT, ELINT, MTI, and SAR.  Next, rules were defined for when each type of detection opportunity would begin and end.  A target always has some temperature, and so it always has some IR signature.  It also always has some visual signature.  This means that a target is potentially always detectable by an EO or IR sensor.  Looked at simplistically, this would mean that there would be one EO and one IR detection opportunity for each target, and that it would last for the entire duration of the simulation.  Defining opportunities in this way would make it difficult to distinguish between a system that saw a target once and never detected it again and one that regularly detected the target and maintained good situation awareness, since both systems would have made detections in 100% of the available opportunities.  To correct this, we defined the rule for EO/IR detection opportunities to be that each opportunity lasted for 60 seconds, at which time it would end and a new opportunity would begin.In the case of COMINT opportunities, the rules are somewhat different.  A radio can only be detected by a COMINT sensor when it is emitting a signal, so detection opportunities only exist when a target radio is transmitting.  Radios can also transmit for different lengths of time, and it didn’t seem reasonable to count a single 5 second transmission as equivalent to a continuously transmitting commercial radio station that would be emitting for the entire 8 hour duration of the scenario.  The rule for COMINT detection opportunities was that a new detection opportunity would begin whenever the target began emitting a signal, and if the transmission lasted for more than 60 seconds, the current opportunity would end and a new one begin with each 60 second period.  The rules for ELINT detection opportunities were similar, except that the opportunities were associated with target radar systems instead of radios.Similar rules were defined for MTI and SAR detection opportunities, each taking into account the conditions under which a target would be detectable to a sensor of that type.3.1.3 Federation Design to Support Node Level ReportingWe implemented the detection opportunity concept by adding detection opportunity counters and end times to target objects.  Whenever a new opportunity of any type began for a target, its opportunity counter would be incremented, the end time for the new opportunity would be set, and the new values would be published via an object attribute update.  Target platforms contained EO/IR, SAR, and MTI detection opportunity counters and end times.  Target radios contained COMINT counters and end times, and target radars contained ELINT counters and end times.  Whenever one of the propagation federates created a signal interaction, it would include the current opportunity type, counter value, and end time in the interaction parameters.  A node level interaction, called NodeSimDetect, was created for the ACS operational models to use to report information in support of these secondary analysis requirements.  At the conclusion of any detection opportunity of any type, any ACS sensor or signal processor that had processed at least one interaction related to that opportunity was to issue a NodeSimDetect interaction summarizing results for the entire opportunity.  If a sensor had processed more than one interaction from the same opportunity, it was still only required to issue a single NodeSimDetect interaction.  On the other hand, if several sensors or processors had processed interactions from the same opportunity, all of them were required to issue NodeSimDetect interactions.In addition to recording which detection opportunity had been processed, the NodeSimDetect interaction reported other node level data such as which piece of ACS equipment had handled the opportunity, whether or not any detections had occurred at the node level, and if so, how many times the signal had been detected during the opportunity.  If no detections occurred, the OM was to report the reason that the signal was not detected (signal too weak, signal parameters outside detectable limits, signal ended before it was examined, etc.).  Whether or not a detection occurred, the OM was also to report the largest probability of detection computed during the detection opportunity.  The NodeSimDetect also included among its parameters, the interaction IDs of the first interaction processed during this opportunity and the first one detected, if a detection did occur.  The summary nature of the NodeSimDetect interaction was intended to strike a balance between providing enough information to support the evaluation without generating such a flood of interactions that it would overwhelm the federation and the analysts.3.2 MOE #2 – Capability of the ACS System to Locate TargetsThe goal of this MOE was to show how accurately the ACS system would be able to locate targets of interest.  As was the case with “capability to detect”, the precise definition of “capability to locate” was also open to interpretation.   Discussions between the government and contractor teams established that this capability would be evaluated by measuring how quickly and accurately the ACS system could improve and maintain the end user’s knowledge of relevant target locations. This would be another system level measure, and a second system level interaction, called SimSystemLocate, was defined to carry the data.  One federation design issue related to this measure was how to characterize location accuracy.  Location accuracy was defined by two means, the first was that the OM was to report a predicted target location, which could be compared against the ground truth target location.  The second was that the OM was to report error ellipses corresponding to 50%, 75%, 90%, and 95% confidence levels.  This would provide a measure of the ACS system’s own confidence in the accuracy of its predictions.A second issue raised was what the rules should be for when and under what circumstances the interaction was to be issued.  The decision here was that a SimSystemLocate interaction was to be issued whenever the information on a target’s location changed at a reporting node of the ACS system.  In other words, whenever new information reached a reporting node that changed what it could report about a target’s location, a new SimSystemLocate interaction was to be generated for that target.   As was the case with all of the interactions, these would be time stamped, so we could examine the changes in reported target locations and uncertainties over time.3.2.1 Node Level AnalysisIn order to better understand the system level results, the analysts wanted to be able to understand which individual system components were contributing to changes at the system level, what the contributions were, and when they were being made.  In order to support this, a new node level interaction, the NodeSimLocate interaction, was defined.  The NodeSimLocate was to be produced whenever a sensor or signal processor in an ACS system processed any interaction that produced new location information.  Unlike the NodeSimDetect, this was to be produced whenever new location data were computed, not on a once per detection opportunity basis.  The reason was that processing of successive interactions within a single detection opportunity might each produce a change in system location accuracy, and we wanted to be able to see every change.Like the SimSystemLocate, it included reports of the sizes and orientations of the four error ellipses.  However, a number of the contractors pointed out that there are many classes of sensors that generate lines of bearing (LOB), not error ellipses, so a line of bearing parameter was added to the NodeSimLocate, and the OM model had the option of either providing a LOB or the error ellipses.  In order to trace back to the ground truth state and signal interactions that produced the NodeSimLocate interaction, it also included the ID of the signal interaction that caused it to be generated.  NodeSimLocate interactions could be sent from signal processors as well as sensors.  We had to accommodate processing architectures where a signal processor might combine inputs from several sensors to produce a location report, so the NodeSimLocate interaction allowed for the reporting of multiple interaction IDs.Whenever a NodeSimLocate interaction was issued, the issuing federate assigned it a unique report ID.  We modified SimSystemLocate, the system level report, to include a list of all of the NodeSimLocate report IDs that had contributed to the SimSystemLocate.  This provided traceability all the way through from ground truth to signal interaction, to sensor/signal processor results to system level results and would allow us to see the quality and timing of contributions from individual sensors and signal processors.3.3 MOE #3 – Throughput Capability of the ACS SystemThe goal of this MOE was to evaluate how well the ACS concept system’s information processing architecture would handle a dense battlefield environment.  It was important to try to understand where bottlenecks in the system architecture would be and what the system’s limits were.Of all of the measures addressed by this federation, this one was probably the most difficult to model.  We initially began with a federation design that included explicit modeling of the transmission of each message through the system.  The contractor teams quickly pointed out that the system’s internal communications architecture would not be well defined so early in the program, as it would depend on the design and performance of sensor and signal processing components, as well as what data compression algorithms were chosen.  They also pointed out that the prioritization and timing of messages would depend on specific mission tasking and could not be modeled in a general way.  All of this meant that an explicit message passing model could not be used.Instead, we settled on a more stochastic approach.  All of the competing teams agreed that, given a volume of incoming signals of any particular type, they could come up with models of how much “downstream” message traffic would be generated.  This could then be fed into models of the communications links between the nodes of their systems to provide information on data throughput rates, transmission times, and data losses.One additional issue of concern to the government was that if the ACS aircraft were flown in such a way that they lost contact with one another, or their communications were degraded, the behavior of the OM should reflect this.  In discussing this with the contractor teams, we learned that the communications links between ACS nodes would not be expected to come up or down very frequently, a fact that we were able to use in designing this aspect of the federation.3.3.1 Federation DesignWe defined a new object class, CommThroughput, to handle the reporting of information processing performance.  The attributes of this object included the outgoing data rate, amount of data currently queued up for transmission, the amount of time that would elapse between when a new byte of data was added to the output queue and when it would be received at the other end of the communications link, and the amount of data that was discarded during the previous 60 seconds.  Each ACS OM was to create one CommThroughput object for each communications link in its system.  Each link was assigned a channel number.  During the course of a simulation execution, each OM was required to publish updates to the CommThroughput object’s attributes every 60 seconds, thus providing a running record of system throughput during the simulated mission.  This information could then be correlated in time with the activity of the ground truth simulation and the numbers of signals received at each ACSnode.The issue of data link degradation was handled by having the RF propagation federate periodically compute the propagation loss between the two ends of a communications link.  This was done by defining an ACScomm emitter class to indicate the transmit end of a communication link and an ACScomm sensor class to indicate the receiving end.  Both object classes included a channel number among their attributes to indicate which emitters and sensors were connected.During the simulation execution, the RF federate would subscribe to all of the ACScomm objects and the platforms that carried them.  Every 60 seconds, it would compute the signal propagation loss between every pairwise combination of ACScomm emitters and sensors on the same channel and would send this information in a DatalinkConnectivity interaction to both ends.  The ACS OM would then be able to determine whether or not the loss was sufficient to degrade the communications over the link and adjust its behavior accordingly.3.4 MOE #4 – Capability of the ACS System to Classify, Recognize, and Identify TargetsThe goal of this MOE was to provide information on the ability of the ACS concept system to classify, recognize and ID relevant targets that have been detected.  In this context, “classify” is the coarsest level of identification, distinguishing among targets at the level of buildings vs. tracked or wheeled vehicles vs. fixed or rotary wing aircraft, etc.  “Recognize” is the next step up in precision, distinguishing among various classes of wheeled vehicles (cars vs. buses vs. light or heavy trucks, etc.), various classes of tracked vehicles, etc.  “ID” is the most precise level of identification, distinguishing among targets down to the level of specific models (AH-64A vs. AH-64D helicopters, for example).Three different levels of identification were defined because the initial discussions with the contractor teams on this MOE brought out the fact that different sensor systems would not all provide the same level of precision in how they identified targets.  These discussions also pointed up the fact that at any level, identification is rarely certain.  At any time, the system’s ability to identify a target is better characterized in terms of probabilities or uncertainties.  Another issue raised during the discussions was that imaging sensors rarely produced automated ID of targets.  Instead, they produced images, whose quality would affect the ability of a human observer to identify targets within the image.3.4.1 Federation DesignIn order to record ACS system identification data, we created the SimSystemID interaction.  This system level interaction was to be issued whenever information reached a reporting node that changed the ACS system’s degree of identification of a target, at any of the levels defined above.  For information from non-imaging sensors, the interaction contained parameters that held the system’s declared values for classifying, recognizing and identifying the target.  This would allow the evaluators to examine the numbers of targets correctly identified over the course of a simulated mission.  In addition, the OM was to report a probability vector at each level of ID, consisting of a list of the possible types that the system determined that the target could be, along with the system’s computed probability that the target was of that type.  For example, at the classify level, a target could be reported as having a 50% probability of being a tracked vehicle, 40% wheeled vehicle, and 10% other.  This would provide insight into how quickly and how well the system was able to converge on a correct identification.  For information from imaging sensors, a NIIRS rating was to be reported instead of the probability vectors.  The NIIRS rating is a 10 point scale of increasing image quality.As with the detect and locate MOEs, there was a secondary analysis requirement to provide information on the contributions of individual sensors and signal processors to the overall system level result.  This was addressed by defining the NodeSimID interaction.  Like the SimSystemID interaction, this contained probability vectors at the classify, recognize, and ID levels, along with parameters recording the sensor or signal processor and the interaction that was processed.  The OM was to issue a NodeSimID interaction whenever a signal interaction was processed and ID information was produced.4. Analysis Tool4.1 Design GoalsDuring simulation runs, all of the RTI traffic from all of the federates was collected into either Access or Oracle databases.  This included object attribute updates for all target platforms, radios and radars, ACS platforms, ACS emitters, and ACS sensors.  It also included the interactions generated by the propagation federates and all of the node and system level interactions generated by the ACS models.This produced an enormous amount of data, which needed to be analyzed.  In order to help with this, the federation design team developed a database query tool that could be used to run a consistent set of queries across all of the hlaResultsTM databases that were collected.We had a number of objectives for this tool:  It needed to be easily usable by analysts who were not database experts and were not HLA experts.  It had to be able to easily connect to each of the different data sources that would be collected.   It had to easy to update so that it could be kept in sync with changes to the federation and its FOMIt had to be easy to implement, refine, and validate the database queries.It had to be able to present data in a variety of formats, depending upon the query being executed.4.2 StructureThe tool that we developed can be described in terms of three functional areas.  The first is a query building function.  This allowed us to create, test, modify, and save a library of queries.   The data collection tool that we were using, hlaResultsTM, provides a mapping from the FOM to the internal Oracle or Access database structure.  This mapping is embedded in each collected database.  We were able to take advantage of this and have the tool read the FOM data and display the available attributes and parameters using their FOM names, as opposed to using the labels assigned by the database schema.  This significantly improved the readability of the queries.  It also meant that the queries were independent of whether the data source was an Access or Oracle database.  A variety of different operations were built into pull-down menus in the query builder portion of the tool to make developing queries faster and less error prone.  Functions were available for producing histograms of data, with user-selectable bin sizes, for generating time histories, cumulative values, differences between two values over time, sliding averages, or first or last instances of specified data.The second functional area was the part of the tool that ran the predefined queries.  When executed, this function would bring up a table listing all of the available queries, along with a short text description of the query and a category (which MOE it supported, or whether it supported validation of the ground truth data).  To run a query, the user merely selected it and clicked on the “run query” bar at the bottom of the table.  If the user had not already selected a data source, a menu of available sources would be automatically displayed for selection.The third area was the display portion of the tool.  Queries could be executed in static mode, which meant that they were run once and the results displayed, or they could be run in dynamic mode, where they were periodically executed and the display updated.  The dynamic mode was intended to be executed at run time to provide the capability to monitor the progress of the simulation.  One design decision that we made was that when the tool was running in dynamic mode, it worked by repeatedly querying the collected database instead of sitting on the network as another federate.  We did this for two reasons.  The first was to avoid increasing the amount of data going through the RTI.  The second, more important reason was that querying the database would allow us to confirm that we were actually collecting the data into the database, which we couldn’t do by merely sitting on the network and watching the data go by.To date, we have implemented three different display formats for query results;  a 2-D map display, a bar chart, and a “gas gauge” chart, which is primarily used to dynamically monitor performance data in support of MOE #3.These three functional parts were encapsulated within a graphical interface that included menu items for common functions such as selecting a data source as well as items for selecting the query builder or the predefined query library.5. Lessons Learned5.1 Iterate, iterate, iterate… and then iterate some moreAs should be obvious from the number of times this paper refers to discussions between the federation development team and the government analysts or the contractor teams, it would have been impossible to produce this federation without extensive communications with all of the involved parties.  The federation development team did not have the subject matter expertise in all of the systems and sub-systems being simulated that the contractor teams had, and we also did not have the detailed knowledge of the analysis objectives that the government team had.  It was only by repeatedly walking through our federation design with all of these groups as their own models and analysis plans evolved that we were able to arrive at the final design.5.2 Don’t let the tail wag the dogOn more than one occasion, one of the contractor teams would make a comment or ask a question that indicated that they were taking the FOM as being fixed and were trying to shoehorn their system concept into it, whether it fit or not.  We had to emphasize repeatedly that it was our job to adapt the FOM to accurately model their system concepts, and not the other way around.  5.3 One picture is worth a thousand database recordsThe value of having tools to readily visualize the input, intermediate, and output data in this federation would be hard to overstate.  Running through the exercise of translating analysis requirements into desired charts, graphs, and data tables, working out what the database queries had to be to produce those outputs, and then actually implementing them on the current FOM was an excellent way to spot missing parameters in interaction data, or to determine whether data was being collected often enough and in enough detail to produce meaningful results.6.  ConclusionsThe ACS Concept Exploration federation is providing a useful insight into the competing ACS system concepts because it has been focused throughout its development on answering a specific set of questions.  Keeping the analysis requirements at the center of the ACS CE federation development effort fundamentally influenced the architecture as well as the details of the federation and its constituent federates.    One key to this was keeping the analysts themselves involved.   Anyone undertaking a similar effort will benefit tremendously from spending the time and energy required to make the end users sufficiently HLA and FOM literate that they can verify for themselves that the federation continues to address their needs as it evolves.    A second key to this was involving the competing contractor teams in early discussions to define the analysis requirements that their models would have to support.   This is critical to avoiding costly misunderstandings due to factors like differences in nomenclature, hidden assumptions about the required level of modeling fidelity, or contractor concepts that wouldn’t produce outputs consistent with the questions being asked.7.  Ongoing WorkImmediately upon completing Phase I, the JPSD team will begin supporting Phase II of the ACS program.   The federation developed under Phase I will continue to be used during Phase II, but will be enhanced to provide larger, more complex scenarios, more realism in the threat behaviors, and support for human operators in the loop.  The simulation environment developed under Phase I is also being used as the baseline architecture for the Joint Virtual Battlespace (JVB).  Author BiographyDR. GARY EISERMAN is a Senior Engineer at Virtual Technology Corporation, Alexandria VA.  He is the technical lead for JPSD’s ACS Testbed effort.  He is also supporting VTC’s sensor modeling and federation design tasks for the Joint Virtual Battlespace.  Dr. Eiserman has more than 14 years of experience in modeling and simulation.