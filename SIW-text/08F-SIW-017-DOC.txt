Sensor Fusion for Live Training Augmented RealityKatherine Scott & Doug HaanpaaCybernet Systems Corporation727 Airport BlvdAnn Arbor, MI 48104(734) 668-2567 HYPERLINK "mailto:kscott@cybernet.com" kscott@cybernet.com,  HYPERLINK "mailto:doug@cybernet.com" doug@cybernet.comFrank Dean Research, Development and Engineering Command Simulation and Training Technology Center (RDECOM STTC) 12423 Research ParkwayOrlando, FL 32826-3276(407) 384-3877 HYPERLINK "mailto:frank.dean@us.army.mil" frank.dean@us.army.milJames ToddProgram Executive Office for Simulation, Training & Instrumentation12350 Research ParkwayOrlando, FL  32826-3276(407) 384-3905james.todd3@us.army.milKeywords:Augmented Reality (AR), Live Training, Simulation, Sensor Fusion, Inertial Measurement Unit (IMU), Pose Detection, Head Mounted Display (HMD), Image Processing, Escalation of Force Abstract:  Recently, there have been discussions within the Army’s research and training community regarding the realistic presentation of various types of targets in the live training environment.  While serving the U.S. Soldier well for decades, pop-up types of mechanical targetry may soon give way to (or share duty with) more cutting edge methods, which go beyond providing purely marksmanship-oriented training.  These newer, more sophisticated methods of target presentation would provide virtual-based synthetic targets that would act and or react in manners representative of the asymmetrical combat environment in which our troops currently find themselves engaged.  Augmented Reality (AR) could prove to be an efficient and cost-effective way to perform certain training objectives for the U.S. Soldier.  AR methods would provide the flexibility and situational realism of existing simulation technology, while providing the physical realism of Multiple Integrated Laser Engagement System (MILES) or instrumented shoot houses.  We are currently developing an Augmented Reality (AR) live training system that uses a sensor fusion approach:  our system uses both a video-based pose detection system and a collection of Microelectromechanical systems (MEMS) based accelerometers and gyroscopes to determine a robust pose value.  The advantage of this approach is that each sensor system can compensate for the shortcomings of the other, in order to provide a more robust AR system. These two pose detection systems are mounted on the head and weapon of the training participant, and are used to simultaneously extract pose data for AR rendering and to determine the rifle’s pose. Since both the participant’s gaze direction and rifle direction are known, it is also possible to determine how the participant is holding the rifle and use this information to create realistic responses to non-player characters within the AR simulation. Augmented Reality can be seen as a natural extension of traditional computer simulation, and using High Level Architecture (HLA, IEEE Standard 1516) the AR training systems could present a real-world view of simulated operations conducted in OneSAF or JSAF.1. Introduction“The Army’s strategic goals are to remain relevant and ready by providing the Joint Force commander with essential capabilities to dominate across the full range of military operations.  The Army’s training goals are to produce highly professional Soldiers capable of military operations in contemporary operating environments against aggressive and adaptive enemies, and [sic] providing rigorous/realistic–training venues to receive expert feedback on performance, and to validate readiness on core competencies or rehearse for specific missions”[8].“Train like you fight” is a common phrase utilized within the US Army to indicate that training must emulate (and be relevant to) the conditions of the fight, and that the trainees should be equally encumbered in training as in deployment.  One element that significantly impacts the “train like you fight” ideology is the inability of threatening target simulations to accurately replicate adaptive and reactive behaviors. Recently, there have been discussions within the Army’s research and training community regarding the realistic presentation of various types of targets in the live training environment.  While serving the U.S. Soldier well for decades, pop-up or cutout types of mechanical targets may soon give way to (or share duty with) more cutting edge methods, which go beyond providing purely marksmanship-oriented training.  These newer, more sophisticated methods of target presentation would provide virtual-based synthetic targets that would act and or react in manners representative of an asymmetrical combat environment.Augmented reality (AR) could prove to be an efficient and cost-effective way to perform certain training objectives for the U.S. Soldier.  Instead of simply reviewing a scorecard with numbers of successful hits on targets, AR methods would provide new avenues for training feedback and reviews that would focus on the decisions made, the timing of those decisions, and the application of doctrine.  In order to fulfill the Army’s strategic and training goals, adaptive training solutions and emerging technologies must be leveraged to accurately “Train for the fight.”1.1 Augmented RealityAugmented reality (AR) is a concept and method of presenting data/information, graphics and 2D/3D virtual objects (generated by virtual and constructive simulations) to a subject or trainee, while operating in a live field environment.  Rather than viewing and operating while immersed in a synthetic virtual environment, a subject or trainee operates in the live field environment while receiving real-time data or information into his field of view that is relevant to the mission.  Additionally, the operator/trainee would interact with virtual targets, personnel or vehicles (that are controlled by virtual and constructive simulations) as though they were real.  In the future, these same concepts and technologies would potentially allow for supplementary situational awareness, communications, map data, etc. to be provided to Warfighters in complex operational environments without impeding their live view.1.2 Live Training For Combat ReadinessFrom hand-to-hand combat to live fire maneuvers, Army training is designed to prepare Warfighters to successfully complete their mission.  A fully-trained Warfighter must be prepared physically, technically and psychologically, and all three of these areas should be interwoven into the fabric of military training to provide a firm foundation for combat readiness.Physical fitness and technical training on weapon systems and other equipment has long been a mainstay of the basic and advanced individual training (AIT) of the individual Soldier.  However, in light of current operations in the Middle East and Southwest Asia, as well as the asymmetrical type of warfare that U.S. Forces face in the ongoing War on Terror, the psychological element of training is becoming ever more important to consider while developing training plans.The urban environment is replete with physical cover and concealment for an elusive enemy, and is additionally populated with civilians that may change from friendly to hostile in a matter of moments.  As such, the ability to make ‘snap judgments and decisions’ concerning the use of force in any given situation is paramount to exercising the appropriate level of force against the enemy, as well as preserving the lives of Soldiers and innocent civilians.1.3 Realism in the Live Training EnvironmentThe live environment domain is particularly effective in preparing Warfighters psychologically, by training them in conditions as close as possible to how they will fight.  However, planning and preparation for live training events can be costly in terms of dollars, time/schedules, personnel, and other resources.  Notwithstanding these large investments, training and rehearsing in a live environment brings a level of realism that is orders of magnitude greater than other training domains.  Writing in the Military Review in 2006, then National Training Center (NTC) commander BG Robert Cone stated, “One of the greatest challenges we have faced [at the NTC] . . . is developing adequate realism and robustness in training environments to support the problem sets we are encountering in Iraq and Afghanistan.”There have been many important changes that the U.S. Army has worked to implement over several years at training centers such as the NTC, as well as at the home stations of our combat units.  One particular area that leadership has focused on is, “the growing significance of small-unit actions, and what is required to increase the rigor and fidelity of training at the small-unit level.” 1.4 Battle Space Target PresentationOne piece of the puzzle necessary to bring psychological stress and realism to training is realistic target presentation. Mechanical targets are a mainstay on most training ranges, but are there other methods or techniques that can fill in existing technology gaps – thus providing tools and techniques for increased realism in certain aspects of Army training.Current Army targetry and associated systems are limited in their ability to act and react in a manner that is similar to how the actual threats present themselves in combat, particularly within unconventional and asymmetrical environments.  For example, threats tend to mix freely with other entities within the battle space (e.g. hostile and friendly personnel, vehicles, buildings such as churches, mosques and other infrastructure, etc.), so target entities can travel alone, within groups or convoys, start and stop often, use terrain and manmade structures to block detection, etc.  In other words, in the Contemporary Operational Environment (COE), real threats do not present themselves in the exact same manner in terms of location, direction of movement, posture, etc; as such, neither should training targetry. Additionally, whether training for deployment or within the theater, the Soldier should train against threats that are as realistic as possible, requiring that he use all senses (visual, auditory, olfactory, tactile, etc.) to gather information and analyze the threat.  The Soldier must not only identify the immediate threat, but understand the immediate battle space conditions in which the threat is operating – he must determine whether the identified threat should be engaged or passed off to other weapon system assets or units, and may also determine how a threat will react to various forms of engagement.There are many questions and concerns that run through Soldiers’ heads when faced with a threat.  The ability to quickly analyze that threat, develop courses of action (COA), and implement a given COA (all within minutes, if not seconds) is vital in order to gain the most favorable outcome.  Considering that the Warfighters/non-combatants’ lives and the success of the mission are at stake, it is critical to continue to develop more robust live training that physically, technically, and psychologically challenges and immerses trainees into a realistic training scenario.  Along these lines of reasoning, the presentation (realism and fidelity) of targetry in the live training environment has been the focus of a recent joint effort between the RDECOM STTC and PEO STRI, Orlando, FL.  1.5 Army Research VisionAt present, the U.S. Army does not have a definitive requirement for the use of augmented reality.  The concept of Live Augmented Environments has the potential to create interactive, multi-sensory, non-linear environments that provides the Warfighter with unparalleled and realistic live training and experiential learning.  Such a capability would potentially provide commanders and trainers with creative and flexible solution sets for the ever-changing and fluctuating mission and training requirements of the modern Warfighter?1.6 SBIR ResearchThe remainder of this paper discusses the research results of a Phase I Small Business Innovation Research (SBIR) project contracted to Cybernet Systems Corporation and co-managed by RDECOM STTC and the PEO STRI.  The following are the results of this research.2. Augmented Reality Training System For Cybernet’s augmented reality live training system we made a number of assumptions about how the system should be configured and work within the live training environment.  For example, we concluded that the training would occur in a large, clutter-free open area, the number of user interface devices would be minimal, and we would have access to the augmented reality area to create an initial 3D reconstruction of the environment and place fiducials.  Our goal was to use augmented reality to train Soldiers for the correct escalation of force – specifically, we wanted to show how rifle posture could be used to indicate a Soldier’s hostility to a target.Our AR live training system currently consists of two hardware assemblies that attach to two desktop computers, each running separate software applications that communicate via a UDP broadcast protocol. The rifle pose detection system uses a single Inertial Measurement Unit (IMU) and a single camera, which are attached to the computer using standard USB and Serial ports.  Before conducting the AR training session, both the camera and IMU require calibration (the camera requires this only once, while the IMU must be calibrated much more often).  In the case of camera calibration, we employ a simple checkerboard pattern to create an intrinsic camera model (See Figure 2A). The intrinsic calibration parameters model the skew, distortion, focal length, and principal point of the camera system.  Calibration of the IMU consists of measuring the drift of the gyroscopes and accelerometers while the unit is stationary, and using this data to establish a simple model of overall system drift.  This process is also repeated with the camera and IMU attached to the HMD unit.The first step in the AR system is to determine if fiducials are visible in the scene viewed by the camera, and if so determine the camera’s position and orientation (pose) with respect to these fiducials.  After digitizing the video using a frame grabber or the camera’s internal hardware, the augmented reality module (see Figure 1) accomplishes this reconstruction task by completing the following steps:First, the line scanner module locates the barcode within the potentially cluttered scene.  This module determines the borders of the fiducial by scanning the image luminosity signal at two frequencies, the frequency of the pixels themselves and a lower, smoothed frequency (Figure 2B).  The places where these two signals cross are used to determine edges (Figure 2C); the proportion of distances between subsequent edges is used to read the barcode number.  This process is able to find the barcodes at different viewing distances and angles because the proportionality of the bars is largely preserved when the camera moves with respect to the barcode.After extracting the edges of the barcode fiducial, we then locate corners of the barcode precisely.  This is accomplished by starting from the first and last edges of the barcode of the middle scan line.  From these two seed points, the software skirts the edge in both directions (up and down) until it finds the corner.  A sub pixel corner-finding operation is then employed to achieve very precise results (Figure 2D).Using the four corners located in the previous step, and the known geometry of the barcode (i.e. its length and width) an extrinsic calibration determines the position and orientation of the barcode with respect to the camera.  Since the barcode pose can be represented by homogenous transform matrix, the camera’s pose can be calculated by inverting the transformation (Figure 2E) from the barcode to the camera.Finally, the pose of the camera with respect to the reconstructed fiducial is improved by using a simple low-pass temporal filter.  This filter helps to eliminate small errors in the reconstructed pose caused by inconsistencies in corner locations, and also eliminate improper pose data caused by blurred images. To accomplish robust pose reconstruction and registration, we use a sensor fusion hybrid approach that makes use of a camera-based AR system, as well as our proprietary I3M inertial measurement unit (see Figure 1). We took this approach because the two sensor systems are complimentary – with the camera-based system yielding accurate but slow position and orientation measurements, and the IMU providing precise but drift-prone position and orientation information.  While the IMU provides precise measurements at rates exceeding 100Hz, the system is prone to gyroscopic drift that significantly degrades its accuracy over time.  For this reason, we developed a pose handoff coordinator module to exploit the strengths of both pose reconstruction methods.  The handoff coordinator works by first determining if a fiducial is visible within the scene – if the fiducial is visible, the system opts to rely on the AR pose extraction module; if a fiducial is not visible, the IMU data is used to determine an orientation offset from the last known good orientation and position determined by the AR module.  This scheme is used by both the rifle and HMD handoff module. Using the reconstructed pose information from the hand-off module, the system then attempts to register virtual imagery onto the live video.  The camera’s pose with respect to the scene is used to construct a pose quaternion for the synthetic imagery camera.  For rendering the synthetic imagery we have opted to use the Open-Source Graphics Rendering Engine (OGRE)[1], which we have found to be a useful method for quickly and easily creating AR simulation environments.The data from both the rifle and HMD handoff module is also used to improve the simulation’s realism by detecting how the AR participant is holding the rifle – or, in other words, the participant’s stance.  The stance detector module works by examining the pose of both the rifle and HMD software modules to return an estimate of stance.  To accomplish this, the cross product of the unit orientation vectors from both the HMD and the rifle are used to estimate the participant’s stance. For example, if the view angle is directly at a fiducial, and the rifle’s pose has been yawed past a threshold with respect to the fiducial, the system would determine that the rifle is across the participant’s chest. The data from the stance detection module is then fed into a non-player character (NPC) controller module.  This module uses the detected stance to drive the behavior of the non-player character in our simulation. For example, if the simulation participant swings his rifle across his chest, the NPC is free to move towards the participant, as a rifle at rest is a sign that Soldier is non-confrontational.  This NPC behavior data is then fed into an NPC controller module that renders the NPC animations and controls their movement.  The results of this module are then used to conduct the final system render.  Occlusion handling, or the overlap of physical objects over synthetic imagery, is completed in our system by using static 3D maps of the training environment.  In creating our AR system, we have chosen to ignore small occluding objects (for example, clutter on a desk) and instead focus on larger occluding objects (i.e., the desk itself).  These 3D maps are generated before the simulation takes place, and make use of a number of COTS 3D modeling tools.  For this project, we used Google’s free 3D modeling software called SketchUp. The motivation for this decision being that SketchUp is extremely user friendly and connected to Google Earth – which allows us to harvest a wide variety of existing, geo-tagged, 3D building models. Once an environment model is created in SketchUp (or downloaded from the web), importing it into the simulation is accomplished using a simple format conversion, and then aligning it within the simulation 3. Results & ConclusionsOur initial results are promising – the system is able to perform reasonably well and maintain an average simulation frame rate of 60 FPS.  Based on this, we believe we have created an initial framework for testing more advanced augmented reality techniques and tools.  Figure 3A shows a screenshot from the application with the NPC character. With respect to pose detection, we believe the camera system is capable of estimating position to within 10% of distance from the fiducial up to a limit of about 5m. The camera-based orientation system is much more accurate, and can determine the camera’s orientation to within 5% of ground truth.  The performance of these systems is related to the quality of the camera’s initial calibration, with poor camera calibration subsequently leading to poor system performance.The IMU (which is currently only used for orientation estimation) is very accurate over short periods of time, but the accuracy of its measurements becomes significantly inaccurate after 60s without a fiducial pose update.  We believe that we can reduce both these error rates by using higher performance hardware – our current system uses basic web cameras and COTS IMUs, so higher resolution and frame-rate cameras should provide significantly better results. The reason for this is that during times of rapid movement, our current 30Hz camera is not able to capture stable, non-blurred images for processing.  Additionally, higher resolution cameras will allow us to determine the position of the fiducial with a much higher degree of accuracy.   The system’s interactivity is currently sufficient for demonstration purposes, but it will require more robust pose estimation from both the HMD and rifle systems to improve performance.  Better interactivity can also be accomplished by improving the overall simulation environment, in the form of modules for NPC artificial intelligence, physics modeling, and audio input and output. Our approach to occlusion handling appears to work for simple, uncluttered, indoor scenes.  Figure 3B and 3C show our simple environment map, as well as its occlusion handling capabilities.  In figure 3C, the whitened areas show occluding regions – in this case, the wall occludes the player’s view with the exception of the door.  We estimate that our static occlusion handling system is accurate to within 5% of ground truth – if we were to use an edge detection algorithm to align the occlusion map and the imagery, we could significantly improve occlusion handling and pose reconstruction. Our system still lacks a dynamic occlusion tracking system (e.g., a system that can determine if the door is open, closed or ajar) and this will need to be addressed before proceeding with further testing. Finally, with respect to our system’s hardware, we had some difficulties finding suitable stereo camera, video-see-through head mounted displays at a reasonable cost. The HMD camera, and the displays of our Trivisio HMD, left much to be desired both in terms of resolution and frame-rate (see Figure 3D).  The HMD’s display resolution of 800x600 pixels and 40-degree diagonal field of view were very limiting – the unit’s field of view alone limits the human eye to about 30% of its natural field of view.  Furthermore, the integrated camera’s narrow field of view made it necessary for the simulation participant to maintain a direct view of the fiducial at most times.     4. Discussion & Future WorkWhile we have successfully made this proof of concept AR for live training system, there are still a number of key concerns to be addressed before the system will be ready for full scale testing.  Our primary concerns are improving the systems registration, occlusion handling, and human interface – all while simultaneously integrating it into an LVC simulation environment.  With respect to both registration and occlusion handling, we feel that the system would benefit from newer hardware that exploits recent advances in parallel computing, and in particular general-purpose graphics processing units (GPGPU). We believe that GPGPU techniques can be used to do real-time, or near real-time, 3D environment mapping using sequences of video imagery [3][7].  Concurrently, with this 3D mapping process we should be able to create a marker-minimal AR system to eliminate, or at least reduce, the system’s dependence on fiducials.  It is most likely the case that fiducials will remain in the final system, serving as precise GPS waypoints within a larger augmented reality environment that help smooth the transitions between interior and exterior scenes. Using GPGPUs coupled with newer frame grabber boards, it should be possible to achieve AR processing frame rates in excess of 240Hz.  These new methods will help to reduce motion blur and improve the overall system registration, while new markerless AR techniques will allow us to support larger AR environments. Newer 3D mapping techniques should allow us to dramatically improve the system’s static occlusion handling capabilities [7].  It is our belief that the greatest benefit from these systems will come from merging existing geo-tagged 3D buildings with either statically or dynamically acquired 3D models of the objects inside.  Dynamically occluding objects, like cars or other AR simulation participants, can be tackled by using a variety of image processing techniques to detect, track, and then build 3D models of the dynamic objects. While these methods will address the performance of the augmented reality, there is still a great deal of research required to create a compelling user interface for the AR simulation [6].  New simulation environments, like augmented reality, require new types of user interfaces that address the user’s other senses in order to improve the overall simulation realism.  Auditory cues are an important component to training, and they must be addressed within our simulation. Furthermore, the NPCs in the simulation should respond naturally to commands issued from the participant – and likewise, the AR simulation should account for the participant’s non-verbal communications (such as hand gestures and gait).  In the case where the AR simulation is used in the place of (or in addition to) live-fire training, the participant must receive tactile feedback to make them fully aware of the danger associated with these situations. Another significant challenge, is incorporating AR training systems into existing training groups within the Armed Services, and providing them with tools and capabilities that significantly improve and foster acceptance of such novel training systems.  Of paramount importance is the issue of usability, requiring us to refine the system’s interface so that both users and trainers can quickly and easily create new and exciting AR scenarios that target specific training objectives.  A quick sweep of the training area with a video camera, plus the addition of a few GPS-located fiducials, should be all that is necessary to create a compelling AR training environment.  After completing the scenario, both educators and participants should have access to the video, audio, and virtual representation of the mission for after-action review. 5. ReferencesOpen-Source Graphics Rendering Engine (OGRE)  HYPERLINK "http://www.ogre3d.org/" http://www.ogre3d.org/Ferrari, V. Tuytelaars, T. "Markerless Augmented Reality with a Real-time Affine Region Tracker" in Proc. of the IEEE and ACM International Symposium on Augmented Reality (ISAR), New York, New York, October 2001, pp. 87-96S., Frahm J. “GPU-based video Feature Tracking and Matching.” Technical Report 06-012. Department of Computer Science, University of North Carolina. May 2006.Hol, J.D.; Schon, T.B.; Gustafsson, F.; Slycke, P.J. “Sensor Fusion for Augmented Reality” Information Fusion, 2006 9th International Conference on, July 2006 Page(s):1 – 6.Klein G. Murray D. ”Parallel Tracking and Mapping for Small AR Workspaces.” In Proc. International Symposium on Mixed and Augmented Reality (ISMAR'07, Nara) November 2007.Livingston, M. Brown, D. “Mobile Augmented Reality: Applications and Human Factors Evaluations.”  HYPERLINK "http://handle.dtic.mil/100.2/ADA473342" http://handle.dtic.mil/100.2/ADA473342  June, 2006.Seitz, S. M. Curless B. A “Comparison and Evaluation of Multi-View Stereo Reconstruction Algorithms”, CVPR 2006, vol. 1, pages 519-526.AR 350-1; Army Training and Leader Development, 3 Aug 2007Author BiographiesFRANK DEAN is a Science & Technology Manager with the Simulation & Training Technology Center (STTC), RDECOM, Orlando, Florida.  Mr. Dean manages R&D projects related to the training of dismounted Soldiers through the enhancement of experiential learning by using augmented and mixed reality technologies.DOUG HAANPAA received his B.S. in Applied Physics in 1991 and his M.S. in Computer Science in 1993 from Michigan Technological University. He is currently primary investigator for Cybernet’s Virtual Reality group He contributed much of the design of this product as well as implementation of pose detection algorithms.KATHERINE SCOTT is a research engineer at Cybernet Systems Corporation. Katherine received dual B.S.E degrees in Computer Engineering and Electrical Engineering from the University of Michigan, Ann Arbor in 2005. Ms. Scott is the project manager for Cybernet System’s Augmented Reality research projects and has help to develop these systems. Ms. Scott has a background computer vision, simulation, robotics, and biosensor design. JAMES TODD is a lead systems engineer with the Program Executive Office - Simulation, Training & Instrumentation (PEO-STRI), Orlando, Florida.  Mr. Todd manages the implementation of the Future Army System of Integrated Targets (FASIT) and upgrades and improvements to the Enhanced Remote Equipment Target Systems (ERETS) as part of the Live Training Transformation (LT2) Standardization efforts.Figure  SEQ Figure \* ARABIC 1: AR for live training system schematic Figure  SEQ Figure \* ARABIC 3: (A) Completed system screen shot with NPC. (B) SketchUp model of the simulation environment for environment modeling. (C) Example of occlusion mapping from environment model. (D) Completed system hardware.Figure  SEQ Figure \* ARABIC 2: Pose Extraction Process. (A) Intrinsic camera calibration. (B)Luminosity of the scan line (yellow) high frequency signal green, smoothed signal in blue. (C) Extracted barcode lines. (D) Completed line scans for corner detection. (E) Reconstructed bar-code pose. (F) The system performs well even in low lighting conditions (contrast enhanced). 