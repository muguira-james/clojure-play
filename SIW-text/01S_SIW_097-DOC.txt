“Missing Links” in the Simulation Based Acquisition Tools CollectionJames KesterCACI International Inc2850 Presidential Drive, Suite 300Fairborn, Ohio  45324937-429-8311jkester@hq.caci.comPeter EirichThe Johns Hopkins University, Applied Physics Laboratory11100 Johns Hopkins RoadLaurel, MD  20723240-228-7264peter.eirich@jhuapl.eduKeywords:Simulation Based Acquisition, SBA, SBA environments, modeling & simulationAbstract:  It is generally accepted that success in applying a Simulation Based Acquisition (SBA) approach will require the use of some form of collaborative, automated modeling support environment.  At this early stage of maturity for SBA, each new SBA-based DoD or service program usually develops its own new environment, and each tends to be at least somewhat different from those that have come before.  However, if the broad DoD vision of SBA is to be realized, different program-specific environments will require much more commonality and compatibility than has been the case up until now.  Compatibility is necessary for enabling the reuse of models and data from one program to the next, and for ease in comparing results of studies using models.  Commonality is also necessary for reducing the cost of implementing each new SBA environment.  This paper attempts to identify what types of enabling work must take place before the desired level of commonality and compatibility among SBA environments can be achieved, and adoption of the SBA approach becomes the norm.PREFACE:  Previous works by each author have described two related concepts.  The Smart Product Model (SPM) [1] is a data collection describing a complex weapon system, which could support simulation and analysis; any data derivable from the minimum set of information (for instance, deriving a signature profile from shape and material) can be obtained by running a software model.  Changes to system design can be captured in the SPM and will be reflected in all dependent information (for instance, manufacturing cost, system effectiveness, maintenance staffing) by re-executing the modeling codes.  Since multiple items of dependent information derive from single SPM data items, the system cannot be fully object-oriented, for the reasons discussed in [2]:  changes to a single datum can affect several features, and the system cannot be regarded as an assemblage of independent, non-interacting objects.  The SPM becomes analogous to a data warehouse, with dependent information regarded as different views through the data collection.  The maturity of the SPM viewing mechanism can drive technology development; for example, can acceptable software models (and other enabling technologies) be developed to produce all desired SBA views?The authors’ goal is not to reinvent or redirect the SBA Roadmap [3], but to take the view of SBA to another level of detail, down to the tools specialists currently use and will use in the future to create systems and acquire them.  If the SPM is defined and the various needed domain-specific software models (e.g., for engineering, total life-cycle cost, maintenance, replacement parts, mission effectiveness, human factors, etc.) are itemized, it becomes possible to identify enabling technology shortfalls which may inhibit program success.  It is also possible to search for and identify technical obstacles to implementing SBA.  Based on the types of considerations indicated above, this becomes the main thrust of the paper.This paper, an extension of a line of analysis first presented by the authors in [4], proposes that SBA will not achieve its stated vision any time soon unless there is progress in identifying obstacles, and then in developing enabling technologies specific to SBA needs.  This paper summarizes the SBA approach and identifies areas where there is the greatest need for new enabling technologies.Introduction The vision for Simulation Based Acquisition (SBA), adopted in 1997 by the Acquisition Council of the Department of Defense (DoD) Executive Council for Modeling and Simulation (EXCIMS), is “an acquisition process in which DoD and Industry are enabled by robust, collaborative use of simulation technology that is integrated across acquisition phases and programs.” [3]  Goals for the acquisition life cycle under this vision are to reduce time, resources, and risk and to increase quality, military worth, and supportability.  A further goal is to reduce total ownership cost. This also will improve system acquisition under integrated product and process development (IPPD).  [5]What indicators identify a successful new process?  Among them would be:a growing stream of industry, academia, and government agencies using the process,a growing catalogue of commercially-available tools supporting the process,educational opportunities in learning the process, and the skills and tools important to the process,inclusion of knowledge of the process in resumes of a wider circle of job applicants, showing awareness of the technological leading-edge, andrecurring conferences for the exchange of experiences with the process.  The DIS/HLA technologies, for example, show all of these indicators.  However, even though the SBA vision has been around since before 1997, there does not seem to be a stampede of programs, tools, and training opportunities to indicate it is reaching takeoff velocity, or even picking up significant momentum.  Directives, DoD endorsement, and financial investment have all been present.  Potential payoffs have been acknowledged and recognized for quite a while.  There is no lack of supporting technologies.  The DoD has successfully done its part with a kickoff effort that produced a roadmap for SBA [3].  Then, why isn’t there more rapid and more evident progress in SBA use?  What has been getting in the way of the widespread implementation of SBA?  Are there inherent technical obstacles present?  Are key element(s) for overcoming any obstacles missing?  We will explore these questions in this paper; call it “The Case of the Missing Links.”The Quest for ObstaclesA search for the reasons would be worthwhile.  If we can identify the impediments to progress, then money and official attention can be brought to bear and the impediments removed.  If the problem is a missing tool or key technology, we can work to see that the tool/technology is researched, developed, and fielded.  If the problem is human behavior, changes in the acquisition and engineering culture can be encouraged, and training can be introduced to reduce pockets of resistance or misunderstanding.  How would Sherlock Holmes have approached this case?  Perhaps, by looking into some of the more evident “suspect” obstacles, eliminating from consideration the unlikely ones, and then investigating closely the remaining suspects.  We’ll take that approach here, beginning with some of the usual suspects; call these troublesome characters “The Unlikely Seven.”Architecture Complexity?Is the problem an architecture with too many components?  Consider the architecture overview shown in Figure 2.1.  According to the SBA Road Map [3], multiple sources of data and models are stored in the Distributed DoD/Industry Resource Repository, and supplied to a program’s Collaborative Environment (CE), which consists of the people, processes, information resources, and program-unique standards for the acquisition, as well as the CE-unique support tools for the task.  The tools are also supported by Distributed Product Descriptions (DPDs) to help determine the driving functions for Cost and Performance/Benefits.  The CE tools talk to these DPDs by way of Data Interchange Formats (DIFs).  Too many components?  Not compared to many successful major weapons systems programs.  Overall, this is not that complex an architecture, and complexity in and of itself is not the problem.Too Many Disciplines?Is one of the problems a system with too many involved technical and managerial disciplines?  … e.g.,WarfightersLogisticiansSystem TestersOperational TestersSystem DevelopersEngineers (production, aero, EE, ME, civil ... )TechnologistsScientistsManagers (acquisition, cost, operational, production)Analysts (cost, vulnerability, effectiveness … )PlannersManufacturersTraining SpecialistsFigure 2.1:  Top-Level View of SBA Systems Architecture (Ref [3])This is not a probable cause.  The domains of knowledge represented in SBA are the same domains represented by people in most any acquisition program, and all of those have had algorithmic models or simulation systems to aid in their work.  In fact, all these skills have been used for decades in new system procurement and use.  They have managed to work together and succeed before this.  The new item present is extensive simulation, more pervasive and more disciplined than before.  This admittedly adds some new disciplines directly related to simulation activities, but this does not significantly affect the other core disciplines.Aggregation?Perhaps one of the problems is the mechanics of aggregating (or conversely expanding) data for multiple levels of management and analysis, with associated collaborative environments, as illustrated in Figure 2.2.   EMBED PowerPoint.Slide.8  Figure 2.2:  Levels of Collaborative Environments  (adapted from Ref [3])Acquisition data span the spectrum from the individual weapon system on the manufacturing line, and as represented in CAD/CAM design systems, up through using agencies, to senior decision makers in the Pentagon (or in other federal agencies).  However, decisions have been made in this manner for decades, even well before the advent of information technology.  National strategy, assessments, doctrine, funding, and other constraints flow down the levels of the spectrum to the acquisition agencies and further down to the designers and vendors [6].  Test information, actual cost data, and user evaluations flow up and are aggregated for each higher level.  Automation has only increased the amount of data, not the need for or technology of aggregation. As noted in Kester’s 1996 paper [6], data is summarized as it moves upward, losing operational detail but being converted into forms the higher echelons can use.  The reverse flow causes higher level direction to be fragmented out using doctrine, strategy, and the current situation to implement the specifics of the orders.As will also be seen for the other “suspects” yet to come in this search for obstacles, simulation and information technology are the only new elements.  Aggregation of data up-channel, and the converse for flows down-channel, have been going on for years using manual techniques; automation simply allows higher volume flows.  Therefore, aggregation is not the culprit.Too Many Layers of Data and Modeling?There are four layers of data shown in Figure 2.3.  At the root of the decision process, there is Fundamental, very basic data, which cannot be derived from any other data.Figure 2.3.  Layers of Data and ModelsSuch data include the basic physical shape of the system being acquired, and the composition (materials, coatings such as paint, color and markings).  The next layer up involves Directly-Dependent Data, such as weight and balance information, the manufacturing process required to produce the object, cargo capacity, crew count and passenger count, electrical load, fuel capacity, etc.  Some  of these can be observed directly and others must be derived with software.  The third layer, Modeled Data, departs from the physical and enters the abstract, where basic and dependent data are used to produce analysis numbers such as Pk, maintenance staff loading, production costs, annual fuel consumption, 30-day spares estimates, and the like.  These are all, at bottom, statistical data with accompanying estimates of variance/RMSE/confidence.  Various domains have their own techniques for estimation of cost drivers.  Engineering has a wide variety of constructive models, including not just deterministic models but also those using permutation of variable effects to produce a distribution of possible outcomes.  Software for process development (of both manufacturing and management) is abundant.  At the fourth layer, Model-Based Information, the two overarching information categories are Cost (unit or life-cycle) and Effectiveness (in whatever use the system is being bought to achieve).  Too many layers?  Four is not really too many.Too Many Decision Domains?Are there too many decision domains?  If we look at the uses of simulation and models within the multiple applications making up the ultimate decision (see Figure 2.3), it really just comes down to three domains:  (1) How well does the thing work?  Normally identified with constructive simulations, this results in effectiveness values.(2) How will it look?  This is done with virtual simulations and focuses on how the system appears and behaves to both its operators (maintainers fit in this context as well) and its adversaries.  The data from this step feeds into both effectiveness and the cost side (maintenance staffing, crew staffing and training, base facilities, …).(3) What will it cost, either per unit, per operating echelon, or “total buy” life cycle cost?  Life cycle cost encompasses more than just the equipment buy and the weapons and gas; basing, staffing of crews, training, maintenance, and ground support drive other contributors to the LCC.  Both the effectiveness and the “look and feel” contribute to these pieces of cost.  Too Many Interrelationships?Figure 2.4 shows the notional interrelationships between the models in the domains shown in Figure 2.3.  Is it possible there are too many inter-relationships?  “Too many” might mean too many to keep track of, or it might mean connected in ways that require convergence loops or feedback. For the most part, in the breakout of dependencies shown in Figure 2.4 there is no looping Figure 2.4.  Interrelationships Among Models and Dataexcept for deliberate restarts to incorporate modifications to Fundamental data (shape and appearance) or to direct dependent data (mix of basing, crew, planned payload).  These modifications are induced based on the feedback provided by crews and maintainers from the virtual simulations, or by analysis results from the constructive simulations.  Once the changes are made, data flows up the chart from bottom to top.  Data in-flows need to be aggregated or reformatted to the expectations of the receiving model/algorithm.Note here that because of the multiple domains and the interrelationships leading to the cost/benefit summations, we cannot manage by exception, or by marginal analysis.  While none of this will appear as a closed set of equations, the interactions are non-linear, and small changes do in fact lead to large changes in effectiveness and/or cost.  Add one more antenna to the outside of an airplane, with associated internal avionics boxes, and multiple characteristics change (weight/balance, drag, signature, fuel flows, EMI, power supply loading, …), leading to potentially non-linear changes in handling qualities, performance, effectiveness, logistics, and cost.This is all the more reason for using simulation and by extension, SBA.  The coupled sets of models, tools, data, and simulators can allow the decision-makers to more easily assess the impact of single changes or multiple compensating changes than with the old mostly-manual process.  However, good systems have been built for years, despite these complexities, using primarily manual processes.  Equally important, the essential processing is straightforward, just a lot of careful bookkeeping, and essentially a linear flow from basic design data through models to the cost/benefit bottom lines.  It is difficult to imagine how the addition of simulation technology would increase the difficulty of this obstacle beyond what it was, so this is unlikely to be the culprit.Information Overload?Perhaps the problem is that simulations provide too much information for the decision maker to easily cope with.  If we look at the set of options for a given acquisition, each design option leads to different system cost and performance characteristics based on the technologies it incorporates, and can be positioned accordingly in a multi-dimensional trade space.  Some of the experimental system design sessions described in [7] suggested that there might well be limitations on the amount of data that a decision maker would want to deal with simultaneously.  However, for this to be a credible culprit, there would have had to have been some previous, data intensive decision support system that decision makers had tried for a time, but then abandoned.  Until that happens  and if it has, the authors are not aware of it – it is hard to view this as being a key suspect.The Combined Impact of all the Above?Even if none of the above is likely, individually, to be the impediment that has stymied SBA, could their combined net impact still turn out to be the problem?  Could the adverse effects from these “Unlikely Seven” factors be layered, one on top of the other, until one becomes the “straw that broke the camel’s back?”   This cannot be ruled out absolutely as a sufficient cause, but none of these look to be a showstopper, even when taken in combinations.  Since programs have had sufficient time, experience, and history to overcome problems such as these, it is hard to believe that these causes – even taken together  could be enough to put a firm roadblock in the path of SBA.  Moreover, program managers have DoD guidelines and directives for reference, web sites to serve as resources, and conferences.  At working forums such SIW, they can share ideas, review available data and tools, discuss do’s and don’ts, and present examples of successful SBA programs.  These resources should be more than enough to help program managers address multiple factors.  So, it’s not the combined impact of “all of the above."Now, for Some of the More Likely “Suspects”With those acknowledged areas of difficulty ruled out, what is left?Database and Tool/Model Integration?Perhaps the problem lies with database and tool/model integration.  Recently, one of the authors (Eirich) worked on a small-scale prototype to help explore SBA issues [8].  To try to create a realistic SBA situation, a number of tools and models were envisioned as contributing to a core system data base:   Reliability calculationCost calculationLogistics calculationConfiguration performance simulationsMOE/MOP calculationSimulation sequence guidanceConfiguration option definitionCustomer/Operator trade-off preferencesSensitivity analysesOverall evaluated performanceQuery and reportingDespite everything that was included on this list, the thoughtful observer notes that nowhere in this collection does computer-aided design data appear!  The “Fundamental” data about system makeup and shape, from which all other models should derive their outputs, is not part of the combined data bases of model and tool data.  Despite a desire to construct a realistic SBA mini-environment, CAD/CAM is where the author had to draw the line; that became the “bridge too far” for the project.  The fact that CAD/CAM was missing becomes a clue to the answer to this quest, much like the famous dog that didn’t bark."It is of the highest importance in the art of detection to be able to recognise out of a number of facts which are incidental and which are vital. . . . I would call your attention to the curious incident of the dog in the night-time." "The dog did nothing in the night-time." "That was the curious incident." — Sherlock Holmes and Inspector Gregory,in The Memoirs of Sherlock Holmes, "Silver Blaze"3.2	A Culprit Found?CAD/CAM is normally the domain of the product design community, as distinct from the systems simulation community that is the natural home of those items that did make the above list.  In addition there is a community of systems acquirers and operators, a.k.a. the customers, whose success depends on the successful execution of simulations that are applied to the results of the product design work.After considering the implications of the “no CAD/CAM” situation, the authors began to wonder if any serious obstacles to the success of SBA might be found in terms of disconnects or other coordination/communication difficulties among these three communities, as illustrated in Figure 3.1.  Perhaps, to continue the detective analogy, a “smoking gun” might be found in the lack of standard interfaces across the boundaries of these three distinct user and technical communities.Consider the domains of activity for the systems simulation community vs. the product design community.  The simulations used by the former range from the Platform level of detail up through the Strategic level (see Figure 3.1), while the interest of the latter addresses the Platform level down to the System and Subsystem level, and sometimes down to the component level.Figure 3.1.  Communities in SBAThese groups have three significant primary data sources for their focus:Requirements, in the form of text, for the systems acquirers and operators;Product description data, in CAD/CAM/CAE form, for the product design community, andsimulated performance results matched to testing results for the systems simulation community.  There exists today no common interface, nor effective data synchronization tools, to assure that all three sources are based on the same fundamental data.3.3	 Lack of Common Bridging Standards?Let’s take an open minded look at the situation.  Is it possible that a (and perhaps the) major problem curtailing SBA use is a lack of standards and tools to allow the product design community, at the platform and system level, to share their data with the system simulation community in a common form?At the shape and material level, isolated commercial and DoD tools exist for deriving some model characteristics from basic CAD formats.  Scene generation software, using standard formats for describing objects, has matured.  This contributes to simulation visualization.  Simulation technology has been around for decades, and is reasonably mature.  The apparent SBA problem is that there is no widely-available tool to transform the CAD/CAM design into a format for the simulation community, since the simulation community has no accepted formats for using data at this level of detail.  This would appear to be the problem requiring a fix.  “When you have eliminated the impossible, whatever remains, however improbable, must be the truth.”	— Sherlock Holmes, in “The Sign of Four”This problem should not be insurmountable, and lack of such a data-sharing interface need not continue indefinitely.  For example, consider the example of a workable interface as described in Kester [2]: Electronic designers can use graphical automated tools to lay out printed boards or chips with complex circuitry, perhaps using libraries of industry-accepted components, then use integrated software to exercise the resulting simulated electronic design with a family of input conditions to assure that the design produces the required outputs.  With some commercial design software, the engineer can also have the simulation check for EMI and for the distribution of heat loads.  The vetted design can be saved or exported in industry-standard CAE file formats.IF there were comprehensive, accepted bridging standards for (1) mapping from requirements into product design features, (2) transforming product design features and attributes into simulation parameters, and (3) relating simulation outputs back to the originating requirements, THEN a closed feedback loop would be created.  Program managers and designers already have the motivation to ensure consistency across these domains.  With bridging standards (supported by design tools) they would then also have both the means and opportunity to bring their system definition data “full circle," checking it at each stage until consistency and completeness were achieved.  (That’s about all that Sherlock Holmes would need to solve a case.)  Finally, IF a capability for achieving cross-domain consistency were in place, THEN this could be the framework to which other SBA elements could be attached, and around which a functioning SBA infrastructure could be assembled.  Then we could say “Case Closed.”  However, we need to identify some near-term, practical steps that people can take to start a process in the right direction toward such a cross-domain capability.4.	Possible Resolution of the ObstacleThe gap between CAD/CAM formats and simulation/federation formats could be reduced if we can take steps to simplify finding areas of congruence between these three domains of data.  The requirements would be threefold:Require that newly purchased DoD system design tools and models must share/publish their import and export data formats or provide their direct access APIs;Require a formal logical data model covering all imported, exported, and API-accessible data from these tools or models; andRequire a formal domain model (i.e. a conceptual model) to restate and confirm the tool’s or model’s view of simulation/modeling requirements.These three, taken together, would make a unified system definition database possible.  The database could contain requirements descriptions, simulation parameters, and key product characteristics.  The database would be fed by and would support published formal data models defining the system, simulated performance vs. testing results, requirements descriptions in text form, and product description data in conventional CAD/CAM/CAE form.The steps to this unification would be:An open and available, web-accessible, source of recognized DoD terminology, to help with data item naming and identification.  The DoD data dictionary is a useful beginning, but similar access to other terminology references would add further value.A common DoD file format for tool-to-tool and tool-to-model data interchange.  This is the equivalent for tools and batch-mode models of what HLA accomplishes for time-driven simulations.A common DoD representation in which a tool or model must “publish” its exportable data structure.  The field is wide open for a solution here, although there are some likely candidates to serve as a basis or starting point.A specialized “tool kit” to analyze legacy data file structures and code-generate a translator to XML.  The best solution to this would be Open Source, with user-community improvements being developed over time.IF the difficulty in sharing data between manufacturing systems and simulation systems could be removed, THEN Simulation Based Acquisition could become affordable in time and dollars.5.	What is Currently Missing, but Still Needed? Figure 2.4 illustrated a notional attempt to show contributors to either side of the cost-benefit decision for DoD systems.  It could be tabularized or converted to a DBMS.  Are all the pieces there?  Are all the (major) interactions there?  Are additions to this model interconnection diagram needed to help identify data requirements not currently served, and models and simulations which do not exist?  A next level of detail for the diagram would be to evaluate whether there is a complete mapping of inputs and outputs for the populated model set.  Perhaps models exist but need to be improved to support a common cost-effectiveness process.  Are there times in the non-automated acquisition process where data are needed but are not reliably available, hence, approximations are made?While a number of different catalogs of models and simulations exist within the DoD and among the services, these are intended for human browsing rather than for access by tools.  They do not provide for automated access to their data flow definitions by other tools.  Therefore, the ability to apply automated analyses to help answer the questions above is very limited.Illustrating a GapThe arrows in figure 2.4 illustrate an existing concern:  there are no data flows or models depicted for the contribution of component reliability to effectiveness; only (through several stages) to life cycle cost – just one part of the usual cost/effectiveness tradeoff.  Further, at the engagement level (categorized under effectiveness) and in virtual simulations, the possibility of subsystem failure is often not modeled to show operators and maintainers how the system might respond in actual use.  In fairness, some current models do simulate failures or substandard operation.  Failures can range from outputs having unacceptable biases or variances, or total power-off failure, to behavioral variances due to manufacturing variance.  Simulator operators already induce system problems as part of operator training.  On the cost side, failure rates are a key driver to the derivation of spares usage rates, the other major driver being battle damage.  We will need the probability distribution of failure, part by part, or at least subsystem by subsystem, for the effectiveness side.  Aggregate failure rates will be needed for addressing the cost side.  [6] [9]6.	The FutureIn addition to the example in section 5, what other gaps need to be addressed?  What other discriminants would alter this automation of cost-effectiveness analysis?  We have an informed hunch that cutting-edge technologies are hard to cost, have hard to estimate manufacturing processes, and are difficult to estimate for the full range of operational usage.  Are there any other fuzzy areas?  How about operations outside the planned envelope?  And with the non-linear phenomena, how can we reasonably estimate the error brackets on the final data?  Are there basic research areas where some advances would assist in addressing these questions?  Answers to these questions are essential if SBA is to achieve its full and lasting potential.“Our ideas must be as broad as Nature if they are to interpret Nature.” 	— Sherlock Holmes, in “A Study in Scarlet”References:[1]	Eirich, Peter, “From Data Environments to Smart Product Models," Spring 1999 Simulation Interoperability Workshop (SIW) of the Simulation Interoperability Standards Organization (SISO), paper 99S-SIW-124, March 1999.[2]	Kester, J.E., "Some Limitations of Object-Oriented Design," Proceedings of NAECON 1993, pp 573-576.[3]	“A Road Map for Simulation Based Acquisition," Report of the Joint Simulation Based Acquisition Task Force, (Acquisition Council Draft for Coordination)," Joint SBA Task Force, December 4, 1998.[4]	Eirich, P.L. and Kester, J.E.  Presentation  “Enabling Technologies Needed For SBA," 6th Annual Joint Aerospace Weapon Systems, Support, Sensors And Simulation (JAWS S3 ) Symposium & Exhibition, San Antonio, Texas, 26-30 June 2000.[5]	Lutz, R. and Keane, J.,  “An Architecture for Simulation Based Acquisition," Spring 1999 Simulation Interoperability Workshop (SIW) of the Simulation Interoperability Standards Organization (SISO), paper 99S-SIW-113, March 1999.[6]	Kester, J.E., “An Approach to Aggregation and Deaggregation for JMASS System Models," 15th Workshop on Distribute Interactive Simulations, paper 96-15-011, September 16-20, 1996.[7]	Eirich, P.L., “Experiments in Decision Analysis Techniques for Simulation Based Acquisition,” in Proc. Interservice/Industry Training, Simulation and Education Conference 2000, Orlando, FL, 27 Nov  1 Dec 2000.[8]	Eirich, Peter, “Systems Architecture for a Low Cost Collaborative Environment for SBA Decision-Making," Spring 2000 Simulation Interoperability Workshop (SIW) of the Simulation Interoperability Standards Organization (SISO), paper #00SSIW128, March 2000.[9]	“Simulation Based Acquisition:  A New Approach," Report of the Military Research Fellows DSMC 1997-1998, LTC M.V.R. Johnson Sr, USA, LTC M.F. McKeon USMC, LTC T.R. Szanto USAF, Defense Systems Management College Press, Fort Belvoir, 1998.Author BiographiesJIM KESTER is a Senior Simulation Engineer working several DoD programs at the Fairborn, Ohio, offices of CACI International Inc, an international information technology products and services company headquartered in the Washington, D.C. metropolitan area.  He received BS (1966) and MS (1974) degrees from MIT.  He is the Dayton office chief engineer on JMASS matters and previously worked on the JSIMS program. A former U.S. Air Force Acquisition Meteorologist and Systems Analyst, his experience includes C++ and Ada software development, human interface development, graphics/imaging, and project management under a mature CMM process.  PETER EIRICH is a member of the Senior Professional Staff at The Johns Hopkins University Applied Physics Laboratory, where he works on projects involving Simulation Based Acquisition environments and on procedures for the Verification, Validation, and Accreditation of models and simulations.  He received BS and MS degrees in Electrical Engineering from MIT in 1970, an Engineer’s Degree in Electrical Engineering from MIT in 1974, and an MS from MIT’s Sloan School of Management in 1974.  Previously, he has been active in the development of both US and international standards, with leadership roles in advisory groups, working groups, and technical committees.  He chaired the subcommittee on Electronic Data Interchange and Design Automation of an international Advisory Committee on Electronics and Telecommunications.