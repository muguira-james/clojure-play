M&S Infrastructure Characterization Preliminary ResultsMax Lorenzo1, Jesse Nunez2, Jonn Kim3, Ken Allred4, Ben Matthews3, Chris Kwasneski3, James Lewis3, David Christianson3Joint Test and Evaluation1; Army Development Test Command HQ, White Sands Test Center2; GaN Corporation3; KEnetics, Incorporated4Keywords:Modeling and Simulation, Simulation Environment, Infrastructure, TENA, MATREX HLA, 3CEABSTRACT:  To optimize the design and performance of a distributed simulation environment, it is imperative to fully understand the characteristics of the mechanism that facilitates the data transport between simulation applications and components. The Army Headquarters Developmental Test Command (DTC), Architecture Focus Group (AFG); Army Cross Command Collaboration Effort (3CE); and the Office of the Secretary of Defense (OSD),  Developmental and Operational Test and Evaluation (DOT&E) is sponsoring an infrastructure characterization task to facilitate the understanding of the  fundamental information transport mechanics.  “Infrastructure” includes physical and logical information transport mechanisms such as the network (physical and IP layers), middleware, encryption devices, applications, interoperability standards, and operating systems. This paper discusses the objectives of the infrastructure characterization and provides preliminary results and analysis of ongoing infrastructure characterization to date. IntroductionModeling and Simulation (M&S) is being used worldwide by military, industry, and academia as a “technological enabler to enhance training, analysis, and acquisition activities  REF _Ref125534099 \r \h  \* MERGEFORMAT [1].  A Simulation Environment consists of the operational environment surrounding the simulation [2]. The M&S Infrastructure includes M&S systems and applications (e.g. simulations); communications; networks; architectures; standards and protocols; and information resource repositories  REF _Ref125527079 \r \h  \* MERGEFORMAT [2].  Distributed simulations require data and information to interoperate.  A key enabler that the M&S Infrastructure provides is the physical and logical mechanisms for data and information transport. This paper presents preliminary results on the characterization of current M&S Infrastructure components that facilitate data and information transport focusing on real-time simulation applications.  These infrastructure components are the network and middleware.By characterizing middleware and the underlying network, an understanding of performance and abilities is gained. The network was characterized using multiple network/middleware configurations and protocols.  Two common middleware solutions were characterized; the Test and Training Enabling Architecture (TENA) version 5.1  REF _Ref125534264 \r \h [3] and High Level Architecture (HLA)  REF _Ref125534388 \r \h [4].  This paper presents the preliminary characterization test results of the network and TENA/HLA in multiple network configurations.  A recommendation or endorsement was not the purpose of this study.  The intention was to provide an objective view of each middleware and its behavior in a particular test environment.  M&S Infrastructure Characterization OverviewThe purpose of the M&S Infrastructure characterization is to explore “As-Is” Army M&S capabilities and limitations, and to provide repeatable test methods and processes that can be used across DoD.  Lessons learned from characterization events will be used to advance the development of future “To-Be” DoD M&S capabilities.  As the first of a series of such events, the results contained in this paper begin to provide data to characterize the “As-Is” Army network and middleware.  Future events will begin to characterize other infrastructure components to fully assesses M&S capabilities/shortfalls, and evaluate the state of simulation interoperability  REF _Ref127613147 \r \h [11] across applications and organizations.The “As-Is” Army infrastructure includes the Wide Area Network (WAN), middleware, computer resources, and operating system(s) used by the various Army commands.  To assess “As-Is” capability, network and middleware system performance parameters such as throughput, scalability, jitter, and latency are examined.  The effects of Operating System(s) and network configuration variables (i.e., payload size, update rate, entity count, and delivery method) are also measured.The characterization test planning activity started under the auspices of the Army Developmental Test Command (DTC), Architecture Focus Group (AFG).  The venue for the first execution of the characterization plan was the Cross Command Collaboration Effort (3CE) Environment Characterization #1 (EC1), with additional funding from DOT&E.  3CE EC1 consists of four phases.  The individual Phases and objectives for EC1 are:Phase 0 is a characterization of network performance from endpoint to endpoint using TCP/UDP Multicast sockets to transmit data. Phase I verifies middleware connectivity by executing a simple test case using a simple object model to facilitate message passing between applications.Phase II is a characterization of middleware performance from endpoint to endpoint, but with middleware applications publishing and subscribing. Phase III incorporates various M&S assets in a TRADOC approved FCS operational scenario.This paper addresses Phases 0, I, and II.EC1 Phase I and II examine two middleware alternatives currently in use by the Army for distributed M&S applications: TENA and HLA.  The HLA Run Time Infrastructure (RTI) used was the Modeling Architecture for Technology and Research Experimentation (MATREX)  REF _Ref125534523 \r \h  \* MERGEFORMAT [5] RTI 1.3NG version 4.2. The LAN testing was performed in Huntsville, Alabama while the WAN testing used the Defense Research and Engineering Network (DREN)  REF _Ref125534657 \r \h [6]. The following sites participated in the testing: Redstone Technical Test Center (RTTC), White Sands Test Center (WSTC), and Electronic Proving Ground – Fort Lewis (EPG-FL) for ATEC; Topographic Engineering Center (TEC) Ft. Belvoir for RDECOM; and Battle Command Battle Lab (BCBL) Ft. Leavenworth for TRADOC.  WAN environment data will not be extensively discussed due to the sensitive nature of the data.Environment Characterization 1 (EC1)EC1 was built on previous studies and events such as Distributed Test and Evaluation Architecture (DTE-ARCH), Distributed Test Event 5 (DTE-5), and 3CE Near Term (3CE-NT); EC1 leveraged each of these previous events to advance technological understanding and help define requirements.EC1 expanded the scope of these previous events and studies in several ways.  EC1Characterized the 3CE environment beyond the peering points to the system level (endpoint to endpoint) in a classified environment.Implemented time synchronization by directly interfacing to GPS receivers.Used time synchronization and time stamping to measure one-way latency.Addressed the effect of the operating system on timing variability and time measurement.Included characterization results of MATREX RTI and TENA middleware 5.1.Data was collected to support the analysis of performance measures including throughput, reliability, latency, and jitter. In addition to the above measures, EC1 examined unique features that each middleware provides. For TENA, a feature called “publication state update caching” was examined. The feature allows subscribers to receive the latest data. A HLA feature called “bundling” and its effects on latency was also examined.MethodologyCharacterization of the network, TENA and HLA were accomplished by the development and use of simple object models to send data across the network. The generic representation of the object model is called the Performance Benchmark Object Model (PBOM), seen in  REF _Ref125528636 \h  \* MERGEFORMAT Figure 1.Figure  SEQ Figure \* ARABIC 1.  PBOM ClassEC1 testing included control variables that were systematically changed to study a wide variety of parameters: system configuration, delivery method, payload size, entity count, and message update rate.    The system configuration determined how many publishers and subscribers were in the test configuration.  System configurations could either be 1 publisher to 1 subscriber (1 to 1), 1 publisher to 4 subscribers (1 to N), 4 publishers to 1 subscriber (N to 1) or 5 publishers to 5 subscribers (N to N).  In N to N test configurations the subscriber and publisher applications where collocated on the same physical system; in all other configurations the applications where located on separate physical systems.The remaining control variables can be combined into 120 unique test configurations.  Delivery method was either a Reliable or Best Effort.  Reliable was TCP based, while Best Effort was based on UDP multicast.  There were 5 payload sizes varying from 128 bytes to 9000 bytes.  Entity count could take on the values of 1, 10 or 100.  This was the number of instantiations of the PBOM that were being updated.  Update rate was the planned rate that each entity was to be updating at.  This variable could take on the values of 10 Hz, 20 Hz, 60 Hz or 120 Hz.  Each of these 120 test configurations was executed for each system configuration.    REF _Ref125867632 \h  \* MERGEFORMAT Table 1 shows a subset of the 120 test configurations containing the 10 entity cases.  These test configurations will be examined in later figures in the data analysis section.  The test configuration number shown in column 1 is used for configuration identification and will be shown on the X axis on many of the figures.  It is important to note that the test configurations were grouped by entity count (40 – 79 = 10 entities), then by update rate (40-49 = 10Hz, 50-59 = 20Hz, 60-69 = 60Hz, 70-79 = 120Hz), then by payload size (40 & 41 = 128B, 42 & 43 = 512B, etc…), and finally by delivery method (Reliable/TCP is odd, Best Effort/UDP is even).  This will become important when looking at the graphs in the data analysis section.  Test Config #Entity CountUpdate Rate (Hz)Payload Size (Bytes)Delivery Method 401010128UDP411010128TCP421010512UDP431010512TCP4410101024UDP4510101024TCP4610102048UDP4710102048TCP4810109000UDP4910109000TCP501020128UDP511020128TCP521020512UDP531020512TCP5410201024UDP5510201024TCP5610202048UDP5710202048TCP5810209000UDP5910209000TCP601060128UDP611060128TCP621060512UDP631060512TCP6410601024UDP6510601024TCP6610602048UDP6710602048TCP6810609000UDP6910609000TCP7010120128UDP7110120128TCP7210120512UDP7310120512TCP74101201024UDP75101201024TCP76101202048UDP77101202048TCP78101209000UDP79101209000TCPTable  SEQ Table \* ARABIC 1.  Subset of the EC1 Test ConfigurationsData was collected to support the analysis of performance measures.  These performance measures included throughput, reliability, latency, and jitter.   All measurements were made at the application layer.  Throughput (or actual throughput) was defined as the amount of data that the publishing application could publish to the subscribing application.  Throughput was measured in megabits per second (Mbps).  Theoretical throughput was defined as the expected throughput based on the multiplication of entity count, update rate, and payload size.  Reliability was defined as the ratio of unique updates received by the subscriber application to the updates published by the publication application.  Latency was defined as the difference in time from the timestamp in the publisher application’s ‘send’ function to the timestamp in the subscriber application’s ‘receive’ function.   Jitter was defined as the standard deviation of latency.  Jitter was not thoroughly analyzed at the time of this paper and is therefore not discussed.To attain accurate one-way latencies, the EC1 test systems provided by EPG-FL contain a Tenacious Timekeeper Position Plus (T2P2) timing card, used for time synchronization and time stamping.  These cards were synchronized by using a GPS receiver. The stated accuracy for the time card with the GPS receiver is ± 1 milliseconds (ms).  REF _Ref125529250 \r \h [7]Preliminary ResultsEC1 collected nearly 300 million data points in the LAN environment; 75 million in Phase 0, 50 million in Phase II MATREX HLA and 150 million in Phase II TENA.  Due to the massive amount of data collected and analyzed, it is difficult to document the comprehensive analysis in a single paper. This paper describes a sample analysis of trends from the 1 to 1 system configuration, focusing on ten entities.  It is believed that this is a representative sample of the entire set of data collected for EC1 for 1 to 1, 1 to N and N to 1.  This sample data does not represent N to N system configurations or trends associated with N to N system configurations.  Parties interested in a detail discussion of the data analysis are referred to the paper entitled “3CE EC1 Phases 0-II Test Report” available by request from: Mr. Jesus Nunez Jr.White Sands Test Center HYPERLINK "mailto:Jesus.Nunez@us.army.mil" Jesus.Nunez@us.army.mil Phase 0For Phase 0 Network Characterization, the socket application actual throughput coincided with the theoretical throughput.  This was seen in Reliable,  REF _Ref125531293 \h  \* MERGEFORMAT Figure 2, and Best Effort,  REF _Ref125531297 \h  \* MERGEFORMAT Figure 3, communication.  Only in the 100 entity test cases (not shown) when theoretical throughput exceeded the physical limit of the network (i.e. 100Mbps Ethernet) did the actual throughput fall below the theoretical.As update rate increased no significant effects were seen on the latency of the socket application.  However, an increase in payload size consistently had the effect of increasing latency,  REF _Ref125531461 \h  \* MERGEFORMAT Figure 4.Reliability stayed near 100% across all of the test configurations.  No test parameter was seen to have a significant and consistent effect upon the reliability. SHAPE  \* MERGEFORMAT Figure  SEQ Figure \* ARABIC 2.  Socket, TCP, 1 to 1, Throughput SHAPE  \* MERGEFORMAT Figure  SEQ Figure \* ARABIC 3.  Socket, UDP, 1 to 1, ThroughputFigure  SEQ Figure \* ARABIC 4.  Socket, 1 to 1, Latency vs. PayloadPhase IIThroughput in Phase II often matched the theoretical throughput for test configurations with one entity.  At ten entities MATREX HLA shows the trend of being unable to match the theoretical throughput for larger payload sizes (Test Configurations 49, 59 69, 79) in Reliable communication,  REF _Ref125531868 \h  \* MERGEFORMAT Figure 5.  TENA conversely was able to match the throughput at the 9000B payload size.  Under Best Effort communication,  REF _Ref125531872 \h  \* MERGEFORMAT Figure 6, both middleware solutions were able to consistently match the theoretical throughput throughout the test configurations.  This trend with Reliable and Best Effort communication continues throughout the test data with some variation.  Figure  SEQ Figure \* ARABIC 5.  MATREX HLA & TENA, Reliable, 1 to 1, ThroughputFigure  SEQ Figure \* ARABIC 6.  MATREX HLA & TENA, Best Effort, 1 to 1, ThroughputTENA often exhibited lower latencies than the equivalent MATREX HLA test case.  A sample of this can bee seen in  REF _Ref125533045 \h  \* MERGEFORMAT Figure 7 for Reliable communication and  REF _Ref125791335 \h  \* MERGEFORMAT Figure 8 Best Effort communication.  Due to the use of bundling under MATREX HLA, HLA exhibited larger latencies than TENA.  Bundling is a feature of MATREX HLA that allows the middleware to send multiple updates under a single message, sacrificing latency for throughput. Generally, update rate has little effect upon the latency of an update for TENA.  The effect of update rate on HLA MATREX was magnified due to bundling and its effects, causing the step pattern seen in the figures.  Across both middleware and the socket application increased payload size often results in an increase in the latency of an update.  This can be seen in  REF _Ref125533045 \h  \* MERGEFORMAT Figure 7 as the move from test configuration 41 (128B) to test configuration 49 (9000B) shows an upswing in the average latency.  Again this trend continues throughout the test data with some variation. SHAPE  \* MERGEFORMAT Figure  SEQ Figure \* ARABIC 7.  MATREX HLA & TENA, Reliable, 1 to 1, 10 Entities, Latency SHAPE  \* MERGEFORMAT Figure  SEQ Figure \* ARABIC 8.  MATREX HLA & TENA, Best Effort, 1 to 1, 10 Entities, LatencyMATREX HLA successfully delivered near 100% of its updates on the LAN.  Conversely TENA often failed to deliver 100% of its updates due to a unique feature called “publication state update caching”  REF _Ref125534776 \r \h [10].  This feature allows subscribers to receive the latest data by overwriting a previously received update that is contained in the buffer with the most recent update.  Through this feature, TENA allows the latest data to be shared (as in real time test event) whereas HLA attempts the delivery of all data points, as shown by the high reliability rates in the data.  A sample selection is shown in Reliable communication,  REF _Ref125534914 \h  \* MERGEFORMAT Figure 9, and Best Effort,  REF _Ref125534915 \h  \* MERGEFORMAT Figure 10.  showing the lower reliability of TENA for these configurations.  This trend becomes more pronounced at the higher entity counts and other system configurations (not shown here).Figure  SEQ Figure \* ARABIC 9.  MATREX HLA & TENA, Reliable, 1 to 1, 10 Entities, ReliabilityFigure  SEQ Figure \* ARABIC 10.  MATREX HLA & TENA, Best Effort, 1 to 1, 10 Entities, ReliabilityIn general, both MATREX HLA and TENA behaved similarly in the WAN environment as they did in the LAN environment. SummaryThis characterization task provided a valuable experience to understand the effects of the infrastructure in terms of key performance attributes. New processes and automation tools such as the Test Harness were developed and designed for efficient execution of EC1 events. The test harness is an infrastructure characterization test and simulation environment development and integration tool that is currently in the initial development phase.  EC1 provided the venue to start the development of an evolutionary prototype of this tool for the purpose of infrastructure characterization testing. These automated processes and tools allow data to be collected and analyzed with minimal human operator interaction. Another key technology insertion was the introduction of 64-bit computers for data processing.  These technology advances will be reused and mature as the events mature.  Despite some key successes, several shortcomings and limitations were observed. For time synchronization across the three commands, a level of uncertainty was introduced by using non-real-time operating systems. To minimize this uncertainty, it is recommended that a real-time OS be incorporated in the future for better timing accuracy.  Given these limitations, advances should be made in this area.  Infrastructure characterization is critical to distributed simulation and object model design.  The characterization methods and processes need to be improved to facilitate the rapid and automated characterization of the infrastructure before an event is executed.  Characterization is critical in assuring that the design and execution expectations and assumptions are being met.  Future TasksTo achieve the maximum benefits from the lessons learned in this event, it is anticipated that a number of characterization events will be executed, eventually becoming part of the norm for distributed event design and execution.  Some of the topics of interests for the future characterization events are:Interoperability characterization.  Plan and conduct interoperability characterization on the simulation environment to assess the maturity of current interoperability.  Survey on the User environments and requirementsCharacterize HLA MATREX Without BundlingPerform HLA RTI Characterization (Other than MATREX RTI NG 1.3)Characterize and Conduct GRE Tunneling over the WAN Characterize Encryption Devices (Taclanes)Conduct Testing Between a Large Number of Sites/NodesCharacterize Other Operating Systems (LSI specific, RTOS, hybrid OSs)Real-time time synchronization, time stamping, and network sniffing. Characterize Hybrid Protocol  (DIS, TENA Middleware)64-bit processingTest Harness  ReferencesDMSO: “A Modeling and Simulation Primer” < HYPERLINK "http://www.education.dmso.mil/ms_primer.asp?a=s4&b=view&c1=272" http://www.education.dmso.mil/ms_primer.asp?a=s4&b=view&c1=272>, Jan 20 2006.DMSO: “Online Glossary” < HYPERLINK "https://www.dmso.mil/public/resources/glossary/results?do=get&ref_list=g" https://www.dmso.mil/public/resources/glossary/results?do=get&ref_list=g>, Jan 20 2006.TENA SDA Group: “TENA-SDA Homepage”  < HYPERLINK "https://www.tena-sda.org/home.php" https://www.tena-sda.org/home.php>DMSO:  “High Level Architecture” < HYPERLINK "https://www.dmso.mil/public/transition/hla" https://www.dmso.mil/public/transition/hla>Lorenzo, M:  “MATREX System Architecture Description, v0.8.”  High Performance Computing Modernization Program:  “Defense Research and Engineering Network.” < HYPERLINK "http://www.hpcmo.hpc.mil/Htdocs/DREN/index.html" http://www.hpcmo.hpc.mil/Htdocs/DREN/index.html>Mark Hines: “Electronic Proving Grounds (EPG) Technology Snippets.”  ITEA Journal, Dec 2003/Jan. 2004.Noseworthy, J. Russell: “The TENA Middleware” < HYPERLINK "http://www.omg.org/news/meetings/workshops/RT_2003_Manual/Presentations/2-2_Noseworthy.pdf" http://www.omg.org/news/meetings/workshops/RT_2003_Manual/Presentations/2-2_Noseworthy.pdf>Buss, Arnold & Jackson, Leroy: “Distributed Simulation Modeling: A Comparison of HLA, CORBA and RMI” < HYPERLINK "http://www.informs-cs.org/wsc98papers/109.PDF" http://www.informs-cs.org/wsc98papers/109.PDF>TENA SDA Group: “The TENA Middleware, Release 5.1 Release Notes:  Release 5.1 of the TENA Middleware.” < HYPERLINK "https://portal.tena-sda.org/doc/5.1/ReleaseNotes/rn_51.html" https://portal.tena-sda.org/doc/5.1/ReleaseNotes/rn_51.html>Carnegie Mellon Software Engineering Institute < HYPERLINK "http://www.sei.cmu.edu/isis/guide/introduction/lisi.htm" http://www.sei.cmu.edu/isis/guide/introduction/lisi.htm>Author BiographiesMAX LORENZO has over 20 years experience in Modeling and Simulation (M&S).  Currently, Mr. Lorenzo is Technical Director for the DOT&E JTE Joint Testing and Evaluation Methodology (JTEM) Feasibility Study and the Chief Architect and Architecture Focus Group Chairperson for the Developmental Test Command (DTC) Virtual Proving Ground (VPG) program. His other recent roles include the Chief Architect for Cross Command Collaboration Effort (3CE) Long Term and was  RDECOM's MATREX program Chief Architect from 2002-2003. As a Team Chief for the US Army Night Vision and Electronic Sensor Directorate (NVESD), he led the development of Paint the Night (PTN) which pioneered the use of real-time image generation and distributed network protocols to simulation tactical imaging sensors. He has published numerous technical papers at the SISO Simulation Interoperability Workshop and Conference on Computer Generated Forces.  JESUS NUNEZ JR. is employed at White Sands Test Center.  He is currently serving as the branch chief of Cross Command Collaboration (3CE) Infrastructure Integration and Verification Branch and is a Co-Chair to the Tools Focus Group for the Developmental Test Command (DTC) Virtual Proving Ground (VPG) program.  He holds a BSEE from New Mexico State University.  He has completed the US Army Air Defense Basic and Advanced Officer Courses, the Combined Arms Service Staff School and the Intermediate Level Education at Fort Leavenworth.JONN KIM is founder of GaN Corporation, which specializes in distributed Modeling and Simulation, Future Combat System testing, and sensor system design. Dr. Kim holds three patents in ultra-wideband (UWB) communications. He has served as the technical lead for Cross Command Collaboration Long Term (3CE-LT). Dr. Kim also supported ATEC Architecture Advisory Council (AAAC) and System of System Virtual Framework (SVF) author panel. He has published numerous papers at IEEE and ITEA conferences on various topics including human instrumentation, signal processing, UWB, and distributed M&S.KEN ALLRED is a support contractor to the Army Developmental Test Command (DTC).  Mr. Allred has worked in the areas of EO sensor and missile testing, hardware-in-the-loop facility development, high performance computing, distributed testing, and architectures to support integration of modeling and simulation into live and virtual environments.  Mr. Allred is currently supporting the DTC and DOT&E with distributed testing architecture design and integration efforts.BEN MATTHEWS, CHRIS KWASNESKI, JAMES LEWIS, DAVID CHRISTIANSON are engineers at GaN Corporation.  They comprised the main analysis group for the 3CE EC1 effort. The Test Harness is a framework that will verify the interoperability of simulation components and aid in the development, integration and characterization of simulation infrastructures.  This framework task is already in progress and being evaluated for use in future infrastructure characterization tests.   The Test Harness is currently in the conceptual design phase.8070605040100908070605040302010071-79 = 10 Hz: 128, 512, 1024, 2048, 9000 B61-69 = 10 Hz: 128, 512, 1024, 2048, 9000 B51-59 = 10 Hz: 128, 512, 1024, 2048, 9000 B41-49 = 10 Hz: 128, 512, 1024, 2048, 9000 BTCP, 1 to 1, 10 EntitiesThroughput for Socket ApplicationTest ConfigurationMbpsTheoreticalSocket - 10EntitiesThroughput for Socket ApplicationUDP, 1 to 1, 10 Entities40-48 = 10 Hz: 128, 512, 1024, 2048, 9000 B50-58 = 10 Hz: 128, 512, 1024, 2048, 9000 B60-68 = 10 Hz: 128, 512, 1024, 2048, 9000 B70-78 = 10 Hz: 128, 512, 1024, 2048, 9000 B0204060801004050607080Test ConfigurationMbpsTheoreticalSocket - 10EntitiesLatency for MATREX HLA and TENAReliable, 1 to 1, 10 Entities41-49 = 10 Hz: 128, 512, 1024, 2048, 9000 B51-59 = 20 Hz: 128, 512, 1024, 2048, 9000 B61-69 = 60 Hz: 128, 512, 1024, 2048, 9000 B71-79 = 120 Hz: 128, 512, 1024, 2048, 9000 B11010010004050607080Test ConfigurationsAverage Latency (ms)     .HLA - 10entitiesTENA - 10entitiesLatency for MATREX HLA and TENABest Effort, 1 to1, 10 Entities40-48 = 10 Hz: 128, 512, 1024, 2048, 9000 B50-58 = 20 Hz: 128, 512, 1024, 2048, 9000 B60-68 = 60 Hz: 128, 512, 1024, 2048, 9000 B70-78 = 120 Hz: 128, 512, 1024, 2048, 9000 B1101004050607080Test ConfigurationsAverage Latency (ms)     .HLA - 10 entitiesTENA - 10entities