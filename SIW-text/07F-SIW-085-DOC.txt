A Foundation for Semantic InteroperabilityDr. David Gross; Mr. William V. TuckerThe Boeing CompanyMS/ JR-08499 Boeing Blvd.Huntsville, AL 35824256-461-3294David.C.Gross@Boeing.Com Keywords: Interoperability, semantics, conceptual modelingABSTRACT.  Interoperability has emerged as critical “ility” for successful simulation development and deployment.  The ever increasing complexity of the challenges to be addressed by simulation, the continuing high cost to develop simulations, and the difficulty of validating simulations all create pressures to create new simulation capabilities by composing interoperable, proven simulation assets.  Interoperable simulations therefore deliver greater utility.But such interoperability is not easily provided.  Certainly, technologies such as the High Level Architecture have addressed part of the interoperability challenge, specifically providing a basis for component simulations (i.e., federates) to physically connect and exchange data through those connections.  This so-called technical interoperability is a necessary but hardly sufficient part of the full interoperability challenge.  To achieve full interoperability, simulation compositions have to (in addition to technical interoperability) determine that the composed representations provide adequate, accurate, and consistent simulated representations that adhere to the principles of “fair fight”.  This demands that those composing simulations from interoperable components understand the meaning of the information exchanged, i.e., be able to define and measure the semantic interoperability present in the composed simulation.Semantic Interoperability is a technology whose goal is to make composition of simulations from existing models or simulations easy, reliable, and cost effective.   In the state of the practice today, such higher order interoperability is assessed through applied engineering judgment.  However such engineering judgment, no matter how well informed, is not adequate to deal with the complexity of today's complex simulations. As a result, credible, cost effective, comprehensive large scale simulations are not accessible to the engineering work force, although it remains critical to their success. Deploying semantic interoperability technology would enable an order of magnitude reduction in the time required to develop complex simulations.This paper summarizes our progress to date in developing a foundation for semantic interoperability technology, addressing the problem from five findings. First, that simulation interoperability is a specialized systems engineering problem, that is, it is a system of systems problem, which requires techniques unique to this special class of systems.        Next, that simulation interoperability can fail according to a finite set of interoperability anomalies we have defined.   Next, that our fidelity framework provides a basis for measuring and comparing representations to validate composed representations.  Next, that specific simulation metadata can be captured and processed to directly provide for semantic interoperability.   Finally, that simulation interoperability can realistically exist only with relatively strictly defined domains, meaning that product line engineering techniques such Domain Specific Graphical Languages, are the approach to exploiting the potential of simulation interoperability.IntroductionSemantic Interoperability is a technology whose goal is to make composition of useful simulations from existing models or simulations easy, reliable, and cost effective.   In the state of the practice today, such higher order interoperability is assessed through applied engineering judgment.  However such engineering judgment, no matter how well informed, is not adequate to deal with the complexity of today's complex simulations. As a result, while credible, cost effective, comprehensive large scale simulations are critical to the success of the engineering work force, they remain inaccessible.  Deploying semantic interoperability technology would enable an order of magnitude reduction in the time required to develop complex simulations.The technical approach of our project has been to apply current and emerging fidelity and composability theory to develop and demonstrate architectural approach to semantic interoperability. Fidelity theory explains how to measure and specify the fidelity of a simulation. Fidelity is "the degree to which a model or simulation reproduces the state and behavior of a real world object or the perception of a real world object, feature, condition, or chosen standard in a measurable or perceivable manner; a measure of the realism of a model or simulation; faithfulness". Composability theory seeks to explain how pre-existing components may be dynamically combined into new systems, especially systems of systems, and how to evaluate the capability of those new systems. [1]Our fundamental objectives are to provide a capability predict semantic interoperability from simulation design data, and then to measure semantic interoperability from simulation results data.What is Interoperability?From a review of the literature, it is clear that interoperability is a complex, multi-faceted issue. Many of the papers reviewed attempted to address the entire scope of interoperability for example including in the discussion capabilities to support solutions to issues relative to the interoperability of organizations, frameworks, protocols, processes, systems, and their components, as well as cultural and regional issues related to epistemology.  This resulted in new, and often, inconsistent definitions. We attempt to use the most general, well established definitions by the technical community at large by using Department of Defense Directive (DoDD) 5000.59, DoD Modeling and Simulation (M&S) Management, defines interoperability as “The ability of a model or simulation to provide services to and accept services from other models and simulations, and to use the services so exchanged to enable them to operate effectively together.”   [2]DoDD 5000.59 self-evidently distinguishes between basic communication (exchange of data) and the effective use of the data exchanged.  We follow this distinction and introduce a formal distinction with the following definitions:Syntactic Interoperability, the capability of multiple M&S to physically connect and exchange data through those connections.Semantic Interoperability, the ability of systems to exchange information on the basis of shared, pre-established and negotiated meanings of terms and expressions and to use the information so exchanged to enable them to operate effectively together.This distinction has been adopted by other authors.  For example, Dalhmann defines Syntactic Interoperability as “the capability of multiple M&S to physically connect and exchange data through those connections”.  [3]Syntactic, and its root syntax, in the traditional sense, is about structure. For digital systems we are talking about the interchange of “data.” For Syntactic Interoperability, we require that the data interchange be valid (i.e. correct). For example, if the output of one M&S is a floating point number, and it is passed to another M&S, the input of the second M&S should accept a floating point number. This can become somewhat more complicated because the transfer of information can be direct (e.g. by value) or by any level of indirection (e.g. a pointer).Semantic interoperability in contrast, as the ability of systems to exchange information on the basis of shared, pre-established and negotiated meanings of terms and expressions and to use the information so exchanged to enable them to operate effectively together. Semantics is defined as the meanings of terms and expressions.  For M&S, information, terms and expressions are captured in “data.” This explicitly conforms to the second requirement for Interoperability in DoDD 5000.59.For M&S, data can be external to the entity that we are interested in (the environment), it can define the initial conditions for the system, it can define the behavior of the entity, it can control the entity we are interested in (stimulus) or it can be the output (response) of the entity. Each of these uses of data has different effects on the entity and its response to some input. For Semantic Interoperability, we will have to ensure that the use of the data in all of its forms between multiple M&S is appropriate.By appropriate, we do not mean that the type of data is identical. For example, the output of one simulation can set a parameter to establish the environment for another simulation. However, if the output of one M&S is the displacement of an object from a reference in meters, and it is passed to another M&S, the second M&S should react to the fact the value of the displacement of the object (as measured in meters) from the reference has changed.An Enabling Context for Semantic interoperabilityOur work to date in developing semantic interoperability technology has led us to five assertions about the nature of the problem.  1.	That simulation interoperability is a system of systems problem.2.	That simulation interoperability can fail according to a finite set of interoperability anomalies.   3.	That the fidelity framework provides a basis for measuring and comparing the semantic content of representations.  4.	That specific simulation metadata can be captured and processed to directly provide for semantic interoperability.   5.	Finally, that simulation interoperability can realistically exist only with relatively strictly defined domains.The following briefly addresses each of these assertions in turn.Systems of SystemsAlmost by definition, interoperability means the problem at hand is a system of systems problem. While the challenges associated with making systems work together is hardly a new problem, today’s systems engineering techniques do not satisfactorily address the system of systems challenge.It has been said, that the primary required skills for systems engineering are:1.	breadth of conception (ability to hold in mind the span and depth of SoS taxonomic organization) and 2.	attention to detail (identification of interfaces and the development and satisfaction of functional requirements, worked out to the lowest level of SoS architecture. [4]The challenges associated with systems of systems challenges each of these required skills.First, system of systems challenges systems engineering on the basic issue of "breadth of conception".  The "standard system hierarchy" (parts integrated into components, integrated in turn to subassemblies, integrated in turn to assemblies, integrated in turn to systems) does not even address systems of systems.  This standard system hierarchy is the common worldview adopted by systems engineers, and creates barriers for system engineers in dealing with system of systems. System of systems are distinguishably different than systems -- in fact, they are more distinguishable than parts or assemblies.  We need to extend the standard system engineering ontology to clearly differentiate each level in the hierarchy and ensure that this worldview accommodates systems of systems.  Figure 1 illustrates the standard system hierarchy and an extension for system of systems. Figure 1: An Improved System HierarchySecond, system of systems challenges systems engineering in regards "attention to detail".  The complexity of each layer today is much greater than the complexity of historical systems.  Complexity is important, because it increases (hopefully) the scope of missions the system can accomplish and (certainly) the chance for design errors to prevent mission success.  One example of this complexity is the shift from a tightly integrated set of consistent elements within the system, to a system composed of constituent parts.  Instead of parts having single uses within their resident systems, they have utility in potentially many systems.  In today’s systems, tight physical coupling is giving way to lose coupling in physical space combined with tight coupling in information space. As a result of these increases in complexity and changes in the nature of integration, systems engineers need extended/new systems engineering techniques.  Figure 1 illustrates both of these shifts in the standard systems hierarchy worldview.  First, note that parts/components/assemblies/ are shown as “composed”, rather than integrated.  Second, note that systems are shown as interoperating to form systems of systems.  If we can understand how to compose systems, and how to make the resulting systems interoperate, then we can make more effective systems. Interoperability AnomaliesIf interoperability is an ability or function to be provided by a composed system, then it should be possible to classify what could go wrong with that function.  By analyzing a series of interoperability failures, we assert that interoperability problems can arise in the following seven areas [5]:Range ConsistencySensitivity ConsistencyTemporal RepresentationState ConsistencyEvent Phase OrderingValidityAdherence to Performance, Input, Output and Environmental RequirementsArmed with these categories or classes of interoperability anomalies, we developed a series of analytical tests to detect the presence of such anomalies.  We present the analytical tests in two sets.  The first set, specific to interoperability but applicable to any/all of the coupled simulations, consists of the classes shown in Table 1. [5]Table 1: Analysis Classes Applicable to All Coupled SimulationsClasses DiscussionScenarios ClassThe effect of external conditions or scenarios for the interoperating simulations must be consistent. This class of analysis actions compares scenario dependent data, setup or responses common to scenarios input across the simulations.Timeline ClassA validly interoperating simulation set will execute functions and generate data in the same order as the “real world” system. Therefore the within the integrated simulations, state changes, data / message generation should follow the same characteristics as the system. A comprehensive approach to validate that the coupled simulations function the same as the system is to define timelines in this class of semantic interoperability metrics to measure this capability.Action - Reaction ClassEnd-to-end timelines are usually too complex to allow detailed stimulus-response or causality analysis. In addition to timelines, this class addresses the properties of Action-Reaction and Causality. Action-Reaction and Causality are both required to define necessary and sufficient conditions for timing interoperability. Explicit validation of Action - Reaction (Stimulus-Response) is required because it is possible for coupled simulations to exhibit a consistent timeline and causality without semantic interoperability. This condition occurs when a control variable is passed from one simulation to induce a response in a second simulation. There is no violation of Causality yet the lack of a response from the second simulation, when there should be one, indicates that the integrated simulations are not interoperable.Causality ClassAn interoperable coupled simulation set will execute functions and generate data in the same order as the “real world” system. Therefore the within the integrated simulations, state changes, data / message generation should follow the same characteristics as the system. Action-Reaction and Causality are both required to define necessary and sufficient conditions for timing interoperability. Transmission of a control variable from one simulation does to a second simulation could stimulate a response that exhibits a consistent timeline and satisfy Action-Reaction but could violate Causality if the simulated time of the response occurs prior to the simulated time of the stimulus. To complete the analysis of timing, we must address causality in addition to the timeline and Action-Reaction. While stimulus - response ensures that there was a response to an input, verifying causality ensures that any of the functions did not execute prior to the point in time where it should have.Temporal Accuracy Class“Temporal Accuracy” is a combination of the domain, range, sensitivity and resolution of time as it is maintained by or passed between simulations. This class of semantic interoperability metrics addresses the characteristics and use of time with in each simulation.Natural Law ClassThis class of semantic interoperability metric ensures that the simulations “Follows the Applicable Laws of Nature.” Analyses in this class specifically address the underlying process, chemistry or physics represented by the simulations.State Consistency ClassIt is possible that the same object / entity is represented in two different simulations. This class of semantic interoperability metrics refers to the states of entities that may be duplicated in two different simulations and ensures that the states of the entity in each simulation are with acceptable tolerances.The second set, specific to the interface(s) between simulations and thereby required to use the information in the interface effectively, consists of the classes shown in Table 2. [5]Table 2: Analysis Classes Applicable to Interfaces between Coupled SimulationsClassDiscussionData Type Consistency ClassThis class of semantic interoperability metrics refers to the data exchanged by the coupled simulations. Since we have established syntactic interoperability, we know that the data is correctly passed between the simulations.Here we ensure that the type of data is consistent with its use in each simulation. A complete set of analyses for this class would explicitly validate every data item that is exchanged between the simulations.External Input ClassThis class addresses the operating environment for the integrated simulations and explicitly addresses the “Domain of the External Input.” Since we assume that the intended use that drove the development of the individual simulations did not include the intended use of the coupled simulations, it’s entirely possible that the operating environment of the coupled set of simulations is greater than either of the individual simulations. Consequently, we need to validate that the following data are properly used:InputsInitial ConditionsParametersScenariosOutput ClassThis class of semantic interoperability metrics addresses the “Range of the Output” to make sure that a simulation will not generate an output that is passed to a second that exceeds its domain.  The “range” of the output in this context can also include the:Norm (magnitude of a scalar, vector or matrix)OrientationRate of ChangeInput ClassHere we address the “Domain of the Input” to ensure that a simulation can correctly accept and utilize the input. For interoperability, the domain of the simulation must be a superset of the range of simulation which output the value to be used as input. We need to consider this because the domain and range of the variable can be dynamic and uncoupled.Output Precision ClassPrecision is defined the level of resolution or granularity with which a parameter can be determined.  In this class of semantic interoperability metrics we consider the “The Precision of the Output” to ensure that small changes in the state of a simulation results in outputs or changes to its outputs of that would be seen by a receiving simulation.Input Sensitivity ClassSensitivity is defined as the ability of a component, model or simulation to respond to a low level stimulus. In this class of semantic interoperability metrics we consider the “The Sensitivity of the Input” to ensure that changes in the inputs to a simulation are observed by the simulation and result in an appropriate response. Input Resolution ClassResolution is defined as the degree of detail and precision used in the representation of real world aspects in a model or simulation.   Resolution therefore matches the magnitude of the spatial and temporal units in the simulations.Domain Constraint ClassThis class of semantic interoperability metrics is designed to address those kinds of issues that are unique to one of the simulations by explicitly adding any domain constraints and testing for the violation of this limit.The Fidelity FrameworkFidelity has historically been the “elephant in the living room” for simulationists.  Fidelity is almost in the same category as what used to be said about politics and religion– something polite people do not discuss!  But fidelity is a key issue for understanding simulations, because it is at the heart of the simulation question: “how much representation is enough?”  Fidelity is at the heart of what simulationists mean when they refer to the problem of a “fair fight” between models hoping to interoperate – and therefore critically important for creating simulation interoperability.  Even though it is a critical problem, fidelity assessments are typically performed in an ad-hoc subjective manner, often after the simulation has been fully developed and implemented and rarely formally related to making simulations interoperate..  In contrast to the typical approach, the Fidelity Conceptual Framework, illustrated in Figure 2 (and detailed in reference [6]), defines fidelity in terms of a mathematical explanation of its relationship to observables characteristics of simulation solutions.  This framework combined with our understanding of the interoperability analogies form the basis of what meta-data can and should be collected to determine the presence and validity of semantic interoperability. [7]Figure 2: The Fidelity Conceptual FrameworkSimulation MetadataClearly, specific interoperability analyses require information about the specific data input and output by the simulations, or metadata.   “Metadata” is data about data.  For example: "12345" is data, and with no additional context is meaningless. But if "12345" is given a meaningful name (metadata) of "ZIP code", one can understand (at least in the United States) that "12345" refers to the General Electric plant in Schenectady, New York.Based on consideration of these interoperability anomalies and the observables identified in the Fidelity Conceptual Framework, we assert the set of metadata identified in Table 3 is the correct set to be gathered for analyzing interoperability. [4]Table 3: Metadata Enabling Analysis of Semantic Interoperability ClassMetadataDiscussionMetadata for each OBJECT in the SimulationInitial ConditionsStatic scenario inputs (not generated by other simulations) State VariablesVariables internal to the simulation which determine the simulation’s state, or ability to process inputs and produce outputs.Resolution identify the level of representationCapacity how many of this object may be representedExistence of / potential for functional compositions when the computation of one or more object states in one simulation depend upon the data provided by another simulationExistence of / potential for manifold representations occur when two or more simulations represent the same state or behavior of the same object & interact either directly or indirectlyMetadata for each CONTROL (input) of each OBJECTRangethe set of values the control may take onFrequencythe update rate for the controlPrecision the smallest change possible in the controlMetadata for each BEHAVIOR (output) of each OBJECTError the difference between an observed, measured or calculated value and a correct valueSensitivity the smallest non-null change possible in a model’s outputsPrecision the smallest input change that produces a change in the model’s outputStrictly Defined DomainsWe assert that since interoperability is a combinatorial problem that the explosion of different meanings in context means that any analysis of interoperability anomalies based on collected simulation metadata will effectively be constrained into a relatively small domain.  In a sense this is freeing, because in actual practice, few simulationists attempt to create interoperability between models from completely unrelated domains.  In fact, if we restrain the domain sufficiently, techniques such as domain engineering and domain specific graphical languages suggest an approach for how such metadata can be efficiently and effectively gathered, and analyzed to determine the validity of intended interoperability.  Domain Engineering, also called “Product Line Engineering” is the entire process of reusing domain knowledge in the production of new software systems. [8]  A domain-specific programming language is a programming language designed to be useful for a specific set of task such as Generic Eclipse Modeling System (for creating diagramming languages), Csound (used to create audio files), and GraphViz, (used to define and create visual representations for directed graphs).  Figure 3 proposes a tool leveraging the concept of domain engineering and constructed around a domain specific graphical language.  Figure 3: An Effective Process for Creating Interoperable Capable Models & SimulationsDomain OntologiesOur objective in all of this work is to develop objective measures for the extent of semantic interoperability present in a given composition of simulations.  While the foregoing work describes the theoretical underpinning of what semantic interoperability is and what information would be necessary to detect it’s presence, it is not clear that this yields a practical approach to actually considering real simulation compositions.  The amount of information present in any useful simulation threatens to overwhelm the ability of simulation developers to actually make practical use of it for determining the validity of a compositions interoperation.If simulation developers are to make practical use of semantic interoperability, we will need to develop tools that enable the efficient and effective collection and manipulation of the data in question.In this light, the ongoing work related to the semantic web and in particular web ontology language (OWL) emerges as a very exciting development.  The semantic web is an evolving extension of the World Wide Web in which web content can be expressed not only in natural language, but also in a format that can be read and used by software agents, thus permitting them to find, share and integrate information more easily. It derives from World Wide Web Consortium (W3C)director Sir Tim Berners-Lee's vision of the Web as a universal medium for data, information, and knowledge exchange.  Tim Berners-Lee originally expressed the vision of the semantic web as follows [9]:“ I have a dream for the Web [in which computers] become capable of analyzing all the data on the Web – the content, links, and transactions between people and computers. A ‘Semantic Web’, which should make this possible, has yet to emerge, but when it does, the day-to-day mechanisms of trade, bureaucracy and our daily lives will be handled by machines talking to machines. The ‘intelligent agents’ people have touted for ages will finally materialize.” The web ontology language (OWL) is technology which may enable this vision to come to a reality.  OWL is a language for defining and instantiating ontologies whose definition enables operations on those definitions which are very important as a basis for understanding the semantic content of those ontologies.  OWL is not a “settled” technology, so discussions of what it provides and does not provide have a certain risk of being overcome by events.   But we can make some generally useful observations about where this technology is headed and its relationship to our problem of semantic interoperability.An ontology is a data model that represents a set of concepts within a domain and the relationships between those concepts. It is used to reason about the objects within that domain.  Ontologies are used in artificial intelligence, the semantic web, software engineering, biomedical informatics and information architecture as a form of knowledge representation about the world or some part of it. Ontologies generally describe:Individuals: the basic or "ground level" objects Classes: sets, collections, or types of objects[1] Attributes: properties, features, characteristics, or parameters that objects can have and share Relations: ways that objects can be related to one another Events: the changing of attributes or relations OWL provides syntax for capturing this information, but more importantly enables reasoners which operate on this data.  A reasoner is simply a computer program that makes inferences about unstated attributes, relationships, and events from what is stated, according to rules of logic it adopts.  For example, if it is stated that all humans have mothers and “Sam” is a human, then a reason can infer that Sam has a mother. [10]OWL reasoners are typically open-world reasonsers, which is the inverse of how we typically think about inferences.  In closed world reasoning, the assumption is that two items are related only if they are expressly stated to be.  In open world reasoning, two items are assumed to be possibly related unless expressly stated not to be.  In closed world reasoning, all items in the ontology are presumed to refer to different referents; in open world reasoning, any two instances might be referring to the same thing.  Closed world reasoning leads to single, definitive models of the world, whereas open world reasoning leaves open as many possibilities they have not been expressly foreclosed. Closed world reasoners answers question when definitive facts are know; open world reasoners can return possible answers, in the absence of sufficient facts to provide a definitive answer.  Closed world reasoner say “it is”; open work reasoners say “it could be”. [10]Finally, most implementations of OWL support federating of ontologies and thereby reasoning across the federation.  All of the individuals, classes, attributes, relations, and events in any of the federated ontologies are available to the reasoner. [10]. We can use the reasoner to discover new possibilities in the federated ontologies, and to discuss inconsistencies between them. The relevance to semantic interoperability of composed simulation should be apparent.   By definition, an ontology provides an technique for simulationists to express a conceptual model, and associated open world reasoners provide a way to explore questions about that conceptual model to discover what’s possibly true about that model.Consider an example. Suppose we express part of two different ontologies expressing part of the conceptual model of two different models as shown in Figure 4.  Discussions of graphical symbology for expressing ontologies is beyond the scope of this paper, but let it suffice for now to note that the rounded corner rectangles represent individuals of the classes shown by the square corner rectangles.  So, “seattleUSA” is an individual in the City class.  The ontologies in the figures are “geo”, expressing the “Geography Model” and “travel: which expressed the “Airports Model.Figure 4: Partial Ontologies Expressing Two Different Conceptual ModelsNow, we know from our independent knowledge of the real world that the geo:huntsvilleUSA individual is actually the same thing as the travel:Huntsville individual, that is they are the same city.  That is, they share a common referent.  Now, suppose we federate these ontologies and assert the single new fact that geo:huntsvilleUS = travel:Huntsville as shown in Figure 5 (the Airports Model Ontology has been flipped vertically for the purposes of simplifying the graphic).Figure 5: Federated Ontologies In this case, an open world reasoner can infer a number of additional relationships by applying the information that was separate before.  Figure 6 shows the new relationships that would be inferred by an OWL open world reasoner.  OWL tools exist that can discover these relationships, document them, and provide an easy basis then for validating that the inferences are in fact correct, and that these two conceptuals have valid semantic interoperability for their originally contrasting views of Huntsville. Figure 6: New Relationships Realized by Federating OntologiesSummary and Future Work:We have shown in this work that semantic interoperability for simulations is a fundamental problem in the components of any system of systems.  Actually, M&S has an advantage over non-M&S systems of systems because M&S by definition incorporate models of other systems which inherently are incomplete and inaccurate (or stated positively, their fidelity is less than perfect).  We have identified and classified the kinds of interfacing problems that might occur when models or simulations are composed to a great whole, and proposed tests or diagnostics to determine if such exist.  Our tests require certain meta-data about the models or simulations, and we have experimentally shown that the fidelity framework largely defines the metadata to be captured.  We have argued that semantic interoperability can only be defined and therefore shown in a limited domain, which again is an advantage for M&S because we already know that our models or simulations are valid only for specific purposes.  Finally, we have hypothesized that emerging technology for the semantic web, in particular the Web Ontology Language (OWL) and associated open world reasoners will provide a practical basis for defining conceptual models based on fidelity metadata and reasoning as to the correctness of semantic interoperability between composed models.We believe that this approach offers promise for providing a sound basis for progress in one of the hardness problems in M&S today – establishing that semantic interoperability is present in a composed model or simulation and correct.  We hope other researchers will join with us in investigating the potential of this approach.ReferencesM. Biddle and C. Perry, (2000). “An Architecture for Composable Interoperability”, Proceedings of the Fall 2000 Simulation Interoperability Workshop, 03S-SIW-073. DoD Modeling and Simulation (M&S) Management, 4 January 1994. (Department of Defense Directive (DoDD) 5000.59).Dahlmann, J. (1999) “HLA and Beyond: Interoperability challenges,” Proceedings of the Fall 1999 Simulation Interoperability Workshop, 99F-SIW-073.Dunn, Mike. (2006). “Systems of Systems: An Historical Perspective”, Personal Communications.Caughlin, Don (2005).  “An Approach for Identifying M&S Interoperability Anomalies”, Personal Communications.Gross, D. and Z.C. Roza and S. Harmon.(1999). “Report out of the Fidelity Experimentation ISG,” Proceedings of the Fall 1999 Simulation Interoperability Workshop, 99S-SIW-167. Harmon, S. Y. and S. M. Youngblood (2000). “Leveraging fidelity to achieve substantive interoperability,” Proceedings of the Spring 2000, Simulation Interoperability Workshop and Proceedings, Orlando, 00S-SIW-09201S-SIW-114) “Domain Engineering: A Model-Based Approach”, Carnegie Mellon University,  HYPERLINK "http://www.sei.cmu.edu/domain-engineering/domain_engineering.html" http://www.sei.cmu.edu/domain-engineering/domain_engineering.htmlBerners-Lee, Tim; Fischetti, Mark (1999). Weaving the Web. HarperSanFrancisco, chapter 12. ISBN 9780062515872.   McComb, Dave and Simone Robe (2006).Designing and Building Ontologies Workshop, Seattle, WA.Author BiographiesDr. David Gross is a modeling and simulation systems engineer and associate technical fellow of the Boeing Company’s Analysis, Modeling, Simulation and Experimentation organization.Mr. William Tucker is the chief scientist of the Boeing Company’s Analysis, Modeling, Simulation and Experimentation organization.