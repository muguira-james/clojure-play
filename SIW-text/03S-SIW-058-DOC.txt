Challenges in Automating theProvisioning of Parametric Initialization Data to Simulation ApplicationsLee W. LacyMichael PuhlmannGabriel AvilesWayne RandolphDynamics Research Corporation3505 Lake Lynda Drive, Suite 100Orlando, Florida  32817407-380-1200 x104LLacy@DRC.comJin KwonU.S. Army Materiel Systems Analysis ActivityATTN: AMXSY-J392 Hopkins Rd.Aberdeen Proving Ground, MD 21005-5071(410) 278 2787kwon@amsaa.army.milMajor Matt ChesneyU.S. Army TRADOC Analysis Center – MontereyNaval Postgraduate School, PO Box 8692Monterey CA 93943-0692 (831) 656 7575Matt.Chesney@trac.nps.navy.milKeywords:XML, DIF, Semantic Web, OWLABSTRACT: Simulation applications are increasingly data driven and require a variety of data to support initialization.  Data should be provided by authoritative data sources and be vetted through a verification, validation, and certification process.  Research to date has focused on resolving syntactic challenges of sharing data using the Extensible Markup Language (XML).  However, other complex challenges require further attention.  These issues involve semantic interoperability, semantic mapping responsibility, explicit tags vs. meta-model approach, standard nomenclature, entity type enumerations, versioning / traceability, storage methods, distribution methods, and the standards development process.  Although XML can solve syntactic interoperability challenges, differences in the semantics between producers and consumers should be addressed in an automated fashion.  Semantic web technology offers potential solutions to semantic interoperability.  Interoperability agreements must be developed in a collaborative environment and published for adoption by a group such as SISO.  Decisions must be made regarding whether to delegate transformation requirements to the producer or the consumer.  Similarly, decisions must be made between a meta-model or explicit Data Interchange Format (DIF) approach. Updates to data must be carefully configuration managed and versioned.  Another challenge is the distribution of baseline versions of data to consumers.  This paper describes these challenges and potential solutions using currently-available technologies.  These issues should be considered as the number of XML DIFs in the simulation community continues to explode.  This paper suggests a number of solutions, and potentially new standards, to proceed with data interchange in the modeling and simulation community with XML and semantic web technology.IntroductionSimulation applications are increasingly data driven and require a variety of data to support initialization and execution.  Much of the data that drives these simulations is parametric initialization data.  Interchanged data includes: initial conditions data, Vulnerability/Lethality (V/L) data, equipment characteristics and performance (C&P) data, and scenario data (e.g., unit laydown, unit order of battle) (Lacy, 2001) (Hopkins, 1999).  The data used by these simulations should be provided by authoritative data sources and be vetted through a verification, validation, and certification process.  Simulation data is typically “provisioned” by authoritative data sources such as the Army’s Materiel Systems Analysis Activity (AMSAA) and National Ground Intelligence Center (NGIC) for the equipment C&P data and the U.S. Army Research Laboratory (ARL) for the V/L data.  Consuming applications typically convert the source files into other forms, or store the provided files in repositories (DaCosta, 2003a).BackgroundSharing simulation data supports model requirements.  Data management policies encourage this sharing, which has been accomplished using Data Interchange Formats (DIFs).Higher Resolution Model SupportThe trend and direction of both legacy and future Army Models and Simulations (M&S) is towards a higher resolution-modeling environment.  Whereas in the past, direct engagements were the only explicitly modeled feature, the models and simulations have progressed to include many more parameters (e.g., line of sight, line of fire, angle of strike, variations upon level of damage).  Other features are also being planned for future models that include a higher resolution of C4ISR entities, environmental impacts upon these entities and massive increases on the terrain complexity, to include mobility on various unique terrains and three dimensional representation of man made structures.    This evolution causes a challenge for the model and simulation community not only within the Army but all Services.  At the user level, the current paradigm requires many man-hours dedicated to reconfiguring source data so that a legacy model can parse and interpret the data.  An analyst or programmer may spend the majority of their time configuring data for a model rather than developing it. This means a significant portion of the development cost must be allocated to data preprocessing.  One person’s sole occupation can easily become that of a data preprocessor.Data ManagementAccording to Army Regulation 5-11, “Management of Army Models and Simulations” (Army, 1997), paragraph 7-2, the Army’s data management goals are to:Provide a common set of verified, validated, and certified data which can be shared by Army M&S activities;Facilitate internal, joint and combined interoperability through the standardization and use of common data; andImprove data quality and accuracy to minimize the cost of data production and data maintenance according to DoDD 8320.1 and DoDD 8320.2.The regulation directs the Army M&S community to:Share valid data to all M&S data consumers,Develop standards to use common data, andMinimize Cost of Data.These three bullets are at the heart of the challenge for today’s Army M&S community.  The existing Army M&S “culture” suffers from not meeting these directives and from being inefficient in its current data management processes.  Specific examples for each bullet can lead to insight of some of the inefficiencies.State of the PracticeAs for shared data, the current process normally involves a single data source or producer catering to many different customers.  This alone forces the data producer conduct more data formatting instead of performing their true job of data production and research.  The number of customers that must be served multiplies this particular inefficiency.  Developing standards for data also has its challenges when dealing with different authoritative data producers.  Each has unique data production, model and format standards.  For example, NGIC has used a meta-model approach that is similar to a catalogue or library of equipment.  AMSAA uses a five key approach that primarily maintains performance data.  The fifth key links the munition to a unique target and therefore leads to numerous pairings.  As defined by AR 5-11, Data Standards is: “A capability that increases information sharing effectiveness by establishing standardization of data elements, data base construction, accessibility procedures, system communication, data maintenance and control.”This definition represents a goal that has not been fully realized by both the data producers and consumers.  Current weaknesses include consistency in the naming of data elements, differing data base construction and different accessibility procedures.  	An incorrect assumption is that these faults lie only with the data producers.  Data consumers (e.g., Army M&S) develop data requirements.  These requirements can be very dynamic even when dealing with one unique model.  Time must be taken to develop a list of all known data requirements, and then develop a data interchange format.  This can be done concurrently with the development of future models.  The third directive, above many initiatives, minimizes the cost of data production by:Establishing standards and standard methods for data interchange.Using the latest technology to develop a Data Interchange Format in XML for all legacy and future models.Educating both data producers and consumers of the benefits of the above two initiatives.Data Interchange FormatsSimulation information can be shared through the use of Data Interchange Formats (DIFs).  DIFs are “formal specifications of the structure and format of data interchanged between data producers and consumers” (Gravitz, 1999).  The development of DIFs is a data engineering task with similar challenges to developing software.  As with other aspects of simulation development, a goal in provisioning simulation data is to simplify the process and minimize costs.DIF research to date has primarily focused on resolving syntactic challenges of sharing data using the Extensible Markup Language (XML) (Lacy, 1998) (Randolph, 2002) (Stytz, 2001).  However, other complex challenges require further attention.  DIF Implementation Challenges and Potential SolutionsA variety of data sharing issues face simulation developers and simulation data architects.  These issues involve:semantic interoperability, semantic mapping responsibility,explicit tags vs. meta-model approach,standard nomenclature,entity type enumerations,versioning / traceability, storage methods,distribution methods, andstandards development process.  The following sections describe these challenges and potential solutions using currently-available technologies.  These issues should be considered as the number of XML DIFs in the simulation community continues to explode.  Semantic InteroperabilityThe power of a standard syntax for neutral interchange has been demonstrated by Hyper Text Markup Language (HTML).  Publishers serve up HTML that can be represented in browsers by consumers.  Similarly, the XML metalanguage is used to define tagging sets for particular applications.  The specified tags help specify the format and structure of the contents of conforming data files.  Although XML can help solve syntactic interoperability challenges, differences in producer and consumer semantics (the meaning of the data) must be addressed in other ways.  Preferably, solutions should support automated processing. Semantic differences may be as simple as the use of different names for the same items.  These naming differences can be overcome by standardizing nomenclature.  However, more complex differences involve different levels of fidelity or resolution for representing information.  For example, one representation of the probability of kill (Pk) for an M1A1 tank firing against a T72 tank might take into account the specific cannon type, munition type, weather conditions, and angle of impact.  Another representation might simply provide a percentage value based on the firing tank type and the target type.  Another example includes level of granularity issues (e.g., movement speed for a platoon of tanks vs. the movement speed for individual tanks in a platoon).  Producers and consumers may provide/expect data with different qualifications (e.g., units of measure).Solutions to semantic interoperability challenges include standardizing on data models and providing explicit semantics.  Standardizing on common data models is being investigated for Army and NATO data interoperability.  Explicit semantics can be provided by using ontologies described using standard languages such as the World Wide Web Consortium (W3C) Ontology Web Language (OWL).  OWL ontologies are encoded using XML and Resource Description Framework (RDF) Schema technology.  These ontologies explicitly define classes of simulation objects and their properties.  They also formally relate classes to each other (e.g., subclass relationship).Semantic Mapping ResponsibilityData producers and consumers may have known semantic differences in their data that require mapping by humans for resolution.  For example, equivalent items may be represented with different names or multiple parametric attribute/value pairs in the producer’s data might be equivalent to a single structure in the consumer’s data model.  Although a DIF provides a standard data format, both producers and consumers will likely require a mapping process to translate their data to/from their data models into the DIF’s semantics.  A data map provides a definition of how a producer’s data is interpreted by the consumer.  This essential definition may be captured in a Data Map document, which contains a cross reference and translation of the data elements.  Decisions must be made regarding whether to delegate transformation requirements to the producer or the consumer.  The onus will likely be on the data consumer to generate and use a Data Map since producers may not know consumer requirements a priori. The data provider might need to generate and use a Data Map only if a common dictionary exists.  A common dictionary is our term for a negotiated agreement on the semantic expectations of the DIF content.  If a common dictionary does not exist, the provider has no target semantics.  However, the DIF should allow the data producer to pass along enough metadata for data consumers to generate the Data Maps without a dependency on the data provider.  A data provider will need to make a Data Map and use it in order to export conforming data if a common dictionary exists.  Whether or not a common dictionary is used, the export routines will likely be tailored to the DIF.  In the absence of a common dictionary, data consumers will need to generate Data Maps based on the meta-data provided with the data.  The import routines will likely be tailored to the consumer formats. Explicit Tags vs. Meta-modelDIF designers must choose between using explicit tags or a meta-model approach. Explicit DIFs use tag names that contain the identifier of the data value being passed (e.g., “<weight>”).  The meta-model approach involves using a single tag name (e.g., “<identifier>”) and passing the data value identifiers as text string data (e.g., “<identifier>weight</identifier>”).  The difference in approaches is similar to XML export techniques for Oracle databases in which the developer is given a choice of serializing a table with tags that are the column names or having the column information including the data value name being  provided as tag data.  The approach also mirrors the data model approach taken by various data producers.  For example, AMSAA database uses explicit tables while NGIC uses a meta-model table approach.Explicit tags make the identifiers visible to XML-aware processing software.  This simplifies automated searching and manipulation of the file.  It also enables validating software to ensure that tags are legitimate and correctly structured.  The problem with explicit tags is that the format/structure definition specified in the XML DTD or XML Schema becomes very complex and requires frequent maintenance and versioning.A meta-model allows for more flexibility in the formal DTD or XML schema representation of the DIF.  Data items can be added and deleted from a DIF without required modification to the formal encoded description.  An associated, less formal, encoding standard should be used with the meta-model approach to encourage consistent use of metadata values. A similar approach has been used with SEDRIS and Distributed Interactive Simulations (DIS).  Although this approach enables a much simpler format, the companion encoding standard must be maintained, and users of the DIF must add another layer of information to produce or consume the data.The decision between marking up data with explicit tags or a meta-model approach is similar to that faced by producer database designers.  The information that is exchanged is too voluminous and dynamic to change the database schema every time the contents change.  Therefore, other than extremely well-structured complex structures, this information would normally be treated as data.  Similarly, a meta-model DIF approach should be used except in cases where the items being exchanged are fairly static.  For example, vulnerability/lethality information based on a well understood combination of keys lends itself to explicit tagging.Regardless of the DIF design decision, the consuming applications are more likely to use an explicit tag approach.  For example, the OneSAF Objective System program wants explicit DIFs within their program (DaCosta, 2003b).  Performance concerns discourage the indirection introduced by the meta-model approach.  However, performance issues do not normally impact DIF processing because it is done during simulation development or initialization, not simulation execution.One compromise is that very standard/critically required items could be standardized and made explicit, while supporting a meta-model approach that enables exchanging any information.Standard NomenclatureCommon naming is a significant, yet easily solved challenge in sharing simulation data.  A variety of schemes are available.  Several initiatives have been undertaken to specifically address this issue.  They include the Army’s standard nomenclature database (SND) for equipment and munition naming used by Army analytical community in support of Army studies and the DMSO common semantics and syntax effort for other parametric descriptions.  The U.S. Army analytical community, consisting of  Training and Doctrine Command Analysis Center (TRAC), AMSAA, ARL, Center for Army Analysis (CAA), and NGIC, has been developing and maintaining the SND and an associated website on the Secure Internet Protocol Network (SIPRNET).  The SND consists of platform, weapon, munition, and sensor names along with descriptive information.  Also, included in this database are linkage tables which link selected sensors to platforms, weapons to platforms, and munitions to weapons.  This nomenclature database is used in the C&P data request process to submit M&S weapon system data requirements to the providers.  The nomenclatures contained in the database adhere to a naming and syntax convention agreed upon by the Army analytical community.  This convention establishes well-defined ground rules for developing names.  These ground rules were established so that these names would be easily recognizable and identifiable to the M&S data community.  The guiding principle was to achieve a common understanding in the community on the meaning of each name.  The database and website on SIPRNET continue to be developed, enhanced, and maintained to support the M&S community.  The identification of standard nomenclature drives the definition of explicit tags and encoding standards for meta-model data.Entity Type EnumerationsOne subset of the standard nomenclature challenge is the assignment of unique identifiers to simulation object types.  There are two common systems for assigning unique identifiers to simulation objects – Modernized Integrated Data Base (MIDB) numbers and IEEE DIS Enumerations (Byrd, 2002).The Defense Intelligence Agency (DIA) maintains the MIDB.  MIDB numbers are five character alphanumeric strings.  The first three characters contain hierarchical information about the platform.  A published document explains the hierarchy of assigned character values.   The last two characters are ordinal.   MIDB numbers are used by JSIMS and its sub-ordinate family of simulations (e.g., WARSIM, WIM, NASM).IEEE DIS Enumerations are documented in  “Enumeration and Bit Encoded Values for Use with Protocols for Distributed Interactive Simulation Applications” (UCF, 2002).  The DIS Enumerations are longer in format and contain far more information, including country of origin.  The DIS Enumerations are also more comprehensive than the MIDBs, with far more categories covered (e.g., terrain features, dismounted platforms, lifeforms, cultural features).   DIS Enumerations are used by the Close Combat Tactical Trainer and by many of the naval and air force simulations.The MIDB is a much more carefully maintained and managed source of encodings than the IEEE document and should be the first choice for delineating entity types .Versioning / TraceabilityParametric initialization data changes as real world objects change or what is known about them is improved.  The changes in the data affect the simulations that use the data.  The pedigree or provenance of the data is especially important to verification and validation agents.  They want to know the source of data and how it has been modified.  Changes to data are just as important as changes to simulation software code.  Updates to data must be carefully configuration managed and versioned.  The solution for managing changes in data is to provide metadata with the data that indicates the version and pedigree of the data.  The metadata may be needed down to the individual data items level.  However, extensive metadata embedded with the data can provide performance issues to consuming applications.  Solutions to the file size bloat caused by metadata include stripping out the metadata prior to use or extracting metadata into a separate documentation file.A standard set of metadata tags, called the Dublin Core tag set, provides a set of industry-accepted standards for defining tags for metadata.  The simulation community should consider adopting this tag set and making their files more interoperable with Dublin Core-aware tools such as search engines.Data Storage The storage and distribution strategy for simulation data must support the versioning and traceability requirements described above.  Data is normally maintained by providers in databases.  However, serialized views of the database are normally used to transfer the data to consuming applications.  Data storage challenges include large file sizes and classification levels.The OneSAF program has identified performance challenges in parsing large XML documents (DaCosta, 2003b).  The program has developed DIF development guidelines to address this issue.  These guidelines include using short tag names, using tabs instead of spaces, limiting embedded comments, using explicit DIFs, and subdividing documents.Data Distribution Data consumers need simple mechanisms to access their required data.  Historic approaches have involved producers creating a magnetic tape or disc and shipping the data to the consumer.  Faster access is required to the data.  At one extreme, the data used by a consuming application might be updated prior to each execution of the simulation.Data provisioners are beginning to provide portals to service their consumers.  For example, both AMSAA and NGIC have web sites that provide access to portions of their databases.  AMSAA is developing a web portal on SIPRNET called The Army M&S Data Portal that will allow consuming applications to select and download data.  The Army M&S Data Portal is envisioned to be a one-stop shop for M&S data.  Its tools will allow study agencies to order and retrieve data from one source.  The portal dispatches agents to act on the customer’s behalf.  The agents interact with the data producers to gather the required information and provide it to the customer.  When the customer gets the data, they can access tools (e.g., XML Stylesheet Language Transformation (XSLT)) to transform the Extensible Markup Language (XML) data sets into a M&S acceptable format.  The portal is an open, standards-based tool that facilitates the study process from beginning to end.  The ultimate objective of the M&S Data Portal is to improve Army and Joint studies by reducing the turnaround time for requesting, generating and distributing the data.  Web technology could eventually be used to provide a single source to an enterprise’s data.Access to authoritative data should require minimal human intervention.  At one extreme, simulation applications could query whether all data was up to date and download any new data.  This could be accomplished using web services.  Data producers could provide web services that would automatically respond to consuming software requests for data.Standardization  ProcessThe most complex issue in defining DIFs involves garnering agreement from various producers and consumers of the data to jointly develop and adhere to the standard.  Interoperability ontology and other agreements must be developed in a collaborative environment with input from various interests and compromises on the approach.  The resulting DIFs and associated documents should be published for adoption by industry groups such as SISO.  SISO provides a forum for developing and publishing simulation standards, and these standards should include data interchange standards.SummaryThe development of simulation DIFs requires careful consideration of design issues and challenges.  These issues involve both the representation and management of simulation data.  Semantic interoperability issues can be addressed by using the explicit semantics encoded using the Ontology Web Language.  Semantic mapping challenges can be overcome by using standard dictionaries.  A meta-model approach with optional explicit tags can provide an extensible manageable formal definition for DIFs.  The Army’s Standard Nomenclature Database can serve as a starting point for common names for the simulation community.The DIA’s MIDB numbers provide a standard encoding for equipment.  This scheme can be leveraged by the simulation community.  Metadata can be used to support versioning and traceability.  The Dublin Core metadata tag set provides a source of reusable tags.  DIF size issues can be addressed by   various techniques including compression and short tag names.  DIFs should be made available through producer portals or web services that enable simulation applications to quickly and easily access the latest available information.  Sharing simulation data will continue to require standardization efforts that should be supported by SISO.ReferencesArmy Regulation 5-11, “Management of Army Models and Simulations”, July 10, 1997.Byrd, Richard, “A Brief Primer on Unique Enumeration Systems in Simulations”, internal memo, July 30, 2002.DaCosta, Boaventura, Lucas, Tim, Outar, Robin, Helton, Doug, “OneSAF Repository Framework: Defining, Storing and Interchanging XML data”, Proceedings of the Spring 2003 Simulation Interoperabilty Workshop, Orlando, FL. (2003a)DaCosta, Boaventura, Outar, Robin, “OneSAF: XML Performance in Simulation”, Proceedings of the Spring 2003 Simulation Interoperabilty Workshop, Orlando, FL. (2003b)Gravitz, Peggy, "Common Activities in Data Interchange Format (DIF) Development. Proceedings of the Spring '99 Simulation Interoperability Workshop, Paper 99S-SIW-177.  March, 1999.Hopkins, M., Haddix, F. , “Unit Order-of-Battle Data Access Tools”, Simulation Interoperability Workshop, 99F-SIW-151, Fall 1999.Lacy, Lee and Cynthia Tuttle, "Interchanging Simulation Data using XML", Proceedings of the 1998 Fall Simulation Interoperability Workshop, http://www.sisostds.org/siw/98fall/view-papers.htm, September, 1998.Lacy, Lee, Dugone, Theodore, Youngren, Robert W., “Standard Data Exchange Methods for Equipment Characteristics And Performance Data”, Proceedings of the Interservice/Industry Training, Simulation and Education Conference (I/ITSEC), November 26-29, 2001, Orlando, Florida.Randolph, Wayne, King, Dany, “Scenario Generation: XML To The Rescue”, Proceedings of the Spring 2002 Simulation Interoperabilty Workshop, Orlando, FL.Stytz, Martin, Banks, Sheila, “XML in Distributed Simulation”, Proceedings of the Fall 2001 Simulation Interoperabilty Workshop, Orlando, FL.UCF Institute for Simulation and Training, “Enumeration and Bit Encoded Values for Use with Protocols for Distributed Interactive Simulation Applications”, April 22, 2002, on-line:  HYPERLINK "http://www.sisostds.org/doclib/doclib.cfm?SISO_CID_41" http://www.sisostds.org/doclib/doclib.cfm?SISO_CID_41.Author BiographiesLEE LACY is the Director of Orlando Operations for Dynamics Research Corporation.  He has worked on major simulation programs including WARSIM 2000 and the Close Combat Tactical Trainer (CCTT).  He serves as the Vice Chairman of the Board of Directors of the National Center for Simulation. His research areas include XML and the Semantic Web.  He received an M.S. and B.S in Computer Science from the University of Central Florida.  He has received his certification as a Certified Modeling and Simulation Professional.MICHAEL PUHLMANN is a Senior Software Engineer.  His primary focus has been data and systems modeling.  He has worked on several major simulation systems including the Close Combat Tactical Trainer (CCTT) and the International Space Station (ISS) Training Facility.  He received a B.S. in Mechanical Engineering from Clarkson University and an M.S. in Computer Science from the University of Houston - Clear Lake.GABRIEL AVILES is an independent software consultant with nine years of experience designing and developing solutions for modeling and simulation applications.  His most recent work is focused on semantic web technology.  He received a B.S. in Electrical Engineering from Boston University.WAYNE RANDOLPH is a Staff Systems Analyst and Systems Administrator at DRC and served over twenty-two years in the U.S. Army. In addition to his logistics, aviation, and education background, his expertise lies in instructional systems development, training and computer integration and information technology. He holds a MS degree in Adult and Continuing Education and MS in Computer Information Systems.JIN KWON is an Operation Research Analyst at the U.S. Army Materiel Systems Analysis Activity.   His primary duties include being Project Manager and Administrator for Army Modeling and Simulation Data Portal and Army Performance Estimates Data System (APEDS) as well as serving as the Data Standards Category Coordinator for U.S. Army Modeling and Simulation Office.  He holds B.S. degrees in Applied Mathematics (Operations Research) and Industrial Management from Carnegie Mellon University and M.S. degree in Systems Management (Operations Research) from Florida Institute of Technology.   MAJOR MATTHEW CHESNEY is an Army Officer with over 15 years of commissioned service.  He graduated with a B.S. degree in Aerospace Engineering from The University of Colorado, Boulder in 1987 and was commissioned in the Army Corps of Engineers.  He completed an M.S. in Operations Research / Management Science from George Mason University in 1998.  He is currently assigned to the Army's TRADOC Analysis Center (TRAC) in Monterey, California.  His previous Army research assignment was at the Center for Army Analysis (CAA).