Forensic Analysis and Process Control in Distributed SimulationMonte L. Porter Jr.Michelle R. HermanHirome FujioComputer Sciences Corporation4815 Bradford DriveHuntsville AL 35805Tel: 256-837-7200, ext 236Fax: 256-837-4301Email: mporter@csc.comKeywords:Process Control, Forensic Analysis, Modeling and SimulationIntroduction.  Distributed Simulation Technology is being used throughout DOD to evaluate the military utility of advanced warfighting concepts.  Many recording processes capture evidence for each experiment.  In most Distributed Interactive Simulation (DIS) based experiments, a datalogger attempts to record every protocol data unit (PDU) that was passed on the experiment net.  This log provides an opportunity to conduct a “forensic analysis” of the experiment and to improve the process.  This paper describes the forensic process and gives lessons learned for the future of simulation and testing.Forensic Analysis Defined.  Webster’s New World Dictionary defines the word forensic as being “characteristic of or suitable for a law court or public debate.”  There are many views of what happened during a given DIS exercise.  A forensic analysis is a careful analysis of the evidence in the simulation log files to provide insights on what happened.  The PDUs recorded in the log file provide a basis for analysis because they contain predefined message sets and have specified validity rules.  The essence of a forensic analysis is the parsing of this data which can be checked for completeness, timeliness, and validity.Methodology.  The forensic methodology uses the PDU data file as its basis.  In order to establish the relationship between the various PDUs within the file, the investigators focus on the time the PDU was received at the logger measured in seconds past the beginning of the log file.  This resolves two problems.  The first is synchronizing the myriad simulations that participate in an exercise.  The difficulty of synchronizing computers over a DIS wide area network (WAN) produces an unacceptable level of variation in time representation.  The second problem is that the time stamp recorded by the creator of each PDU is expressed in seconds past the hour.  When an exercise lasts for several hours, the time stamp within the PDU is unusable in establishing a linear relationship between specific events.  By relating each PDU to the time received at the logger, the investigators are able to establish an audit trail and a point of reference for calculations.  In the extraction process, there are two options.  One can either open a port and listen to the DIS net and process the PDUs as they are received from a playback, or read each PDU in the log file without a playback.  The latter method is preferred since it allows faster than real-time processing of the data.  It allows for all PDUs that were recorded to be processed.  During the development process, the investigators found instances of missing PDUs. This was due to the overloading of the logger or network contention issues.  There were Fire PDUs recorded with no corresponding Detonate PDUs and there were Detonate PDUs with no corresponding Fire PDUs.  By reading each PDU sequentially within the file, the processor was never overloaded and the investigators achieved a higher certainty that the PDU records were correctly extracted.The DIS Extractor (DISE) provided data about Entity State PDUs, Fire PDUs, and Detonate PDUs.  The entity state file tracked the specific bumper number of each vehicle for which an Entity State PDU was generated, regardless of entity number.  This provided a record of each incarnation of a given entity.  Multiple records with the same bumper number indicated an entity has been instantiated more than once (e.g., brought back to life, reloaded after a system crash…).  Additionally, information about the time of the last valid PDU for each entity is given.  With this data in hand, the investigators were able to link Fire PDUs with Detonate PDUs using the Event Identifier Record and create a record of each engagement.  The output from this process was then inspected manually for completeness and validity.  The completeness check made sure that all of the PDU fields were filled out and identified where there was missing data.  The validity check examined the engagement record to verify that the engagement made sense from a technical and practical perspective.  Errors in the extraction process were corrected and checked again.The output of the extractor system included the following files:  entity state,  detonate, artillery beaten zone, and unit movement.  These files were comma delimited, and each record was time stamped with the time expressed in seconds past the start of the log file.  Using this time stamp, the investigators were able to move through the log file in playback mode and verify events.Limitations and Assumptions.  One of the inherent limitations of a large scale DIS exercise is the use of  “broadcast” as the primary method of exchanging data. This results in a “best effort” data log which is not reliable to record every PDU.  Some simulations may get loaded down and not publish a specific PDU, packet collisions may occur at several places on the net, and a logger may miss a packet due to I/O limitations.  For these reasons, the heavy DIS traffic experienced in the Field Experiment (FE) had a negative impact on the completeness of the data.  There are instances throughout the data where, for a given Fire PDU, there is not a matching Detonate PDU.  In some instances (notably SADARM), the lack of matching Fire and Detonate PDUs is explainable.  But in many others, the lack of a match must be attributed to the PDU dropping into the “bit bucket”.A second limitation is the experimental simulation management PDU (which reports specific killers upon change of state) not being implemented in all simulations. Therefore, it could not be used reliably to indicate specific killer-victim lash-ups.  This caused a problem when multiple missiles from different platforms hit the same target.  To arrive at a killer for each killed target, the investigators were forced to infer from the data in the PDU record.  When an entity changed its state to “dead”, the investigators looked for a system that impacted the target within one second of the entity state change.  By comparing detonate records with entity state changes, the investigators were able to make the inference that shooter A killed victim B (but it was impossible to be sure).To conduct the analysis, the investigators had to assume that the data elements resident within the data log comprised a  “fair representation” of what happened.  The investigators assumed that participants in the battle created the PDUs that showed up. The data within the PDU is final arbiter of the events that took place.  If the tank simulated by Station A fired a SABOT round but published the enumeration for a Maverick missile, then the rest of the exercise believed and acted as though a Maverick missile was launched.  A second factor which assists in validating the ”fair representation” assumption is the activities of the battlemaster.  Typically, the battlemaster ensures a fair fight and that the process is in control (appropriate data is being used by each simulation, no one else is on the net, and no apparent anomalies are seen by the battlemaster).  However, the battlemaster can not “catch” everything. Analysis ActivitiesPDU completeness and validity:  In this effort, the investigators looked for matching Fire and Detonate PDUs, where appropriate.  Additionally, they looked to see if the PDUs were filled out properly.  In some instances, the DISE algorithm assumptions had to be changed to accurately report the data in the PDU.  Lastly, in this effort, the investigators looked for enumeration/bumper number errors.  The entire effort was an iterative process between the investigators and the DISE developers.  There were nine passes through this process.Horseblanket to entity list crosswalk:   Not all of the entities that were planned showed up for each scenario.  To manage the entity representation, a horseblanket (so named because the data was laid out on a single paper of the appropriate size) was developed that graphically depicted every entity, its organization, and, most importantly, its bumper number – that number which allowed the investigators to follow an entity uniquely through the entire run.  A crosswalk between what was planned (on the horseblanket) and the entities recorded in the Entity State PDUs gives a definitive list of where differences existed between the plan and execution.  Figure 1 shows an example of a horseblanket. Figure 1.  HorseblanketShot analysis:  In this effort, the investigators determined for each unit the total number of rounds fired, the number of hits, and the number of secondary hits based on the PDU record.  This is typical killer-victim scoreboard information but gave the investigators a point of reference for the engagements that happened.  Fire PDUs without Detonate PDUs and Detonates PDUs with no corresponding Fire PDUs were listed as engagements and their numbers added to the totals.  Secondary hits refer to those targets impacted by the same type of munition more than once.  This gave the investigators insight into the effectiveness of target distribution rules and engagement procedures.  Figure 2 shows an example of a shot analysis.Figure 2.  Shot AnalysisMultiple hits analysis:  In this analysis, the investigators examined the targets that were hit multiple times (two or more) by direct fire systems.  With typical probability of kill (Pk) values ranging between .4 and .65, it was not considered abnormal for a target to be hit two or three times.  When the number of multiple hits exceeded 3 in the data records, this signaled the possibility of a special cause.  One cause of this phenomenon was targets that were intentionally left invulnerable so that they could trade off and become live targets a little later in the scenario.  Other causes could include data voids in the PDU records or incomplete vulnerability tables.	Figure 3 shows an example of individual vehicles that were hit multiple times for various reasons.  Engagements against these targets were analyzed for cause.Figure 3.  Multiple Hit AnalysisMiss analysis:  In this effort, the investigators determined the number of rounds fired with a subsequent miss.  This analysis proved to be just as important as the hit analysis.  The typical killer victim scoreboard does not allow the tracking of misses in the aggregate by unit or bumper number.  The investigators were able to identify individual entities that missed in multiple engagements.  When multiple misses occurred for a specific weapon system or type of missile or type of munition, the investigators were able to examine the data within the simulations to determine cause.   Figure 4 shows an example of this analysis.  In instances where the number of misses appears to be outside of expected values, the engagements by systems using those missiles were examined for cause.Figure 4.  Miss Analysis	Visual analysis (missile fly-out):  One of the files created by the DISE was the movement file for each instance of a specified missile flown throughout the exercise.  This was especially useful when examining weapons with man-in the-loop control of the missile flight.  Examination of the missile flight paths identified trends, anomalies, and potential airspace management issues.  Figure 5 shows an example of flight path data.Figure 5.  Missile Fly-out AnalysisVisual inspection of playback:  Visual playback of the exercise was essential for investigating anomalous behavior.  Two workstations were set up for this purpose.  A data logger was used to play back PDUs collected during the simulation runs, and ModSAF was used to display a plan view of the battle based on these PDUs.   Using the log time, the investigators were able to fast forward in the log to a specified point and observe the playback.Number of shots by bumper number or by unit:  In this effort, the investigators looked at how many shots were fired by each blue system looking primarily for logistics and firing rate problems.  Examining rounds fired by individual system showed where reload rules were not followed and showed patterns in engagements by unit.  Figure 6 shows an example of this analysis.  Total onboard load should have been 5 missiles.Figure 6.  Number of shots by bumper number or by unitBeaten zone analysis:  This effort determined the number of missions fired by a given unit and rated the impact location of each round to determine if targets were in harm’s way at the point of  detonation.  Given a mission, the DISE provided a record of any entity that was within n meters of the detonation where n is a variable based on the size and type of munition.  Using this method, investigators were able to evaluate the number of missions fired by unit and the effectiveness of those fires.  This gives some interesting insights to the fire management process.  Figure 7 shows an example of a beaten zone based on mission analysis.  Figure 8 shows an example of a beaten zone based on shot group or barrage analysis.Figure 7.   Beaten Zone Analysis – Missions FiredFigure 8.   Beaten Zone Analysis – Shot GroupingsTime history analysis:  Another tool which was viable to this analysis was the time history file.  Using time in seconds on the x-axis and range from shooter on the y-axis, the investigators were able to examine detonation data by unit in the context of the battle.  By plotting the detonations and connecting them, the investigators were able to visualize the flow of the battle.  They identified time gaps between system and platoon reload, skirmishes within the battle, and the relationship between the fires of adjoining systems or units.  Figures 9,10, and 11 show time history files representative of those that were used in the analysis.  Figure 9 shows four individual systems within a single platoon whose intended tactic was to fire in volley at the same target array.  It is clear from the chart that while there were some instances where the systems fired together, as a whole, the unit did not follow the intended tactic.  Figure 9.  Time History – Target ArrayFigure 10 shows systems that fired as a platoon at the same targets.Figure 10.  Time History – Same TargetsFigure 11 depicts an example of a procedural rule not being followed.  The rule required a platoon to return to a central ammunition plant, spend 10 minutes reloading, and then return to its engagement area.  Due to a number of factors, this logistics requirement was not coded into the simulation. Figure 11.  Time History – Procedural Rule Observations and Findings.   In the forensic analysis, it was important to present just the essential facts that are identifiable in the PDU record.  Measures of Effectiveness (MOEs) like Loss Exchange Ratio (LER), Force Exchange Ratio (FER), and the killer-victim (KV) scoreboard were intentionally avoided.  The findings are grouped by weapon system and are presented as exhibits in annotated chart form.Lessons Learned.  Some of the key lessons learned as part of this forensic analysis are provided below:Unique Bumper Numbers:  The efficacy of unique alpha-numeric bumper numbers was very instrumental in conducting a thorough analysis.  While the battle was ongoing, the Battalion/Company/Platoon markings were easy to use because they were familiar to soldiers.  The bumper number communicated clearly the unit organization of each participant.  When the battle-master queried his display, he could immediately determine unit type and organization.In the post-process analysis, the bumper number allowed the investigators to trace a specific entity through a series of simulation crashes.  The unique bumper number enabled the correlation of the intended unit with the multiple site:host:entity numbers for the same system.  It also allowed a comparison between specific vehicles planned for the experiment and the numbers and types of vehicles that actually participated.  Last, but most importantly, the unique bumper numbers allowed the investigators to conduct analysis by unit to whatever level of granularity was necessary for a meaningful analysis.Re-supply:   During the exercise, the investigators discovered that supply procedures were handled differently by the various player personnel.  Player units used differing rule sets  from “reload at will”, to “go to FAARP to reload”, to “reload only when you show a logistic capability to do so” (cross-leveling ammo).  The PDU record showed that, in many cases, the weapon systems fired beyond the number of rounds there were viable for the exercise.  This reduced or negatively impacted the level playing field necessary to insure validity of the results.  To ensure a level playing field, there is a need to be able to monitor munition expenditures in real-time and to enforce logistics constraints.Anomaly Impact:  During the exercise, the battlemaster was able to identify indicators of abnormal behavior within the simulation.  However, he was unable to identify the scope and impact of each anomaly in real-time/near real-time.  Part of the problem is the cognitive limits of human ability.  In a large exercise, numerous entities appear on the screen.  Small, erratic moves or other anomalous behaviors are difficult to discern.  In some cases, the plan view display shows appropriate behavior, but the underlying data within the PDU could affect the battle.  In other instances, the data within the simulation itself might be erroneous or missing and cause inappropriate effects within the overall battle. A combination of real-time validity checks of each PDU and real-time expenditure monitoring could reduce this problem.Stable Process:  A key finding of this study was the overriding requirement to ensure that the simulation process is in control.  Some measures include:Are the PDUs being filled out correctly between all participants?Do fire and detonate PDUs match up?Are system time constraints (Javelin/TOW rate of fire) being violated?Are all entities planned for the scenario actually simulating?  are there extras?Intelligent Technology:  The myriad of tasks required to monitor a brigade-sized distributed battle simulation indicates a need to use intelligent technology or expert system to assist the exercise controllers in monitoring and controlling the simulations.  A real-time monitoring capability would evaluate each PDU and conduct the appropriate counting, tracking, and assessing functions, triggering alarms and updating displays to assist the battlemaster in maintaining a process that is in control and enabling a fair fight.Conclusion.  The forensic analysis process is effective for determining if the process is in control.  Continued use of this process and implementation of real-time monitoring capabilities will give the modeling and simulation community the means with which to effectively assess, monitor, and improve the process.  The issue of process control becomes more important as federations expand to connect more and more individual federates.  Forensic and real-time monitoring tools will be valuable for ensuring appropriate interactions between federates.As part of the presentation, a member of the team will demonstrate a real-time monitoring tool prototype based on Gensym Corporation’s G2 software.  G2 is a real-time intelligent system whose strengths include detection of abnormal behavior, monitoring, and process control.  CSC has developed a DIS bridge to enable real-time monitoring of DIS experiments.  A commercial off-the-shelf HLA bridge is also available.9.4%D/2-50 INF3/5250D532/5250D521/5250D51PL250D563/4250D432/4250D421/4250D41PL250D463/3250D332/3250D321/3250D31PL250D363/2250D232/2250D221/2250D21PL250D263/1250D132/1250D121/1250D11PL250D16MissesTotal Fired2.5%87.5%38.5%5.6%1.7%20.5%77.5%29.8%1.3%500450400350300250200150100500ZILBMP-2 T-72 2S19BMP-22S-12724211815129Missile 10Missile 9Missile 8Missile 7Missile 66Missile 5Missile 4Missile 3Missile 2105missile50CalD16D13D12D11D56Missile 14/D3/D2/D1/D201522313Bn 2Bn 11/D5/DD53D52D51D42D41D33D31D26D23D22D21D16D13D12D11 EMBED Excel.Sheet.8  Munition AEntity Impact (Platoon 2)Miss (Platoon 1)100009000800070006000500040003000200010006000500040003000200010000Missile-Fired Time (sec)