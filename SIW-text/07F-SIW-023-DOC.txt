Effects of long-haul network connectivity on the visual fidelity of real-time flight simulationMichael SlaterChristine M. CovasUnited States Air Force Research Laboratory6030 S. Kent StreetMesa, AZ 85212480-988-6561 ext. 241, 547 HYPERLINK "mailto:Michael.Slater@mesa.afmc.af.mil" Michael.Slater@mesa.afmc.af.mil,  HYPERLINK "mailto:Christine.Covas@mesa.afmc.af.mil" Christine.Covas@mesa.afmc.af.milKeywords: Network Transport Delay, Latency, Distributed Mission Operations, Visual AnomaliesABSTRACT:  This paper presents the results of two experiments designed to assess visual discrepancies produced by network delay in a real-time distributed simulation environment.  Due to the imperfect nature of networks over which communications for these simulations take place, visual anomalies occur at varying levels.  We assessed the subjective visual fidelity of a fast-flying simulation over a long-haul network with varying data loads.  Network load was manipulated across two different flight paths as a pilot identified and rated the visual discrepancies.  Results of these experiments show a direct relationship between the network data load and the presence of visual discrepancies.  We ran a second experiment to determine actual position error of an aircraft model based on differing levels of network delay, network jitter, model smoothing, and predict-ahead techniques.  Through these experiments we have determined that a small amount of network delay results in large position discrepancies and that although a visual model may be perceived to fly a smooth path its actual position error may be large.1. IntroductionMany distributed simulation exercises in the US Air Force are conducted with simulated aircraft beyond visual range.  However, there are requirements to train tasks within visual range such as visual identification of targets, visual formation flight, aerial refueling, air combat maneuvers, and basic flight maneuvers [1] [2].  In Air Force distributed simulation, entities typically travel at high velocities, thus delays in data communication have a greater effect on visual accuracy and scene realism than if entities were slow-moving vehicles.  Data delay results in entity position errors as well as visual anomalies such as unrealistic position and orientation changes.Geographic separation of training devices affords many benefits, such as lower cost and ability to conduct training that would normally be very complicated to coordinate.  However, this separation introduces a number of challenges due to the delay of data on the communication networks especially when a large number of entities are involved. The Applied Visual Research Team at Mesa Research Site (MRS) has conducted a set of experiments to determine and categorize the visual artifacts caused by network transport delay in a distributed simulation environment.  We hypothesized that some visual inaccuracies would occur, though we were unsure of the severity or quantity.  Here we describe the results of an experiment where the rate of occurrence and severity of these visual anomalies was identified.  In addition we describe results of a second experiment where we used a network simulation to quantify the position errors introduced by data delay in the network.1.1 BackgroundTo enable cost efficient and realistic team training, the US Department of Defense (DoD) has been interconnecting simulators since the initiation of SIMNET (SIMulator NETworking) in 1983 [3].  In the mid 90’s the US Air Force established the Distributed Mission Training (DMT) and later the Distributed Mission Operations (DMO) programs to take advantage of the team training environment afforded through simulator interconnection.  A large amount of distributed simulation research has focused on protocols and architectures such as Distributed Interactive Simulation (DIS) and High Level Architecture (HLA) [4] [5].  Maintaining and updating standards for these protocols has been an enormous undertaking and has resulted in a formal set of standards widely used by today’s simulation and training community [6] [7].Another major area of research has focused on exploring different algorithms and techniques to compensate for data delay in distributed simulation [8] [9].  Within the realm of transport delay research, the primary focus has been on the effect of latency on manual control in stand-alone simulators [10] [11].  Some research on network transport delay has been completed, but the focus has been primarily in gaming environments using the internet [12] [13].  The majority of the visual research completed in this area has focused more on collaborative learning environments rather than real-time simulation applications [14].  Here we attempt to bridge the gaps in the research of visual discrepancy to include and explore long-haul, distributed, real-time, flight simulation.An understanding of current DMO architectures is necessary to comprehend the challenges we are up against today.  To facilitate communication between simulators, the DIS standard dictates the use of Protocol Data Units (PDUs).  There are 27 different types of PDUs which contain information such as unique identifiers, position, appearance, and weapons and radar effects.  When change occurs in the simulated environment, i.e. a weapon is fired or an entity leaves a predictable motion path, the entity that caused the reaction or first experienced it transmits a message that informs others of the event and describes what has occurred.  The other simulation participants decide whether an event is applicable to them and either incorporate or discard PDUs accordingly.  Data networks take some amount of time to transmit these PDUs and do not have infinite data throughput.  Hence, there is always some degree of position and orientation error between the connected simulators.  To compensate for the lack of infinite network throughput, predict-ahead techniques are used to predict model position between updates.  When an error threshold is met between the actual and predicted position, a new update is transmitted and the entity is moved to the new position.  To eliminate an obvious jump in position the entity is typically moved to the new position over a predetermined amount of time.  This technique is referred to as “smoothing” the entity.   For an overview of the issues related to data delay in a DIS environment see deVries and Padmos [15].2. EXPERIMENT 1 2.1 Methods2.1.1 DesignExperiment 1 was designed to determine the categories and severity of visual discrepancies as a function of flight axis and network load.  Networks and especially the internet are notoriously hard to simulate [16].  Thus we decided not to simulate the network but to use a network currently being used for long-haul communication in distributed simulation.  MRS has a distributed simulation network in place between our site in Mesa, Arizona and the Defence Science and Technology Organization (DSTO) in Melbourne, Australia.  The network connection goes from MRS on the US Defense Research Engineering Network (DREN) to a peering point with the Australian Academic Research Network (AARNet) then on a dedicated T1 to the DSTO site.Our experimental design included two flight scenarios and three different network load conditions (0, 250 Kbps, and 450 Kbps).  We expected that the severity and quantity of visual discrepancies would increase with greater network load.  We also expected that the more complicated of the two flight paths would have more complex visual discrepancies due to the high velocities and multiple direction changes of the aircraft during this maneuver.  2.1.2 Stimuli and ApparatusWe used the MRS research Computer Generated Forces (CGF) program eXpert Common Immersive Theater Environment (XCITE) to create an F-16 entity that would follow DIS standards while flying two different predetermined paths.  We then used a DIS record and playback system to playback the flight from the DSTO site in Melbourne to MRS.  Within XCITE we used dead reckoning algorithm number 4 with thresholds of 3 degrees for orientation and 1 meter for position [4].  We used two flight paths with varying levels of motion.  The first flight path, which we will refer to as jink, consisted of the aircraft starting at a velocity of 400 knots and then performing a series of random high and low G turns and changes in speed consistent with a jink defensive maneuver.  The other flight path consisted of the aircraft also starting with a velocity of 400 knots and performing a bombing run maneuver in which only altitude and velocity changed; no turns were incorporated.  We refer to this scenario as the bomb run scenario.  Each flight scenario lasted approximately 4 minutes.To determine the effects of varying network loads on the visual scene, we generated background DIS traffic to be replayed along with the entity of interest.  We measured the throughput of the network as 450 Kbps using Ixia’s QCheck network performance measurement tool.  We used three levels of congestion traffic: no traffic, low traffic (250 Kbps), and high traffic (450 Kbps).  Round trip latency was also measured using QCheck, and was found to be an average of 227 ms.  We did not thoroughly test the data jitter or the change in latency due to network traffic.Figure 1. The Experimental Deployable Tactical Trainers (XDTT) Simulators used in experiment 1We used the MRS Experimental Deployable Tactical Trainers (XDTT) in Mesa as depicted in Figure 1 to fly with the entity played back from Melbourne.  The XDTT has a high-fidelity stick and throttle with LCD touch-screen simulated instruments.  The out-the-window scene was generated using the SDS International AAcuity® PC-IG and was displayed on three 30-inch Apple Cinema LCD displays.  The model entity smoothing value was set to the value typically used at MRS, that of 1 second or 60 steps.A former pilot with 3000+ F-16 flight hours and years of simulator experience flew along with the entity generated from Melbourne and rated the visual discrepancies of the aircraft.  Visual discrepancy is defined here as any unrealistic visual behavior of an entity model that would be impossible for a real aircraft to perform.  Most visual discrepancies are seen when a model instantaneously jumps to an updated position or orientation.  The pilot was asked to first identify the discrepancy and then rate the impact of the discrepancy on task performance using a Likert scale of 1-5, with 1 being little impact and 5 being extreme impact.  Both the audio responses from the pilot as well as the display of the visual scene were recorded onto videotape for analysis purposes.  A log of all network traffic was also recorded.2.2 ResultsThe audio and video recording was analyzed by viewing the videotape and noting the time, type of discrepancy, pilot comments and rating of the visual discrepancy.  From this data, the different types of visual discrepancies were broken out into general categories as shown in Table 1.  Category data is collapsed across both flight scenarios and all three network loads.The forward jump discrepancies were classified as any jump either forward or back or both forward and back by the model.  To obtain measurements of the forward jump discrepancies, we used radar information (the pipper) from the heads-up-display in the simulator captured on the videotape to measure the distance from eyepoint and the distance jumped.  Radar was not always available due to radar tracking losses.  These losses were either due to the presence of network delay or the viewpoint of the visual scene as captured by the video camera.  Table 1. Visual Discrepancy CategoriesNameDefinitionNumber of OccurrencesForward JumpJump forward or back or both forward and back58Roll JitterSmall jitter in roll, wing rock or multiple perturbations9DisappearAircraft disappears from view completely3Vertical JumpJump in vertical axis or incorrect change in pitch10Multi-Axis JumpJump multiple directions of motion (i.e. forward jump in ascent, forward jump and slide back, forward jump and roll jitter)8Diagonal JumpJump on a diagonal3Left/Right JumpJump in wing axis of motion 1Direction Change  (no roll)Unrealistic change in direction that would require the aircraft to roll in the real world2The measured distance from the pilot’s eyepoint to the F-16 model used to assess forward jump discrepancies was between 300 and 7000 feet. The distance jumped varied between 3 and 100 feet.  A large number of the cases of the distance jumped measurements were between 25-50 ft (22 cases).  The majority of ratings were 1 (40 cases), with a distance range between 1000 and 2000 ft and an average jump range between 25-50 ft.  For the 2 rating, the average distance range was 1800 feet and the average jump was 30 ft (11 cases).  For the 3 rating, the average distance was 2700 ft and the average jump was 48 ft.  The highest ratings of 4 and 5 (total of 3) had an average distance of 5000 ft and an average jump of approximately 100 ft.  Pilot rating generally increased as a function of the average jump distance.Roll jitter was classified as any small visual variation in smoothness of roll, any unrealistic wing rock, or multiple quick jerks in the roll dimension of the model.  For the roll jitter discrepancies the same measurement procedure was used for the distance from eyepoint measurements.  Roll jitter was only present during the Jink scenario as the bomb-run scenario did not contain any aircraft roll.  Overall, there were nine occurrences of roll jitter, five of which were rated as a 3 and six of the total roll jitters appeared under high network load.  Three model disappearances occurred during the experiment.  All disappearances were rated as five and all happened under the high network load condition.  The time the model was actually gone from the visual scene ranged from 3 to 5 seconds.  This was reported by our pilot observer to be the one discrepancy that could cause a “definite negative transfer of training” in a simulator training environment. The remaining documented discrepancies were not as disruptive as the final discrepancy shown in Table 1, the direction change without roll.  Direction change without roll was classified as an unrealistic change in direction that would require the aircraft to roll in the real world.  One comment made by our pilot observer regarding one of the direction changes without roll was “coming out of that (roll) without resetting the wings would be physically impossible”.  This statement provides a good example of how unrealistic the majority of these discrepancies would be to a long-haul training exercise.Figure 2 shows visual discrepancy ratings as a function of both flight scenarios and all three network load conditions.  The y axis in this figure depicts the ratings and the x axis depicts the percentile of ratings.  With no other traffic on the network (i.e. the 0 network load condition) there were a total of 12 visual discrepancies noted in the Jink scenario and 17 in the Bomb-run scenario.  While the bomb-run scenario had more discrepancies, most were rated low.  This is due to the different packet update rates of the scenarios.  Because the Jink scenario was turning more sporadically, it was breaking threshold more often and sending more position updates than the bomb-run scenario.  When a high level of network congestion (450 Kbps) was added, the number of discrepancies rated as 1 increased by 40 percent in the bomb-run scenario while the number of higher discrepancy ratings remained low.  When high network congestion was added to the Jink scenario, the number of discrepancies rated as 1 actually decreased, but the number of 3s and 4s increased substantially.Figure 2.  Visual discrepancy rating as a function of flight scenario for all network load conditions; y axis shows ratings and x axis shows percentile of rankingsFigure 2 also demonstrates that the amount of visual discrepancies increase when the network reaches its capacity.  However, when the network capacity is still within limits the visual scene doesn’t degrade.  We are not sure why the low level of network congestion has fewer discrepancies than the no network load condition. However, the presence of this effect may be attributable to statistical error as only one pilot rated the stimuli for this experiment.2.3 DiscussionOverall, the results of this experiment show that, even on a dedicated network without any other traffic, the number of visual discrepancies in the scene was quite large.  For a four minute segment of time this represents one visual discrepancy every 14 seconds.  While the majority of these visual jitters were small, there are training scenarios where even small discrepancies in the visual scene may be unacceptable, such as in aerial refueling or close formation flight.Adding congestion to the network while transmitting a flight path that does not change rapidly increases the number of small visual discrepancies, but does not necessarily make the discrepancies worse.  Conversely, adding network congestion to a flight path that rapidly changes direction and speed significantly increases the amount and levels of visual discrepancy.Based on the number of visual discrepancies found in this experiment, we wondered why this hasn’t been a bigger issue in the simulation research literature.  One reason may be that it is just thought to be impossible to train certain tasks in a widely distributed simulation network.  If this were the case, only tasks that did not involve entities within visual range would be trained.  Another reason may be that in many cases work-a-rounds have been used.  As we went through the software necessary for the experiment we found that in both the CGF software and the XDTT simulator software the dead reckoning thresholds were set lower than the DIS recommendations.  We believe that resourceful simulation engineers have reduced dead reckoning thresholds and increased smoothing periods to make the visual scene look better at the expense of increasing network traffic and relative position error. 3. Experiment 2After analyzing the results from the first experiment we wanted to conduct an additional evaluation to gain an understanding of the actual errors in position and velocity introduced by network transport delay.  Thus, we designed an experiment to quantify the amount of position error that occurs due to latency in the network.3.1 Methods3.1.1 Stimuli and ApparatusTo understand the actual position errors caused by network delay we again used the CGF program XCITE to produce an F-16 entity to fly a predetermined flight path.  The flight path consisted of 60 seconds of a variety of low and high G turns and changes in altitude and velocity.  We then used a free Linux-based network emulator, Netem, to emulate a real network by delaying the PDUs in a realistic fashion to another XCITE station as shown in Figure 5.  The dead reckoning thresholds were identical to those used in the first experiment.  The source code on both XCITE stations was modified to store position, velocity, acceleration, and roll and pitch angles at 60Hz.  Both XCITE stations were also modified to wait for a “start test” command from another computer.  This enabled a direct comparison of the position of the entity when it was generated with the position as calculated by the receiving XCITE station.  The setup facilitated the use of a realistic DIS architecture while maintaining control over network parameters.  With both XCITE stations initializing at the same time, we also had a common time reference.  However, the XCITE frames were not synchronized, so we had a maximum error of one frame or 16 milliseconds (60 Hz).  With this setup we were able to compare all the important position parameters of an entity as seen from two simulators during any point in time.Figure 5.  Depiction of experimental computer connectionsThe latency values used within the network emulator were based on measurements of five different distributed simulation networks.  We decided to emulate network parameters of a US-only type simulation with values of latency of 50 ms with a standard deviation of 5 ms on a normal curve.  We also emulated an intercontinental-type network scenario with a latency value of 150 ms with a standard deviation of 15 ms.  No packet loss was introduced to either network scenario by the network emulator.3.2 ResultsFigure 6 depicts box and whisker plots showing the distribution of position error.  With latency in the network emulator set to zero, median position error was found to be 2.65 feet with a maximum of 12.6 feet and a minimum of 0.  With the emulator set to 50 ms, median position error was found to be 30.5 feet with a maximum of 39.2 ft.  When the emulator was set to 150 ms, median error was 88.9 ft with a maximum of 132 ft.Figure 6.  Box and whisker plots depicting the distribution of position error.  3.3 DiscussionA majority of the position errors can be explained simply by the time it takes for a position update to reach the receiving simulation.  For example, if the aircraft was flying at 400 knots and the transport delay was 150 ms, the minimum error would be the distance it traveled in that time or 101 ft.  The presence of variability in position error is the result of network jitter and lower update rates.The use of smoothing techniques can also increase position error.  Smoothing an entity in a visual scene makes it appear more realistic; however, it increases the amount of position error which could have undesired effects on weapons and sensors.  Figure 7 shows the position error in feet for the zero latency condition with both smoothing and no smoothing.  The smoothed flight had an average error of 2.65 ft, while the non-smoothed flight had an average error of 1 foot.  The few spikes are due to the frames of the two CGF stations not being synchronized.While smoothing is typically used in distributed simulation, there is some debate as to whether there are some situations where its use is undesirable.  For example, during within visual range tasks it is difficult for a pilot to determine if another entity is being moved to a new location by the remote controller or if it is being smoothed to the new location.  If the model were not being smoothed, the jumps may just be accounted as artifacts of the simulation and ignored.Figure 7.  Position error with 0 ms latencyFigure 8.  Position error with 50 ms latencyFigure 8 shows the actual position error in feet for the 50 ms latency condition for both the smoothed and non-smoothed conditions.  The red dashed line in the figure shows the position error if PDUs were issued at the frame rate of the simulation (60 Hz).  This line represents only the error caused by network delay and eliminates the error caused by dead reckoning and low update rates.  The other lines show the actual position errors with PDUs issued at DIS standard rates both with and without smoothing.  The error was calculated as the square root of the sum of the squares of error in the X, Y and Z axis.  Because we used the straight line distance and not the accumulative error in X, Y, Z, the error actually increased when an update was received.  This happens during a turn where the aircraft naturally get closer together.  It can be seen from Figure 8 that even small amounts of latency cause significant position errors.  With 50 ms delay, the greatest error is 40 feet.  Small amounts of delay variation (jitter) can cause large amounts of position error.  Figure 9 shows a case where every PDU was delayed by a constant 150 ms.  Ignoring the error caused purely by delay, the average error with a 150 ms delay and no network jitter was 1.25 ft.  Figure 10 shows the same 150 ms delay condition with a standard deviation of 15 ms to emulate network jitter.  The average error for this condition was 6.44 ft.  The maximum error with no delay variation was 3.8 ft from the baseline, while the maximum error with a standard deviation of 15 ms was 26.8 ft.Figure 9.  Position Error with 150ms latency and no network jitterFigure 10.  Position error with 150 ms latency and 15 ms of network jitterWhile we did not directly test the visual discrepancies caused by network jitter, our data suggests that it is the variation in delay and not delay itself which causes the greatest amount of unrealistic visual behavior of an entity in the scene.4. Conclusions and ImplicationsThe initial reasoning behind this experiment was to scope the problem of visual anomalies caused by long-haul networks in distributed simulation.  We hypothesized that some amount of visual discrepancy would occur, but we were uncertain of the amount.  To ensure the applicability of our results, we established our experiment on a network currently being used at MRS for distributed simulation.  We were careful to apply DIS standards to all software in the experiment.  We did not attempt to understand the effects these visual discrepancies would have on training, but rather to see if they existed and to what magnitude.We found that a substantial number of visual anomalies manifested as position and orientation jumping of an entity model, even on a network with no other traffic.  The number and severity of the visual discrepancies did not increase until the network reached a level of saturation of its available throughput.  Also, more erratic flight paths tended to have increased visual anomalies with a saturated network.Simulation Engineers are adept at creating a realistic visual scene.  However, they may be inadvertently producing high levels of traffic on the network by decreasing dead reckoning thresholds.  Also, they may be increasing position error through the use of long smoothing periods.Overall, we found that position error increases linearly with the speed of the model and the latency in the network.  Our results also show that it is not the periodic delay but the variation of delay or network jitter that is the likely culprit in introducing visual anomalies of the model in the scene.  While smoothing was found to increase the position error in the simulation, its use is generally desirable as it increases visual motion quality.Through this research we have established an understanding of the visual discrepancies associated with increases in network load.  The effects of network latency on weapons and Electronic Warfare (EW) still need to be explored.  Areas of future research include effects of network latency on accurate weapon detonations, fair fight, Radar and EW.This area of transport delay research also has implications in the Live, Virtual, and Constructive (LVC) arena.  Because live participants typically have much less bandwidth in their communication networks, an understanding of these implications will become increasingly important as they are brought into distributed training.5. AcknowledgementsWe wish to thank Lt Clinton Kam for programming support, Craig Eidman for subject matter expertise, Mitch Zamba for network connectivity, Samuel Hasenbosch for engineering support at DSTO and Lt Megan Shamp for assistance with data analysis.  This work was supported by U.S. Air Force contract FA8650-05-D-6502.  6. References[1]	Air Force Instruction 11-2F-15 Volume 1, 18 January 2007, F-15 Aircrew Training[2]	Air Force Instruction 11-2F-16 Volume 1, 19 January 2007, F-16 Pilot Training[3]	Miller, Duncan C. and Thorpe, Jack A. SIMNET: The Advent of Simulator Networking, Proceedings of the IEEE, Vol. 83, No. 8, August 1995[4]	DIS Steering Committee, The DIS Vision, A Map to the Future of Distributed Simulation, Institute For Simulation and Training, IST-SP-94-01, May 1994[5]	Dahmann, Judith S. High Level Architecture for Simulation, Distributed Interactive Simulation and Real Time Applications, 1997, First International Workshop on, January 1997[6]	Distributed Interactive Simulation Committee of the IEEE Computer Society, IEEE Standard for distributed Interactive Simulation, IEEE Std 1278 series[7]	Simulation Interoperability Standards Committee of the IEEE Computer Society, IEEE Standard for Modeling and Simulation High Level Architecture, IEEE Std 1516 series[8]	Cai, W., F. B. S. Lee, and L. Chen. An Auto-adaptive Dead Reckoning Algorithm for Distributed Interactive Simulation.  Proceedings of the 13th Workshop on Parallel and Distributed Simulation, ACM Press, 1999[9]	Durbach, C. and Fournean, J.M. Performance Evaluation of a Dead reckoning Mechanism.  Proceedings of the Second International Workshop on Distributed Interactive Simulation and Real-Time Applications, pages 23-32, Montreal, Canada, July 1998[10]	Gawron, Valerie J. Bailey, Randall E. Knotts, Louis H. Comparison of Time Delay During In-Flight and Ground Simulation, Proceedings of the Human Factors Society 33rd Annual Meeting, 1989[11]	Middendorf, Matthew S. Fiorita, Annette L. McMillan, Grant R. The Effects of Simulator Transport Delay on Performance, Workload, and Control Activity during Low-Level Flight, AIAA Flight Simulation Technologies Conference, 1991[12]	Beigbeder, Tom Coughlan, Rory Lusher Corey, Plunkett, John Agu, Emmanuel Claypool, Mark The Effects of Loss and Latency on User Performance in Unreal Tournament 2003 SIGCOMM Workshops, Sept, 2004[13]	Kick, Matthias Wellnitz, Oliver Wolf, Lars Analysis of Factors Affecting Players’ Performance and Perception in Multiplayer Games, NetGames ’05, October 10-11, 2005[14]	Park, Kyoung Shin, Kenyon, Robert V. Effects of Network Characteristics on Human Performance in a Collaborative Virtual Environment, IEEE Proceedings of Virtual Reality, 1999[15]	DeVries, S.C. and Padmos, P. (1997). Perceptual-motor aspects of Distributed Interactive Simulation (DIS). TNO Human Factors Research Institute Report TM-97-A028[16]	Paxson, Vern, Floyd, Sally, Why we don’t know How to Simulate the Internet, Proceedings of the 1997 Winter Simulation Conference, 1997Author BiographiesMICHAEL SLATERMr. Slater is a Computer Systems Engineer with the Air Force Research Laboratory, Warfighter Readiness Research Division in Mesa, Arizona.  He received an M.S. degree in Computer Engineering from North Carolina State University and a B.S. from Utah State University.CHRISTINE COVAS Ms. Covas is an Associate Research Psychologist in the Visual Systems Research Laboratory at the Air Force Research Laboratory in Mesa, Arizona.  Her research focuses on visual perception, particularly as it relates to simulation and training.  She is pursuing a Ph.D. in Psychology from Arizona State University and holds a M.S. (2005) and B.S. (2003) in Applied Psychology from Arizona State University.