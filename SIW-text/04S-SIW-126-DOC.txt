Building a Theater-Level Ground Combat Referent from SME KnowledgeS.Y. HarmonZetetixP.O. Box 705, Myrtle Creek, OR  97457(541) 863-4639 HYPERLINK "mailto:harmon@zetetix.com" harmon@zetetix.comM.L. MetzInnovative Management Concepts, Inc.21400 Ridgetop Circle, Suite 210, Dulles, VA  20166(703) 318-8044 HYPERLINK "mailto:mmetz@imcva.com" mmetz@imcva.comS.M. YoungbloodDefense Modeling and Simulation Office1901 N. Beauregard Street, Alexandria, VA  22311(703) 824-3436 HYPERLINK "mailto:syoungblood@dmso.mil" syoungblood@dmso.milKeywords:referent, subject matter experts, validationABSTRACT:  This paper reports the progress of an effort to develop techniques for building objective simulation referents from subject matter expert (SME) knowledge.  Our approach to this problem consists of sampling SME opinions on particular warfare situations using survey techniques then amalgamating these opinions into a single referent using statistical techniques.  We have conducted an experiment that exercises this approach around a simple Blue vs. Red armor combat scenario.  In this experiment, we followed a conventional survey regimen for constructing a questionnaire to sample SME opinion on the conditions and outcomes of this scenario.  This regimen involved drafting an initial questionnaire then vetting it in a focus group of SMEs.  The information we collected from this focus group enabled constructing a detailed questionnaire that we then exposed to a set of SMEs in structured interviews.  These interviews improved the quality of our questionnaire but also increased the number of questions to 162 for a single scenario involving direct fire only with two variants (direct fire + organic mortars, direct fire + organic mortars + field artillery),.  We then distributed the resulting questionnaire to a sample SME population, received responses from seven individuals, and characterized the means, standard deviations, skews and kurtoses of several key outcomes of the battle.  The backgrounds of our respondents were broad and relevant to the ground combat scenario.  The results from this process varied widely.  For example, the SMEs estimated that the direct fire battle would take from less than 30 min. to 1000 min with a mean of 420 ± 411 min.  Blue losses varied within 0 to 60% of their forces with a mean of 17 ± 24% and Red losses varied within 25 to 100% of their forces with a mean of 59 ± 26%.  We have learned several significant lessons from this experiment.  Most importantly, we learned that we must query the SMEs at a higher level of abstraction to reduce the questionnaire burden, to improve the precision of the results, and to generate a more broadly applicable referent (i.e., not limited to a single narrow scenario).  We learned, despite the encouragement from our focus group, that gathering SME opinions through remotely administered questionnaires is very difficult.  We distributed our ground combat questionnaire to several organizations and received only a very small response and only received that because of our extensive and aggressive follow-up.  We also learned the value of following the wisdom from the survey research community for constructing questions and questionnaires.  In summary, the technique that we proposed worked well enough to encourage us to run another experiment in a different domain with questionnaires that solicit SME opinions at a different level of abstraction.1.  IntroductionCurrently, users and developers of theater-level simulations must employ subject matter experts (SMEs) for validation.  The impossibility of collecting a rich set of experimental data describing actual warfare at a theater level makes the use of SME opinions for validation a necessity for the foreseeable future.  Obviously, the actual warfare with future doctrine and future forces cannot be conducted and measured on the battlefield.  Other results validation methods, including use of test and evaluation data and comparison to other model and simulation results also do not provide a sufficient referent for comparison.  Yet, the importance of theater-level simulations and the decisions made from their results increases the emphasis upon the validity of those results.  In order for new warfare simulations under development to help senior Department of Defense leaders make multi-trillion dollar decisions, they must be validated to the maximum extent possible.  New techniques must be developed for handling SME contributions that improve their independence, consistency, repeatability and, ultimately, credibility.This paper describes one of the new techniques to help make using SME opinions in validation more independent, consistent, and repeatable.  It describes the technique and the results of its application to an existing theater-level warfare simulation (i.e., the Joint Warfare System (JWARS)).  It also describes the lessons learned from this experiment and proposes changes to the technique resulting from these lessons that can be applied in future experiments and eventually integrated with VV&A practice.  The result of this effort will be a technique to improve the ability of V&V practitioners to objectively assess the validity, and thus the credibility, of complex theater-level warfare simulations.2.  ExperimentIn the experiment exploring SME-derived referents, we Developed an initial approach to the problem of constructing SME referents, Chose an actual simulation upon which to exercise this approach, Executed the approach, then Analyzed the results from this process.  Two previous papers describe our initial approach and the simulation that we chose for the experiment (i.e., JWARS) [1, 2, 3].  This paper considers only the execution of the approach on the JWARS simulation and our analysis of those results.2.1  Experiment designOur experiment explored a specific approach to constructing SME-based referents (i.e., essentially the experiment’s hypothesis).  This approach leverages well understood survey techniques for collecting SME knowledge independent of a specific simulation and statistical techniques for characterizing the uncertainty associated with that knowledge.  We executed the following steps in this experiment: Developed the questionnaire scenarios;Identified the possible respondents from which we drew SME knowledge;Designed a questionnaire for eliciting that knowledge; andCollected and analyzed the responses to that questionnaire.Our previous papers detail the scenarios that we developed for this experiment and our selection of the respondents from which to extract SME knowledge [1, 2].  Briefly, we chose a simple meeting engagement of two division-sized armored forces on featureless terrain under good weather conditions during daylight as our basic scenario.  We explored three variants of this scenario:Direct fire only – the forces engaged with only their direct fire weaponsDirect fire and mortars – the forces could use organic mortars to supplement their direct fireDirect fire, mortars and artillery – the forces could employ field artillery as well as mortars and direct fire.The final report for this experiment contains the details of the basic scenario and its variants [4].The JWARS Program Office suggested several SMEs as participants in our survey.  All of our respondents came from these suggestions even though we attempted to collect from a broader sample (see the lessons learned below).  Our final report also details the selection and collection processes we employed [4].The survey research community has evolved questionnaire design into a well understood art [5, 6].  We carefully followed the established procedure to develop the questionnaire used in this experiment in the following steps in order to reduce the effects of various known error sources [6]:Devise an initial questionnaire,Expose the questionnaire to a focus group and modify it from their comments and recommendations.Conduct structured interviews using the revised questionnaire and modify it from the results of those interviews.Distribute the final questionnaire to the population sample.Our initial questionnaire considered a realistic scenario focusing upon ground combat and drew from well-understood interviewing techniques [5, 6].  The questionnaire described the scenario and asked a number of questions about this scenario to elicit three types of information:•	The respondent’s understanding of the scenario situation and the factors that affect its outcomes,•	The respondent’s predictions of the likely outcomes of the scenario situations, and•	The area and level of expertise of the respondent.A few questions sought the same information from different perspectives to provide the initial means by which to test the consistency of the SME predictions.  The questionnaire also included questions that encouraged the SME to characterize their own areas and levels of expertise. We directed considerable attention on questions that led to predictions with uncertainties described by normal distributions wherever possible. We then exposed this initial questionnaire to a focus group consisting of six SMEs associated with the JWARS Program.  This focus group meeting guided the design and evaluation of the survey questions [5, 6].The discussion in this group considered three areas:Assessing the respondent’s background;Designing the questionnaire scenario; andFormulating specific questions that probe the respondent’s knowledge about the scenario situation.We collected the comments and recommendations from this group and massively modified the initial questionnaire from this information.  For example, using a very simple basic scenario then systematically adding weapon assets to the battle, as reflected in the scenario variants, grew from a suggestion originating in the focus group.  This approach  builds upon the baseline scenario (i.e., the direct fire battle) and was supposed to better delineate how the different assets play into the battle.  Originally, we anticipated systematically adding additional assets beyond field artillery,  The SMEs participating in the focus group suggested combat air support, unmanned aerial vehicles, logistics and non-organic sensors in addition to exploring the influences of soft factors (e.g., morale, troop condition).  But, the questionnaire had already grown to over 150 questions and we felt that the respondent burden had already become too high.  The diminished responses to the later questions in our final survey seems to support this persuasion.  As a result, we only represented the three battle situations given above.With the revised questionnaire, we conducted three structured interviews.  These interviews provided us with the opportunity to understand a respondent’s reasoning processes when answering the survey questions [5-7].  This insight helped us further tweak the questions to improve their coverage and validity.  Aside from correcting a few minor errors and improving clarity with further structuring and a few additional questions, the actual content of the questionnaire changed little after the structured interviews.  It did, however, grow to 162 questions.  We distributed the resulting questionnaire to a set of prospective respondents.  Our final report contains these three questionnaires, summaries of the focus group and structured interview results, and the raw responses to our final questionnaire [4].We analyzed the responses as described in our first paper [2].  We received so few responses to our distribution that we needed to combine the responses we received from the structured interviews with those from the final questionnaire distribution to obtain a statistically stronger sample.  We do not expect this to be necessary in any employ of the actual technique.  The results of our analysis of the raw SME predictions defined a prototype referent for the JWARS simulation.  As our previous publications describe, this referent describes the mean predictions and the nature of the uncertainties associated with each of these prediction sets.  In addition, it identifies areas where SME predictions did not agree and so highlights the areas where the referent is most uncertain.  Such uncertainties may reflect the need to collect further information through additional surveys.  The following section summarizes our experimental results.2.2  Experiment resultsIn the course of this experiment, we collected data in three distinct phases that corresponded with each of the questionnaire development phases described above:Focus group results,Structured interview results, andSurvey results.In the interest of conserving space, this section focuses upon the results and analysis of our survey results and we refer the interested reader to our final report for the results from our focus group and structured interviews [4].  We do describe what we learned from applying these mechanisms in the lessons learned below.We distributed the final questionnaire to several SMEs but received very few responses.  We aggressively tried follow-up calls with little success.  As a result, the response rate was very small.  We received only four responses and one of these respondents did not answer the questions but rather provided a running commentary on the inadequacy of the questionnaire and how that prevented their answering the questions.  In order to raise the sample size for the statistical analysis, we combined the responses we got from the structured interviews with those we received from our distribution.  This increased our maximum number of responses to six and, in several questions, to five since one respondent did not complete the questionnaire past the questions on the direct fire battle.Unfortunately, the small sample size made us jealously guard the number of samples that we did have.  As a result, we did not to test for biases associated with the respondent backgrounds, something that could be corrected only by discarding responses to diminish the effects of the bias.  This choice negated the aspects of our original approach that involved tests for independence and bias, at least for this experiment.  This choice does not, however, change the need for such testing when constructing actual referents.Thus, our sample consisted of six SMEs.  Five of those either were serving or  had served in the military, one at O-6, two at O-5 and two at O-4 levels.  Their specialties included infantry, artillery, intelligence, aviation and armor.  All of the respondents had experience in military operations research, both in their military service and as civilians.  All respondents have extensive experience in analyzing ground combat operations, much of it through applying simulations.  Four of the respondents had actual experience in live exercises and three experienced actual ground combat.  All respondents had experience in simulating ground combat for various purposes.  In short, aside from the small sample size, this group of SMEs appeared ideally suited for this experiment.Tables 1 and 2 summarize the results from the killer-victim scoreboards for the three scenario variants.  Table 1 describes the Blue force losses and Table 2 describes the Red force losses.  Each cell in these tables shows the mean percentage of the total number of that asset lost together with the standard deviation associated with the data distribution.  The numbers in the square brackets represent the skews and kurtoses of each of the distributions.  Both the skew and the kurtosis are given in their non-dimensional forms [8].  Our final questionnaire included some question redundancy that we used to test for response consistency in the tank vs. tank, tank vs. infantry fighting vehicle (IFV), IFV vs. tank, and IFV vs. IFV engagements.  We used the results of these tests to correct the loss predictions for these engagements [2].  Regrettably, the small sample size with which we worked became even smaller when the corrections were applied.  Our questionnaire did not provide us with the same consistency-testing ability for the other interactions so we did not correct those loss predictions.  Further, in some cases our limited sample size prevents our computation of standard deviation, skew and kurtosis.  We have omitted this information in the tables below where the respondents did not provide it or we could not compute it.The final questionnaire sought considerably more information from the respondent SMEs than just their predictions of asset losses.  Table 3 captures most of these other characteristics about the battle situations depicted in the questionnaire scenarios. Table 3 facilitates ready comparison of the changes to these characteristics over the different battle situations.  However, one must exercise due care when making these comparisons because of the large uncertainties associated with each of these parameters.  In order to better understand the effects of these uncertainties upon the statistical confidence that one should have in the validity of any differences, we applied the Student’s t-test to the distributions of these parameters.  Table 4 shows the results of this in terms of the likelihoods of the significances of the differences seen in the characteristic means.  In this table, lower numbers imply less likelihood in the significance of a difference between the means of the characteristics of two different battle situations.As Tables 1 and 2 show, even in the simplest battle (i.e., direct fire only), SME estimates of killer-victim outcomes had large standard deviations (i.e., 53% - 153% of the mean).  Further, the distributions in this battle case showed high skews (i.e., 0.33 – 2.3) and kurtoses (i.e., 0.6 – 5.6).  A distribution with a large skew and a large kurtosis has a shape very different from a normal distribution and that tends to diminish the accuracy of standard deviation as a measure of error.  In this case, the computed standard deviation tends to overestimate the error bound as the data in Tables 1 and 2 reflect.  The scores least likely to suffer from the problems of distorted distributions are Blue tank losses from Red tanks, Red tank losses from Blue tanks and Red IFV losses from Blue tanks.  Red IFV losses from Blue IFVs showed a relatively small standard deviation (for the cases shown in Table 2) but its distribution was peaked (i.e., large kurtosis) and showed a moderate skew (i.e., 1.0).  The peakedness of this distribution overcame its skew to produce a relatively small standard deviation.These observations do carry some implications for the resulting referent.  A large distribution skew suggests that the arithmetic mean does not accurately portray the distribution maximum and that another measure needs to be applied to better reflect the composite SME view (possibly the median or geometric mean).  However, this choice complicates the characterization of error somewhat.  A common solution to the skewed data problem transforms the data set into a normal distribution, computes the mean and standard deviation (or error) then transforms those values back into values that more accurately represent the actual data [9].  This alternative requires understanding the detailed nature of the initial data set so one can choose the correct transform and that generally requires more samples than we were able to collect in this experiment.  That said, more data might actually change an apparent non-normal distribution into a more normal distribution.  Whatever the case, more data will improve our situation.Kurtosis simply measures the peakedness of the sample distribution [6, 7].  A peaked distribution has a large kurtosis and a gently sloped one has a small kurtosis.  One desires a large kurtosis for a normal distribution because that implies a small standard deviation and corresponding uncertainty.  However, kurtosis as a measure in itself says nothing bad about the information and needs no adjustment.  For near normal distributions (i.e., small skews), standard deviations accurately reflect the uncertainty associated with the mean as a single value measure of the distribution and, therefore, of the aggregate SME opinion.  Large standard deviations imply large variance in individual SME opinion and thus greater uncertainty in the knowledge associated with the referent constructed from their knowledge.  Aggregate values computed from large sample sizes (as this one was not) with large standard deviations imply a large irresolvable uncertainty associated with the expert population’s knowledge of that particular phenomenon.  One cannot avoid this uncertainty as it represents the limits of human knowledge.  However, our experiment suffers from a very small sample size and this leads one to suspect, rightfully so, that its large standard deviations come more from measurement error rather than any inherent uncertainty in the aggregate knowledge base.Tables 1 and 2 show the killer-victim outcomes for direct fire engagements corrected for consistency.  This correction involved omitting inconsistent answers.  These corrections decreased the standard deviation proportions (i.e., standard deviation / mean) from the uncorrected results in most cases.  With these corrections, the ranges of standard deviation percentages tended to shift toward zero and decreased in span.  Correcting increased the individual skews as much as it decreased them but it decreased the average skew.  This suggests some reduction in the skew effects in the overall data.  These changes in standard deviation and skew suggest some improvement in data quality.  The correction process decreased the kurtoses in six out of eight instances (as only the direct fire results were corrected).  The instances with standard deviation percentages and kurtoses changing in the same direction appear uncorrelated.The data collected about the battle with direct fire and mortars suffers from several problems, most notably, the lack of responses from those that responded to the other scenario variants.  In a few cases, all of the respondents ignored questions about the effects of Blue mortars on Red forces or responded with no change where there had previously been no relevant question.  These problems changed the collected sample sizes adversely.  As a result, we could not compute kurtoses in some cases (because it requires a minimum data set of four samples), skew in a few (because it requires a minimum of three samples), and standard deviation in one (because it requires a minimum of two samples).Despite these problems, we were able to analyze the collected data. Standard deviations ranged from 0% to 156% of the mean values.  The zero standard deviation resulted from only two samples with identical values (i.e., for Blue mortar losses from Red direct fire).  Discarding this case, the standard deviations ranged from 52% to 155% of the mean values for Red tank losses from Blue tanks and Blue tank losses from Red IFVs, respectively.  Where they could be computed, the skews ranged from 0.1 (for Red tank losses from Blue tanks) to 2.1 (for Blue IFV losses from Red tanks and for Red tank losses from Blue IFVs).  Where they could be computed, kurtoses ranged from -0.19 (for Red IFV losses from Blue tanks) to 4.7 (for Red tank losses from Blue IFVs).  Clearly, the data describing Red tank losses from Blue IFVs suffered from the combination of high skew and high kurtosis (i.e., highly peaked but skewed data).  The areas of greatest certainty (i.e., low standard deviation proportions together with relatively small distribution skews) appeared to be in tank vs. tank interactions.In the direct fire and mortar battle, like the direct fire only case, correcting the outcomes decreased the standard deviations in six out of eight instances.  The standard deviation percentages of the corrected data ranged from 25% to 142%.  Thus, correcting shifted the standard deviation percentages down (a desirable thing) but widened it as well (not necessarily a bad thing as long as most values decreased which appears so).  The corrected results also decreased the skews in six out of eight cases.  The corrected data skews range from 0.27 (for Red IFV losses from Blue tanks) to 1.7 (for Blue tank and IFV losses from Red tanks and for Red IFV losses from Blue tanks and IFVs).  The instance of Red tank losses from Blue tanks actually has a skew of 0 but that comes from only three well-distributed samples and probably results from a limited sample size.  There was only weak correlation between the decreases in standard deviation proportion and skews.  The kurtoses could not be compared due to the lack of sufficient samples.  The data collected about the battle with direct fire, mortars and artillery also suffers from non-response but less so than the battle with only direct fire and mortars.  In this data set, the standard deviation percentages ranged from 50% (for Red artillery losses from Blue indirect fire) to 200% (for Blue artillery losses from Red direct fire and for Blue tank losses from Red mortars).  The Blue tank losses from Red mortars were relatively small so large relative errors are not surprising.  The skews ranged from 0.0 (for Red mortar losses from Blue direct fire) to 2.2 (for Blue artillery losses from Red direct fire, for Blue tank losses from Red mortars, and for Red tank losses from Blue IFVs).  The kurtoses ranged from 0.1 (for Red IFV losses from Blue tanks) to 5.0 (for Blue tank losses from Red mortars).  The greatest certainty (for this limited a sample size) (i.e., small standard deviation percentages together with small skews) again lies in the outcomes of tank vs. tank interactions.  In the direct fire, mortar and artillery battle, correcting the outcomes decreased the standard deviation proportions in five out of eight cases (with one case remaining unchanged) and decreased the skews in five out of eight cases (with one case remaining unchanged).  The corrected standard deviation proportions ranged from 25% (for Red IFV losses from Blue IFVs) to 142% (for Blue IFV losses from Red tanks).  Thus, the correction process shifted the standard deviation proportions down and narrowed the range.  The skews of the corrected data set ranged from 0.0 (for Red tank losses from Blue tanks) to 1.7 (for Blue tank losses from Red tanks, Blue IFV losses from Red tanks, Red tank losses from Blue IFVs, and Red IFV losses from Blue IFVs).  There was a strong correlation between the changes in standard deviation proportion and skews.  The kurtoses could not be compared because of the lack of sufficient samples (i.e., >3) in the corrected results.  The greatest certainty remains in Red tank losses from Blue tanks.From Table 3, the Blue and Red travel rates showed reasonably good statistical characteristics with relatively small standard deviations, skews and kurtoses when compared with the values seen in the killer-victim outcomes.  The travel rates before engaging varied between the direct fire only, the direct fire and mortars, and the direct fire, mortars and artillery battles but these variations were not statistically significant when considering the sample size.  Also in Table 3, the rates of direct, mortar and artillery fire showed good statistical characteristics but with slightly larger variances, skews and kurtoses than the rates of travel.The lengths of the battles showed relatively small skews and kurtoses but had relatively high standard deviation proportions.  Interestingly, the mean length of the battle did not vary much (i.e., only 84 minutes) from the direct fire battle when mortars and artillery were introduced.  Further, the predicted outcome of the battle remained static over the different scenario variants.Applying the Student’s t-Test to determine the confidences in the similarities between the different battle situations for the various parameters provides some interesting information (shown in Table 4).  The correlation between the direct fire only situation and the direct fire, mortars and artillery suggests that the likelihood of not being able to distinguish between these two cases is almost 80%.  The likelihood of not being able to distinguish between the other two cases is less.  Thus, the most distinguishable difference rests between the direct fire and mortars case and the direct fire, mortars and artillery case.   However, the mean speed depicts a confusing picture, increasing with the addition of mortars to the battle then decreasing with the addition of artillery.  Thus, the safest conclusion is that the Blue travel rate before engaging does not change significantly from one battle situation to another.  This seems consistent with the intuitive expectation.  The Red travel rates before engaging support this leaning, no change in travel speed before engaging.Travel speeds after the engagement are somewhat more confusing.  The Blue travel speed increases then decreases while the Red travel speed remains almost the same.  Confidences in the correlation between the Blue travel speeds are highest between the mortars case and the mortars plus artillery case.  The confidence that one cannot distinguish between the results of the two distributions is highest between the direct fire only case and the direct fire plus mortars case.  We interpret this evidence, together with the standard deviations and skews, that the support for a statistically significant difference between the Blue travel speeds after engaging for the different battle situations is weak.  Red travel speeds after engaging come with even stronger evidence for the lack of significant difference between the different battle situations (i.e., 85%, 100% and 85% likelihood that there is no difference).  This conclusion is certainly counter to the intuition that suggests that the effects of indirect fire should slow force advance rates.Changes in the rates of direct fire paint a different picture.  These show persistent changes with the addition of indirect fire and the confidences support this conclusion.  However, the confidence in the Blue similarities between the mortar and artillery cases is somewhat less than that for the Red direct fire.  This result suggests that the SMEs predict that any indirect fire can reduce direct fire rates but that they are less confident in the effects of Red indirect fire upon Blue direct fire.  This result mirrors the picture depicted by the killer-victim predictions.  The effects of battle situation on the rates of mortar fire suffered from the small responding sample size and could not be estimated from the existing data.The length of battle for the different battle situations appears not to change significantly with the addition of mortars to direct fire assets.  However, the decreasing likelihood in the similarity and the increasing separation of the means suggests, but only weakly, that the addition of artillery does affect the average battle length although by only a less than one hour (25% of the battle situation involving artillery).  The increased skew and standard deviation proportion further weaken the connection between the mortars only and mortars plus artillery cases.A few peculiarities come from the analysis of the battle situation parameters.  The lack of significance in the differences and the significance in the similarities in the rates of travel before engaging suggest that these rates vary little for the different battle cases.  One would expect this outcome.  However, the likelihoods in the similarities in the rates of travel after engaging also show little difference in the expected travel rates.  This is peculiar since the volume of fires should increase with the addition of the different resources.  The rates of direct fire show a trend similar to that displayed in the travel rate data.  Again, one could reasonably expect these rates to decrease with the addition of the different assets to the battle.  Also surprisingly, the lengths of battle correlate over the different battle situations.  This suggests that the integrated SME opinion predicts little effect of mortars and artillery on the overall battle length, a non-intuitive result.3.  Lessons LearnedWe learned four primary lessons in the course of this experiment.The small size of responses to our survey contributed to the large uncertainties associated with our data.  Collecting a larger number of samples might not decrease the uncertainty of our referent but it would definitely increase our confidence in the accuracy of the resulting uncertainty estimates.  This observation reinforces the need to collect a statistically significant sample size (i.e., one representative of the sampled population [5-7]).  In future experiments, we will pursue the collection of larger sample sizes much more aggressively.Our questionnaire suffered from two obvious flaws, high respondent workload and soliciting information at an incorrect abstraction level.  The high workload conclusion comes from the number of questions in the final questionnaire (> 160).  This reduced our ability to validate it through structured interviews because the interviews became far too long (with 3/4 to 1 hr. recommended).  Effects of the high workload also showed in respondent choices to either not answer questions later in the questionnaire or to respond “No Change” later in the questionnaire even when that response is inappropriate.  The comments we received from the respondents to the final questionnaire support the wrong abstraction conclusion.  Two of the respondents refused to answer the questions and only said that they were unable to do so for the lack of information.  Had we provided more information and additional question options (or worse, additional questions), our questionnaire workload would have grown even more.  Finally, the level of the resulting referent reinforces the abstraction conclusion.  We succeeded in constructing a referent that applies to only a very limited aspect of ground combat.  Our questionnaire did not vary the effects of terrain, time of day, levels of troops, effects of soft factors, mission changes (even in allowing different tactical breakpoints) or various support options.  The only way to create referents with sufficient generality to justify the expense of collecting sufficient samples is to raise the level of abstraction of the questions.  However, it is not clear to us right now exactly how to accomplish this and exploring this question will focus our future experiments in this area.We did not exercise the power of statistical techniques very much in this experiment.  Our limited sample size and available resources prevented us from finding and applying the transforms that would enable us to account for the effects of the large skews in many of our data distributions.  In future experiments, we will anticipate the need to apply these transforms.  Our limited sample sizes also made us wary of applying the tests for independence that we proposed and correcting the data distributions more from the results of these tests.  Improving data independence is an important part of dealing with experimental data.  We did not do this in this experiment but certainly intend to do so in later experiments.The current best practice of the survey research community proved invaluable in designing and evaluating our questionnaire.  In the future, we will more carefully control our focus group discussions to concentrate on examining question consistency and will spend more time collecting and analyzing the structured interviews.  We recognize that all of these efforts improve the validity of our survey results, an important attribute of any simulation referent.  We will continue to follow this best practice and do so more rigorously in the future.In summary, the technique that we proposed for constructing simulation referents from SME knowledge still seems promising.  Our experiment supports this optimism by showing that we could construct and administer a questionnaire and build a meaningful simulation referent from the responses to that questionnaire.4.  ReferencesNote:  website URLs were accurate on 1 July 2001.[1]	M.L. Metz & S.Y. Harmon, “Using Subject Matter Experts for Results Validation of a Complex Theater Warfare Simulation: A Progress Report,” Paper 02S-SIW-095, Proc. SISO Spring Simulation Interoperability Workshop, Orlando, FL, March 2002, np.[2]	M.L. Metz & S.Y. Harmon, “Using Subject Matter Experts for Results Validation of a Complex Theater Warfare Simulation,” 2001 Fall Simulation Interoperability Workshop Papers, September 2001.  Available at:   HYPERLINK "http://www.sisostds.org/index.htm" http://www.sisostds.org/index.htm[3]	Joint Warfare System (JWARS) Office Executive Overview Briefing, August 27, 2001.  Available at  HYPERLINK "https://www.jointmodels.army.mil/jwars/library.html" https://www.jointmodels.army.mil/jwars/library.html.[4]	S.Y. Harmon, M.L. Metz & C. Gauvin, An Experment in Constructing Simulation Referents from Subject Matter Expert Knowledge, Defense Modeling and Simulation Office, Alexandria, VA, 31 December 2003.[5]	F.J. Fowler, Jr., Improving Survey Questions – Design and Evaluation, Applied Social Research Method Series, Vol. 38, Sage Publications, Thousand Oaks, CA, 1995.[6]	P. Salant & D.A. Dillman, How to Conduct Your Own Survey, John Wiley & Sons, Inc., New York, NY, 1994.[7]	R. Tourangeau, L.J. Rips & K. Rasinski, The Psychology of Survey Response, Cambridge University Press, Cambridge, UK, 2000.[8]	M.G. Bulmer, Principles of Statistics, Dover Publications, New York, NY, 1979.[9]	G.E. Dallal, Student's t Test for Independent Samples, 23 October 2000, at  HYPERLINK "http://www.tufts.edu/~gdallal/STUDENT.HTM" http://www.tufts.edu/~gdallal/STUDENT.HTM.5.  AcknowledgementsThis research was funded by the Verification, Validation and Accreditation Program of the Defense Modeling and Simulation Office through the Naval Air Weapons Center – Training Systems Division.Author BiographiesSCOTT HARMON, president of Zetetix, a small business specializing in modeling complex information systems.  Mr. Harmon has been developing rigorous techniques for the validation of simulation federations and human behavior representations.MICHAEL METZ, Vice President of Innovative Management Concepts, is the Technical Director of the Joint Warfare System (JWARS) Verification and Validation program. He is a specialist in the design, development, verification, validation, and accreditation of simulations.  Mr. Metz is a member of the Defense Modeling and Simulation Office’s Verification Validation and Accreditation (VV&A) Technical Support Team (TST).  In his work for the TST he is a contributing author of the Department of Defense VV&A Recommended Practices Guide.SIMONE YOUNGBLOOD is a member of the Senior Professional Staff at the Johns Hopkins University Applied Physics Laboratory.  She is currently on assignment at the Defense Modeling and Simulation Office (DMSO) serving as the Technical Director for VV&A.  Ms. Youngblood currently serves as chair for the Simulation Interoperability Workshop's (SIW)VV&A Forum and co-chair for the DMSO VV&A Technical Working Group. Ms. Youngblood has supported several VV&A efforts, including the Navy's Naval Simulation System, the Joint Warfare System (JWARS), and the development of the current DoD VV&A Recommended Practices Guide.Table 1.	Blue Losses (in percentage of total assets in the force) for the Different Battle Scenarios.OutcomeDirect Fire BattleDirect Fire & MortarsDirect Fire, Mortars & ArtilleryTank Losses from Tanks14 ± 17 [1.3, 2.7]15 ± 21 [1.7, -]15 ± 21 [1.7, -]Tank Losses from IFVs4.1 ± 4.0 [1.7, 3.2]4.5 ± 4.8 [1.5, -]4.5 ± 4.8 [1.5, -]Tank Losses from Mortars-10.2 ± 0.4 [2.2, 5.0]Tank Losses from Artillery--1.7 ± 2.0 [1.3, 1.6]IFV Losses from Tanks17 ± 22 [1.9, 3.6]19 ± 27 [1.7, -]19 ± 27 [1.7, -]IFV Losses from IFVs8.4± 8.7 [0.99, -0.41]4.5 ± 4.8 [1.5, -]4.5 ± 4.8 [1.5, -]IFV Losses from Mortars-2.2 ± 2.6 [1.1, -]1.5 ± 2.1 [1.4, 1.2]IFV Losses from Artillery--3.3 ± 3.8 [2.1, 4.4]Mortar Losses from Direct Fire-10 ± 03.6 ± 3.8 [1.7, 3.5]Mortar Losses from Indirect Fire-7.2 ± 11 [1.7, -]9.4 ± 12 [1.9, 3.4]Artillery Losses from Direct Fire--2.2 ± 4.4 [2.2, 4.8]Artillery Losses from Indirect Fire--5.7 ± 8.2 [2.0, 3.9]Table 2.	Red Losses (in percentage of total assets in the force) for the Different Battle Scenarios.OutcomeDirect Fire BattleDirect Fire & MortarsDirect Fire, Mortars & ArtilleryTank Losses from Tanks39 ± 16 [0.44, 1.2]40 ± 20 [0, -]40 ± 20 [0.0, -]Tank Losses from IFVs7.5± 2.9 [0.0, -6.0]8.3 ± 2.9 [-1.7, -]8.3 ± 2.9 [-1.7, -]Tank Losses from Mortars-0.5 ± 0.70.9 ± 1.7 [1.9, 3.7]Tank Losses from Artillery--2.3 ± 2.4 [1.0, 0.7]IFV Losses from Tanks36 ± 25 [1.0, -0.001]42 ± 27 [0.27, -]42 ± 27 [0.3, -]IFV Losses from IFVs16 ± 9 [1.7, 2.6]12 ± 3 [1.7, -]12 ± 3 [1.7, -]IFV Losses from Mortars-3.2 ± 2.53.4 ± 4.2 [1.2, 0.7]IFV Losses from Artillery--6.8 ± 5.2 [1.2, 0.7]Mortar Losses from Direct Fire-N/A5.0 ± 4.1 [0, 1.5]Mortar Losses from Indirect Fire-N/A27 ± 17 [0.4, -2.1]Artillery Losses from Direct Fire--2.8 ± 4.4 [1.5, 1.7]Artillery Losses from Indirect Fire--24 ± 12 [0.2, -1.1]Table 3.	Summary of Various Other Battle Characteristics Collected.ParameterDirect Fire OnlyDirect Fire & MortarsDirect Fire, Mortars & ArtilleryBlue Travel Rate < Engaging (km/hr)19 ± 11 [1.6, 3.0]16 ± 4.8 [-0.8, -1.3]21 ± 11 [1.5, 2.8]Red Travel Rate < Engaging (km/hr)16 ± 7.7 [1.4, 1.5]14 ± 4.3[0.7, 0.3]25 ± 7.1Blue Travel Rate > Engagement (km/hr)5.6 ± 3.5 [0.7, -1.9]6.3 ± 3.5 [0.4, -3.1]4.5 ± 3.3 [1.6, 2.5]Red Travel Rate > Engaging (km/hr)3.4 ± 2.4 [0.1, 0.1]3.7 ± 2.6 [-0.3, 0.6]3.7 ± 2.6 [-0.3, 0.6]Blue Rate of Direct Fire (rounds/min)3.5 ± 5.6 [2.4, 5.8]1.3 ± 0.7 [-0.4, -3.9]1.2 ± 0.5 [0.02, -0.7]Red Rate of Direct Fire (rounds/min)3.3 ± 5.7 [2.4, 5.9]1.0 ± 0.4 [-0.8, 2.4]1.0 ± 0.6 [0.5, 1.5]Blue Rate of Mortar Fire (rounds/min)-3.4 ± 3.3 [1.3, 1.1]2.9 ± 3.1 [1.6, 2.2]Red Rate of Mortar Fire (rounds/min)-1 ± 00.9 ± 0.1 [1.7, -]Blue Rate of Artillery Fire (rounds/min)--1.4 ± 0.7 [0.8, -1.8]Red Rate of Artillery Fire (rounds/min)--1.0 ± 0.4 [0.5, 1.2]Length of Battle (min)420 ± 411 [0.3, -1.9]427 ± 441 [0.8, -1.1]336 ± 416 [1.3, 1.0]Outcome of BattleRed Forces disengage & retreat while Blue Forces pursue (0.83)Blue and Red Forces both disengage and fall back to temporary positions (0.17)Same (0.8, 0.20)Same (0.8, 0.2)Table 4.	Significance of Characteristic Differences between Different Battle Cases (computed from Student’s T Test)ParameterDirect Fire vs. Direct Fire & MortarsDirect Fire & Mortars vs. Direct Fire, Mortars & ArtilleryDirect Fire vs. Direct Fire, Mortars & ArtilleryBlue Travel Rate < Engaging0.540.400.79Red Travel Rate < Engaging0.630.240.28Blue Travel Rate > Engaging0.740.420.61Red Travel Rate > Engaging0.851.00.85Blue Rate of Direct Fire0.380.640.34Red Rate of Direct Fire0.360.980.37Blue Rate of Mortar Fire-0.82-Red Rate of Mortar Fire-0.18-Length of Battle0.980.760.75