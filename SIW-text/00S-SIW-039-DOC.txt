Multicasting Fast Messages in RTI-KitAlex Koh Jit-Beng, Francis Lee Bu-Sung, Wentong Cai and Stephen J. TurnerNanyang Technological UniversitySchool of Applied ScienceBlk N4, #2A-32, Nanyang AvenueSingapore 639798alexkoh@hotmail.com, {ebslee, aswtcai, assjturner}@ntu.edu.sgKeywords:Reliable Multicast, Fast Messages, RTI, RTI-Kit, FDK, DIS, HLAABSTRACT: In any distributed simulation environment, the underlying network and its protocols play an important part in defining the key issues of scalability, reliability as well as the runtime performance of the simulation. The Runtime Infrastructure (RTI), which implements DMSO’s High Level Architecture (HLA) Interface Specification services, must address these issues appropriately. Multicasting is a technique commonly employed to improve scalability and runtime performance in distributed simulations. This paper describes and discusses the implementation of a reliable multicast protocol over UDP/IP for the RTI implementation in the Federated Simulations Development Kit (FDK). The performance of such an implementation and its implications are benchmarked and explored, marking an impressive 40% increase in performance in TAR benchmarks on 10Mbps Ethernet.1. IntroductionHigh Level Architecture[1,2,3,4] was proposed for the support of next generation  of modelling and simulation projects. The new RTI [6], which supports a broad range of simulation communities, is currently been designed and build in DMSO and the Architecture Management Group (AMG). Several critical RTI functionality highlighted include performance, scalability and reliable multicast. In order to support large-scale federations, multicasting is often used to improve scalability. However, multicasting is often done by means of a TCP exploder, which is basically a dedicated process to copy the multicast data and send to the subscribers one by one using TCP/IP unicasts. The latencies involved in multicasting using a TCP exploder can thus be quite substantial when sending data to a large group. One solution is the use of network-level multicast that is possible through UDP/IP. However, as UDP/IP is unreliable, efficient reliable transport protocols on top of UDP need to be realized in order for multicasting to be of use in distributed simulations. Currently, there are already efforts underway to incorporate reliable multicasting technology in DIS/HLA environments. One example is the Selectively Reliable Transmission Protocol (SRTP) [7,8,9]. Other multiple-sender, multiple-receiver protocols also exists in the research and commercial communities. However, these protocols, including SRTP, are often quite complex to implement and exhibit high overheads and latency. This paper introduces a simple but effective reliable multicast protocol, Pseudo-Reliable Multicast Protocol (PRMP) as an alternative to reduce bandwidth requirements in DIS/HLA environments.2. Federated Simulations Development KitThe Federated Simulations Development Kit (FDK) [5] is developed and distributed by Georgia Tech Research Corporation. It consists of a set of libraries to support development of RTIs. This collection of libraries, known as the RTI-Kit [10], consists of the following modules, all of which implemented in C:Buffer Management and Queues Library.Time Management Kit (TM).Multicast Kit (MCAST).Fast Messages (FM).In addition to the RTI-Kit, the Federated Simulations Development Kit contains two HLA Interface Specification complaint RTI implementations:Baby RTI (BRTI), C implementation.Debbie’s RTI (DRTI), C++ implementation.While these two implementations are not complete realization of the HLA Interface Specification, sufficient HLA services are already in place for simple simulations and benchmarking. Figure 2.1 shows an architectural overview of FDK and its interconnects with the Federate and the underlying network.  EMBED Word.Picture.8  Figure 2.1: Architectural Overview of FDKThe MCAST and FM modules are of particular interest in this paper. The MCAST module is responsible for the management of multicast groups and group communications while the FM module provides the low-level primitives for communications on the underlying network. 2.1 FM moduleFast Messages [11] is developed in the University of Illinois for the High Performance Virtual Machines [12] project, in which FM acts as the core communication layer. FM is a low-level messaging layer designed to deliver underlying network's hardware performance to the application. It provides the following key guarantees:Reliable delivery.Preserved transmission order.De-coupling of communication and computation.Freedom from communication deadlock.FM consists of the following Application Programming Interface (API) function calls shown in Figure 2.1.1: EMBED Word.Picture.8  Figure 2.1.1: Fast Messages PrimitivesThe following sequence illustrates a send using FM: FM_begin_message is called by specifying the destination node ID, maximum size of the message and destination handler ID. FM_begin_message will setup and return a FM stream, basically a data structure, for sending of the message. This is followed by a call to FM_send_piece, specifying the FM stream to use for sending, the data and its size. FM_send_piece may be called multiple times with different data content as long as the accumulated size of these contents does not exceed the maximum size of the message specified earlier. Finally, FM_end_message is called to indicate the end of the message and the message is sent out. As can be perceived, FM is not designed with multicasting in mind. 2.2 MCAST moduleOriginally developed for the Myrinet [13], a high-speed cluster computer network, MCAST utilizes the FM module for its communications. This will mean that any data to be multicast will be performed as a series of unicasts in FM. However, as the number of subscribers increase, unicasting will be inefficient. While unicasting is very fast in cluster computer platforms like Myrinet, the same cannot be said for the Ethernet platform. And when FM is ported to TCP/IP in FDK, no effort is made to include network-level multicasting capabilities. 3. Multicast FMTo implement native multicast in the FDK, both MCAST and the TCP port of FM in FDK are modified. This is to minimise changes to the FDK code as well as ensure support for exiting FM API calls.  The new FM APIs are backward compatible and thus  simulations need not be rewritten. These new APIs are written such that minimum changes are needed to make use of the multicast capabilities. Figure 3.1 shows the extensions, developed and proposed by us, to the original set of FM primitives in supporting multicasting. EMBED Word.Picture.8  Figure 3.1: FM extensions for multicastingFM_scanFDThis primitive allows the upper layers to inform FM which multicast address to expect multicast data from. This is usually called when a subscription occurs by supplying the multicast socket file descriptor, multicast address and multicast port.FM_multicastThis primitive is called to setup the FM stream for multicasting. To send multicast data, the following sequence of calls are made:FM_begin_message, with both destination and handler IDs as dummy values.FM_multicast, with parameters: FM stream, destination multicast address, destination multicast port and type of transmission – reliable or non-reliable.MCAST_SendID (discussed later)FM_send_piece, with parameters just as normal when sending FM unicast data.FM_end_message, with parameters just as normal when sending FM unicast data.FM_removeScanFDThis primitive needs to be called when an unsubscription occurs to free system resources as well as to stop expecting multicast data from the particular multicast group.FM_shutdownThis primitive needs to be called, either in FM_finalize or by the process, before the process is terminated to free system resources. MCAST_SendIDAs the handler IDs (SubIndex) of different subscribers at different nodes may differ, MCAST must send the handler IDs of each and every subscriber along with its corresponding node number in the multicast packet. This information will be needed at the receiver end to invoke the correct handler. To reduce the amount of data sent, bitmaps are used instead of sending the node and handler ID pairs. The bitmaps and the handler IDs should be sent after FM_multicast is called using the MCAST_SendID. Figure 3.2 shows the format of the bitmaps and handler IDs. EMBED Word.Picture.8  Figure 3.2: Mapping Node & Handler IDsIf a particular node has subscribed to the multicast group, its corresponding bit would be set otherwise it is cleared. The SubIndex values are then concatenated after the bitmaps, starting with the SubIndex of the lowest subscribed node ID.The node ID of a particular ith bit, which is set in a bitmap can thus be computed by:Node ID = 8 ( bitmap# + iwhere 0 ( i ( 7 and LSB=0, MSB=7The byte offset of a node SubIndex value, from the start of the Multicast header, is computed using the formula below:Offset = 1 + # of bitmaps + ( j ( 1 ) ( k byteswhere j is the number of number of set bits in the bitmaps preceding the node specific bit. 1 ( j ( maximum number of nodes and, k = size of an unsigned long variable in bytes (typically 4 bytes)MCAST_RecvIDThis function does the inverse of the MCAST_SendID, where each receiver would retrieve their respective handler ID. The receiver would then use the handler ID in subsequent FM_receive calls to then extract the rest of the message from the network.It should be noted here that though MCAST_SendID and MCAST_RecvID are not part of the Multicast FM module, these two functions are crucial to the operation of Multicast FM. These functions are currently located in MCAST because MCAST currently keeps the handler IDs rather than FM. Efforts are underway to keep these information in FM so that MCAST_SendID and MCAST_RecvID are operations within Multicast FM. This will mean higher modularity of Multicast FM module.4. Transport Modes in RTI-KitA total of three transport modes are offered in Multicast FM to upper layers, including the RTI. These are:Reliable point-to-point using TCP/IPUnreliable multicast using UDP/IPReliable multicast using UDP/IPRTI developers are therefore able to make use of these transport modes to cater for different types of HLA traffic requiring different levels of reliability. This section provides detail discussion of the various transport modes offered in FM, including the protocol developed to achieve reliability in UDP/IP multicast.4.1 Reliable FM unicastsThis transport mode is offered in the original FDK v2.0. As previously discussed, it emulates multicasting via multiple TCP unicasts, hindering scalability. Thus, this transport mode should be used sparingly. The most likely scenario of using this transport mode is for control and setup messages. 4.2 Unreliable FM multicast using UDP/IPThe unreliable multicast is a simple UDP/IP multicast implementation with no error detection or recovery efforts. Packets are delivered on best-effort basis with error handling left to the higher layers. This provides the necessary infrastructure in FM for building reliable multicast protocols at higher layers when the reliable multicast protocol provided by FM does not suffice. This transport mode is very useful for sending multicast data that is updated with very high frequency periodically, with the condition that lost or corrupted messages will be superceded by the next message. This is particularly true for frequent state updates, which are offsets from an earlier reference state. These state updates form up to over 90% of total simulation traffic in SIMNET and the first generation of DIS [14]. 4.3 Reliable FM multicast using PRMP over UDP/IPThe reliable multicast protocol developed in FM is known as Pseudo-Reliable Multicast Protocol (PRMP). PRMP aims to provide a high degree of multicasting reliability in RTIs implemented using the FDK. Some important features of PRMP include:Preserved transmission order.Hybrid sender-based and receiver-based reliable protocol.ACK and NACK suppressions.Resends are multicast and filtering of duplicates done at the receivers.Supports many-to-many multicasting, and sender outside multicast group.No heartbeat or keep-alive control messages.Simple with minimal overheads.PRMP is developed for the purpose of simulation. This entails the assumptions:High speed networks with low loss probabilities.All network routers involved in the simulation are IP multicast capable.Maximum number of consecutively lost packets is less than the maximum sequence number (ranges between 8 – 256 and determined by the RTI developer). PRMP must be able to discover and handle lost messages in order to ensure reliability. The protocol needed to achieve this is presented next in this section.PRMP is currently only implemented for MCAST User Messages in RTI-Kit. MCAST User Messages refers to federate data, which forms the majority of simulation traffic [14]. 4.3.1 Handling a receiver detected lost packetSubscribers detect a lost packet when there is a gap between the sequence numbers of packets received. Subsequently, the subscriber will send back a negative acknowledgement (NACK) using a receiver-initiated protocol as illustrated in Figure 4.1 and described below. Due to the fact that UDP/IP packets may be received out-of-order, subscribers do not send the NACKs immediately on discovering a gap between sequence numbers of packets received. Moreover, NACK-implosion, an event where the sender is flooded with incoming NACKs from the subscribers, may occur. Instead, each subscriber suspecting a lost packet does a random timeout, tR. This ensures that every subscriber has an equal chance of sending a NACK first, in case the packet in question did not arrive within the timeout period. The subscriber whose timer expires first, would send back a NACK via TCP/IP and multicast the NACK via UDP/IP. The NACK includes the subscriber’s ID and lost packet sequence number. Any subscriber awaiting timer expiry to send NACK for this particular lost packet will abort the NACK. Thus, NACK suppression is enforced. On reception of a NACK, the sender would multicast the requested packet again. The NACK resend packet is then read by subscribers waiting for the packet. Otherwise it is discarded.Figure 4.1 illustrates a scenario in handling receiver detected packet loss in PRMP.Nodes B, C and D are part of a multicast group. Node A is outside the group.Node A is going to send 6 multicast packets.Packet 0 is received by all.Packet 1 is received by B but not C and DPacket 2 is received by all. This triggers C and D to suspect a packet loss. C and D initiate random timeouts of t1 and t2 respectively (t1 ( t2), to wait for packet 1 in case it is received out-of-order.In the meantime, packet 3 is received by all.Node D’s timer expires first and packet 1 is not received still. D sends a NACK back to the sender using TCP. D also multicast this NACK to the group.At t3, C receives the NACK sent by D for the same packet number. Thus, C suppresses its NACK.On reception of NACK, node A multicast a resend of packet 1.Node A continues sending packets 4, 5 and 6.Packet 4 is received by C and D but not B.Packet 5 is received by all.Node B initiates random timeout t4 after receiving packet 5.However at t5, B receives the out-of-order packet 4. Thus, B aborts its NACK.Finally, packet 6 is sent and received by all. EMBED Word.Picture.8  Figure 4.1: Handling receiver detected packet loss in PRMP4.3.1 Handling a lost ‘last’ packetBy eliminating heartbeat control packets to conserve bandwidth, there is a need to handle the loss of the ‘last’ packet. The loss of a ‘last’ packet is undetectable by the subscribers since there is no other packets arriving next for them to discover a gap in the sequence number. Even if a next packet arrive later, the time lapse may be too long and real-time requirements of the simulation may be violated. For this purpose, PRMP uses a sender-initiated protocol as illustrated in Figure 4.2 and described in this section. A timeout timer, tA, is set after every multicast message is sent and reset every time a new message arrives to be sent out to the network. Eventually, after the ‘last’ message is sent, the timer would expire. On expiry, PRMP would request for a report from the subscribers by multicasting the report request with the last sent sequence number. To prevent ACK-implosion, an event where all subscribers acknowledge the sender, PRMP uses an ACK suppression mechanism. When a subscriber receive the report request, it does a random timeout, tR. This ensures that every subscriber gets an equal chance to reply first. On expiry of timeout tR, the subscriber unicasts a report back to the requester via TCP/IP, this report is also multicast to the group via UDP/IP. This report specifies the number of packets expected from the last sequence number of the report requester. Thus, a reply of 0 means there is no packet lost, and 2 means that the subscriber did not receive the last two packets. All other subscribers with the number of lost packets less than or equal to that indicated in the multicast report, will suppress and abort their reporting. Any subscriber with the number of lost packets more than that indicated in the multicast report, will continue with its random timeout, and send the report after the timeout, unless preempted by another multicast report with a higher number of lost packets. Meanwhile, the report requester will wait for a limited time (tB) for all reports to arrive. Any reports arriving after this will be ignored. If there are no reports, the report requestor will retry the request up to a predetermined maximum number of retries. After which, it will abort the report request and continue, assuming all packets are received. This ensures that deadlock will not occur. If this is not desired, higher layers can trap this error and provide the required error handling.Figure 4.2 illustrates a scenario of the PRMP in handling ‘last’ packet loss. Nodes B, C and D forms a multicast group. Node A is not part of the multicast group.Node A sends packet 0 and is received by B, C and D. Node A also starts a timeout timer of tA.Before timer tA expires, Node A receives another packet to send. Thus, node A sends packet 1 and reset the timeout tA.However, only nodes B and C receive packet 1.Node A has nothing else to send. After the expiry of timer tA, node A sends a report request, R. Node A also starts a timeout of tB.On reception of R, nodes B, C and D does a random timeout (tR) of t1, t2 and t3 respectively, where t1 < t3 < t2.Node B unicast to node A indicating number of lost packet as 0. Node B also multicast to the group indicating current report of 0 packet lost.At tC, node C will abort its timer since it also lost 0 packet.Node D, however, continues its countdown since it lost 1 packet ( ( current report of 0 ). On expiry, node D unicast to node A indicating number of lost packet as 1. Node D also multicast to the group indicating current report of 1 packet lost.Node A waits for other reports before continuing upon timeout of tB. Since 1 packet is lost, the last packet (packet 1) is multicast again. EMBED Word.Picture.8  Figure 4.2: Handling loss of ‘last’ packet in PRMP5. Performance MeasurementsBenchmark results varies with different network configurations and equipment. To justify and validate the benchmark results, the network configurations and equipment setup of the Ethernet test-bed is presented in Figure 5.1. By isolating the machines on these network configurations, benchmarks are unaffected by other network traffic. Doing so, benchmark results can be verified and replicated.Equipment used:2 x D-Link Systems DFE-904, 4 ports, 10/100Mbps Dual-speed Hubby5 x Intel-based PCs with configuration:Intel 450Mhz Pentium II (3 machines) Intel 450Mhz Pentium III (2 machines)128 Mbytes of RAM3COM 3C905B-TX 10/100Mbps Network Interface CardsSun Solaris 2.6/x86 and Sun Solaris 7.0/x86 EMBED Word.Picture.8  Figure 5.1: Ethernet Test-bedTwo benchmarks are used to measure the performance of DRTI using Multicast FM. These are the Latency Benchmark and the Time Advance Request (TAR) Benchmark.5.1 Latency BenchmarkThe Latency Benchmark measures the one-way latency between two federates. This latency is calculated by taking the round-trip latency and dividing it by two. Round-trip latency is measured as the delay it takes from an UpdateAttributeValues invocation to send a message until the time the same federate receives an acknowledgement message from the second federate. The message sent by UpdateAttributeValues contains a wall-clock time value and a payload of N bytes. The acknowledgement message sent by the second federate via the ReflectAttributeValues callback contains a time stamp without any payload. This is an unusual way to measure the one-way latency since the acknowledgement message contains no payload. However, this approach used is consistent with the existing DMSO software and methodology. Several terminology are used in this section:“FM-TCP” shall mean emulating multicast via a series of TCP unicasts. “PRMP” shall mean IP multicasting with PRMP.Figure 5.1.1 shows the results of running the benchmark over the Ethernet test-bed with various payload sizes from 100 bytes to 1500 bytes, at speeds of 10Mbps and 100Mbps. EMBED Excel.Sheet.8  Figure 5.1.1: Latency BenchmarkAs compared to FM-TCP (tcp10, tcp100), PRMP (r/udp10, r/udp100) incurs an additional latency of about 400 to 500 (s in both cases. This is as expected since PRMP has to assemble, process and buffers messages to attain reliability. A large part of this latency is incurred in context switching as Multicast FM is a multi-process implementation.5.2 TAR BenchmarkThe Time Advance Request (TAR) Benchmark measures the performance of the time management services and the time required to perform LBTS computations. The benchmark involves N federates, in which, federate i subscribes to federate (i+1) and federate (N-1) subscribes to federate 0. In effect, each multicast group contains a publishing (sending) federate and a subscribing (receiving) federate. Each federate will then repeatedly perform an UpdateAttributeValues call and a following TimeAdvanceRequest call with the same time parameter. The number of time advance grants (TAGs), per second of wall-clock time, as observed by each federate is then measured. To measure the effects of utilizing IP multicasting, a variation of the TAR benchmark is used. In this modified TAR benchmark, federate 1 to federate (N-1) will subscribe to federate 0, and federate 0 does not subscribe to any federate. In effect, there is only one multicast group comprising of a sending federate (federate 0) and multiple receiving federates (federate 1 to N-1).For 10Mbps Ethernet in Figure 5.2.1, PRMP (r/udp) consistently outperforms FM-TCP (tcp). The higher latencies incurred by PRMP, as established by Latency Benchmarks earlier, is outweighed by PRMP benefits, resulting in the higher performance observed. For 2 subscribers and above, PRMP registers a 40% gain in performance over FM-TCP!  EMBED Excel.Sheet.8  Figure 5.2.1: TAR Benschmark (10Mbps Ethernet)When the network speed is switched to 100Mbps, FM-TCP is observed to offer a better performance, as shown in Figure 5.2.2, when there are few subscribers. This is because at 10Mbps, the network latencies are higher and forms the bulk of the latency incurred. Multicasting using FM-TCP thus incurs substantially higher latencies than the processing overheads introduced in PRMP. On the other hand, at 100Mbps, network latencies are much lower and the latencies involved are mainly due to processing overheads. Since PRMP has higher processing overheads than FM-TCP, the latter will offer a better performance. However, with two or more subscribers in the multicast group, performance of multicasting using PRMP starts becoming comparable to that of FM-TCP on 100Mbps Ethernet.  EMBED Excel.Sheet.8  Figure 5.2.2: TAR Benchmark (100Mbps Ethernet)At 4 subscribers, PRMP surpasses and outperforms FM-TCP. It is expected that the trend will continue, with PRMP performing better than FM-TCP and widening that difference as the number of subscribers increase. This results shows that PRMP is better suited for large multicast groups. 6. ConclusionsBy utilizing network-level multicasting with PRMP, higher scalability and performance can be achieved in distributed interactive simulations. In addition, Multicast FM directly benefits current and future FDK-based RTIs.7. Future WorkImplementation of multicasting in the TM module is also being pursued. It is hoped that multicasting in TM will help accelerate the Lower Bound Time Stamp (LBTS) computations.MCAST and FM are closely coupled in the current implementation. Efforts are underway to keep multicast information in the FM rather than MCAST. This would allow RTI developers to bypass MCAST completely and use Multicast in the FM directly, thus ensuring greater efficiency.Implementation of Multicast FM to be multi-threaded using shared memory, instead of multi-process using pipes, is being investigated to reduce context switching latencies and further improve performance.8. References[1]	Defense Modeling & Simulation Officehttp://www.dmso.mil[2]	High Level Architecturehttp://hla.dmso.mil[3]	IEEE 1278.1-1995, Standard for Distributed Interactive Simulation - Application Protocols[4]	IEEE 1278.2-1995, Standard for Distributed Interactive Simulation - Communication services and Profiles[5]	Federated Simulations Development Kithttp://www.cc.gatech.edu/computing/pads/fdk.html[6]	S.T. Bachinsky, L. Mellon, G.H. Tarbox and R. Fujimoto, RTI 2.0 Architecture, Proceedings of the Spring Simulation Interoperability Workshop, Mar 1998[7]	J. M. Pullen and V. P. Laviano, A Selectively Reliable Transport Protocol for Distributed Interactive Simulation, Proceedings of the 13th Workshop on Standards for Distributed Interactive Simulation, 1995[8]	J. M. Pullen, V. P. Laviano and M. O. Moreau, Creating a Light-Weight RTI Using Selectively Reliable Transmission as an Evolution of Dual-Mode Multicast, Proceedings of the Fall Simulation Interoperability Workshop, Sep 1997[9]	V. P. Laviano and J. M. Pullen, Selectively Reliable Transmission Protocol, IETF Internet-Draft, http://nac.gmu.edu/~vlaviano/draft-laviano-srtp-01.txt[10]	R. Fujimoto and P. Hoare, HLA RTI Performance in High Speed LAN Environments, 1998 Fall Simulation Interoperability Workshop, Sep 1998 [11]	Fast Messageshttp://www-csag.ucsd.edu/projects/comm/fm.html[12]	High Performance Virtual Machines Projecthttp://www-csag.ucsd.edu/projects/clusters.html[13]	The Myrinet – A brief technical overviewhttp://www.myri.com[14]	M. Bassiouni et al., Performance and Reliability Analysis of Relevance Filtering for Scalable Distributed Interactive Simulation, ACM Transactions on Modeling and Computer Simulation, Jul 19979. Author BiographiesKoh Jit Beng received his B.A.Sc.(Hon) from Nanyang Technological University (Singapore) in 1999 and is currently a research staff in the School of Applied Science there. His interests include Internet Technologies, Broadband Networks and their applications. He is currently pursuing his M.Phil in Nanyang Technological University (Singapore).A/Prof. Lee Bu Sung received his B.Sc.(Hon) and Ph.D. from the Electrical and Electronics Dept., Loughborough University of Technology, U.K. in 1982and 1987 respectively. He is presently a lecturer in Division of Computing Systems, School of Applied Science, Nanyang Technological University. He is a member of the Asia Pacific Advance Network committee and is the Application Manager of Singapore Advance Research and Education Network project. His interest are in Broadband networks and its applications.Wentong Cai is currently an Associate Professor in the School of Applied Science, Nanyang Technological University (Singapore). He received his BSc in Computer Science from Nankai University (P.R. China) in 1985, and his PhD also in Computer Science from the University of Exeter (UK) in 1991. He was a Post-Doctoral Research Fellow at Queen's University (Canada) from 1991 to 1993.Dr Cai has served as program committee member in many international conferences, and was a coordinator of the Parallel and Distributed Simulation Mini-track in HICSS'32 (1999). He is a member of IEEE, and has published over 50 refereed papers in journals, books and conferences. His current research interests include parallel and distributed simulation, parallel programming environments and tools, parallel algorithms and architectures and system performance analysis.Stephen Turner received his MA in Mathematics and Computer Science from Cambridge University (UK), and his MSc and PhD in Computer Science from Manchester University (UK). He is currently a visiting Senior Fellow in the School of Applied Science at Nanyang Technological University (Singapore), on leave from Exeter University (UK), where he is Director of the Distributed Systems Group.His current research interests include: parallel and distributed simulation, visual programming environments, parallel algorithms and architectures, distributed computing and agent technology. He is Co-General Chair of the PADS 2000 (Parallel and Distributed Simulation) conference and a member of the advisory committee of DS-RT (Distributed Simulation and Real Time Applications).