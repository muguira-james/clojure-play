VV&A Lessons Learned - A Developer’s Perspective David JenksRaytheon Systems CompanyJoint National Test Facility730 Irwin Ave., M/S 270Schriever AFB, CO 80912-7300719-567-8071 HYPERLINK mailto:dave.jenks@jntf.osd.mil dave.jenks@jntf.osd.milKeywords:VV&AABSTRACT: The single most important piece of the VV&A puzzle is the people.  Each organization brings its unique perspective, and biases, to the effort.  In order to be successful, a VV&A team needs to be integrated from these distinct parts.  This paper identifies lessons learned from participating in an independent VV&A of a major new simulation program.  The VV&A practices used were based upon the Defense Modeling and Simulation Organization’s (DMSO) published recommended practices for the verification, validation, and accreditation of models and simulations. IntroductionThis paper is written from the perspective of a developer giving advice to a program sponsor on how to best establish an independent verification, validation, and accreditation (VV&A) of the program products.  The lessons were learned from participating in the VV&A of the Wargame 2000 program at the Joint National Test Facility at Schriever AFB, Colorado.BackgroundWargame 2000 is the Ballistic Missile Defense Organization’s (BMDO) next generation,  human-in-the-loop, command and control simulator.  The primary mission of the simulator is to allow the missile defense communities to explore concepts of operations (CONOPS) through the use of human-in-the-loop experiments.  The VV&A team is lead by BMDO/DEI in Washington D.C.  and is responsible for coordinating and tracking all of the VV&A activities.  The Verification Agent is NSWC, based in Dahlgren VA, and is responsible for planning and implementing the verification activities.  The Verification Agent has two full-time employees co-located with the development team.  The POET coordinates the validation effort and is based at the Applied Physics Laboratory of Johns Hopkins University.  assigned as the Verification agent.  The POET coordinates conceptual model reviews using subject matter expert (SME) from across the country.One important fact to note is that the VV&A of Wargame 2000 is being performed as the product is being developed.   Block 20 of Wargame 2000 was accredited to support the National Missile Defense C2Sim99 wargame in November of 1999.  Future blocks are planned to support Theater and Air Missile Defense (TAMD) and will undergo VV&A in the same manner as Block 20.VV&A DefinitionsThe following definitions are provided for context, and are not intended to replace those provided in the Defense Modeling and Simulation Organization’s (DMSO) Verification, Validation, and Accreditation Recommended Practices Guide (hereafter referred to as the RPG).  The goal of verification, in colloquial terms, is to answer the question, “Did I build the thing right?”  While the goal of validation is to determine, “Did I build the right thing?”  Finally, accreditation is the act of an end user approving a simulation for a specific intended use. Lessons LearnedThe following subsections provide a summary of the lessons learned in the VV&A of Wargame 2000 Block 20.  Each subsection is intended to stand alone, but there may be some common themes throughout.Establish a Primary ObjectiveA primary objective is a very important, yet subtle concept.  A primary objective is much like a mission statement in that it provides focus for the team.  The concept is based on two assumptions.  The first is that is that the team is much more likely to achieve its goals if they are communicated clearly.  The second assumption is that all of the team members are willing to work together to meet the customer’s need.  One common pitfall in VV&A is not to clearly establish a primary objective because it is considered obvious.  This is very dangerous because if goals are not explicitly defined then the team is likely to define their own (usually based on their own past experience).  For example, consider the following two software development teams.  The first team has been working for NASA on the Space Shuttle program where each defect has the potential to make headlines.  The second team has been developing high fidelity weather models.  If both of these teams were given the same set of functional requirements, the products they produced would still be quite different.  The product produced by the NASA team would most likely take longer to develop and have fewer code defects than that produced by weather team.  In the absence of a primary objective, each team relied on its past experience.   Given DMSO’s  definitions of verification, validation and accreditation, what is the primary objective of VV&A?  From the Verification Agent’s perspective, it may be to find defects in process and products produced by the developer.  From the Validation Agent’s perspective, the primary objective may be to find defects in underlying design approach. The sponsor’s objective may be to procure a system adequate for its intended use.  The developer’s perspective may be to produce a system which meets all of its requirements. Each of these perspectives is valid, but without a primary objective which has been clearly stated and agreed to, it is likely that each of these organizations will proceed with a slightly different objective. A common objective in V&V is to find defects in products. If a V&V team is asked to find defects, it is highly likely that a large list will be produced.  This kind of objective produces a “bulk rate” mentality which assumes that all defects are equally important, and that the sponsor is not getting his moneys worth unless a lot of defects are found.  Experience indicates though that all defects are not created equally.  For example, a spelling error in the User’s Guide is not as important as a sign error in a navigation algorithm.  Each will probably cost the same to fix, but the latter has an impact on the ability of the system to meet its intended use.  Some V&V practitioners contend that they do not add value unless they find a certain number of defects.  The search for their quota of defects becomes their primary objective and results in a long list of defects which have to be sorted through, and formally addressed by the developer.  A large percentage of these “defects” are issues of documentation style, rather than functional defects which could inhibit the system from meeting its intended use.  All of that being said, what is a good primary objective for VV&A?  The basic goal of VV&A is to ensure that the product is adequate to meet its intended use.  To meet this goal, all potential issues raised by the VV&A team should be categorized and prioritized based upon the system’s ability to meet its intended use.  This kind of categorization and prioritization ensures that real functional problems are not hidden in a forest of style issues.  This is not to say that all defects should not be reported, but rather to track these defects through different channels.  This methodology attempts to promote system quality over credit for raising issues.The I in IV&VThere are some who take the “independence” in IV&V to an extreme.  One prevailing theory is that a team of independent V&V practitioners travel to the developer’s site, perform some audits, write their reports for the program sponsor, and return home.  The auditors in this paradigm are independent of the program, and therefore do not care whether the program continues or not.  The reports are honestly written based upon all of the information which was made available, so the consciences of the auditors are clear.  This kind of audit may make sense in the accounting world, but does not make sense in the modeling and simulation world.  How can a developer make fifteen years of modeling and simulation experience in a single domain clear to an external auditor?    No wonder developers handle this kind of review with “kid gloves”, and are considered paranoid.  Is a developer really paranoid when his job and program are on the line?  Most developers seek constructive criticism from domain experts who understand the intended use of the simulation.  Unfortunately, domain experts who understand the intended use of the simulation are a very rare commodity.  Is a developer really paranoid if the independent auditor does not understand the domain, or the intended use of the simulation?   The RPG cautions that a “…common (mis)perception holds that V&V must be conducted completely independent of the M&S developer, lest the results be tainted…”  [ Ref 3, p. 1-27] The RPG further warns that too much independence can lead to an adversarial relationship “with unnecessary baggage that you will ultimately have to pay to carry, all in the name of independence.”  Unfortunately the RPG does not give guidance as to how much independence is enough.   Developers need independent V&V to counteract two common areas of oversight:  Pride of Ownership and Perspective.  Pride of Ownership can sometimes limit a developer from seeing certain classes of problems.  The developer is focused on getting a product out the door, and can benefit from independent reviews.  Having to answer “did you think about…” questions takes the developer out of production perspective and back into a design perspective.  EMBED PowerPoint.Show.8  Figure  SEQ Figure \* ARABIC 1 The Perceived “Value” of IndependenceFigure 1 is a notional depiction of how perceived value differs as a function of independence from the developer’s perspective to the V&V practitioner’s perspective.  The target area indicates a fuzzy region of compromise, within which both groups are willing to operate.  There is no single correct solution to this problem, but there are many bad solutions.   It should be the responsibility of the sponsor to facilitate the compromise process, document and then enforce the agreements.  Regardless of how the compromise is reached, the goal should be to foster an environment where the developer is not afraid to show the warts on a product.  Developer buy-in to V&V activities is much more likely if the Developer is an equal teammate in the planning rather than a afterthought at the end.   A VV&A steering group chaired by the sponsor, and supported by the developer and V&V practitioners should be created to review processes and products.  This group would also be responsible for renegotiating the compromises as the situation changes over time.Most important of all is to ensure that at least part of the VV&A team be geographically located near the developer.  Near enough such that there can be face to face interaction on a daily/weekly basis.  This does a not guarantee perfect communications, but is does a lot to facilitate effective communication..  Invest in the TeamCalling a collection of people who get together a few times a year a team is a recipe for failure.  It is not enough to identify qualified personnel and inform them that they are now a team.  Effective teams go through some standard phases as they evolve over time.  These phases are Forming, Storming, Norming, and Performing [Ref 2].  The details of each phase is not important here, but the realization that effective teams do not “just happen” is very important.Building an effective team requires some up front investment.  These investments come in two flavors: personnel and infrastructure.  It is not reasonable to expect a group of geographically separated people to become an effective team.  It is important to provide team members opportunities to build trust and confidence in each other, through face to face meetings, during the forming phase.  Face to face interaction during the forming phase provides a basis of understanding which will benefit the team throughout the duration of that team.  People who work together on a regular basis tend to develop understanding and even trust. Since it is very difficult for distributed teams to meet face to face on a regular basis, investments need be made into communications infrastructure.  These investments do not need to be large, but they need to be explicitly planned.  Some interfaces are highly complex, others are fairly straightforward. The spectrum of communications includes face to face, video teleconference, telephone, e-mail, and snail mail. Using face to face meetings to report status may be a waste of resources, while face to face meetings for complex technical material is not. Each interface should be evaluated to determine the basic nature of that interface, and plans should be made establish an appropriate communication mechanism.Plan RealisticallyOne of the first questions which needs to be answered is “to what degree will independent verification and validation be performed?”.  This is largely a question of how much the project can afford in terms of cost and schedule.  DMSO’s Recommended Practices Guide identifies methodologies which might be used in verification and validation.  Each of the methodologies listed brings a certain degree of credibility, but also levies a certain cost.  For example, performing design audits of all design elements is much more costly than performing design audits of fifty percent of the design elements.  Once the approach has been defined (e.g., how many reviews of what types), it is very important to agree on how the approach will be implemented.  Many questions are still unanswered, for example: What materials are to be reviewed?  When will materials be distributed to the reviewers?  When will the materials be ready?  How long will the reviews take?   It is very easy to fall into a trap in which the V&V schedule is independent of the development schedule.  It is important to identify the dependencies between the VV&A team and the development team.  One way to examine these dependencies is through a detailed schedule which identifies cross group dependencies.  A schedule of this type can help to identify the impact on the V&V effort due to a change in the developer’s schedule. Establish Checks and BalancesIndependent Verification and Validation will probably always be somewhat contentious.  Many of the issues raised simply do not have one “right” answer.  Therefore it is important to establish some mechanism of checks and balances within the V&V team.   Many of the V&V methodologies in the RPG involve some kind of review and comment cycle.  Since most of the products are reviewed by multiple people, it is important to provide some kind of quality control on the comments.  For example, the VV&A steering group might complete a review of all comments before they are submitted for the developer.  Since the developer has representation on the steering group, it gives them the opportunity to question some of the comments before they are made formal.  Additionally, the source of all comments should be provided.  This way the steering group has a means to track where the most valuable comments are generated.  This approach will result in a smaller set of comments requiring a formal response from the developer.  Furthermore, these comments should be kept in some kind of database which is accessible to the entire team.  This would enable the tracking of metrics based on the category, priority, status etc. of all comments.Another example of a check and balance is to have the developer assess the effectiveness of each phase of the V&V process.  Assuming that the goal of the V&V effort is to ensure a quality product, the developer is best situated to identify what is helping and what is not.Subject Matter ExpertsThe use of independent subject matter expert reviews can greatly enhance the credibility of a model from an accreditation viewpoint.  To get the most out of a SME review it is imperative that the SME not only be an expert in their field, but also to be intimately familiar with the intended use of the simulation.  For example, a SME with thirty years of experience in radar technology, but with no familiarity with the intended use of the simulation may not be beneficial to the overall effort.  Unfortunately, it not easy to find independent SMEs who are recognized experts in their field, who are familiar with modeling and simulation, and who understand the intended use of the simulation.Another pitfall with the use of SMEs is that they are expected to put their reputation on the line based on limited information.  For example, a SME is given a conceptual model to review relative some acceptability criteria and is expected to approve or disapprove the model.  It is not surprising then that many SMEs will not stand behind a model, regardless of how good it is.  This risk can be mitigated by providing a thorough education on the intended use of the simulation.  Specific attention on how data produced by the simulation will be used is required.Another point of contention with the use of SMEs is the selection process.  One methodology is to allow the Validation Agent to select whomever they deem appropriate.  SME selection is another area where checks and balances are appropriate.  The basic purpose of validation is to determine if the model or simulation will support its intended use.  Since the accreditation decision is ultimately made by the accreditation authority, it makes sense then to involve the them in the selection of SMEs.  In general, it is a good idea to require the use of SMEs from the project office responsible to provide the system being represented.  This is not always possible, but should always be attempted. Acceptability CriteriaThe RPG defines acceptability criteria as “how well required M&S functions (or representations, or entities, as well as the interactions between them) must correspond to the real world for the M&S results to be acceptable for the purpose at hand.” [Ref 3, p1-21]  In a perfect world the user would define the acceptability criteria for their particular intended use.  These acceptability criteria would identify what the simulation must do (i.e., functional requirements), and how well (i.e., fidelity requirements) these tasks must be performed.Unfortunately we do not live in a perfect world.  Users have full time jobs, and often do not have time or interest in formally documenting their requirements.  Yet acceptability criteria are key to performing many V&V tasks.  The VV&A team, including the Developer, then must step up and draft acceptability criteria and get buy-in from the end user.  This is not optimal, but is much more realistic than expecting the end user to draft acceptability criteria which are usable by the VV&A team.  RecommendationsThe single most important piece of the VV&A puzzle is the people.  Each organization brings its unique perspective, and biases, to the effort.  In order to be successful, a VV&A team needs to be integrated from these distinct parts. The following recommendations are directed towards building an integrated VV&A team:Establish a VV&A Steering Group with equal representation from the development and VV&A teamsEstablish a primary objective for the VV&A teamCreate a VV&A database which tracks the status, category, priority, and originator of all comments.  Use the database to create metrics to determine how much value is being added from the VV&A effortCreate a master plan which identifies all interactions and dependencies between the VV&A and development teams relative to specific productsExplicitly examine the nature, frequency, and complexity of communications between the VV&A and development teams.  Select the communication method most appropriate for each interaction.  Use face to face communication as much as possible up front to facilitate team building. Use program office supplied subject matter experts whenever possibleEducate subject matter experts on the simulation technology and intended use of the simulation before any review takes placePeriodically, ask the developer for feedback relative to the value add of the VV&A activitiesEnsure that Acceptability Criteria are explicitly documented, and are understood by the end user and all members of the VV&A team.References[1] 	Donald Gause, Gerald Weinberg:  “Negotiating a Common Understanding”, Exploring Requirements:  Quality Before Design”, p.1-2, Dorset House Publishing, 1989[2] 	Eric Honour: “Planning and Controlling Collaborative Teams” Tutorial presented at the Thirteenth International Conference on System Engineering, August 1999.[3]	Department of Defense: “Verification Validation and Accreditation (VV&A) Recommended Practices Guide”, November 1996[4] 	Dale Pace, “Use of Subject Matter Experts (SMEs) in Simulation Evaluation”, 99F-SIW-018Author BiographyDAVID B. JENKS is a Principal System Engineer for Raytheon Systems Company, C3I Modeling & Simulation, at the Joint National Test Facility, Schriever AFB, Colorado.  He is currently the System Engineering Manager for the Wargame 2000 program.DRAFTDRAFT