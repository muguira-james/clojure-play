DIS:  Does Interoperability Suffice?A Need to Set a Higher StandardBrian T. SchreiberLumir Research Institute195 Bluff Avenue, Grayslake, IL 60030847-946-2171 HYPERLINK "mailto:Brian.Schreiber@lumirresearch.com" Brian.Schreiber@lumirresearch.com Eric A. WatzLumir Research Institute31045 Grassy Parke DriveFernandina Beach, FL 32034480-254-9692 HYPERLINK "mailto:Eric.Watz@lumirresearch.com" Eric.Watz@lumirresearch.com Winston Bennett, Jr.Air Force Research Laboratory6030 South Kent Street, Mesa, AZ 85212480-988-9773 HYPERLINK "mailto:Winston.Bennett@mesa.afmc.af.mil" Winston.Bennett@mesa.afmc.af.mil Keywords:Resource, utilitarian tools, higher standard, network data qualityABSTRACT:  Distributed Interaction Simulation (DIS) and High Level Architecture (HLA) standards have revolutionized training in simulated environments.  Instead of simply training in stand-alone simulators on routine activities like emergency procedures, the training community now has the ability to bring vast numbers of entities together, for example, to conduct a small military battle.  This in turn allows for training entire experiences never before possible without using real assets.  However, the interoperability community often settles for what works.  That is, contracting companies, having met an interoperability demonstration “milestone”, simply stop or slow down development.  Company and contractual objectives have been met, so the need to expend more money and resources in fixing behind-the-scenes issues translates into unwise business practice, unless a clear need or opportunity exists to justify otherwise.  Furthermore, customers, observing that a demonstration was successful, may overlook the underlying robustness or adherence to interoperability standards.  They simply see a demonstration worked and the contract is therefore satisfied.  Successful demonstrations can use shortcuts, patches, or contain holes or major stress points that should be addressed, but often are overlooked.  Examples can be readily found in distributed military simulation exercises and include such issues as missing data packets, no entity ID, incorrect entity IDs, improper marking strings, substandard modeling, etc.  This paper illustrates how these problematic errors, often swept under the rug during and after a demonstration, lead to “work-arounds”, create additional manpower requirements, and generate obstacles for the evolving use of interoperability standards as a resource.  These issues ultimately limit the training utility and potential of distributed environments.1 IntroductionDistributed simulation has come a long way in the past several years, and it holds great promise for enabling training of our military.  Distributed Interactive Simulation (DIS) and High Level Architecture (HLA) standards have revolutionized training in simulated environments.  Instead of simply training in stand-alone simulators on routine activities like emergency procedures, the community now has the ability to bring vast numbers of entities together.  This in turn allows for training entire battle experiences.  These experiences have been identified by operational warfighters as critical to being fully prepared for combat (Colegrove & Alliger, 2002).  Simulation interoperability standards have had a pivotal role in creating a transformation in warfighter training.However, the interoperability community settles for what works.  If it interoperates, it “works.”  But, what works for most situations and purposes, does not work for all.  Interoperability affords more than simply seeing one another, more than seeing weapon fly-outs or targets destroyed—i.e., it affords a more behind the curtain view than what operators and observers see and experience in real-time.  Interoperability, specifically the inherent shared data, can be a valuable resource.2 Network Data as a ResourceAs one specific example, interoperability affords tremendous potential for assessing human performance of operators performing on the network.  It allows organizations to (a) calculate return on investment of their simulation training equipment, (b) provide more detailed feedback to operators, (c) objectively assess performance, (d) enable competency-based training, (e) evaluate/pinpoint specific operator skill deficiencies, (f) evaluate and compare new, alternative training techniques or tactics and their objective impact on performance, and (g) standardize performance measures across installations for cross-comparing scientific data (Schreiber, under review). To fully realize this performance assessment capability, we summarize the need for stricter adherence to (and the expanding of) interoperability standards.  In this paper we attempt to illustrate how interoperability standards afford more than simple entity interoperation, but how that resource does not meet a high enough quality standard.The Air Force Research Laboratory in Mesa, AZ performed early research on developing human performance assessment tools using the network data as a resource, and were successfully capturing such data in 2001 (Schreiber, Watz, Bennett, & Portrey, 2003).  A number of other utilitarian uses of network traffic have also emerged over the past few years (e.g., see Nixon, 2004; Perrin, Biddle, Dargue, Pike, & Marvin, 2006; Stacy, Merket, Freeman, Wiese, & Jackson, C., 2005; Biddle, Perrin, Dargue, Lunsford, Pike, & Marvin, 2006; Carolan, & Bilazarian, 2004; Biddle, 2006), as well as other efforts to use the network data as a resource in performing studies on distributed mission operations (DMO), such as documenting the effectiveness of DMO (Schreiber & Bennett, 2006).  These efforts use the interoperability network data, directly or indirectly, as a raw data resource.  The quality of these efforts, which are clearly growing in number and need, depend on the quality of the raw network data. Table 1.1 is an example of how network data can be used as a resource for other, non-interoperability essential functions and lists just a few common assessments generated from the raw network traffic, along with some of the logic performed to produce those assessments.  These examples are all extremely useful as feedback not only to warfighters after a mission, but also in areas such as marketing, studies examining training usefulness, organizational reports, publications, effectiveness evaluations, etc.  They are just a tiny fraction of the potential outputs and uses network traffic could serve when used as a resource.  However, the quality of that resource must be high enough and comprehensive enough to enable other tools to produce reliable output.Table 1.1:  Sample of assessments derived from network traffic.Outcome assessmentsRed and blue kills:  Tracked by counting adversary weapon hits in PDUs. Red and blue weapon hit rates:  Tracked by counting weapon hits compared to fires.Bomb distances off target (absolute, long/short, left/right):  Tracked by capturing position and heading of shooter at fire PDU, drawing parallel and perpendicular lines through target, and then measuring (at detonate PDU) the long/short, left/right, and absolute distance of bomb miss from target.Process (skill) assessmentsTime spent in enemy weapon envelopes, weapon launch parameters (altitude, airspeed, mach, 2D, slant range, platform g-loading, clear avenue of fire, etc.), mutual support violations, communication step-overs, etc.3 The General ProblemSimply stated, network traffic provides a tremendous resource, but its potential is far from realized.  Network issues degrade the quality of the network traffic data—degrading the quality of it as a resource.  These issues, in turn, limit more utilitarian uses of the network as a resource.  Some current issues are listed in Table 2.2.These issues create problems for using the network traffic as a resource.  Each issue in Table 2 is discussed in turn.  The examples provided have occurred in actual (and multiple) DMO training events with operational warfighters (not test personnel).  Specific simulators/sites, however, have been kept anonymous.  Table 2.2.  Types of network data issues.Network IssuesIncomplete Data:  Defined as data not present on the network (e.g., missing fire/detonate PDUs, missing target identification, missing velocity, etc.).Inaccurate Data:  Defined as data not used/represented correctly (e.g., incrementing event ID for a fire and detonate for same weapon, unspecified force ID when it is red or blue, inaccurate entity field marking strings, late data due to system lags, etc.).Customized implementations:  Defined as intentional “tweaks” when implementing interoperability standards (e.g., proprietary voice data, unsupported tactical data links, weapons load variations in data PDU, non-standard enumerations in Data PDUs, etc.).Unrealistic Data:  Defined as poor representations of reality, often—but not always—the result of less-than-adequate modeling (e.g., aircraft performing 35G turns, missiles flying twice as far as they should, aircraft flying underground, etc.).Console operator “work-arounds”:  Defined as actions taken by console operators to work-around one or more of the issues mentioned above or other system limitations (e.g., killing off entities, reincarnating entities, relocating entities, using shields because weapons models are inaccurate, etc.).Incomplete data is one of the more serious issues in trying to use network traffic as a resource.  Unique to incomplete data issues, a number of omissions are “known” because they are typically expected (e.g., a fire event should have preceded a missile entity appearing on the network), making it somewhat easier to overcome their challenges.  But, not all omissions can be logically deduced and therefore pose more serious challenges.  For example, when detonate information is missing (e.g., by being “dropped” or otherwise not received), very important mission summary statistics from Table 1.1 may get calculated incorrectly.  That is, when simply trying to count up how many “red” or “blue” entities were “hit” and killed for calculating kill ratios—a key metric in any warfighter mission—may be error-prone.  Similarly, efforts trying to count or link how many adversaries were killed by which entities and weapon types would also be error-prone.A dangerous assumption is that all the existing network data is accurate when trying to use it as a resource for other tools.  Inaccurate data has negative consequences.  For example, when trying to simply count how many weapons “red” or “blue” employed during an exercise, an inaccurate “unspecified” munition ID label poses an issue.  A munition, in the vast majority of exercises, is almost always fired from either red or blue and should be labeled as such.  Not doing so requires further intervention in the form of additional logic code or human editing of the output.  Or when an organization modifies an AIM-9 missile model to use as an adversary Archer missile, but not change the entity marking string to say Archer missile.  Without the description change in the entity marking string, a utilitarian application reading network data would then read the marking string as an AIM-9 and subsequently output that a (friendly) AIM-9 missile was shot from a red (adversary) entity.  In a warfighter debrief with a tool using the network data as a resource, warfighters would then see that an adversary fired an AIM-9 missile!  This is, of course, definitely not possible in real life and it blatantly stands out to warfighters.  Consequently, errors such as these publicly damage the credibility of the simulator, the debrief system, and the utilitarian tool(s) being used.  Note also in the last example, the marking string-to-model description mismatch has zero negative impact on pure mission interoperability success or failure, and is allowable within the current interoperability standards.  It only becomes a problem when trying to use the network traffic as a resource for other efforts.Similar to, but different from inaccurate data, customized implementations also create issues.  These customized implementations are typically the result of not enough specification in the interoperability standards.  Consider using the network traffic as a resource to accurately calculate if one entity penetrated another entity’s weapons envelope.  In order to calculate this, one needs to know the number and types of weapons the adversary is carrying.  In DIS, there exists a Quantity of Stores enumeration for the Data PDU, along with enumerations for ammunition quantities.  Using the Quantity of Stores, one can specify weapons type and number.  Specifically, the problem is that the standard does not specify how to use enumerations such as Quantity of Stores (i.e., no standard format defined).  The Quantity of Stores enumeration would need a weapons type enumeration included to specify quantities of a particular weapon.  It is important to note that the enumerations in this example are part of the standard; the issue is that the standard does not specify a linkage between items such as Quantity of Stores and weapon type.  Unique to customized implementation issues, utilitarian tools using network data will likely still work at a single site on a local area network (LAN) exercise because the engineers at a single site all adhere to the same customized implementation.  But, the utilitarian use of the customized implementation breaks down as soon as more sites and simulators with different customized implementations join into the exercise.  Then the ability to calculate weapons envelopes disappears for all entities that are not at that same site using the same customized implementations.  This issue bears no negative impact on the ability to interoperate, but detracts heavily from other utilitarian uses of network data.Unrealistic data, though frequently not an impediment to sheer interoperability, also creates significant problems.  First and foremost, warfighters will notice that underlying models are not realistic.  Justifiable or not, a close association is often made that a lack of realism detracts from the ability to train.  The problems can manifest themselves with a number of different end effects, such as unrealistic tactics employed by threat generation systems (e.g., threats moving too fast), unrealistic tactical responses needed by the warfighter, or—to again use a debriefing feedback example—may reveal that shot ranges in a shot log were an order of magnitude too far to “hit” their target.  The end result is that one or more poor underlying models in the simulation require the warfighter to think substantially different than he/she would during a live mission (e.g., to take into account “sim-isms” of individual sites and/or models); credibility of the DMO environment/system is brought into question and corrective actions must be taken.  It often requires a trained eye, such as an operational warfighter, to observe unrealistic data, so spotting the issue usually cannot be done by an engineer.  But, the consequences from this category run deceptively deep, often directly resulting in the next--often the most severe--category of issues for utilitarian tools of network data.With the ripple effect from unrealistic data, console operators oftentimes create “work-arounds” in an attempt to regain realism and credibility for the warfighter.  Interestingly enough, depending on the tool’s purpose for using network data as a resource, these human actions can easily be the most serious as they insert extremely unrealistic data into the network.  By extremely unrealistic, we define these human work-arounds in the form of God-like actions.  Because a missile model may be unrealistic, for example, a console operator may work-around this by deleting the missile mid-flight, minimizing warfighter exposure to the poor model.  Or the console operator may use “shields” so that the optimistically long range missile does not kill the warfighter, again avoiding warfighter exposure to inadequate models.  Or, an entity model may not be burning fuel at the correct rate, leading the console operator to cover up, or work-around, by either “freezing” the fuel or replenishing it.  As yet another example, a scenario generation tool may not have the capability to generate an entity quickly and easily in a particular location to quickly and efficiently conduct a given training event.  The work-around is then to initialize that entity as normal from another given scenario, but then quickly kill it and regenerate it in another location, or the console operator may simply “move the entity.”All of these God-like actions are usually the direct result of either poor models and/or system limitations that do not allow enough function to expedite meeting training objectives.  These work-arounds are (hopefully) short-term, human-controlled patches to solving poor models or system limitations.  These God-like work-arounds have very little, if any, impact to sheer interoperability, and the work-arounds do actually, in the short-term, benefit realism for the warfighter.  But the consequences for using network data as a resource are severe.  The results are that additional data are inserted into the network data stream—data that never should have existed, such as Remove Entity PDUs.  This is not to imply that the Remove Entity PDU should never be used, rather that when it is used to cover up or work-around a known simulation issue or system capability deficiency, the quality of the resulting network data is significantly reduced as a resource.  This human work-around solution,  is a much less costly approach in the short-term (i.e., a lower “bid”) than spending extra time and money to either procure or develop more realistic models or adding system functionality, but the consequences of the short-term perspective are severe for the community trying to use the network data in utilitarian ways.  To take just one simple example, imagine trying to automatically count kill ratios using network traffic when operators are forced into frequently playing God in the training exercise with actions such as allowing shields, deleting entities, reincarnating entities, and moving entities.  Add on “normal” network traffic challenges such as network lags creating time-outs and one can easily see how trying to use network data as a resource, in its current quality state, can pose significant challenges.4 More Simulators, More Sites, More ProblemsToday, much of distributed simulation military training occurs largely at a single institution site as part of a LAN.  As is becoming more common, two institutional sites may join together in a simulated exercise.  With growing interest and use of distributed simulation to meet military training needs, even more simulators and more sites are interoperating on wide-area networks, or WAN exercises, including multi-national exercises (e.g., Gehr, Schurig, Jacobs, van der Pal, Bennett, & Schreiber, 2005; Smith, McIntyre, Gehr, Schurig, Symons, Schreiber, & Bennett, 2005).  Unfortunately, this only compounds the problem of using the network data as a resource.  In small numbers, some interoperability standards issues from Table 2.2 within a given LAN can be made without noticeable or drastic consequence, even for some types of utilitarian efforts of using the network data as a resource.  However, WAN exercises, with more simulators and more sites connecting together, the issues with using network data as a resource only grow in number and variety.  The types and numbers of issues at one site are added to the types and numbers of issues at another site.  Even worse, sometimes those issues don’t simply add up, they breed entirely new issues that may not have previously existed at either site.  Some common examples include latencies due to number of entities and/or bandwidth restrictions or security that necessitates the use of lower fidelity models, which in turn may introduce the God-like actions on the part of console operators.  Typically, the larger the WAN exercise, the more engineers aim for simple successful connectivity and interoperation.  This often results in a “lowest-common denominator” approach, meaning that deviations or inaccuracies to DIS standards that impact successful interoperation are worked during testing and resolved between the sites.  Once successful interoperation is achieved, entities typically can interoperate successfully and the engineering “to-do list” is considered largely done.  In Figure 1.1, we represent this engineering goal of successful interoperation with line (B).  Meeting line (B) in Figure 1.1, successful interoperation, can be achieved with a number of the issues in Table 2.2 existing.  And therein is the crux of the problem—if it “works” or interoperates successfully, any behind-the-scenes existing issues may not be addressed.  It is important to note that meeting the next highest quality line in Figure 1, complete adherence to DIS/HLA published standards, can also be achieved with a number of the issues in Table 2.2 still existing, a point to be discussed shortly in more detail.An important distinction, network traffic quality matching this “successful interoperability” line should coincide with the DIS/HLA standards line shown in Figure 1, but in practice meeting successful interoperability either matches or falls short of protocol standards.  In general, the more entities, simulation systems and sites on the network, the greater the likelihood that the network traffic quality will fall short of the standards line in Figure 1.1.  Especially with very large exercises, engineering success is defined by achieving a successful interoperability goal, which in turn usually satisfies contractual obligations.  And, for exercise managers, the casual observer, and even participants, entities/players interoperated successfully and missions were performed.  We argue that simply reaching the interoperability milestone is far from enough, as some outright violations of interoperability protocol (e.g., using munition ID of 0.0.0 for tracked munitions) and some less-than-ideal (but not an infraction) protocol practices (e.g., using an incorrect label for entity marking field descriptions) can and does occur in successfully interoperating exercises, both large and small.  Figure 1.1.  Quality of log files in relation to (A) requirements for using the network data as a resource, and (B) requirements for successful exercise connectivity.Requirements for successful utilitarian uses of network data (line A in Figure 1.1) are even higher than the current interoperability standards.  The reason that line (A) is higher than the published standards line is that many issues still exist, even when the published standards are adhered to 100%.  Restating earlier examples, such issues include marking string mislabeling, customized implementations, God-like actions, latencies, etc.  These types of issues are numerous and necessitate the need for an even higher standard of network data quality.  Problem is, without a voiced need for a higher standard than what is currently published, the network traffic data quality will just asymptote at the DIS/HLA published standards line in Figure 1.1.  Utilitarian efforts using the network traffic data drives a need for setting a higher standard.5 SummaryThere exists a growing need and desire to use network traffic data, directly or indirectly, as a resource for other tools.  The various network quality data issues previously discussed can, and do, occur in exercises that “successfully” interoperate.  Even if there is strict adherence to interoperability standards, the quality of the network data, as shown in Figure 1.1, is still not sufficient for robust use of utilitarian tools that use the network traffic as a resource.  As a result of this suboptimal network traffic data, one of three countermeasure approaches must be followed when pursuing the use of network traffic data as a resource for other utilitarian tools, all three of which have been exercised in the simulation research and development community:  (1)  Additional logic must be formulated to account for the issue and additional (arguably unnecessary) code and resources must be used in an attempt to reconstruct the accurate data, (2) efforts to use the network traffic as a resource are abandoned or shelved due to the complexity these issues pose, or (3) humans are injected into the loop to “oversee” the utilitarian data process and, specifically, revise/edit the output from a tool before it is shown to warfighters as feedback.  This last step is frequently adopted and creates substantial extra costs with the associated additional manpower—in essence, it’s a utilitarian tool work-around.”  So, now we can have ripple effect of utilitarian tool work-arounds as a result of the console operator work-arounds!  Automaticity in these instances (and fewer human-in-the-loop work-around patches) can be achieved with more attention to eliminating issues in Table 2.2.  Robust raw network data would be the result.  It is the primary goal of this paper that the need for using network data as a resource is known and the issues preventing its widespread use are published so that, over time, none of the three afore-mentioned countermeasures need to be taken. Unfortunately, though issues to using network data as a resource are clearly present and abundant and the causes behind them can generally be identified, they cannot always be singled out for a given DMO environment.  Any number of factors can contribute to one or more of the types of issues listed in Table 2.2, just a sampling of which include inadequate modeling, idiosyncratic site requirements, temporary issue with a given simulator, various system limitations, business strategies, priority on “when it works, the job is done”, bandwidth limitations, local or national security policies, simple carelessness, or simply being unaware of the need to use the network data as a resource.  Though the issues from Table 2.2 are often found in network data, we may not necessarily know what to attribute each issue to.  In many circumstances it is a combination of factors that produce the issues in Table 2.2.  Fortunately, with the exception of security, most of the likely root causes could be addressed and the quality of network traffic data would raise much closer to line (A) in Figure 1.1. Looking forward, there could be a means to automatically monitor a simulation network to look for situations that may be potential causes of problems addressed in this paper.  While such an application may not be an end-all solution to fixing all the problem areas discussed here, it would provide a valuable means of identifying the problems when they do occur.  The output from this application would be in the form of reports, which would be available in real-time as well as post-mission, and would flag violations of the DIS/HLA standards by sites, simulations, and players on the network.  The application would be designed specifically to monitor the network in real-time, or by means of pre-recorded data, and flag instances of potential problems.  The application would monitor all outputs from all simulations and players on the network and would possess logic to detect and flag such items as God-like actions, apparent simulator resets, entity string mislabeling, and customized data PDUs.To increase the quality of network traffic data is not an impossible goal.  As previously mentioned, single sites that perform LAN exercises and strictly adhere to protocol standards, utilitarian efforts using the network traffic as a resource can succeed (Schreiber, Stock, & Bennett, 2006).  The challenge is in raising the quality of network traffic across all sites, then eliminating issues that still exist despite strict adherence to interoperability standards.  We speculate that this will require (a) a further maturation of networks and simulation protocol standards that will possibly require several years of further development, and (b) a conscious effort in raising the bar for the quality of network traffic data.  Once achieved, however, the door opens even farther for utilitarian tools using the network traffic data as a resource.  Potentially any data from any simulation could be sent, in specific standardized format, to a utilitarian tool.  ReferencesColegrove, C. M. & Alliger, G. M.  Mission Essential Competencies: Defining Combat Mission Readiness in a Novel Way.  Paper presented at the NATO RTO Studies, Analysis and Simulation (SAS) Panel Symposium. Brussels, Belgium April 2002.Schreiber, B. T.  Transforming Training:  A Perspective on the Need and Payoffs from Common Standards.  Manuscript submitted for publication 2007.Schreiber, B. T., Watz, E., Bennett, W. Jr., & Portrey, A.  Development of a Distributed Mission Training Automated Performance Tracking System.  In Proceedings of the 12th Conference on Behavior Representation in Modeling and Simulation (BRIMS), Scottsdale, AZ 2003.Nixon, A.  A Flexible Software Architecture for Visualising Simulation Data.  Paper presented at SimTecT Papers 2004, Simulation - Better Than Reality? May 24-27, 2004, Canberra, Australia 2004.  Best Paper. Retrieved 9 January 2007 from,  HYPERLINK "http://www.siaa.asn.au/library_simtect_2004.html" \l "category-2396672122" \o "http://www.siaa.asn.au/library_simtect_2004.html#category-2396672122" http://www.siaa.asn.au/library_simtect_2004.html#category-2396672122Perrin, B. M., Biddle, E., Dargue, B., Pike, B., & Marvin, D.  SCORM as a Coordination Backbone for Dynamically Blended Learning.  Paper presented at the Society for Applied Learning Technology (SALT) Interactive Technologies Conference, Arlington, VA August 2006. Stacy, W, Merket, D, Freeman, J, Wiese, E., & Jackson, C.  A Language for Rapidly Creating Performance Measures in Simulators. In 2005 Interservice/Industry Training, Simulation and Education Conference (I/ITSEC) Proceedings. National Security Industrial Association, Orlando, FL 2005.Biddle, E., Perrin, B., Dargue, B., Lunsford, J., Pike, W. Y., & Marvin, D.  Performance-based advancement using SCORM 2004.  In 2006 Interservice/Industry Training, Simulation and Education Conference (I/ITSEC) Proceedings.  National Security Industrial Association, Orlando, FL 2006.Carolan, T. F. & Bilazarian, P.  Automated Data Collection and Assessment for Debriefing Distributed Simulation-Based Exercises (DDSBE).  In Proceedings of the Human Factors and Ergonomics Society 48th Annual Meeting.  Human Factors and Ergonomics Society, Santa Monica, CA 2004.Biddle, E. (Chair).  Automating Human Performance Assessment: Challenges in a Distributed, Joint Training Environment.  Symposium conducted at the 2006 Behavior Representation in Modeling & Simulation (BRIMS) Conference, Baltimore, MD August 2006.Schreiber, B. T. & Bennett, W. Jr.  Distributed Mission Operations within simulator training effectiveness baseline study:  Summary report.  (AFRL-HE-AZ-TR2006-0015-Vol I).  Air Force Research Laboratory, Warfighter Readiness Research Division, Mesa AZ 2006.Gehr, S. E., Schurig, M., Jacobs, L., van der Pal, J., Bennett Jr., W., Schreiber, B. Assessing the Training Potential of MTDS in Exercise First Wave. In The Effectiveness of Modelling and Simulation – From Anecdotal to Substantive Evidence (pp. 11-1 – 11-16). Meeting Proceedings RTO-MP-MSG-035, Paper 11. Warsaw, Poland: RTO. 2005Smith, E., McIntyre, H., Gehr, S. E., Schurig, M., Symons, S., Schreiber, B., & Bennett, W. Jr. Evaluating the Impacts of Mission Training via Distributed Simulation on Live Exercise Performance:  Results from the US/UK “Red Skies” Study.  In The Effectiveness of Modelling and Simulation – From Anecdotal to Substantive Evidence (pp. 12-1 – 12-10).  Meeting Proceedings RTO-MP-MSG-035, Paper 12. Warsaw, Poland RTO 2005. Schreiber, B. T., Stock W. A., & Bennett, W. Jr.  Distributed Mission Operations within-simulator training effectiveness baseline study:  Metric development and objectively quantifying the degree of learning.  (AFRL-HE-AZ-TR-2006-0015-Vol II).  Air Force Research Laboratory, Human Effectiveness Directorate, Warfighter Readiness Research Division, Mesa AZ 2006.Author BiographiesBRIAN T. SCHREIBER is a Senior Research Scientist with Lumir Research Institute at the Air Force Research Laboratory, Human Effectiveness Directorate Warfighter Readiness Research Division in Mesa, AZ and has an M.S. from the University of Illinois in Champaign-Urbana.  For over the past decade, his research has focused on Air Force simulation training techniques and methodologies.ERIC A. WATZ is a Software Engineer with Lumir Research Institute at the Air Force Research Laboratory, Human Effectiveness Directorate Warfighter Readiness Research Division in Mesa, AZ.  He holds an M.S. in Computer Information Systems from the University of Phoenix, and a B.S. in Computer Science from Arizona State University.  His software experience for the past five years has focused on military simulation systems, and he is the lead engineer on the Performance Effectiveness Tracking System.WINSTON BENNETT JR. PhD is a Senior Research Psychologist and team leader for the training systems technology and performance assessment at the Air Force Research Laboratory, Human Effectiveness Directorate Warfighter Readiness Research Division in Mesa, AZ.  He received his PhD in Industrial/Organizational Psychology from Texas A&M University in 1995.