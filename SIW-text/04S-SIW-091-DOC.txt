Command and Control (C2) of Distributed Synthetic Environment InfrastructureJanet McDonaldUS Army Electronic Proving Ground2000 Arizona StreetFort Huachuca, Arizona 85613-7063520-538-4958Janet.McDonald@epg.army.milRobert T. Patelski Jr.Technologies Engineering, Inc.877 Baltimore & Annapolis BlvdSuite 207Severna Park, MD 21146410-518-6600 X104patelskib@tecenginc.comJoseph BenischekTechnologies Engineering, Inc.877 Baltimore & Annapolis BlvdSuite 207Severna Park, MD 21146410-518-6600 X101benischekj@tecenginc.comKeywords:Test and EvaluationTrainingPlanningVerificationInitializationMonitoringControlSituation AwarenessAfter Action ReviewCommand and ControlABSTRACT:  The Virtual Proving Ground (VPG) is a U.S. Army Developmental Test Command (DTC) program to link the unique capabilities of its individual test centers into a distributed, integrated complex to perform material system testing and simulation based acquisition (SBA).   A major component  of the VPG effort is the Synthetic Environments Integration Testbed (SEIT) used to simulate a representation of the expected physical environment influences, both natural and man-made, on a system under test by networking an array of simulations, applications, and computer generated force models (CGF) via the Defense Research and Engineering Network (DREN).  A major challenge of the SEIT effort is the ability to command and control the synthetic environment infrastructure itself.  This includes the ability to monitor, for verification purposes, computer resources and simulations over a widely distributed network; the ability to initialize, synchronize and control the execution of participating applications including DIS and HLA based federates; and the ability to plan and coordinate any simulation Time Order Event List (TOEL).   This paper discusses the existing and planned capabilities within the US Army Electronic Proving Ground (EPG) Starship Suite of tools being used in the SEIT Project  including testing C2 functions like test planning, laydown verification, initialization, control, monitoring, status display, and after action review (AAR).  It also highlights the need for simulation interface standards for Control and Monitoring.Introduction  The Army Test and Evaluation (T&E) Community is confronted with the same challenges facing the rest of United States Army: i.e., to do more with less.  The Army T&E Community’s solution has been to move away from using actual troops and equipment for testing and transition to sophisticated communication technology, distributed simulations, and automation.  In essence to create a synthetic environment in which system testing and Simulation Based Acquisition (SBA) can be accomplished more cost effectively.  Maneuvering troops in the field as part of a test exercise required a command & control (C2) infrastructure as part of the test control mechanism.  Similarly, the synthetic environment infrastructure also needs to have a C2 mechanism to facilitate testing, especially when the synthetic environment is dispersed across a widely distributed network.The Virtual Proving Ground (VPG) is a U.S. Army Developmental Test Command (DTC) program to link the unique capabilities of its individual test centers into a distributed, integrated complex to perform material system testing and SBA.   A major component  of the VPG effort is the Synthetic Environments Integration Testbed (SEIT) used to simulate a representation of the expected physical environment influences, both natural and man-made, on a system under test by networking an array of simulations, applications, and computer generated force models (CGF) via the Defense Research and Engineering Network (DREN) (See Figure 1).  Figure 1 SEIT Distributed InfrastructureA major challenge of the SEIT effort is the ability to C2 the synthetic environment infrastructure itself.  Furthermore, the inclusion of human in the loop activities required in operational testing and training adds elements of flexibility, realism, variability, and at the same time injects potential confusion, and discord.  The ability to plan, initialize, synchronize, monitor, control, and report accurately on a test when people are involved is always challenging.  Currently the T&E Community has no standard set of tools that provide a “bird’s eye view” or Common Operating Picture (COP) of the overall test infrastructure that produces and connects the distributed synthetic environment.  Additionally, there are no interface standards that a set of test C2 tools could use to observe and display the COP of the synthetic environment infrastructure.The purpose of this paper is to describe how C2 functions normally associated with military science is equally applicable to the T&E Community, identify the growing challenges in testing C2 exhibited in SEIT, and discuss how test C2 functionality provided by Army’s Electronic Proving Ground (EPG) C4ISR Test Toolkit Suite is helping the USADTC SEIT Community meet distributed testing challenges.  Test Command and Control (TC2)Command and Control (C2) TheoryWhile this paper in not intended to be a thesis on C2 Theory it is important to detour into the subject enough to demonstrate it applicability to the T&E Community.  The Joint Chiefs of Staff (JCS) Pub 1-02 defines C2 as the exercise of authority and direction by a properly designated commander over assigned and attached forces in the accomplishment of the mission. C2 functions are performed through an arrangement of personnel, equipment, communications, facilities, and procedures employed by a commander in planning, directing, coordinating, and controlling forces and operations in the accomplishment of the mission [2].  C2 is equally applicable to the arena of Army T&E where a test director serves as the central decision authority exercising C2 functions through a test infrastructure consisting of personnel, instrumentation, LAN/WAN communications, and various test facilities including applications, models and simulation enginesC2 Process: The OODA LoopWhile C2 is normally associated with military operations, it is also a fundamental human process for determining a course of action to achieve a desired outcome.  C2 can be as simple as an individual deciding what to eat after his senses tells him his body is hungry or as complex like the actions a nation state must undertake to prosecute global war on terrorism.  In either case, the fundamental C2 process remains the same and is best illustrated by a concept called the OODA Loop depicted in Figure 2 [3].  OODA stands for Observe, Orient, Decide, and Act.  The OODA Loop is an iterative process that a central decision authority, be it an individual, National Command Authority, or test director, cycles through to achieve objectives or testing outcomes.  Figure 2.  Observe, Orient, Decide, Act (OODA) LoopThe OODA Loop process can start anywhere in the cycle but typically begins with developing a composite picture by “observing” the environment and the objects of interests or participants, within a given mission space or enterprise domain.  It requires the central decision authority to have appropriate monitoring capacity and reporting mechanisms to deliver timely and accurate information about the environment and the status of the participants. Within the T&E Community, various instrumentation devices provide visibility over a testing enterprise and enable a test director to observe a particular system under test (SUT) performing within given environmental boundaries.  For the purposes of this paper, we can refer to this instrumented view of the SUT as the common test picture (CTP).The next cycle in the OODA loop requires a decision authority to “orient” on the CTP.  This requires that data being collected by appropriate instrumentation be accurately processed to quickly arrive at the correct understanding of what the CTP is portraying.  This is accomplished through a knowledge acquisition process called the “Cognitive Hierarchy” depicted in Figure 3. Raw unprocessed data is manipulated in various ways to create information which in turn is fused together to derive knowledge.  Finally knowledge is synthesized and adjudicated into understanding.   Comprehension of the CTP by the test decision authority is what forms “Situation Awareness”, that is, understanding of what the sensors and instrumentation devices are telling the test director about the CTP.  The process of converting raw data into situation awareness may involve many different applications to facilitate the reduction of data into information.  Triggers or thresholds can be established to serve notification when certain test conditions or actionable events have occurred.Figure 3.  Cognitive HierarchyOnce situation awareness is achieved, the central test director can “Decide” what action to take.  When situation awareness is fully developed, then various courses of action can be considered and the appropriate decision rendered.  If this is the first cycle through the OODA loop it may involve scripting a detailed test plan.  This could take the form of a Time Ordered Event List (TOEL).  The final phase the OODA Loop is to “act”.  Execution of the test plan or implementation of the test director orders requires a communications apparatus to effectively transmit the actions to the appropriate personnel or execute a control command to an instrumentation device.  This includes the ability to initialize a test or synchronize events over a widely distributed network.  In distributed testing, communications can present significant C2 problems because of the distances and lack of visibility over the testing infrastructure. From the discussion above, it can be derived that the theory, processes, and functionality associated with C2 are also relevant to the T&E domain.     USAEPG C4ISR Test ToolkitThe C2 process is fundamentally the same for any enterprise but what normally distinguishes one C2 system from another is the apparatus that must be employed to effectively exercise C2 over the enterprise’s domain.  The information age, especially the advent of the Internet provides a medium to extend C2 of an enterprise anywhere on the globe so long as it has an interface that can be specified and the information can be properly represented in some useful and meaningful display. The Army Command’s Electronic Proving Ground has developed a suite of C4ISR testing tools that instruments, automates, and facilitates many of the C2 functions discussed above including test planning, exercise scripting, initializing, synchronizing the execution of, monitoring, controlling, and displaying the status of large numbers of simulations, live systems, instrumentation, and individuals operating within a distributed country-wide network.   Starship - Situational Awareness for the Test DirectorThe cornerstone application in the USAEPG C4ISR Test Toolkit Suite is named Starship and consists of the Exercise Manager, Starship Database, and StarGen GUI system components.  Starship is the glue that integrates the EPG C4ISR Instrumentation Suite into a networked Test C2 System.  Starship is primarily a test planning and test C2 tool used for conducting distributed systems of systems testing.  It is a PC-based, MS Windows software suite to command, control, and display the status of anything (e.g., instrument, control, live battlefield system, simulated battlefield entity).  Starship interfaces to distributed test instrumentation via a test communications infrastructure and allows the tester to monitor the status of systems under test and test instrumentation in near-real time. Starship provides graphical and textual reports both during test execution and for daily test reviews.The underlying object-oriented design and eXtensible Markup Language (XML) technology implemented within Starship provide maximum extensibility, scalability, flexibility, and reuse.  Starship is intended to facilitate control and monitoring of test equipment as well as the unit under test for all test facilities and ranges.  This capability will interface to existing control systems at other ranges as well as joint models, e.g., JVB and JointSAF.  Because of its unique design and functional test C2 capabilities, Starship will be integrated into the Interoperability Test and Evaluation Capability (InterTEC), a Multi-Service development effort with the Navy as lead agency.Starship Implementation in SEITDuring the past year, the EPG C4ISR Toolkit Suite played an active role in supporting the DTC SEIT Project during two exercise iterations.  Starship, together with one of C4ISR test instruments called the Remote Reconfigurable Intelligent Instrumentation to Control, Collect, Simulate and Stimulate (RICS2), were employed at several SEIT sites and then networked together through the DREN (See Figure 4). Figure 4.  Starship/RICS2 SEIT ConfigurationStarship and RICS provided situation awareness by displaying health and status on various SEIT infrastructure components.  Starship and RICS exchange information through an XML specified interface.  Through this interface Starship can collect and display the status of remote test instruments and their systems under test.  Starship also enables the user to command the RICS internal process as well.The (RICS)2 includes a number of subordinate applications known as Child Processes.  Two of these child processes have been deployed for SEIT: High Speed LAN Tap (HSLT) and Pinger.  HSLT tracks message types, including tactical messages such as Joint Variable Message Format (JVMF), moving across a LAN.  These data are then reported to Starship for display to the user.  This capability gives the Starship user the ability to monitor the message traffic on a remote LAN and thereby providing an indication of whether simulations deployed on that LAN are executing.  The Starship user can issue remote commands to any HSLT such as to start tracking certain message types.    Pinger issues ping messages to systems identified in its script.  The ping results are reported back to Starship.  The Starship user has views of all Pingers on all LANs and their ping results in a timeline display.  Furthermore, the Starship user can remotely instruct a Pinger to load a particular script.  A Starship tabular view of this information is depicted in Figure 5.Figure 5.  Starship Status Monitoring ConsoleThe current capability of Starship, HSLT and Pinger provide just two types of status information on the SEIT infrastructure.  Starship’s extensible architecture easily accommodates further development of other instrumentation interfaces to expand situation awareness for the other SEIT infrastructure components. Test & Training Enabling Architecture (TENA)In order to better support SEIT, a prototype Starship Replication Manager was developed to enable each Starship instance the ability to communicate with other Starship instances across the DREN Network.  The best technology available to quickly enable Starship to communicate over a widely distributed network was the DoD Foundation Initiative 2010 Test and Training Enabling Architecture (TENA).  The prototype utilized the TENA middleware along with a Logical Range Object Model (LROM) designed specifically to support the exchange of Starship/RICS2 data.The Starship Replication Manager consists of two applications, the Starship TENA Publisher and the Starship TENA Subscriber.    The publisher, acting as a client to a local Starship Engine instance, tracks Starship entities’ properties and publishes this information to the Starship TENA Execution.  The subscriber, participating in the same Starship TENA Execution, listens for entity creation and changes in entities’ states.  It then updates another Starship Engine instance:  As the subscriber discovers newly published entities in the execution it inserts entities into its Starship Engine; as entities of the execution change state, the subscriber updates the engine’s entities accordingly.  This execution can consist of any number of publishers and subscribers, all of which share the common object model.  The Starship TENA Publisher and Subscriber were built with the TENA Middleware, Release 3; they are compatible with Starship 5.1.The Starship Replication Manager was first deployed as an experiment for SEIT IOC 2.5 in November 2003.   In this capacity all Starship Information was collected at one Starship node on the SEIT DREN network and distributed to other Starships deployed at other nodes.  The Starship Engine and the TENA Replication Manager software were deployed at various nodes on the DREN: White Sands Missile Range (WSMR); Electronic Proving Ground (EPG); Yuma Proving Ground (YPG); and Aberdeen Proving Ground (APG).  In this deployment, Starship at WSMR collected information from (RICS)2 instrumentation residing on the SEIT DREN and its Starship TENA Publisher published Starship Entities to the common Starship TENA Execution.  The subscriber at EPG successfully participated in the execution; accordingly, EPG operator obtained the CTP of the Starship instrumentation from WSMR.  A similar experiment was undertaken at SEIT IOC 3 in December 2003.  In this endeavor, WSMR acted as the publishing node and EPG acted as a subscriber.SEIT Lessons LearnedThe Starship capabilities provided just a limited view of the SEIT infrastructure health and status because interfaces for all simulations in SEIT were not available.  Additional information might include the execution status of critical processes of a simulation engine or status of processes upon which it depends.  Critical simulation-specific information is lacking as well.  Starship’s extensible nature accommodates interfacing with unforeseen systems.  Any system that provides a well-defined interface can potentially be monitored and controlled by Starship.  Furthermore, remote control of simulations would enhance the test director’s ability to coordinate simulation execution, which presently is done via voice commands to remote operators.  Another requirement identified by the SEIT Community is the ability to display the network status in the form of a logical network diagram in addition to Starship’s present tabular displays of this information.Future Starship CapabilitiesDuring the next SEIT exercise cycles, Starship will be extended to increase the information being displayed in the SEIT CTP by adding an automatically updated logical network display of the SEIT participants.  Test planning and execution will be improved with the introduction of an enhanced Starship GUI called StarGen.  StarGen uses the Starship Engine to facilitate test planning through a graphical display of a time order event list (TOEL).  The user can both design a test plan and later execute the TOEL from the StarGen GUI.  Additional interfaces will also be developed to enable Starship to monitor and control other SEIT infrastructure participants including.HLA/DIS  Object ModelsOperating System AgentBelow is the planned additional C2 functionality to be provided by StarGen and employed in the next SEIT cycles.Exercise PlanningWhen a test exercise is first conceived, one of the initial steps is to determine the participants including the SUT, an instrumentation suite, and environmental stimuli generator.  Once test objectives are identified, an overall plan must be developed to perform the T&E or training activity.  The plan consists of up front aspects such as the identification of constraints, facilities, participants (i.e., organizations, personnel, facilities, equipment, automation, simulations), organizational/personnel roles and responsibilities, high and low level objectives, general schedule, and budget.  Once these programmatic aspects are determined the designated test or training officer conducts detailed planning.  There are a number of software tools that support this planning process.  Most of the tools (e.g., Microsoft Project) are more than adequate for the planning process only.  The tools require tedious manual input to keep them current and a means to indicate accomplishments and “earned value” per the plan.  No automated update mechanisms exist.  The StarGen capability of the Starship suite facilitates planning by allowing users to import or graphically add participants (simulations, systems, devices, data collectors and other instrumentation, anything with a software interface) to a hierarchical vertical tree structure like the on in Figure 4 which depicts the Participant list and threads/events laid out on a timeline.  To the right of the participant tree is a timeline on which events are dragged and dropped for execution by the participants.  The events can be composed into threads and the threads can be saved for repeated use.  The StarGen capability provides the tools to create both the participants and the events.  Users create events by entering data into a form, using a Wizard, or by recording the event using a macro.  As the event is created, the participant group that can execute the event is designated to help guide the placement of events with participants. The software will warn the user when an event is scheduled for a participant who cannot execute the event.  The user can designate an event to be a recurring event by specifying the frequency and time period or number of occurrences. Figure 6.  StarGen Participants and Thread/Events TimelineOnce the detailed planning process is completed, StarGen allows the users to automatically generate custom reports in Microsoft Word or Excel that depict the participant hierarchy and key metadata such as name, IP address, Unit Reference Number, hierarchy, type, and role.  Additionally, a Visio logical laydown can be generated and modified by the user to support unique requirements.  The purpose of these reports and laydown is to provide enough information for the laydown to be accurately and completely constructed.  More than one user can work simultaneously to build the plan.  StarGen is designed to be multi-user.Exercise Laydown Verification  Although not yet constructed, StarGen will interrogate the laydown to verify that the implementation was correct.  This is the first of three cutting-edge capabilities that are exclusive to the Starship suite of tools.  If there are any discrepancies between the planned laydown and the discovered laydown, the Visio laydown will display the discrepancies.  The discrepancies can then be resolved.  This feature depends upon the existence of means to determine participants’ nets and subnets via available protocols such as Simplified Network Management Protocol (SNMP). Exercise Initialization:  After the laydown is verified, Starship will initialize the participants (i.e., organizations, personnel, facilities, equipment, automation, simulations) according to the events graphically depicted on the StarGen timeline display.  This is the second of three cutting-edge capabilities that are exclusive to the Starship suite of tools.  Colored coded status is used to convey readiness or problems.  This feature depends upon the participants exposing an interface that includes initialization as part of the protocol and transmittal of status information. Monitoring and Control:  After determination that all participants are ready, execution can start.  The StarGen timeline that displayed the plan now becomes a primary interface to monitor adherence to the plan.  After the command is issued to start, a red vertical line appears on the timeline at the correct timeline position (i.e., now).  The red line moves to the right to signify the passage of time.  As the red line crosses events, the events are executed through Starship.  Starship issues the appropriate commands (or instructions) to the participant designated for the event.  As events are executed, the display signifies event status by changing the color of the event (and the color of the thread in which the event occurs) on the timeline.  Figure 2 depicts this process on a sample StarGen display.  Figure 7. StarGen Display during ExecutionPresently, the entire feedback mechanism has not been completed so that StarGen displays via color coding the status of the successful issuance of the event.  In the near term, the feedback will be enhanced between Starship and the participants to gain acknowledgement of the participant’s receipt of the events’ instruction and consequential action and state change.Since StarGen is multi-user, additional copies of the participant list and timeline can be displayed at each user workstation.  The displays are updated within seconds from one repository.  Information is stored in one database and shared by each StarGen instance.As execution takes place, the actual start and end times are journaled to the data base.  This is the third of three cutting-edge capabilities that are exclusive to the Starship suite of tools.  The user can add information and a discrepancy report to any event as depicted in Figure 3.Figure 8. Discrepancy Report and InformationDuring execution, users can move threads and events to the right to delay their execution.  In addition to the scheduled threads and events, ad hoc commands can be issued by right clicking a participant and selecting among the choices displayed.  This is depicted in Figure 4.  Figure 9.  Starship Ad Hoc CommandsStarship provides different monitoring displays that serve assorted purposes.  One display is the status timeline.  This display, depicted in Figure 5, provides color-coded historical status information for any or all participants.  The color coding is determined by user-entered thresholds applied to any property reported by a participant.  Figure 10.  Participant Timeline ChartAdditionally, Starship provides a color-coded grid display of the instantaneous property values reported by participants.  The color coding is again determined by user-entered thresholds applied to any property reported by a participant.  The tripped thresholds can also optionally sound audio alarms.For participants that report location, Starship provides map windows that can be panned and zoomed and on which users can draw simple objects.   Users decide the participants that are displayed in each window.  The under-development Blueprint tool will be able to handle at least 30,000 individual participants.  Blueprint will handle (display on a grid and map) at least 1000 updates including threshold alarm checks per secondA Starship capability that is under development is to allow the users to develop there own graphical display from the Visio drawing developed during the planning process.  The display will be customizable and animated with information reported by the participants.  The user will drag and drop properties of the participants onto the Visio drawing thereby designating the information to be displayed on the drawing when it is animated during run time.  The user will be able to drill down to display additional information much like one presently does on a desktop or laptop computer.  The ability to allow the user to create, customize, and animate their display will be a significant usability enhancement to Starship.After-Action Reporting:  After the test or training exercise, message data collected by the US Army Electronic Proving Ground (EPG) data collectors can be reverse engineered and the results displayed on the StarGen timeline by participant.  The plan is to superimpose the collected data on the planned data to enable a direct visual comparison of planned verses actual.  Starship provides the capability to play back all data it receives from the participants.  The playback works much like a cassette or digital video disk in that the visual and audio settings can be adjusted.  This means that different thresholds can be set during playback so that phenomenon unnoticed during the original live session can be uncovered.Need for Standard Monitoring & Control InterfacesPresently, all of Starship interfaces are custom and unique.  As Starship is used more frequently to control and monitor tests and training exercises that involve simulations, the existence of rich, robust interface standards would facilitate and enhance the user experience and overall usefulness.  Existing simulation interface standards such as the High Level Architecture (HLA) Federation Object Model (FOM) are administrative in nature and hence, of very limited use for meaningful control and monitoring on test ranges.  The data passed between Starship instances use the TENA Middleware.  Data attributes are defined in a Logical Range Object Model (LROM) which is somewhat analogous to an HLA FOM.  However, despite the conceptual similarities between the two, there are significant differences between an HLA FOM and a TENA LROM.  Once an HLA FOM has been defined, information is, for all practical purposes, passed around in the form of messages that have no life span.  In the case of TENA, objects are shared among applications and have a lifespan that is controlled by the publisher.   In addition, TENA objects support remote methods which can be invoked remotely by subscribers.  This can be used, for example, to provide remote control of range instrumentation as implemented in Starship.  It is important to differentiate between the purposes of HLA and TENA.  HLA is a simulation interoperability standard whereas TENA is a test range interoperability standard.  The EPG Starship team is actively defining a strawman Logical Range Object Model (LROM) to be used within the TENA middleware.In order to take full advantage of the extensive capabilities of Starship, custom participant interface software must be written to allow Starship to interface with the new participants.  Note that Starship can specify an interface using eXtensible Markup Language (XML), thereby minimizing the level of effort to add new participants to it.  With sufficient specification, any participant (i.e., organization, personnel, facility, equipment, automation, simulation) could be easily interfaced with Starship and, thereby, be monitored and controlled as discussed above.SummaryC2 functionality is just as relevant to the T&E Community as it is to the Warfighter.  As test and evaluation enters into the synthetic environment realm, interconnected by a widely dispersed infrastructure, the need for adequate Testing C2 functionality becomes more evident.   The EPG C4ISR Test Toolkit Suite has adapted the militaries theory and process for C2 into its Starship C2 Suite.  There are still challenges ahead but Starship’s experience with SEIT and in the future with the InterTEC effort will produce a test tool that will provide enabling test C2 functionality to the test director.References[1]  Joint Pub 1-02. Department of Defense Dictionary of Military and Associated Terms. March 1994[2] The OODA Loop was developed by COL. John R. Boyd, USAF (Ret), "An Organic Design for Command and Control," A Discourse on Winning and Losing. Unpublished lecture notes, August 1987. OODA is an acronym for Observe-Orient-Decide-Act.Author BiographiesJanet McDonald is a Computer Scientist with the Army Electronic Proving Ground at Fort Huachuca, AZ.  She is the Program Manager for portions of the EPG C4ISR Toolkit Suite including Starship.Robert T. Patelski Jr. is a Senior Communications Engineer with Technologies Engineering Inc. Severna Park, MD.  Mr. Patelski is the Program Coordinator for the Starship effort and several other test improvement efforts supporting the Army Electronic Proving Ground at Fort Huachuca, AZ.Joseph Benischek is a Senior Scientist with Technologies Engineering Inc., Severna Park, MD.  Mr. Benischek is the Chief Architect for the Army’s Virtual Electronic Proving Ground at Fort Huachuca, AZ responsible for the development of Starship and EPG’s C4ISR Test Toolkit Suite.