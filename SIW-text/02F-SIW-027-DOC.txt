Natural Language Interaction Between Live and Synthetic Participants in a MOUT EnvironmentKeith GarfieldDonald A. WashburnInstitute for Simulation & TrainingUniversity of Central Florida3280 Progress DriveOrlando, FL 32826-0544407-882-1342, 407-882-1433 HYPERLINK "mailto:kgarfiel@ist.ucf.edu" kgarfiel@ist.ucf.edu,  HYPERLINK "mailto:dwashbur@ist.ucf.edu" dwashbur@ist.ucf.eduKeywords:Natural Language, Natural Language Processing, SAF, vocal interaction, and MOUT[NOTE: Sections of this paper are incomplete as noted.  Also citations and references are not fully in place yet.  A complete version will be uploaded on or before 16 July.]ABSTRACT: This paper discusses issues involved with Natural Language (NL) vocal interaction between live and synthetic participants within a virtual training simulation.  In order for the live participant to get the most benefit from the training experience, all interfaces between the real and synthetic aspects of the simulation must appear seamless and natural.  Current simulations have essentially no capability for live and synthetic participants to engage in vocal exchanges, yet vocal exchanges play a large role in normal human interaction.  Vocal interaction is particularly important in the areas of Individual Combatant (IC) training and team training; virtual simulation trainers are being developed in both of these areas.  This implies that inclusion of natural language vocal interactions between live and synthetic participants will result in more effective training tools.The phases of live-synthetic vocal interaction can be broken down into Speech-To-Text (STT) translation, Natural Language Processing (NLP), and Text-To-Speech (TTS) translation.  This paper will focus on NLP issues.  NLP issues generally concern extracting meaning from text, and thus include semantics, situational awareness, and knowledge representation.  The NLP function must operate in a reverse manner when synthetic entities generate speech and construct text that captures the intended meaning of the speaker.  In order to accomplish this in a natural and non-scripted fashion, the virtual entities must have a degree of situational awareness and assessment capability.This paper discusses these issues in the context of the Institute for Simulation and Training’s (IST) experiences building a voice interaction tool for use with a Dismounted Infantry Semi-Automated Forces (DISAF) product.  This application allows for allows for NLP features to be investigated while limiting the scope of the conversational topics.  We believe that the methods required to establish a sense of self-awareness for the virtual entities can be used in other realms of virtual entity representation, such as modeling effective decision making and generating appropriate behaviors.  We then present conclusions based on our experiences and provide recommendations for future work. IntroductionIn order for immersive simulations to become more effective they must become more realistic. Current simulations primarily immerse the participant visually while engaging the other senses in a limited fashion.  Fully realistic simulations require live participants to converse naturally with the virtual entities in the environment.  Currently, immersive simulations exist that allow human participants to issue commands to Computer Generated Forces (CGF) entities.  The Institute for Simulation and Training (IST) at the University of Central Florida (UCF) has produced one such product with the Voice Federate. No existing simulations allow natural language conversation between the human and synthetic participants in the exercise. This paper presents IST’s plan for providing an initial capability in this area.  This technology can be extended beyond the training arena to operational applications aimed at reducing workload at a crew station or providing the command and control interface for robotic vehicles from a central command location. BackgroundRecently, the Army has seen the need for training simulators for infantry and other dismounted soldiers  REF _Ref12876797 \r \h [1].  A dismounted soldier is simply a soldier that carries out his objectives on foot, outside of any vehicle.  Examples are an infantry foot soldier and a member of a special operations team.  The need for dismounted infantry (DI) simulation has grown out of the new roles that the Army is assuming in recent years.  These include peacekeeping missions, like those in Haiti, Somalia, and Bosnia.  The skills required for these missions are more diverse, and the rules of engagement more dynamic, than regular combat missions.  Another use for DI simulation is the training of team and squad leaders for Military Operations in Urban Terrain (MOUT) missions.  The unique needs of simulations used to train DI forces drive the design to be immersive, whereby a live soldier is operates in conjunction with virtual entities in a virtual environment.  The current techniques rely on visual immersion while the other senses are engaged less fully. The ability to allow voice interaction between live and CGF DI forces has been identified as a critical capability to provide a fuller and more inclusive immersion into virtual environment  REF _Ref12876822 \r \h [2].   A voice recognition and synthesis interface between the individual live and synthetic participants in the exercise is needed to achieve a natural sense of immersive presence for the live participants in the environment.IST has been working on the Reusable HLA Voice Input/Output Capability for Human Simulation project for three years.  This project was funded by STRICOM and is more commonly called the Voice Federate (VF) program.  During the first year of this contract, we established the basic design of the system and allowed the human user to communicate vocally to the SAF entities  REF _Ref12948233 \r \h [3].  During the second year, we extended the VF system in the following ways.  First, DISAF was modified to allow SAF entities to initiate speech and respond vocally to speech appropriately based on their behavior.  Second, the VF was extended to include voice commands for selected new DISAF behaviors, and to allow more natural identification of SAF entities and terrain locations.. Third, the VF was modified to better support multiple human and automated speakers.  During the third year, we added spontaneous speech generated by the SAF entities, allowed more flexible speech patterns to be used when issuing commands, and increased the ability to refer to synthetic entities in a more natural way.  In addition, we improved the way behaviors are instantiated within the SAF program called Dismount Infantry SAF or DISAF.  Overview of the Natural Language ProcessingThis section provides an overview of automated Natural Language Processing (NLP) processes and issues.  The NLP process can be broken up into three phases.  The Speech to Text (STT) phase requires that spoken words be recognized and translated into text.   The Natural Language Processing (NLP) phase attempts to process the incoming text, determine its meaning, process any new information provided by the text, and determine what response is appropriate.  The response itself may be in the form of an action or a spoken reply.  Replies are generated in a text format during the NLP phase and converted to audible speech in the Text To Speech (TTS) phase.  In actuality, the three phases are not distinct.  Unlike formal languages, the syntax and semantics of natural languages cannot be separated.  This means that the STT and TTS phases (primarily thought of as syntactic) require information gained in the NLP phase (primarily thought of as semantic) and vice versa.Speech To TextThe STT phase converts the spoken word to text, essentially by processing a signal from a microphone and pairing it with text words in a dictionary.  Since pronunciation of words vary widely between people and situations this is a difficult task by its nature.  There are several COTS products available for use.  IST has used Dragon NaturallySpeaking (DNS) and IBM’s ViaVoice products in connection with the VF program.  These products can achieve upwards of 90% accuracy rates when optimized for a specific user assuming a specific speaking style.   IST has experienced accuracy rates as low as 50% for the applications at hand in which a new user attempts to issue short commands to SAF entities.  We attribute this to the use of untrained users speaking to a system not optimized for their speech patterns, which is a typical use for a training program with high user turnover.  This is in keeping with accuracy reported in other systems.  For example, users reported a 66% success rate with an automated telephone information system in which callers spoke their requests.  The success rate for a first-attempt request was 30%.  IST has not experienced a change in accuracy due to the presence or absence of background or ambient noises during system use.It is a goal of this project to investigate ways to improve upon the low success rates that can be seen when untrained users interact with a system not trained to their speech patterns.    IST is investigating the use of filtering and flexible parsing software to map perceived speech patterns to those that fit the limited domain of the application.    In one sense, the STT system is the most important component of a project, since it is single point of failure.Text To SpeechThe issues dealt with in this area have to do with producing correct pronunciations and realistic intonations while converting the text to audible speech. There is little chance for producing the wrong word or pronunciation, unless specialized jargon is introduced. Pronunciations for standard words can be stored in a dictionary.  Realistic intonations are difficult to achieve, but typical COTS systems allow for difference in male and female speech patterns and can recognize the difference between statements, questions, and exclamations.  IST is using the Microsoft Speech API product for converting our text output messages to speech.  This product has been sufficient for our needs as TTS was not the focus of our research effort.  Minor modification of the dictionary content was required to obtain understandable output.  One lesson learned is to keep audible error messages short and use them sparingly so as not to interfere with continuation of the exercise.  Knowledge and Awareness[NOTE: This section is incomplete but will sketch common KR techniques.  The main goal is to provide a definition of awareness levels as developed in Pew and Mavor.  Knowledge representation schemes include Rule-based, relational, hierarchical classification schemes.  ]Natural Language Processing[NOTE: Incomplete but will discuss that most systems involve Finite State Machine models. And true NLP systems have the ability to allow queries by the user, request clarifying information from the user, and process elliptical utterances (“Close the window”  “Which one?” “That one” it is understood after the first sentence that we are talking about a window even though it is not mentioned past the first sentence).Focus on tree structure and assignments to parts of speech.  Note that still basically syntactic process. Semi-Automated ForcesThis project is targeting Semi-Automated Forces  (SAF) simulations.  SAF’s are CGF’s that require input from an external entity to react to situations.  SAF simulations such as the OneSAF TestBed (OTB) and the Close Combat Tactical Trainer (CCTT) model individual vehicles and soldiers.  Traditionally, the external entity is a human operator though experiments have been performed whereby an external software agent provides input to control SAF entities. The SAF system we have been using is the Dismounted Infantry SAF or DISAF.  This system was developed and is maintained by SAIC.  It is a  variant of ModSAF.  The DISAF command input, format, and structure is very strict.  Therefore, we have limited our commands to a small subset of the total commands available.  We continue to working with SAIC to have the Voice Federate required changes to be incorporated into the DISAF releases.  The characteristics of SAF simulations are as follows.  The SAF entities have traditionally been controlled by activating existing behaviors via a series of mouseclicks and keyclicks.  The SAF entities have no sense of self-awareness in an intelligent sense; they simply initiate and terminate behaviors as triggered by operator input and simulation events.  In terms of vocal interactions this results in a rigid command structure that may only initiate and terminate existing behaviors with no capacity for an interactive informational exchange between human and synthetic entities.   Natural Language Voice Interaction (NLVI) with Semi-Automated Forces (SAF) Project[NOTE: This section is essentially complete except that the presentations of the system parts each need a brief introductory sentence to place them in context.]ST was awarded the Voice Interaction with SAF project in May 2002.  The goal of this project is to create a  prototype Natural Language Voice Interface (NLVI) allowing natural language communication with Computer Generated Forces (CGF).   There are two main goals for this prototype.  The first goal is to prove than it is possible to correctly interpret, a limited set of non-scripted speech.  The second goal is to develop a flexible system which can easily be modified to add functionality or target different SAF systems.The NLVI builds upon STRICOM’s Reusable HLA Voice Input/Output Capability for Human Simulation (otherwise known as the Voice Federate, or VF) program.  The VF program provided the ability for live participants in a simulation to perform Command and Control over CGF forces, and for the CGF forces to generate limited synthetic speech.  The VF supported a relatively small speech set and was restricted to scripted language patterns.  The NLVI will develop and demonstrate prototypical natural language features to allow live participants to interact in a natural manner with their CGF counterparts.  The goal is to demonstrate the feasibility and effectiveness of allowing live participants in a simulation to converse naturally with their CGF counterparts.  Due to the complexity of this task, the scope of the conversations supported by the NLVI will focus on verbal exchanges inherent in carrying out squad-level and fire team-level missions.A functional diagram of the NLVI is  shown in  REF _Ref151728 \h  \* MERGEFORMAT Figure 1.   The human participant speaks naturally (language Ln).  This speech is converted to a text sentence and then transformed into a sentence in the more restrictive CGF language (language Lc).  This Lc sentence must correspond to a command or query recognized by the CGFP.  The CGFP has the ability to generate speech requests in Lc.  These requests are transformed into natural language text prior to processing by a text-to-speech processor.  The conversion between Ln and Lc are performed with the use of a World Data Base (WDB) that stores context information to associate natural language references with entities in the CGFP.  The result from the human user’s point of view is a natural language conversation with the CGF entities.  This approach makes use of established principles of automated language compilation and transformation, such as used in state of the art compilers.Language RepresentationsHumans communicate using natural languages.  Computer programs, including CGFs, are written using formal programming languages.  Natural languages differ from formal languages in several key ways.  Natural languages are far more expressive, tolerate ambiguity, and intertwine syntax with semantics.  The NLVI must translate between the natural language used by humans and the formal language required by the CGFP.  These languages are referred to as Ln and Lc respectively. Natural Language (Ln): This is the language spoken by the human participants.  It is also the language in which the audible synthetic speech is generated.  The characteristics of this language are flexible syntax and ambiguous semantics.  This language will grow out of the existing VF speech set and be restricted to topics related to carrying out missions at the squad and fire team echelon level. CFG Language (Lc): This is the language used to interface with the CGFP.  It allows commands and queries to be input to the CGFP while the CGFP can initiate synthetic speech cues.  The characteristics of this language are inflexible syntax and unambiguous semantics.  This language will grow out of the commands used within the CGFP.System Input/OutputSpeech to Text Software (STT): This component will make use of existing COTS language recognition software to translate streams of voice into text to be passed on to the natural language processing software.Text-to-Speech Software (TTS): This component will make use of existing COTS software to translate text sentences into audible vocalizations.  Natural Language to CFG Language Translator (Ln-LcT): This software uses recognized language patterns in context with the WDB to transform sentences in Ln to Lc.  The natural speech is supplied to the Ln-LcT as text strings provided by the STT.CGF Language to Natural Language Translator (Lc-LnT): This software transforms synthetic speech cues written in Lc into natural language sentences written in Ln.  The WDB is referenced as part of this transformation.  This software performs the opposite transformation as the Ln-LcT.  This software allows for non-scripted synthetic speech generation to be developed.  Since it is modular, future versions could be extended to include personality and emotional traits without affecting the rest of the system.The goals of this project are to:The NLVI will benefit from the work performed on the VF by reusing as much of the VF products as possible.  In addition, COTS products will be used as much as possible.  The NLVI will be modular in design to support reuse from both the system and component point of view.  The inherent design of the NLVI will not be dependent upon any specific COTS product or CGF platform (CGFP).Figure  SEQ Figure \* ARABIC 1. Functional Diagram of the NLVIA fuller description of each functional component follows.Natural Language Vocal Interface (NLVI): This is the sum of all components described below.  The system allows human participants in a simulation to interact vocally with CGF entities in a natural conversational fashion.World Data BaseWorld Data Base (WDB): This repository associates natural referencing of virtual objects and behaviors to the CGF objects and behaviors.  The WDB provides context and background information required to convert natural language (Ln) sentences into CGF acceptable commands (Lc).  It also is required to convert CGF speech cues into natural flowing synthetic speech.World Data Base Interface (WDBI): This software package provides a graphical interface for scenario designers to populate the WDB with meaningful associations.  It also queries the CGFP to automate the process of building the WDB as much as possible.CGF Interface (CI): This utility software package allows the Ln-LcT and the Lc-LnT to communicate with the CGFP.  This software performs data distribution functions with no language translation.  The CI allows synthetic speech to be directed at specific human participants or broadcast to the group.Natural Language Processor[NOTE: This section will contain a very brief decription of our NLP process, comparing to the types specified in Sect 3.4.]CGF SystemComputer Generated Forces Platform (CGFP): This is the simulation of choice used as a demonstration test bed for the research.  The most likely choice for this role will be SAIC’s Dismounted Infantry Semi-Automated Forces (DISAF) in order to maximize reuse from the VF project and match customer intent.  The remaining components of the NLVI will be able to interface with the CGFP but their design will not be dependent upon it in order to promote code reuse.  The CGFP must be modified to the extent that it can receive and act on commands as well as initiate speech cues to be transformed by other NLVI components into audible synthetic speech.One of our challenges when used this product at the Fort Benning Army Post, was the users ability to direct the SAF member to a desired location.  Therefore, we have since added the ability for the SAF member to follow the team leaders, who is a live participant in the scenario.  In addition, we added commands to have the team move in the eight cardinal directions, which are north, northeast, east, etc.  Previously, we had the user use waypoint for all desired movement locations.  These waypoints were difficult to remember and the user would pause, which caused the STT system to insert words.  We eliminated all waypoints except the center of each building.  Using this waypoint, which can be a logical name like police station, the user can request the SAF team to move relative to the building.  For example, the user would request his team to move northeast of the police station.  We additionally had the SAF entity response to commands given them.  They would reply “Wilco” when a command was understood.  Plus, they would reply “move complete” when the operation was completed.  We recently added spontaneous speech, which is automatically generated by the SAF entity.  For example, they would report a man down or when a new enemy force was observed. CGF Interactions[NOTE: This section will contain a brief description of the types of interactions the human can experience with the CGF entities.  Queries, conversational topics, and command and control capabilities. ]Summary[NOTE: This section restates main points of the paper, particularly the goals, techniques, and future work of the NLVI project and the future role of human-machine vocal communication in simulations in general.]References [Incomplete]Bruce W. Knerr, Donald R. Lampton, Michael J. Singer, and Bob G. Witmer & Stephen L. Goldberg “Virtual Environments for Dismounted Soldier Training and Performance:  Results, Recommendations, and Issues” Army Research Institute for the Behavioral and Social Sciences, Technical Report 1089, November 1998K. Garfield, R. Franceschini, S. Schricker, R. Urich, R. Alberdeston, J. Grosse, P. Dumanoir, B. Comer. "Modeling voice input and output for individual combatant simulations", Proceedings of the Tenth Conference on Computer Generated Forces, pp 447-456, May, 2001, Norfolk, VA.W. Foss, R. Hofer, R. Franceschini, R. Urich, I. Byrne, P. Dumanoir, J. Grosse. "Development of an HLA voice federate for individual combatants", Proceedings of the Fall 2000 Simulation Interoperability Workshop, September, 2000, Orlando, FL.Author BiographiesKEITH GARFIELD is a Research Assistant at the University of Central Florida’s Institute for Simulation and Training.  Mr. Garfield has 16 years of experience in the aerospace community with the Boeing Corporation.  He earned his B.S. in Aeronautical Engineering from Embry-Riddle Aeronautical University in 1983, and is pursuing a Ph.D. in Computer Science at UCF.  His research interests include formal language specifications for use in parallel computation and natural user interfaces with virtual environments.DONALD A WASHBURN is a Senior Research Scientist at the University of Central Florida’s Institute for Simulation and Training.  Mr. Washburn is the Principle Investigator for the Voice Recognition Federate, Haptics for Dismounted Infantry, Voice Interaction with Semi-Automated Forces, and Army Research Institute’s Virtual Environment Research Testbed projects.  He received his B.S.E. in 1982 and his M.S.E. in 1984 both in Industrial Engineering from University of Central Florida and he is currently pursuing a Ph.D. in Industrial Engineering specialty of Interactive Simulation.