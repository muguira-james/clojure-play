An RTI Performance Testing FrameworkRoger D. WuerfelJeffrey S. OlszewskiSAIC5400 Shawnee Road, Suite 110Alexandria, VA 22312703-333-5427 HYPERLINK "mailto:roger.d.wuerfel@saic.com" roger.d.wuerfel@saic.com HYPERLINK mailto:jeffrey.s.olszewski@saic.com jeffrey.s.olszewski@saic.comKeywords:RTI Performance, RTI Comparison, Benchmarks, Metrics. ABSTRACT: This paper outlines a process for developing a federation performance test plan and for conducting federation performance testing. Performance requirements vary for different types of federations and should be clearly defined during early stages of federation development. Common RTI performance definitions can assist federation developers in understanding federation performance requirements. In addition, the Federation Execution Planners Workbook (FEPW) can be used as a tool to characterize the operating conditions of the federation. Together this information can be used to develop a tailored performance test plan. To assist in executing this plan, the RTI benchmarks applications distributed with the DMSO RTI implementations are used. IntroductionPerformance is very often an important factor in achieving the objectives of an HLA federation.  If performance requirements are defined for a federation and the federation is unable to perform at the level of these requirements, then the results obtained from the simulation may be questionable or totally invalid.  In the case of human-in-the-loop training simulations it may result in negative training.  In an analytical simulation, poor federation performance could increase execution time and reduce the amount of statistics that can be collected. Poor federation performance in a testing simulation could result in test data that is unrealistic or inaccurate.The issue of federation performance is addressed by defining performance requirements, objectives, and metrics that are important to the specific federation. This is often a challenging task, and so it is desirable to have a performance framework that can provide a structure for defining federation performance using existing metrics and performance measurement techniques. This paper provides such a framework by outlining a process for determining performance metrics important to a federation, developing and executing a performance test plan, and assessing the results.  This framework can be tailored or extended to meet the needs or the federation by adding, subtracting or customizing performance metrics to meet the needs of the federation developers.Before a framework can be constructed, however, one must first examine the series of steps that are typically needed to specify and measure federation performance. Such a process for measuring performance can be roughly summarized as follows: Determine what aspects of functionality and behavior are most important to the federation.Of these items, determine which items can be quantitatively measuredFor each item which can be measured, define a corresponding performance metric Express the performance requirements of the federation using these metrics, including thresholds or limits on eachDevise a method by which each of the performance metrics can measured in the federation’s operating environmentCreate a performance testing plan which details the data that will need to be collected to assess performanceExecute the plan, collecting performance metric dataAnalyze the performance data to determine whether the federation meets the performance requirements in the operating environmentSuch a process is likely to be iterative. As a greater understanding of the simulation’s performance in the operating environment is achieved, new performance metrics may need to be devised.  If performance testing reveals that the simulation fails to meet the stated performance objectives, the operating environment or the performance requirements themselves may be adjusted and additional performance tests conducted. It is very likely that several iterations of the final three steps listed above will be necessary to achieve the desired federation performance.HLA federations have many common characteristics, which makes construction of a performance framework possible, and can make the framework a valuable addition to the suite of tools available to a federation developer. Each HLA federate contains an RTI component, and so tools for determining how well a particular RTI implementation performs in the target operating environment are useful. HLA federations are typically distributed across many hosts, so performance testing tools and metrics need to take this into account.    The performance frameworkThe sections that follow outline a process for planning and conducting federation performance testing. A performance framework will be developed that will aid federation designers in documenting and measuring the performance of their federation.  This documentation can used to determine if the federation meets its performance requirements.Relationship to the DMSO FEDEPThe DMSO Federation Development and Execution Process (FEDEP) [1,15] describes a common approach for creating new federations. While developing federations using the FEDEP is not a requirement, it does provide a tailorable framework that outlines the tasks typically needed to create a new federation. The performance framework can be integrated with this process, or used independently of the FEDEP. Throughout the discussion below, the relation of phases of the performance process are related to the corresponding phase in the FEDEP.  The result is a process which roughly overlays the FEDEP process.  This technique has been used for other types of federation testing.  See [10, 11, 12, and 13].  The analyst’s role as outlined in [13] is especially important as they will be the authority that determines the validity of the collected data and will most likely be doing the performance analysis.Regardless of the federation development approach used, it is recommended that federation developers begin to address their performance objectives and requirements as early as possible in the development cycle.Defining Federation Performance RequirementsThe first step in the process is to document the performance objectives and requirements of the federation.  It is helpful during this process to make distinctions between performance objectives and performance requirements. A performance objective is a desired performance goal that the federation will attempt to achieve.  A performance requirement is a specified performance target that the federation must achieve.  Performance requirements are much less flexible and are often specified by the sponsor or users, or can be implied by other performance objectives or requirements.  The primary focus of performance testing is to address the performance requirements. These objectives and requirements will accumulate during several phases of the federation development.  The initial set of requirements will come from examining the federation’s objectives and defining the federation conceptual model.  More performance requirements will then be added as the federation requirements and design are documented.Federation ObjectivesDocumenting of performance requirements can begin during the formulation of the objectives of the federation, and can be incorporated into the Federation Objective Statement of phase 1 of the FEDEP.  The Needs Statement of the federation will specify the “high-level descriptions of critical systems of interest, coarse indications of fidelity and required behaviors for simulated entities, key events that must be represented in the federation scenario, and output data requirements”[1].  It will “indicate the resources which will be available to support the federation (funding, personnel, tools, facilities, etc.) any known constraints which may affect how the federation is developed”[1].  This statement will then be refined into the Federation Objective Statement that translates “high-level user/sponsor expectations into more concrete, measurable federation goals”[1].  Fidelity and behavior requirements can often be translated into performance requirements that should be documented in the Federation Objective Statement.  Each measurable goal and any given operational constraints should be examined to determine if they translate into any performance requirements that are known to be unachievable.  Examples of operational constraints include network bandwidth or latency limitations, and limitations on the availability, types and numbers of hosts available for federation execution. Identifying and rectifying these by adjusting fidelity expectations or the operational constraints at this early stage will reduce the risk of sponsor or user dissatisfaction because of performance problems.Federation Conceptual ModelIn the development of the Federation Conceptual Model (phase 2 of the FEDEP), “federation objectives are transformed into a set of highly specific federation requirements for use as success criteria during federation testing” [1].  The fidelity needs of the federation are a good starting place to look for performance requirements.  As stated in [4], "the levels of fidelity provided down to the level of individual transactions between objects in a simulation, and federates in a federation should be explicitly addressed".   The fidelity levels and object transactions should be examined to find performance constraints upon which the fidelity specification may depend.  In addition an analyst should be involved to identify “where the assumptions lie in the conceptual model and how sensitive the results will be to those assumptions” and to define the measures of effectiveness (MOE) and measures of performance (MOP) for the study [13].  Many MOE’s and MOP’s map directly to performance requirements.  A specific quantifiable criterion should be associated with each performance requirement identified so that it can be used to determine if the requirement is satisfied during a later phase.During scenario development, the number and types of entities and their capabilities and interactions over time are defined and documented.  “Dynamic relationships should include (if appropriate) the specification of temporally ordered sequences of object interactions with associated trigger conditions.” [1]  The temporal aspect of these interactions and triggers often imply latency constraints.  Each interaction and trigger should be thoroughly examined for explicit or implied latency constraints between interactions or between a trigger and its causally related  actions.  Also, each interaction should be examined in reference to the simulation environment to determine if there are any latency constraints between an interaction and other part of the system.  This can especially be true when the federation contains humans or hardware in the loop.  Any reasons for which the federation execution should be terminated due on performance problems must be documented as well.  Federation RequirementsThe performance related aspects of the Federation Conceptual Model will serve as the description of the performance requirements that are necessary to be met in order to achieve the objectives of the federation.  These will then lead to testable federation performance requirements that can be used to design and develop the federation performance tests.  Version 1.4 of the FEDEP specifically mentions that fidelity requirements should be considered in the requirements in relation to selection of the federates.  Many performance requirements are related to fidelity requirements, therefore the references to fidelity requirements in the FEDEP generally apply to performance requirements.  See [4] for more details on fidelity requirements.Federation DesignThe first steps of federation design (phase 3 of the FEDEP) are selecting the federates and allocating functionality to those federates.  Ideally, it would be useful at this point to simulate the behavior of the federation at a high level in various configurations in order to evaluate the performance implications of designing the federation in a particular manner. For example, the impact of introducing WANs, the effects of latencies between various federate hosts, and the effects of introducing cluster computing, all can effect the overall performance of the federation. Unfortunately no such model or tool is currently available, so the developer must rely on experience and domain knowledge to efficiently design the federation. However, once the objects are assigned to the chosen federates, the Federation Execution Planning Workbook (FEPW) can be filled out to document the federation and provide input for the last phase of federation design related to performance: the creation of the federation performance requirements checklist, which will be discussed below.Selecting Federates and Allocating FunctionalityThe fidelity and performance requirements should be major factors in determining which federates are selected and what entities they are responsible for.  Federates should be selected based on their ability to model the needed entities with the fidelity and performance characteristics necessary to meet the federations objectives and its performance requirements.  The “Report from the Fidelity Implementation Study Group” [4] should be used as a guide for selecting federates with regards to fidelity.  An analyst should also be involved in the selection of federates to ensure that the fidelity of the chosen federates will generate valid data necessary to calculate the MOEs and MOPs [13].  Also see [19] for recommendations for planning data collection.The valid performance of simulation models is dependent upon the integrity of their input data. Accuracy and timeliness are two aspects of the integrity of input data that are important to the correct execution of a model.  Accuracy can be degraded by dead reckoning, which in turn is affected by the timeliness of the data.  The timeliness of the data is influenced by the message latency of the system, federation design, and RTI use.  Reflected data used as inputs to a model should be examined to determine any performance constraints imposed by the model’s algorithms.  Input data performance constraints vary with the type of model and the algorithms used and can be determined from the model’s documentation, a domain expert, or knowledge of the algorithms used.  The federation designers may need to perform design tradeoff studies at this point to determine if the performance requirements are achievable and possibly relax some constraints based upon the actual achievable performance as long as the federation’s objectives can still be met.Federation Execution Planners WorkbookAt this point the physical and modeling characteristics of the federation should be documented with the Federation Execution Planners Workbook (FEPW). The FEPW provides a series of tables to document the characteristics of each federate in the federation. These characteristics include what object attributes and interactions each federate will publish and subscribe, the hardware and software requirements for each federate, and the federation’s network topology.  See [2] and [6] for more details about using the FEPW to document a federation execution.The object/interaction tables of the FEPW include some important performance-related data.  The entries for each attribute contain the update group that it belongs to and the nominal and maximum rate at which that these attributes will be updated.  This value is usually determined by the rate at which the model that calculates the attribute is updated on the sending federate.The reflected attribute entries contain the maximum tolerable latency for the attribute.  This value is more difficult to determine and could be based upon objectives of the federation or constraints from the models that use the reflected attributes as input as discussed above.  Determine the federate or federates that supply the data for each of these performance constraints.  Add an item to the federation performance requirements list, as defined in the next section, for each constraint.  The item should be defined in terms that can be quantitatively measured. See [3] for a detailed study of simulation latencies and their effects and [5] for an example of determining latency requirements.Once each federate’s characteristics and the federation’s publication and subscription topology is defined, additional performance requirements may be determined.  Some of these requirements, such as the attribute latency requirements, can be recorded in the FEPW, but in order to verify that the federation meets all of the performance requirements it is suggested that a federation performance requirements checklist be created that lists all of the performance requirements.  This checklist can then be used during federation integration and testing to insure that all the performance requirements are met.  This checklist can be incorporated into the federation’s test plan.Federation Performance Requirements ChecklistThe last step of federation design is preparing the federation performance requirements checklist.  This checklist should map the performance requirements discovered in the previous steps with metrics, constraint values, and measurement methodologies.  The checklist should be used during execution and post processing of the data to insure that each condition is checked before the results are deemed valid.  This checklist can be a simple table that lists the following for each requirement:A brief description that summarizes the reasoning behind the requirementA quantitative constraint value that will be used to determine if the requirement has been metWhether it requires pre-execution or run-time verificationThe specific metric and units of measureThe measurement methodologyThe tool used to obtain the measurementsAn indication of whether this is a performance requirement or performance objectiveThe description should include enough detail to trace back to the initial source of the requirement, such as a user federation objective or a constraint upon input data for a model.   It should also describe the source of the constraint value.Each item should be marked as requiring either pre-execution or run-time verification.  A pre-execution verification item needs to be checked to insure that the federation meets its constraints before the federation can be used to collect data to satisfy the objectives.  This type of requirement should be verified during federation testing.  A run-time verification item is a performance requirement that will need to be monitored during execution to determine if the execution should be stopped prematurely. If this item exceeds its constraints then the current execution or the data collected from the current execution should be considered invalid. These values depend heavily upon how the scenario unfolds during the execution.  Some federations may be able to perform worst-case pre-execution verification tests to reduce the need for run-time verification.Run-time constraints are the most critical and difficult requirements to determine and quantify.  Many things need to be considered in defining a run-time constraint.  If the constraint is exceeded during one time-step of the federation, does that invalidate the data or does it need to be exceeded several times?  Is the constraint a hard or soft value?  Is its value dependent upon other values?Next the metric, the measurement methodology and the measurement tool to used for each requirement should be listed. If possible, a common performance metric as documented in [8] should be used since they have well defined meanings, measurement methodologies, and measurement tools.  If it is necessary to create metrics, then they should be documented separately with a definition that follows the format of the common performance metric definitions.  Metrics should have unambiguous definitions and measurement methodologies that can be implemented as a test federate.A wide variety of performance metrics is available to the federation developer. For the purposes of discussion, metrics can be grouped into two categories: transient metrics and summary metrics.   Transient metrics are used to characterize performance during federation execution. Examples of transient metrics include:Sampled latencyHost Memory UsageHost CPU UsageTime Advance Grant RateEtc.Summary metrics are used to characterize performance after the federation execution completes.Mean Update ThroughputMinimum Update ThroughputAverage Packet OverheadEtc.In [8] the authors developed an initial set of definitions of quantitative metrics which can be used to characterize the performance of an implementation of the RTI.  These definitions are useful for looking at raw RTI performance.  However, they do not necessarily represent the performance of a real federate.   Real federates are too varied in their execution characteristics to be represented by a simple benchmark.  The following sections outline the considerations for selecting a performance measurement tool.Common BenchmarksThe benchmarks supplied with the DMSO RTI implementations can be used to collect data for some of the items on the list.  When this is possible, the command line arguments should be used to simulate the actual operating conditions as closely as possible.  The test runs should also be executed on the same hardware and network as the real federates will be using.  Use the Object/Interaction Tables along with the grouping of the attributes and interactions to estimate the average size of the updates that will occur on the sending federates and use this as the update size parameter on the benchmark.  The number of updates per cycle should be large enough to obtain comparable values for different runs and the number of cycles should be large enough to obtain a sample large enough to make valid statistical inferences about the data. Custom benchmarksCustomized benchmarks may have to be written if the common benchmarks don’t simulate the operating environment closely enough or if the needed metric is not measured by any of the common benchmarks.  The benchmarks operate differently that most normal federates and therefore may not be able to accurately predict some behaviors.  The common benchmarks measure fundamental performance of RTI implementations.  Many federation-related metrics may involve measurements of the RTI and federate-related execution therefore the common benchmarks cannot capture all the necessary information.  Nonetheless, depending on the metric, the common benchmark code can be customized to reduce the time spent developing new benchmark applications.  For example, it is a fairly simple exercise to convert the update throughput benchmark code to measure interaction throughput.Federate data collectionPerformance analysis should be built into federates for the run-time verification performance requirements that are critical enough that the federation execution should be stopped or the results can be marked as questionable if the performance requirement is not met.  These measurements can be embedded in the federate’s code and either controlled by conditional execution or eliminated by conditional compilation.  For any item that is critical during execution it may be necessary to allow the monitoring of the value during execution to determine whether the execution should be stopped if the requirement is not met.  This can be accomplished by use of tools such as the Data Collection Tool (DCT). [19]The JADS program found that sometimes the test federates did not accurately represent the performance of the actual federates [7].  They reported that the test federates had a higher stability and that only the actual federates with representative data could accurately predict run-time performance.  It is therefore suggested that even if there are no run-time verification performance requirements that are critical enough to terminate execution, that some performance monitoring be built into the federates to verify that the data collected from the common and custom benchmarks was applicable.RTI SelectionOnce a performance checklist has been determined the critical fundamental performance requirements that are placed on the RTI itself should be determined.  These are necessary as metrics to use as selection criteria for an RTI implementation.  Attribute throughput and message latency are the most common metrics for many federates. The main concern of doing a comparison of RTI implementations is insuring that the comparison is fair.  Internal differences may make a fair comparison difficult.  For example, the 1.3v6 implementation will buffer updates in memory if the network socket cannot be written to.  The updates will not get sent until either the next call to updateAttributeValues() or tick().  This presents a problem in the measurement of throughput if some of the time to send the updates on the network is not measured.  This can inflate the throughput because instead of incurring the overhead of the network send, it only incurs overhead of a memory copy.  Note also that this approach introduces additional federate memory usage that is not taken into account by the throughput benchmark and can increase message latency.Another factor in conducting a fair comparison is to make sure that the RTI’s are configured as closely as possible.  For example, the RTI 1.3v6 and 1.3NG RID file contains parameters to control the bundling of updates.  Each has a bundle size and time out that for a fair comparison need to be set to the same values.  Matching these parameters may not be straightforward.  Using its SingleThreaded mode, RTI 1.3NG uses a select on the socket for reading data, while RTI 1.3v6 uses a polling method.  For RTI 1.3NG the RID file contains parameters to control whether or not the tick() method enters select if tick() was called within the time parameter.  RTI 1.3v6 has no such parameter but does have a polling interval that controls how often the socket is polled during a call to tick().  Since it is unclear how to set these two parameters, each should be optimized for the intended federation.The computers and network should be the same for the comparison.  The extraneous network traffic should be minimized as much as possible during data collection runs.  If it cannot be minimized, it should at least be as consistent as possible for each run. Developing federation performance test plansThe federation performance test plan can be developed now that the relevant information has been documented in the federation performance requirements checklist.  The purpose of the performance test plan is to outline a sufficient set of tests which can determine whether or not the federation can meet it’s performance needs in the operational environment.  A detailed performance test plan will be federation dependent, therefore a general plan will be outlined.The logistics of each test, such as performance test programs needed by each physical site in the federation, how the data should be saved, and what data needs to be collected should be documented in the performance test plan.  The data analysis process should also be worked out to insure that the needed data is collected and can be collated from separate data files if necessary.Depending on the extent of the performance requirements, the performance test plan can range from simple to complex.  For more complex test plans, it is helpful to structure the test plan as a series of dependent tests, ordering them in such a way that the most important performance requirements are tested earlier in the test process. One of the motivations for preparing and documenting the test plan is that since the performance testing is an iterative process, it is very likely that a test plan will be reused.  If the initial testing reveals a certain set of performance bottlenecks during federation execution, steps can be taken so these bottlenecks can be eliminated or their effect reduced.  The test plan can be reused to insure that the performance improvements were helpful and did not introduce other problems.Creating Tests Using the Common BenchmarksThe common benchmarks are useful for performance testing because the simple command line interface allows scripts to easily be developed which will executed in a particular configuration.  Runs can easily be repeated as the operating environment configuration changes, and comparisons can be made between runs to detect changes in performance.As outlined in [9], the benchmarks can be used in a test plan to permit RTI comparisons. A variety of tests are possible using the benchmarks by varying the number of federates as well as the command line parameters.  These parameters include attribute size and the number of objects per federate. Specifying Data Collection Needs Enough data should be collected to enable statistically meaningful conclusions to be drawn from the results.  Reference [13] provides useful guidance on  techniques for collecting federation performance data.The test plan should indicate for each test the number of samples requiredthe format of the output (required fields, precision, sort order, file naming conventions, etc.)The host or hosts on which the test is to execute and where the test data is to be storedAny required post-processing of the dataClearly defining this information can help to minimize confusion and workload during the data analysis phase.Specifying Data Analysis NeedsThe test plan should indicate what statistics will be used for analysis of the data.  These presentation of these statistics should be in the units specified in the performance requirements checklist for each performance metric.  Providing adequate statistical information is very important for a thorough analysis of the results. For stochastic processes, the mean, mode, maximum, minimum, variance, standard deviation, and histogram are typically calculated. Executing the test planIn the Integrate and Test Federation phase of the  FEDEP (phase 4) the federation is tested to insure proper functioning.   While a properly functioning federation is not a necessity for many types performance tests, it will be essential when tests are used which integrate with and measure federate capabilties. The test plan should take into account the availability of the federation applications for testing, and sequence the tests such that any testing not requiring the federate applications are performed early in the testing cycle.  Data collected during this period can be useful to the federation developers during integration.If repeated test execution cycles are likely, the test execution cycle should be automated as much as possible. This will minimize the amount variance in the test data introduced by those executing the tests.  As mentioned above, the common benchmarks are good candidates for scripted execution.  Assessing performance resultsThe results of the test plan execution will yield a series of statistics that can be used to determine the degree to which the federation can meet its performance needs in the target operating environment.  It is unlikely that the initial choice for a federation configuration will meet all these needs, so techniques are needed to identify which of the many possible variables should be adjusted to improve performance.  Again, depending on the complexity of the test plan, this step can range from trivial to complex. For simple test plans it may be a matter of comparing performance data from two or more RTI implementations in the operating environment. For more complex testing, one approach is to construct a hypothesis tree.  A hypothesis tree is an acyclic, directed graph, in which each node is a hypothesis.  A given hypothesis can have sub-hypotheses that are dependent on the empirical outcome of their parent hypothesis. The hypothesis tree can be used to organize a series of tests that are used to incrementally improve performance.  The outcome of a particular test may indicate one or more branches of the hypothesis tree can be ignored, or that a particular path through the tree is of greatest interest.  It can be incorporated into the test plan, and can be used to track the progress of the testing. It can also help to avoid repeating tests unnecessarily. Once a hypothesis tree has been constructed, it can be used to guide the subsequent series of tests. The sequence of steps of the previous sections are then repeated:The test plan is refinedThe test sequence is re-executed The test data is collected and analyzed, and the performance results are compared to the performance requirementsThe data collected after the test execution step in each case is used to refute or support the next hypothesis in the path through the tree.ConclusionThe above discussion outlines a process using a combination of existing and developer-built tools that together allow the federation developer to make decisions about federation performance in the operating environment.  This process will continue to evolve as additional testing tools are developed and new and better testing techniques are discovered.  References[1] “High Level Architecture Federation Development and Execution Process (FEDEP) Model”, Version 1.4, DRAFT, June 9 1999.[2] “Federation Execution Planner’s Workbook Editor” Version 1.3 User’s Guide, 28 May 1999.[3] Monson, Steven L., Johnston, Capt Ron R., Barnhart, Lt. David J., “Latency – The Adversary of Real-Time Distributed Simulation”, 19th IITSEC Conference Proceedings,  December 2-5 1997.[4] Gross, David C., “Report from the Fidelity Implementation Study Group” 1999 Spring Simulation Interoperability Workshop. Paper No.: 99S-SIW-167.[5] Wright, Maj. Darrel and Harris, Clyde, “Determining and Expressing RTI Requirements” 1998 Spring Simulation Interoperability Workshop. Paper No.: 98S-SIW-158.[6] Salisbury, Marnie R. and Seidel, David W., “Execution Planning for HLA Federations” 1999 Fall Simulation Interoperability Workshop. Paper No.: 99F-SIW-092.[7] McCall, Lt. Col. Mark, “Joint Advanced Distributed Simulation Test Results Briefing to RDE Forum” 1999 Spring Simulation Interoperability Workshop.[9] Wuerfel, Roger D. and Olszewski, Jeffrey S., “RTI Performance Metrics” 1999 Spring Simulation Interoperability Workshop. Paper No.: 99S-SIW-100.[10] Horst, Roberts, Old “Testing Overlay For The FEDEP” 1998 Fall Simulation Interoperability Workshop. Paper No.: 98F-SIW-1104. [11] Wright, Maj. Darrel and Zimmerman, Philomena M., “Test and Evaluation Federation Experience and Lessons Learned Using the Federation Development and Execution Process” 1999 Spring Simulation Interoperability Workshop. Paper No.: 99S-SIW-173.[12] Roza. Manfred, van Gool, Paul. Jense, Hans., “A Fidelity Management Process Overlay onto the FEDEP Model”, 1998 Fall Simulation Interoperability Workshop. Paper No.: 98F-SIW-083.[13] Pratt, Shirley, James Totten,. Leroy Jackson, Andrew Melton, “Analysis Process Overlay for the FEDEP” 1999 Spring Simulation Interoperability Workshop. Paper No.: 99S-SIW-142.[14] Zimmerman, Philomena, Kamsickas, Schandua, Youngblood, “FEDEP Tests & Processes” 1998 Spring Simulation Interoperability Workshop. Paper No. 98S-SIW-016.[15] Lutz, Robert. “FEDEP v.1.1” 1998 Spring Simulation Interoperability Workshop. Paper No.: 98S-SIW-236.[16] Page, Babineau. “ALSP JTC Case Study” 1997 Fall Simulation Interoperability Workshop. Paper No.: 97F-SIW-061. [17] Youngblood, S. “A VV&A Overlay to the HLA Federation Development Process”, Presentation at the Fall 1998 Simulation Interoperability Workshop.[18] Neuberger, T. “A Progress Report: Recommended Practices for Data Collection in HLA and Other ADS Environments”, 1998 Fall Simulation Interoperability Workshop. Paper No.: 98F-SIW-129.[19] Perkinson, Paul B., et al. “How to Plan and Execute Data Collection and Analysis for HLA Federations”, 1999 Spring Simulation Interoperability Workshop. Paper No.: 99S-SIW-176.Author BiographiesROGER WUERFEL is a senior software developer in the Distributed Computing Technology Division of SAIC, and is currently working on the development team that is building the RTI 1.3 Next Generation implementation.  He received his B.Sc. in Aeronautical/Astronautical Engineering from The Ohio State University.JEFFREY OLSZEWSKI is a senior computer scientist in the Distributed Computing Technology Division of SAIC, and is currently working on the development team that is building the RTI 1.3 Next Generation implementation. He received his B.Sc. degree in computer science from the University of Pittsburgh, and his M.Sc. in Computer Science from the George Washington University.