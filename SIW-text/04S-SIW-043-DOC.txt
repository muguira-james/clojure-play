Architectures for Modeling the Space EnvironmentRobert A. ReynoldsNorthrop Grumman Information Technology55 Walkers Brook Dr.Reading, MA 01867781-205-7579 HYPERLINK "mailto:robert.reynolds@ngc.com" robert.reynolds@ngc.comKeywords:environment, space, feature, effect, distributed, centralized, architecture, JSB, ionosphere, scintillation, GPSABSTRACT:  The Joint Synthetic Battlespace, Air Force (JSB-AF) sponsored, through its GFY’03 Analysis of Technical Alternatives (ATA) initiative, fundamental research within several areas of simulation technology considered vital to the JSB vision, including improved representations of the natural environment and integrated space system representations.  This paper describes research that Northrop Grumman Information Technology performed for JSB-AF on the analysis of technical alternatives for modeling space environmental features and effects within the context of a distributed simulation.  The paper describes two closely related sub-studies.  The first sub-study focused on identifying the features of the near-space environment that affect the largest class of military communication, navigation and surveillance systems, and the authoritative models available for these features and effects.  The second sub-study focused on identifying a candidate set of simulation architectures for modeling the features and effects selected by the first sub-study.  The candidate simulation architectures, consisting of a number of combinations of centralized and distributed feature and effect models, were systematically evaluated based on considerations of scalability, complexity and adherence to standards.  The paper concludes by recommending two candidate architectures for further study.Features and EffectsThe distinction between environmental features and environmental effects is fundamental to this study.  The definitions used are those developed by the Environmental Effects in Distributed Interactive Simulation (E2DIS) project.  Within the E2DIS definition, features refer to objects which define or describe the state of the atmosphere, ocean and/or space (AOS) environment.  A feature may be a localized entity such as a cloud, thunderstorm or solar flare, or it may be an ambient property such as wind, humidity or magnetic field.  Unlike the terrain, features of the AOS regimes have important time variation across their full spatial extent; these distinctions motivate the different approach usually taken to the modeling and simulation of the AOS regimes relative to the terrain.Environmental effects refer to the impacts that environmental features have on other objects within the simulation and, within the current study, are recognized to fall into one of three broad categories:Effects on Military Entities – examples include effects of turbulence and icing on aircraft dynamics, effects of waves on ship roll, pitch and heading, and effects of temperature and humidity on troops and required water rations.Effects on Electromagnetic and Acoustic Energy and Signals – examples include attenuation of visible, IR and laser frequencies by clouds and water vapor, effects of water temperature and salinity variations on acoustic signal propagation, and ionospheric effects on radio-frequency communication and navigation signals.Effects on Other Features of the Environment – examples include precipitation effects on soil strength and ocean acoustic background noise, and  wind effects on ocean wave height and on the transport of chemical and biological agents.The study described by this paper is focused on  the geospace environment.  The current reality is that warfare is not conducted directly in geospace,  and that coupling between the geospace and atmospheric regimes can be neglected within a simulation context. Hence, this study is concerned solely with effects in the second of the three groups identified above.Structure of StudyThe study described by this paper was conducted as a three-phase effort, with these respective goals:Identify the geospace features and effects that impact military systems, and determine which among these candidate features and effects are the most important to model initially within a JSB prototype.  Identify the models that have been developed by the space science community for the selected features and/or effects, and which would be available to a JSB prototyping effort.Identify a set of integration strategies for incorporating the selected feature and effect models into a JSB prototype simulation capable of supporting simulation based acquisition (SBA) as well as training simulations with real-time performance constraints.Sections 3 and 4 respectively present the results of the analysis done in support of the first and second phases. Section 5 documents the results of the sub-study on alternative integration strategies.Geospace Features and Effects Relevant to Military SystemsWithin this phase, the study leveraged the work done by ARINC Inc. for the Air Force Space Command and 50th Weather Squadron [1].  While developing a Space Weather Training Program, ARINC produced this flowchart --  HYPERLINK "http://www.agu.org/eos_elec/flowlogo.gif" www.agu.org/eos_elec/flowlogo.gif -- which, although it is not a numerical or computer model, has come to be known as the “Space Environmental Impacts Model” (SEIM).  The flowchart pulls together, summarizes, and organizes the work of many researchers in a concise and consistent manner.  Observable space environmental phenomena are identified within the boxes on the left side of the flowchart, and resulting impacts are shown on the right.  Linkages enter the various boxes from the left or at the top, and they leave the boxes to the right or at the bottom.Within the context of the current study, one can view the SEIM as a mapping from 10 geospace features to 25 geospace effects.  The key question: “What subset of these 25 effects are important to model (initially) within JSB?”  Once this subset is identified, the features that need to be modeled can be determined from the chart.  Towards this end, the 25 effects were analyzed and grouped into these three categories:Physical damage to and/or degradation of orbiting hardware and/or personnel:Physical damage to spacecraftIncreased neutral density and drag on LEO satellitesSpacecraft surface charging and dischargeHazards to exposed humans in spaceSingle Event Upset (SEU), satellite disorientationInternal satellite charging / damage to internal electronic componentsDifficulty unloading “dumping” torqueDeterioration of spacecraft surface materials, sensors and solar panelsGeostationary magnetopause crossingDirect thermal input to low temperature (IR) sensorsFalse star sensor readings, attitude control problemsInterruption to and/or degradation of navigation, communication, control and/or radar signals:Range errors on LF and VLF navigation systemsSpacecraft commanding problemsShort wave fadeoutSeverely degraded HF propagation at high latitudesRadio frequency interference (solar noise)Radar auroral clutter, noise and interference Location errors on Navstar GPSSignal fade, change in phase, polarization, angleDepressed maximum usable frequencies (MUFs)Radar range, range rate and direction errorsNon-great circle HF propagationSeverely degraded HF prop. on transequatorial pathsDirect effects of the magnetosphere on electrical and electronic systemsGeomagnetically induced currentsFluctuating geomagnetic fieldThis grouping was found to be very useful in paring down the list of candidate geospace effects to be modeled initially by JSB.  Effects in the first category were given a relatively low priority on the grounds that these effects, while obviously relevant to satellite systems engineering,  do not seem directly relevant to the warfighter.  Training systems do not require models of satellite orbital decay and material fatigue as these processes occur over times much longer than those associated with the training.  Furthermore, it is argued that Simulation Based Acquisition (SBA) of space-based hardware assets is not a near-term priority for JSB, due to the current focus on Command & Control (C2) trades within the JSB objective of more rapid development & delivery of integrated warfighting capabilities.  With respect to effects in the third category, they impact these types of systems:Electric power transmission lines, petroleum pipelines and telecommunication cablesPrecision instruments, manufacturing equipment and computersMagnetic compasses, navigation and survey instruments (homing pigeons)Satellites that depend upon the earth’s magnetic field for reference in spin axis alignmentMany of these systems are civilian and in any case the direct effects of geomagnetic fluctuations are significant only when these fluctuations are enormous, corresponding to relatively rare events.  It is argued that these are not the most important effects to model initially within JSB.Hence, the sub-study focused on effects in the second category.  Figure 1 presents a “reduced” version of the SEIM:  effects in the first and third categories have been removed along with features that contributed only to those effects.  The effects that remain affect a large class of military communication, navigation and surveillance systems as indicated by the (reduced) list on the right side of the figure.Figure 1: Reduced Space Environmental Impacts Model Essentially all of the systems indicated in Figure 1 depend on the transmission and/or reception of energy in the radio-frequency portion of the electromagnetic spectrum (Fig. 2).  Broadly speaking, these systems either depend on the ionosphere to reflect radio signals to support long-range transmissions (VLF, LF and HF bands), or they depend on ionospheric transparency to radio signals to support satellite-based communication, navigation and surveillance (VHF through EHF bands).  The dividing line is the plasma frequency of the ionosphere: 		1)where N is the density of free electrons in the ionosphere,   EMBED Equation.3   is the permittivity of free space, and e and me are the charge and mass of the electron respectively.  The plasma frequency varies between approximately 1 and 10 MHz depending on the electron density.  Ionospheric electron density is a complex function of location, time of day, the state of the magnetosphere and the dynamics of the solar wind, to name only a few of the influencing factors.  Radio signals transmitted at or near the plasma frequency are strongly absorbed by the ionosphere.  Radio frequencies below (above) the plasma frequency are largely reflected (transmitted).Figure 2:  The Radio SpectrumThe analysis proceeded by ordering the remaining geospace effects on the basis of the number of military systems that they impact.  The top four effects are listed here, preceded by the number of systems affected:(11)  Radio Frequency Interference (Solar Radio Noise)(9)    Signal Fade, Change in Phase, Polarization, Angle(5)    Radar Auroral Clutter, Noise and Interference (2)    Radar Range, Range Rate and Direction ErrorsWith this “prioritized” list as a guide, attention was shifted to the problem of feature modeling.  Here the SEIM flowchart allows one to trace each effect back to the feature(s) of the earth-sun system that cause that effect.  The leading question posed during the feature analysis was: “How difficult would it be to integrate a model of a given feature, in combination with a model of the effects of this feature, within a prototype JSB over the initial build cycle planned at six months?”For the initial prototyping effort, the conclusion reached was that modeling Fluctuating Total Electron Content (TEC) and the effects of fluctuating TEC on transionospheric radio signals represent the best “first steps” into geospace modeling for JSB.  The reasons are:This feature/effect pair impacts 9 of the 12 systems identified in Figure 1, second only to radio bursts caused by solar flares.Total Electron Content (TEC) refers to the density of free electrons in the ionosphere; it is the N in Eq. 1 and therefore defines the plasma frequency.  Electron density is a primary parameter for describing the state of the ionosphere.Several models of the ionosphere exist in the public domain.  Integrating an ionospheric model with the JSB is an important step in supporting the modeling of systems that depend on the either the reflection of radio signals by the ionosphere or the transmission of  signals through the ionosphere.Fluctuating electron density, including the important phenomenon known as ionospheric scintillation, was the target of the modeling work done by the Space Warfare Center and USSPACECOM for the 1999 Joint Expeditionary Forces Experiment.  Work done by the environmental modeling and simulation community for that effort is available for reuse by JSB [2].In the next section the models available for TEC and scintillation are identified as well as the ones selected by this study.Models for Ionospheric Electron Density and ScintillationA wealth of Internet-accessible information exists on the subject of space weather and space weather models.  The ionospheric models reviewed within this study included those described on the websites of the National Space Science Data Center [3], Computational Physics, Inc. [4], Rice University [5], NorthWest Research Associates, Inc. [6]  and by the Community Coordinated Modeling Center [7]; the latter provides a centralized source of model information across the NASA, AFMC, AFOSR, AFRL, AFWA, NOAA, NSF and ONR organizations.Based on these reviews, two sets of candidate models were identified, one for ionospheric electron density and one for scintillation.  The identified electron density models are:International Reference Ionosphere (IRI) modelBent Ionospheric Model (1972)Chiu Ionospheric Model (1975)Penn State Mk III Model (1985)Semi-Empirical Low-Latitude Ionospheric Model (SLIM) Model (1985)Fully Analytical Ionospheric Model (FAIM) (1989)Parameterized Ionospheric Model (PIM) (1998)The identified scintillation models are:Scintillation Model (SCINTMOD) (1997)AF-GEOSpace system’s IONSCINT model (2002)The criteria used to select a density and scintillation model from these candidates were:Availability – the model should be available to the simulation community, preferably as source codeTheoretical vs. Empirical – models based on theoretical considerations were judged superior to models based on empirical data.Reuse Considerations – if software to support integration of a given model is available from a previous simulation effort, then this provides a discriminating factor favoring that model.The recommendation made by the study was that PIM be selected for electron density and SCINTMOD for scintillation.Model Integration ArchitecturesUp to this point in the study consideration has been given to a wide range of communication, navigation and surveillance systems, without referring to any one specifically.    However, the primary required output of the JSB-AF ATA effort was the specification of a prototype system, implementable within a six month development effort, that could be used to evaluate the alternative integration architectures deemed promising by the study.  Towards this end a specific military system was selected, both to facilitate a use-case analysis and to focus the initial prototyping effort.The GPS System as a Use CaseThe (Navstar) Global Positioning System (GPS) was selected as the use case.  The GPS system consists of 24 satellites, in well known orbits, continuously emitting navigation signals within the UHF band.  These radio signals travel trans-ionospheric paths to reach earth-based GPS receivers.  These receivers may be embedded in precision guided munitions, integrated with the central navigation systems of ships or aircraft, or hand held by a soldier on a time-sensitive targeting mission.It was judged that a simulation of the GPS satellite constellation could be developed within a six month prototyping effort, along with a simple model of a moving platform carrying a notional GPS receiver.  The prototype would be developed in such a way that a bona-fide GPS receiver model could be integrated under a subsequent effort.The geospace feature that would be modeled is the fluctuating electron density distribution of the ionosphere, constructed as the superposition of large-scale variations, based on the PIM model, with the fine-scale variations modeled by SCINTMOD.The geospace effect to be modeled would be the effect of the ionosphere on GPS signals, and would include 1) the phase delay resulting from the integrated electron density along the propagation path (TEC) and 2) the amplitude and phase distortions resulting from scintillation.  TEC would be based on line-integrals of PIM-computed electron density, while scintillation effects would be computed by  SCINTMOD.  The modeling of the position errors that result from TEC and the contribution of these position errors to, e.g., the modeled behavior of a precision-guided munition, would not be within the scope of the initial prototype.  Similarly, the behavior of a GPS platform that loses lock on a GPS satellite due to scintillation-induced signal distortion would not be modeled directly in the initial prototype.  These higher level effects would be developed in a straightforward manner in a subsequent effort, building on the initial prototype.The system recommended for prototyping is illustrated in Figure 3.  The prototype simulation would be HLA-based and would include 24 GPS satellites moving in their designated orbits.  The satellites would be modeled as objects and their emitted radio signals as interactions.  A configurable number of notional terrestrial receivers, which would move along paths with configurable geometry and velocity, would be modeled as objects.  Receivers would subscribe to radio emission interactions.  Candidate approaches to the integration of the PIM and SCINTMOD models with a synthetic battlespace built on the HLA – the focus of this sub-study – are described in the next section.Figure 3:  GPS Prototype IllustrationCriteria for Good ArchitecturesThe primary criteria used in the study for judging good integration architectures are composability, scalability and adherence to standards.To support composability, the architecture should allow for the construction of a wide range of different scenarios from a relatively small set of simulation components.  For example, one would not generally want a model of a precision-guided munition (PGM) and a model of an aircraft to separately implement an internal GPS receiver model, both because this is inherently inefficient/expensive but also because it is hard to verify that the PGM and the aircraft receiver models are implemented at the same level of fidelity as would normally be required for simulation interoperability.  To support scalability, the architecture should preclude any one simulation component from becoming a “bottleneck” as the number of simulation entities are increased.  For example, one does not generally want a single simulation to be responsible for modeling ionospheric effects on behalf of all of the GPS receivers in the Federation, given that as many as 12 satellites are visible to each receiver, resulting in as many as 12 different propagation paths.  Such a “bottlenecked” architecture would not generally support the real-time performance requirements of training systems.Within this study, an architecture is considered to adhere to standards when the interfaces between all simulation components are standards-based.  Two types of interfaces are of particular relevance to this study.  The first, based on the HLA and its use of a shared run-time data model -- the Federation Object Model – leads to simulation architectures based on many independently executing component simulations (Federates); sharing dynamic models and/or algorithms within this architecture tends to result in centralized “server-like” components.  The second, based on the use of (notionally) standardized Application Programming Interfaces (APIs), supports the concept of “distributed” dynamic models and/or algorithms that are linked with Federate code;  it is argued that these architectures are problematic due to the difficulty of establishing standard APIs across multiple platforms and programming languages.With these criteria in mind, consideration was given to two classes of integration architectures, and to six specific architectures within these classes:Features and effects modeled jointly:Distributed joint modelCentralized joint modelFeatures and effects modeled independently:Distributed feature model, distributed effect modelDistributed feature model, centralized effect modelCentralized feature model, distributed effect modelCentralized feature model, centralized effect modelDefinitions for “Joint”, “Independent”, “Distributed” and “Centralized” feature and effect models are given in the sections that follow.  These sections describe the pros and cons associated with each of these six alternatives.  This paper will then conclude with a recommendations for the  two integration alternatives that represent the best integration architecture.Features and Effects Modeled JointlyA Joint Feature/Effect model is defined herein to be an effect model that embeds a model of the environmental features that create the effect.  The primary reason for considering this class of architectures is the fact that both PIM and SCINTMOD are joint models by this definition.  This is remarkable – consider what is implied if a joint model were sought for the atmospheric regime.  A radiative transfer model such as the Moderate-resolution Transmittance (MODTRAN) code would need to embed  fluid dynamics and thermodynamic models of the atmosphere  (and ocean surface) such as those used by the Coupled Oceanographic and Atmospheric Mesoscale Prediction System (COAMPS).  Of course, because fluid dynamics and radiative transfer involve very different physics, they have historically been modeled independently.  How then is it possible for geospace models such PIM and SCINTMOD to model both features and effects?For the case of PIM, the answer seems to be that the model does not solve the equations of state directly, as COAMPS does, but instead uses sophisticated closed-form analytic expressions.  These are obtained by fitting a series of orthogonal functions to datasets which themselves are produced by a limited number of computationally intensive runs of “deep physics” models; the latter capture the physics at a more fundamental level. Using the closed-form expressions, PIM can readily compute the line-integrated electron density (TEC) along the line-of-sight between arbitrarily-located transmitter and receiver pairs.  TEC is then used to determine the phase delay on the GPS signal caused by the ionosphere; the phase delay results in an error in receiver-computed position.Given the availability of jointly-modeled features and effects, two integration strategies are considered.  The first, referred to as the Distributed Joint Model, is illustrated in Figure 4.  This figure introduces elements of the notation that will be used in the remainder of this section for describing integration alternatives.  Ovals represent component simulations, i.e., Federates. The ellipses (…’s) indicate that there are more Federates of a given type than are shown.  In the case of Transmitters, there are 24.  In the case of Receivers, there will be one for every GPS receiver in the simulation.  The arrows represent emission Interactions published by the Transmitters and subscribed to by the Receivers.A distributed model is defined to be an implementation of a feature or effect model that is shareable across the Federation.  Figure 4 illustrates a “distributable version” of the PIM model as a named partition of the Receiver model; the intent is to suggest a software library that is linked with the Receiver simulation.  Under these definitions, distributed models must provide an Application Programming Interface (API) for client simulations to use to invoke services provided by the model.  Note that the interface between the Receiver and PIM, as indicated by the vertical line in Figure 5, is intended to refer specifically to an API and not a more general/abstract interface that might be implemented by an available interprocess communication (IPC) mechanism.  Within this study, the intention is to restrict  alternatives to those for which all communication between system components is based on either a well-specified and documented API (for intra-Federate interfaces) or on the Federation Object Model (for extra-Federate interfaces).With the notation now defined, the analysis proceeds to the pros and cons of the distributed joint model.  The pros are that:Potentially voluminous electron density distributions do not need to be published over the network, because these distributions are captured in the distributed version of the PIM model.The effect model is tightly coupled with the feature model and as a result it is likely to represent the most accurate and efficient effect model implementation possible for use with this particular feature model.The cons are that:An API to the distributable version of the PIM model must be defined and an implementation developedTo support integration of the distributed joint model with a wide range of legacy and future simulations, the API and its implementation must be ported to a number of different platforms, and/or language bindings must be provided.Because the feature and effect models are joined, it is not possible to support simulations that require ionospheric features for the purposes of computing effects other than TEC.  There is a significant risk that ionospheric features will be implemented (differently) within multiple joint distributed models, each model supporting different effects.If an improved model of ionospheric features or effects becomes available which is not joint, it is difficult or impossible to upgrade existing Federations that are based on a joint model.This integration alternative cannot be applied to the atmosphere and ocean regimes due to its dependence on a joint model.Figure 4: Distributed Joint ModelGiven these considerations, the Joint Distributed Model is ruled out as a candidate architecture for geospace model integration.  Note that here and in the following the decision is not solely a function of the number of pros and cons enumerated!  The decision is based on this set of overarching considerations:A centralized model that provides an interface based on the FOM is generally superior to a distributed model that provides an API.  The reason is one of pragmatics: distributed models must provide APIs usable over a wide range of platforms and programming languages.  Supporting these heterogeneous platforms over time is a significant challenge for the implementers and/or maintainers of the API.  While Java technologies hold promise in this area, there is the risk that Java implementations will port well but suffer from poor performance, and in any case there are many legacy simulations that must continue to be supported.  On the other hand, the HLA interface specification is platform and language neutral along with all of its design tenets.An architecture that supports separate feature and effect models is generally superior to an architecture based on joint models.  The reason is that the Space Weather Reanalysis (SWR) project [8], sponsored by the Defense Modeling and Simulation Office (DMSO), is currently developing a 10+ year archive of self-consistent gridded space environmental datasets based on authoritative space physics models.  The intent is to create a rich collection of representative space weather scenarios for the modeling and simulation community to complement a similar archive that has been developed for the atmospheric regime.  A simulation architecture based on joint models will be difficult or impossible to extend such that it can take advantage of the SWR.An architecture that might support atmosphere and ocean feature and effect models in addition to the particular geospace models selected by this study is superior to an architecture that is specific to these particular models.  The rationale here is that there are a large number of parallels between the problem of modeling ionospheric effects on radio-band emissions and 1) the problem of modeling atmospheric effects on visible, IR and laser emissions, and 2) the problem of modeling the propagation of acoustic energy and signals through the underwater environment.  Hence, the architecture pursued for geospace may be reusable for modeling atmosphere and ocean features and effects.The second architecture within the class of joint modeling architectures is the Centralized Joint Model, illustrated in Figure 5.  Under this alternative, PIM’s joint feature and effect model is enhanced to include an HLA “wrapper” to create the centralized PIM Federate shown in the figure.  This Federate subscribes to all radio emission interactions published by the Transmitters.  It also subscribes to the Receiver object class and tracks the locations of all Receiver objects.  For each received emission interaction, the PIM Federate determines the phase delays of the signal as it propagates along the lines-of-sight to the N Receiver objects.  The PIM Federate then generates N secondary emissions.  The secondary emission interactions include the phase-delays computed by the joint model based on its internal feature model.  The original emissions published by the Transmitters are “consumed” by the PIM Federate and are not seen by the Receivers.The pros for this candidate architecture are:It features a system model that corresponds relatively closely with the real world, in that the ionosphere is modeled as a persistent entity (object) that absorbs and re-radiates incident electromagnetic energy.It is API-free and therefore flexible with respect to supporting heterogeneous FederationsPotentially voluminous electron density distributions do not need to be published over the network, because these distributions are captured in the centralized version of the PIM model.The effect model is tightly coupled with the feature model and as a result it is likely to represent the most accurate and efficient effect model implementation possible.The cons are:It suffers from all of the problems associated with a joint model as described previously.The centralized PIM Federate is an obvious bottleneck and simulations based on this architecture cannot be expected to scale well.  Adding Receivers to the system will quickly saturate the PIM Federate and the system will be unable to maintain a real-time simulation rate.This candidate is rejected and consideration is next given to the non-joint modeling approaches.Figure 5:  Centralized Joint ModelFeatures and Effects Modeled SeparatelyThe four architectural alternatives described below are all based on dividing the PIM joint model into separate feature and effect models, with these models implemented through either a Federate or a software library providing an API.Distributed Feature Model, Distributed Effect ModelThis candidate architecture is illustrated in Figure 6.  Under this architecture, Receiver models subscribe to the direct radio emissions of the Transmitters and determine ionospheric propagation effects using APIs to independent, distributed feature and effect models.  The API to the feature model allows the Receiver to retrieve the electron density distribution for current simulation time, while the API to the effect model is used to compute TEC along the line-of-sight between the Receiver and the satellite that published the emission interaction.  The database icon in Figure 6 is meant to underscore the near-term reality that physics-based models for features of the ionosphere (atmosphere, ocean) are too computationally intensive to be run during the execution of a distributed simulation, and as a result the time-history of ionosphere (atmosphere, ocean) features will have to be generated off-line, before the simulation exercise, and stored in a database.  The database is then distributed along with an API that provides read access.The advantage to this candidate architecture is that potentially voluminous electron density distributions do not need to be published over the network, because these distributions are captured in the distributed database that supports the distributed feature model.  There are three primary problems associated with the publication of environmental feature data over the network:Large object (> 65Kb) updates are not always supported by the simulation infrastructure, especially for simulations with large entity counts [9].If the environmental state is too large to be updated atomically, then a subscribed Federate’s reflection of this state is inherently inconsistent for some interval of simulation time following an update.The network loading that results from a sequence of updates to large environmental objects is typically highly “spiked”, with little or no loading in the intervals between updates and very large loading over the short intervals over which updates do occur.  This is precisely the loading pattern that should be avoided in distributed simulations.The disadvantage is that supporting a distributed feature model via a distributed database is equivalent to scripting the synthetic environment.  For SBA, scripted environments are fine and perhaps even preferable, but for training there are two primary drawbacks to scripted environments:They do not allow changes to be made to the synthetic environment at run-time.  Hence, the exercise controller cannot “surprise” the trainees with sudden and/or unforeseen changes to virtual weather.  Given the nature of real-world weather this particular training experience should be valuable.The virtual environment cannot be made to correlate with the real-world environment if the virtual environment is based on a script.  While it can be argued that it is impossible to create a virtual environment that correlates in detail with the real-world (“live”) environment, many simulation programs nevertheless have requirements to create virtual environments that correlate with the “live” environment to the maximum extent possible.The pros and cons associated with pre-distribution and run-time-distribution of AOS environmental data have been recognized by the environmental modeling and simulation community for some time now.  Because promising solutions have been proposed to the problems associated with run-time distribution of the synthetic environment [9], within the current analysis the arguments on the “pro” side are de-emphasized and this alternative rejected.Figure 6:  Distributed Feature, Distributed Effect ModelDistributed Feature Model, Centralized Effect ModelThis candidate architecture is illustrated in Figure 7.  It suffers from all of the problems associated with a distributed feature model described in the previous section.  It also suffers from the “bottleneck” problem associated with a centralized effect model.  The only significant “pro” is the fact that features do not need to be sent over the network, but this is mitigated by the arguments made in Section 5.4.1.  This candidate is therefore rejected.Figure 7:  Distributed Feature, Centralized Effect ModelCentralized Feature Model, Distributed Effect ModelThis candidate architecture is illustrated in Figure 8.  The solid arrow notation is used to denote the updates that a centralized feature model makes to the objects it owns which encapsulate feature state.  Receiver objects reflect feature state and make this data available to the distributed effects model. The effects model executes locally to compute TEC for lines-of-sight associated with the received emission interactions.Figure 8:  Centralized Feature, Distributed Effect ModelThe advantage to this candidate architecture is that with ionospheric features published by a centralized model (Federate), the synthetic environment can be:Modified at run-time to support the introduction of unforeseen ionospheric conditions.Correlated with available nowcasts and/or forecasts of real-world ionospheric conditions.The disadvantage to this architecture is that the effects model is distributed and must be supported for a wide variety of platforms and programming languages.This candidate is viewed as a promising alternative.  By keeping the feature model centralized, it is possible to support the generation of a synthetic environment which is correlated with the real world, and also to support the run-time editing of the synthetic environment.  This alternative – the centralized feature, distributed effect (CFDE) architecture – is designated as a candidate for prototyping.Centralized Feature Model, Centralized Effect ModelThis candidate architecture is illustrated in Figure 9.  It has the advantage that:With a centralized feature model it can support both run-time editing and live environments.With a centralized effects model it avoids the problems associated with distributed effects models that have been described previously.Its major disadvantage is the fact that its centralized effects model severely impacts the scalability of this architecture.  At this point in the analysis this question was posed: “What can be done to mitigate the poor scalability of architectures that employ a centralized effects model?”  It is argued that if the scalability problem can be solved then this particular architecture is optimal.Note that the centralized feature, centralized effect (CFCE) architecture illustrated in Figure 9 is the architecture that was pursued within the GFY’02 JSB experiments.  Within these experiments the centralized feature model was provided by the Ocean, Atmosphere and Space Environmental Services (OASES) system [10], an HLA-compliant publisher of objects that encapsulate environmental features.  The centralized effect model was implemented by a Federate referred to as Common Synthetic Environment (CSE) Services.Figure 9:  Centralized Feature, Centralized Effect ModelThe bottleneck problem associated with a centralized effect model might be mitigated by hosting the model on a massively parallel computer or computer cluster.  This would probably require the development of a multi-threaded effect model, or some other high-performance implementation based on a distribution of the computational burden across multiple processors.  The drawback to these kinds of solutions is that the Federation comes to depend upon exotic computer hardware that is likely to be expensive and difficult to maintain, and often require customized operating systems, APIs, and/or software solutions in order to work efficiently.Yet, the crux of the CFCE scalability problem is that it only includes one execution thread working on effects calculations for the entire Federation.  For the GPS-based use case, for example, a single Federate must compute TEC for as many as 12 lines-of-sight, for every GPS receiver included in the simulation, and these computations must be repeated relatively frequently for receivers that are carried by rapidly moving platforms (e.g., PGMs).A potentially novel solution to the bottleneck problem was identified during the study.  The proposal is to increase the number of execution threads working on effect computations through the instantiation of multiple instances of the Federate that implements the effects model.  Within the proposed concept of operations, the set of executing effects models would communicate with each other using Interactions.  As a result, the effect model instances would be collectively “self aware” and able to monitor their own collective performance (i.e., able to monitor whether or not they are maintaining a real-time simulation rate).  The effect model instances would perform dynamic load balancing using HLA Ownership Management services – if a model were to start to become a bottleneck, it would transfer ownership of one or more of the Receiver objects it is serving to another model.  If all existing effects models are saturated, a new instance would be created.Further details of the concept of operations are as follows.  At the start of the simulation exercise a single, a centralized effect model Federate would be started and joined to the Federation.  This initial instance designates itself as the master.  Only the master has the authority to create new effect model instances.  Furthermore, the master has the authority to create new slave instances both on its host platform and/or across any number of computer platforms that have been designated pre-exercise as available to the synthetic environment Federates.  Further details associated with dynamic load-balancing and ownership transfer would need to be worked out during the prototyping effort.Note that the Receiver models would not be aware that there are multiple effects model instances running within the Federation.  Receivers would subscribe to the subclass of emission Interactions that are reserved for describing propagated emissions (vs. the direct emissions that the effect model subscribes to.)  Propagated emission Interactions would be tagged with an identifier for the Receiver that the emission is intended for.This approach – based on the use of multiple instances of a single effects model collaborating to effect dynamic load balancing – is hereby classified as a variant of the CFCE architecture and designated as a second candidate for prototyping.ConclusionsIn summary, the study described in this paper has led to a number of recommendations to the JSB program.   First it has been recommended that the fluctuating electron content of the ionosphere, and its effects on transionospheric radio signal propagation, be made the focus of an initial JSB prototype that would also include a model of the GPS constellation and a notional model of a terrestrial GPS receiver.  Second, it has been recommended that the PIM and SCINTMOD models be used respectively for the computation of TEC and scintillation effects.  Finally, two candidate architectures for integrating the PIM and SCINTMOD models within a JSB Federation were recommended; the first is referred to as the Centralized Feature, Distributed Effect architecture and the second as the Centralized Feature, Centralized Effect architecture.  The developed prototype would serve as a testbed for evaluating the scalability of these alternative approaches.ReferencesDavenport, G.R.,Chart Links Solar, Geophysical Events With Impacts on Space Technologies  HYPERLINK "http://www.agu.org/eos_elec/95183e.html" http://www.agu.org/eos_elec/95183e.html. © 1996 American Geophysical Union.Reynolds, R.A., Shohara, N.M., TAOS in Space, Proceedings of the Spring 1999 Simulation Interoperability Workshop, 99S-SIW-101, March 1999, Orlando FL. HYPERLINK "http://nssdc.gsfc.nasa.gov/space/model/models_home.html" http://nssdc.gsfc.nasa.gov/space/model/models_home.html HYPERLINK "http://www.cpi.com" http://www.cpi.com HYPERLINK "http://space.rice.edu/ISTP" http://space.rice.edu/ISTP HYPERLINK "http://www.nwra-az.com/ionoscint/wbmod.html" http://www.nwra-az.com/ionoscint/wbmod.html HYPERLINK "http://ccmc.gsfc.nasa.gov" http://ccmc.gsfc.nasa.govKihn, E.A., et. al., The Space Weather Reanalysis, Proceedings of the Fall 2003 Simulation Interoperability Workshop, 03F-SIW-105, September 2003, Orlando FL.Reynolds, R.A. and Scannell, C.G., Supporting the Transfer of Large Environmental Data Objects Over a Connectionless RTI, Proceedings of the Fall 2003 Simulation Interoperability Workshop, 03F-SIW-088, September 2003, Orlando FL.Reynolds, R.A., Iskenderian, H., and Ouzts, S.O., The Atmosphere, Ocean and Space Environmental Services (OASES) System, Proceedings of the Spring 2001 Simulation Interoperability Workshop, 01S-SIW-047, March 2001, Orlando FL.Author BiographyROBERT A. REYNOLDS is a Principal Engineer at Northrop Grumman Information Technology, Defense Enterprise Solutions, and has been the system architect and lead developer for the OASES system and its predecessor TAOS.  His primary professional interests are in the areas of software/simulation engineering, object-oriented modeling, visualization, distributed simulation and synthetic environments.  He has a BS in Physics from the Massachusetts Institute of Technology and an MS in Nuclear Engineering from Carnegie-Mellon University. Geospace, also known as near-space, consists of the particles, fields and radiation environment from the Sun to Earth’s space plasma environment and upper atmosphere.  Geospace is considered to be the fourth physical geosphere, after solid earth, oceans and atmosphere. (From:  HYPERLINK "http://www.spacescience.org/ExploringSpace/Glossary/1.html#Geospace" http://www.spacescience.org/ExploringSpace/Glossary/1.html#Geospace) A subset of this flowchart is included herein as Figure 1. To be precise, it is classes of systems that are being referred to here, e.g. Communication Satellites, and not specific systems within a class, e.g., AFSATCOM, FLTSATCOM, Milstar, or Intelsat. In some contexts “TEC” is the name used to refer to the total number of electrons encountered by a signal traveling along a specified line-of-sight and is therefore the line-integral of electron density along this line, in units of cm-2.  Hence, the term electron density is used herein, measured in units of cm-3 and the term TEC is reserved for use when referring to line-integrated electron density. Scintillation is the result of rapid, localized variations in ionospheric electron density, and so by definition would be included in a complete description of the ionosphere via some function N(r,t).  However, due at least in part to a lack of understanding of the underlying physics involved, models of ionospheric TEC do not attempt to capture the rapid variations that produce scintillation, and scintillation is modeled separately based on observations and/or climatology. This figure is based on a graphic found at  HYPERLINK "http://www.geog.okstate.edu/gpstools/how_gps_works.htm" http://www.geog.okstate.edu/gpstools/how_gps_works.htm. From this point forward, PIM is used to illustrate the integration concepts, but the concepts apply equally to SCINTMOD. The existence of a network is understood and is not explicitly indicated, hence these diagrams are not of the “lollipop” type. This can be effected by designing a FOM that includes two subclasses of (radio) emissions, one which is direct and one which is reradiated by the intervening plasma in the geospace environment.  The PIM Federate would subscribe to the former while the Receiver Federates would subscribe to the latter.PAGE  PAGE  1PAGE  PAGE  3PAGE  PAGE  7(1)