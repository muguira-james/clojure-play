Simulation Conceptual Model DevelopmentDale K. PaceThe Johns Hopkins University Applied Physics Laboratory11100 Johns Hopkins RoadLaurel, Maryland 20723-6099(240) 228-5650; (240) 228-5910 (FAX)dale.pace@jhuapl.eduKeywords: conceptual model, validation, fidelity, simulationABSTRACT: The conceptual model for a simulation addresses the application domain context for the simulation, the idea for how the simulation will satisfy its requirements, and the way that entities and processes will be represented within the simulation.  Previous work by the author drew attention to simulation conceptual model importance in establishing simulation appropriateness for intended applications and focused on improvement in documentation of simulation conceptual models in order to facilitate more effective and efficient validation of simulation conceptual models.  This paper addresses the more fundamental issue of how to develop a simulation conceptual model.  It focuses upon approaches that enhance simulation conceptual model completeness and correctness.  These methods include a) conceptual model definition processes, which are closely associated with the subject of requirements engineering; b) conceptual model decomposition, which is concerned with the level of detail or aggregation that is appropriate for simulation elements; and c) “real world” abstraction to provide representation in the simulation.  The paper addresses conceptual model development for both unitary and distributed simulations.1.  BackgroundContemporary software developments are limited more by methods for conceptual model development than by implementation capabilities.  “We are not really having a problem coding a solution – we are having a problem understanding what solution to code. . . .  If you focus on requirements and verification and validation, the coding will take care of itself.” [Cook]  To compound the problem, no generally accepted guidelines exist to aid a simulation developer in determining which attributes of a subject should be represented in a simulation or what fidelity is required for their representation, although a format for describing a simulation conceptual model has been recommended [Pace].  In spite of IEEE’s effort to create an integrated collection of standards [Moore], ferment in software standards causes many critical factors, such as Data Item Descriptions (DIDs), to be omitted in some contracts [Sorensen].  This reduces the likelihood of quality conceptual model development.  Because a conceptual model describes how a developer intends for an implementation to satisfy its requirements, guidance about conceptual model development, evaluation, and description is diverse, as illustrated by the following.  Mayhew provided guidelines for conceptual models of software user interfaces.  Teeuw and van den Berg, like Lindland et al, identify quality criteria for conceptual models:  completeness, propriety (pertinence), clarity, consistency, orthogonality (modularity, the independence of aspects of the subject represented), and generality (as implementation independent as feasible).  Soft systems methodology, knowledge-based system (KBS) development, formal methods, and knowledge engineering have approaches for abstraction and expression pertinent to simulation conceptual modeling.  For example, an intelligent brokering service for knowledge-component reuse on the world wide web is envisioned as allowing one to enter the specification of the knowledge-intensive problem, such as an engineering design problem.  Then the broker examines available libraries of software components using shared ontologies to support such component reuse and configures a suitable problem solver.  This shows the kind of benefit possible when development discipline such as shared ontologies is employed extensively [Esprit].  Likewise, the Journal of Conceptual Modeling [http://www.inconcept.com/JCM] contains articles about capabilities and limitations of descriptive and development formalisms, especially those in the object-oriented (OO) arena, for conceptual modeling.  However, there are fundamental limitations on these methods, as illustrated by Menzies et al when they show some qualitative conceptual modeling languages used in knowledge acquisition can interfere with testability and maintainability of knowledge bases.This paper begins with a definition of the simulation conceptual model and then addresses the fundamental issue of how to develop a simulation conceptual model, focusing upon approaches that can enhance simulation conceptual model clarity, completeness, consistency, and correctness.  These methods include a) conceptual model definition processes, which are closely associated with the subject of requirements engineering; b) conceptual model decomposition, which is concerned with the level of detail or aggregation that is appropriate for simulation elements; and c) “real world” abstraction to provide representation in the simulation.  The paper addresses conceptual model development for both unitary and distributed simulations, and includes comments about implementation and control aspects of the simulation concept (simulation space) as well as representational aspects (mission space). Simulation conceptual model development is still an art.  Conceptual model development principles are incomplete and evolving, for both unitary and distributed simulation.2.  The Simulation Conceptual ModelA simulation conceptual model is a simulation developer’s way of translating modeling requirements (i. e., what is to be represented by the simulation) into a detailed design framework (i. e., how it is to be done), from which the software, hardware, networks (in the case of distributed simulation), and systems/equipment that will make up the simulation can be built.  A conceptual model is the collection of information which describes a simulation developer’s concept about the simulation and its pieces.  That information consists of assumptions, algorithms, characteristics, relationships, and data.  Taken together, these describe how the simulation developer understands what is to be represented by the simulation (entities, actions, tasks, processes, interactions, etc.) and how that representation will satisfy the requirements to which the simulation responds.  The more perspicuous and precise the conceptual model, the more likely the simulation development is to both fully satisfy requirements and allow demonstration that the requirements are satisfied (i. e., validation).A simulation conceptual model can be (and should be) a primary mechanism for clear and comprehensive communication among simulation developer design and implementation personnel (systems analysts, system engineers, software designers, code developers, testers, etc.), simulation users, subject matter experts (SMEs) involved in simulation reviews, and evaluation personnel, such as those involved in verification, validation, and accreditation (VV&A).  A simulation’s conceptual model addresses the simulation context, simulation elements, and the simulation concept.  Each of these is discussed briefly below.  The relationship among these is illustrated by Figure 1.   EMBED PowerPoint.Show.8  1)  The simulation context provides “authoritative” information about the domain which the simulation is to address.  Often this part of the conceptual model is merely a collection of pointers and references to sources that define behaviors and processes for things that will be represented within the simulation.  Special care, especially for distributed simulations, must be used when algorithms are taken from more than one source to ensure that sources do not employ contradictory assumptions or factors (such as different models for the shape of the earth, differences in characterizing the environment, etc.).  The information contained in the simulation context establishes boundaries on how the simulation developer can properly build the simulation.2)  A simulation element consists of the information describing concepts for an entity, a composite or collection of entities, or process which is represented within a simulation.  A simulation element includes assumptions, algorithms, characteristics, relationships (especially interactions with other things within the simulation), data, etc. that identify and describe that item’s possible states, tasks, events, behavior and performance, parameters and attributes, etc.  A simulation element can address a complete system (such as a missile or radar), a subsystem (such as the antenna of a radar), an element within a subsystem (such as a circuit within a radar transmitter), or even a fundamental item (such as an atom).  It can also address composites of systems (such as a ship with its collection of sensors, weapons, etc.).  It should be noted that a person, part of a person (such as a hand), or a group of people can likewise be addressed by a simulation element.  A simulation element can also address a process such as environmental effects on sensor performance.3)  The simulation concept describes the simulation developer’s concept for the entire simulation application (all the federates and other pieces in a distributed simulation, everything that comprises the simulation) and explains how the simulation developer expects to build a simulation that can fully satisfy user defined requirements.  The simulation context (discussed above) establishes constraints and boundary conditions for the simulation concept.  If the simulation is concerned with realistic representation of missile flight, then laws of physics and principles of aerodynamics are part of the simulation context, making the simulation concept accommodate conservation of momentum, etc.  Unrealistic, cartoon representation of missile flight would not necessarily be so constrained.  The simulation concept includes simulation elements, i. e., the things represented in the simulation.  The simulation concept is the total of all simulation elements, specifies how they interact with one another, and includes all additional information needed to explain how the simulation will satisfy its objectives.  Such additional information often addresses control capabilities intended for the simulation.  Can the simulation  pause and re-start?  What data can be collected and displayed as the simulation runs?  How can data and simulation control factors be entered into the simulation?  By keyboard?  By voice?  Etc.A primary function of the simulation conceptual model is to serve as the mechanism by which simulation requirements are transformed into detailed simulation specifications (and associated simulation design) which fully satisfy the requirements.  This transformation is easiest and most reliable if both the requirements and the specifications can be expressed in the same descriptive formalism because every translation from one descriptive formalism to another introduces an additional source of potential error, even for mundane transformations.  Such errors may result from failure to translate properly from one set of units to another (this kind of error caused failure of NASA’s Mars Climate Orbiter in September 1999 [http://lunar.ksc.nasa.gov/mars/msp98/orbiter/]).  Such errors can also result from misinterpretation of symbols and factors (especially from the different connotations for items with multiple definitions – as with the statistical term “mean” where the arithmetic mean is quite different from the geometric mean).  Such errors also may arise from the absence of a convenient construct or concept in the subsequent descriptive format to address all aspects of information contained in the format from which the information is being translated (this is a well known problems in natural language translation). The simulation concept has to address both simulation space (simulation operational and functional capability) and mission space (representational capability of the simulation).  This paper says relatively little about simulation space aspects of conceptual models because of length limitations since the paper focuses on mission space aspects.  Simulation space characteristics can range from identification of specific kinds of computing systems (hardware and operating systems that the simulation must run upon) and timing constraints so that real systems can be part of the simulation (such as hardware in the loop unitary simulations or involvement of live forces in distributed simulations) to simulation control capabilities that allow the simulation to be stopped, backed-up in time, and restarted, or that allow parameter values to be changed by controller fiat during simulation execution.Some simulation space considerations are closely related to implementation issues for the simulation.  For example, selection of a parallel computing architecture has implications for the algorithms that can be used to describe simulation elements.Ambiguity nearly always exists in description of the simulation context, simulation elements, and simulation concept because some parameters are generic and others are application (run) specific.  A simulation element for a sensor may contain parameter values and algorithms that fully characterize sensor states, behavior, performance, and characteristics.  For a particular application, it may be desirable to change some parameters from run to run.  One could have a parameter set by simulation inputs at the beginning of each simulation run.  Or one could have a slightly different simulation element for the sensor, one with the appropriate parameter value, for each run.3.  Conceptual Model DevelopmentOnce simulation objectives have been established, development of the simulation conceptual model may begin.  Simulation requirements and conceptual model development are a classic “chicken-egg” pair.  They each stimulate and derive from the other.  Conceptual model development may even begin prior to completion of simulation requirements.  Conceptual model development may reveal problems with requirements for the simulation, especially if there has not been a rigorous validation of simulation requirements prior to initiation of conceptual model development or if the best of contemporary requirements engineering practices have not been employed.  As the simulation conceptual model is developed to fully satisfy simulation requirements, inconsistencies among requirements and lack of balance among the requirements (some very lax and others very stringent in the same general area) may become apparent.  Likewise, development of the conceptual model may also reveal serious holes in the requirements, areas where the simulation developer is left to his own initiative about what the simulation should be able to do.  A good simulation development program will insist upon use of the best contemporary requirements engineering practices, will encourage early formal and rigorous validation of simulation requirements, and will ensure that requirement deficiencies uncovered during conceptual model development are corrected with appropriate modifications to simulation requirements.  Section 4 addresses simulation requirements and conceptual model definition in more detail.There are four basic steps in simulation conceptual model development:  1) defining the simulation context, 2) simulation decomposition by defining entities and processes for representation (similar to enumeration in the DIS paradigm), 3) development of simulation elements, and 4) defining relationships among simulation elements.  These are illustrated in Figure 2. EMBED PowerPoint.Show.8  The first step in conceptual model development is to collect authoritative information about the intended application domain that will comprise the simulation context.  Development of the simulation concept and collection of authoritative information for the simulation context is likely to occur iteratively as the entities and processes to be represented in the simulation are more clearly defined.  Authoritative descriptions of military activities such as contained DMSO’s Conceptual Models of the Mission Space (CMMS) [Sheehan et al] can be used in the simulation context when appropriate for a simulation’s intended application, as can the laws of physics and similar principles of science and engineering.It is unlikely that the formal, documented simulation context, should one exist, will address everything needed to fully describe the domain that a simulation is to address.  CMMS endeavors emphasize a disciplined procedure by which the simulation developer is systematically informed about the real world and a set of information standards that simulation subject matter experts (SMEs) employ to communicate with and obtain feedback from military operations SMEs.  Sheehan et al identify common semantics and syntax, a common format database management system (DBMS), and data interchange formats (DIF) as keys to removing potential ambiguity between the ideas of the warfighting SMEs and the simulation development SMEs.  Significant progress has been made in developing a CMMS Toolset to provide the keys noted above, but information beyond that likely to be obtained in the first level abstraction (i. e., CMMS) may be required for a simulation conceptual model.  SMEs may be “called upon to fill in details needed by [simulation] developers” that are “not provided in doctrinal and/or authoritative sources.” [Johnson]  Unfortunately, caution is required.  It has also long been known that the knowledge engineer in the role of an intermediary between the expert and the knowledge-based systems (or simulation developer) may create as many problems as he or she solves [Gaines].  Many approaches have been developed to address this kind of problem.  For example, Sharp developed a process to ensure that the expert and the knowledge engineer or program developer have the same understanding of the information being acquired and transferred to a knowledge-based system or to some other form of expression.The more complete and clearly stated a simulation context is, the easier it will be to understand how one simulation entity (or simulation in a federation) may differ from another in its assumptions about the domain which is addressed.  This becomes very important when questions of compatibility among simulations (federates) considered for a distributed simulation implementation (federation) are addressed as well as in assessment of coherence and compatibility of simulation parts.The second step in conceptual model development is to identify entities and processes that must be represented for the simulation to accomplish its objectives.  This enumeration process is fundamental in conceptual model development.  It is here that basic decisions are made about the level of detail and aggregation that is appropriate to support simulation requirements.  These decisions determine whether a system (such as a radar) will be represented as a single entity, as a composite of subsystem entities (such as an antenna or receiver), or as a composite of composites of ever smaller entities (to whatever level of detail is needed for the purpose of the simulation).  This is where decisions are made about the level of representation of human decisions and actions.  For example, in the movement of a platform (tank, aircraft, ship, etc.), are the decisions and responses of all the people involved (the crew) represented implicitly as a single aspect of the movement control process – or does each individual involved get represented explicitly (as in a tank simulator with a position for every member of the tank crew)?  Section 5 discusses conceptual model decomposition in more detail.The third step in conceptual model development is development of simulation elements.  A simulation element is needed for each entity or process (or composites of such) identified in step two above.  It is here that decisions are made initially about the level of accuracy, precision, resolution, etc. needed in the representation of the entity or process.  Simulation elements determine functional and behavioral capabilities of the simulation.  Simulation fidelity is a function of both the scope of representation in a simulation (the entities and processes identified in step two) and the quality of entity and process representation in terms of accuracy, precision, etc.  Section 6 below discusses representation abstraction in more detail.The  fourth step in conceptual model development addresses relationships among simulation elements to ensure that constraints and boundary conditions imposed by the simulation context are accommodated, and simulation space issues (such as control capabilities that allow the simulation to be stopped, backed up in time, restarted, etc. or that identify data to be collected during the simulation). These steps will be iterated often in most simulation developments.  Conceptual model development should always be done within the context of simulation theory, such as Application Domain Modeling (ADM) [Horne and Moulding] or Discrete Event System Specification (DEVS) [Zeigler et al] methodology, to ensure that conceptual model development has coherence and can be related directly to simulation development.  Sarjoughian and Zeigler developed a collaborative method for employing DEVS constructs in High Level Architecture (HLA) federation development.  YEROOS, whose acronym is derived whimsically from “Yet another project on Evaluation and Research on Object-Oriented Strategies,” is a collaborative venture of researchers from Belgium, Switzerland, Argentina, and Senegal housed at the QANT Research Unit of the University of Louvain in Louvain-La-Neuve, Belgium, researching and publishing about theoretical foundations for conceptual modeling [http://yeroos.qant.ucl.ac.be/yeroos.html].4.  Simulation Requirements and Conceptual Model DefinitionSimulation requirements are usually developed and approved by anticipated application domain experts who expect to use the simulation.  They seldom are expert in formal methods that would allow the requirements to be stated with the rigor of mathematical precision.  As a result, simulation specifications and the requirements from which they are derived through the simulation conceptual model are typically stated in natural language (even when a CASE tool is used to facilitate tracing requirements to implementation).  Consequently, this informal approach to requirements limits potential for the simulation to be proven correct (that the requirements will lead to the desired objective, and that the conceptual model and its implementation will satisfy the requirements) because of the “ambiguity, context sensitivity, and vagueness inherent in natural language in specification” [Nissanke].  Often the best practices of contemporary requirements engineering, such as described by Gilb, are not applied in developing requirements for the simulation, causing  benefits [Sawyer et al] that derive from such to be missed.It may be impossible to fully express requirements for a simulation, the conceptual model embodying those requirements, and the specifications derived therefrom fully in precise, mathematically formal terms because the simulation subject is too complex, those expressing these have limited fluency in the mathematical languages, or their mastery of the formal descriptive format is incomplete.  “Lack of professionals competent in formal methods, at least in sufficient numbers, continues to be a dominant factor in inhibiting the industrial use of formal methods.” [Nissanke]  When mathematically precise formal methods can not be used fully, even their partial use can provide significant benefit in allowing parts of the simulation to possess provable correctness.  This perspective should be taken for each simulation element and requirements and specifications associated with it as well as for the complete simulation conceptual model and simulation requirements and specifications as a whole.A long term goal of the simulation community should be to increase familiarity and competence within it with set theory, propositional logic, predicate logic, etc. that undergird formal development environments such as employed with languages like the Vienna Development Method (VDM) from IBM laboratories in Vienna or Z developed at Oxford University so that more formal and precise methods can be used with simulation requirements and specifications, as well as with the conceptual model which connects them and the simulation elements contained within the conceptual model.  The more that such formal approaches can be used, the greater the utility of various “automatic” verification tools so that the scarce resource of knowledgeable, competent personnel can focused more on validation issues to ensure that the simulation not only works properly but that it is really working as intended -- a major motivation in NASA’s emphasis upon the value and utility of formal methods, especially for safety-critical and mission-critical systems [http://eis.jpl.nasa.gov/quality/Formal_Methods/index.html].The movement toward greater use of more formal constructs in requirements and specifications is not unique to simulation, but is a general trend of maturing software engineering [SEI Interactive] and is widely understood as essential to allow more use of automation in verification and validation of software and simulation, especially during design and early stages of development [http://archive.comlab.ox.ac.uk/formal-methods.html].5.  Conceptual Model DecompositionBooch, Rumbaugh, and Jacobson identify four basic principles of modeling:  the choice of what models to create has a profound influence on how a problem is attacked and how a solution is shaped, every model may be expressed at different levels of precision, the best models are connected to reality, and no single model is sufficient.  These principles suggest that modeling is essentially an art that has not yet matured to a scientific method.  This is especially true for simulation conceptual model development, but this is not to say that rational processes should not be applied to conceptual model development.Conceptual model decomposition determines what simulation elements are in the conceptual model.  This determines the scope of representation in the simulation and the discernible levels of the simulation.  The following rationale may be helpful in determining what simulation elements should be in the conceptual model.  These six items discussed below can be used as a checklist in conceptual model decomposition as the conceptual model is being developed.  Using this rationale will help a conceptual model to be complete and coherent.  Figure 3 provides a graphic summary of the rationale in this checklist. EMBED PowerPoint.Show.8  There should be a specific simulation element (parameter, entity, etc.) for every item (parameter, entity, etc.) specified for representation in the simulation by simulation requirements.There should be a specific simulation element (parameter, entity, etc.) for every item (entity, task, parameter, state, etc.) of potential assessment interest related to the purpose of the simulation.There should be “real world” counterparts (objects, parameters for which data exist or could exist, etc.) for every simulation element as far as possible.  The potential impact of data, and metadata structures, on simulation elements and the simulation conceptual model should not be underestimated.Wherever possible, the simulation elements should correspond to “standard” and widely-accepted decomposition paradigms to facilitate acceptance of the conceptual model and effective interaction (including reuse of algorithms and other simulation components) with other simulation endeavors.Simulation elements required for computational considerations (such as an approximation used as a surrogate for a more desirable parameter which is not computationally viable) that fail to meet any of the previously stated rationale should be used only when absolutely essential.There should not be extraneous simulation elements.  Elements not directly related to specific items in the simulation requirements, not implied directly by potential assessment issues, and without a specific counterpart in the real world or in standard decomposition paradigms should not be included in the simulation conceptual model.  Every extraneous simulation element is an unnecessary source of potential simulation problem.Rationale for how to decompose a HLA federation (or other distributed simulation) into federates (component simulations) is embryonic.  Pollack and Baker developed HLA-specific software metrics for use in determining the appropriate level of decomposition in a specific HLA application.  The metrics provided a quantitative measure for achieving a balanced federation in the effort to optimize the sometimes competing goals of utility and efficiency.  Applying these metrics to a specific legacy simulation and a baseline federation object model (FOM) identified the need for further decomposition to effectively support the intended application.  As others perform similar assessments, it is likely that a set of metrics which have general utility will emerge and become accepted by the community.  Such do not yet exist.  Until they do, it will be necessary for those designing a distributed simulation to consider the availability of compatible simulation components as well as computational and bandwidth factors in determining how to best decompose the distributed simulation (HLA federation) for the intended application.6.  Representational AbstractionIdentification of simulation elements in conceptual model decomposition determines the scope of subject representation and the discernible levels possible in the mission space.  How the characteristics of the simulation elements are abstracted determines the accuracy and precision of the representation.  As noted earlier in the basic principles of modeling by Booch et al, no single model is sufficient – which is one of the reasons that the unified modeling language (UML) which they developed has nine kinds of diagrams (class, object, use case, sequence, collaboration, statechart, activity, component, and deployment) to express fully the five most useful views (use case, design, process, implementation, and deployment) that comprise the architecture of a software-intensive system.  In a similar manner, representational abstraction for simulation elements is likely to take a multifaceted descriptive approach.Simulation fidelity is a complex function of the scope and discernible levels of the simulation as well as accuracy, precision, and other parameter quality characteristics.  During recent years, substantial attention has been paid to describing simulation fidelity, with progress being made toward standardizing connotations for fidelity related terms and awareness of issues associated with simulation fidelity [Gross].  The light of widely-accepted principles for determining required levels of fidelity and abstraction approaches that will produce them has not yet penetrated this murky topic.Failure to fully account for the uncertainties and errors that may exist in data and information used as the basis for models, algorithms, entity characteristics and behaviors, processes, and other aspects of a simulation is a common problem in simulation verification and validation [Roache].  This issue is closely associated with the subject of simulation fidelity and an important consideration in representational abstraction.Knowledge engineering forms the heart of abstraction for a simulation conceptual model.  Theoretical approaches to knowledge engineering typically break it into three phases:  knowledge acquisition, knowledge elicitation, and knowledge representation.  Such theoretical approaches usually identify three knowledge structures: declarative knowledge (why things work the way they do), procedural knowledge (how to perform a task), and strategic knowledge (the basis for problem solving) -- with different acquisition, elicitation, and representation techniques for each kind of knowledge. Unfortunately, these theoretical approaches do not yet allow abstraction to be performed as a scientific method; it remains an art [http://church.cse.ogi.edu/~walton/know_eng.html].  Review of recent articles in such publications as Elsevier’s  Journal of Data and Knowledge Engineering [ HYPERLINK "http://www.elsevier.com/inca/publications/store/5/0/5/6/0/8/" http://www.elsevier.com/inca/publications/store/5/0/5/6/0/8/] reveals that contemporary researchers in this arena often develop a “new” descriptive language (or dialect of a language) or formalism for the problem at hand since current techniques do not yet have broad, general application capabilities.In order to develop a conceptual model which is clear, complete, consistent, and correct, the reader is advised to use rationale presented in the previous section and employ quality criteria from Teeuw and van den Berg to judge abstractions employed – the criteria are completeness, propriety (pertinence), clarity, consistency, orthogonality (modularity, the independence of aspects of the subject represented), and generality (as implementation independent as feasible).  As one develops a simulation conceptual model and evaluates it by these criteria, it is important to record how one assesses the model and then to note why it changes in response to the evaluation, and how criteria for a quality conceptual model are met more fully.  Otherwise, rationale for some changes (and their benefits) may be lost as time passes, and lessons learned from the conceptual model development will not be so readily available for use in subsequent developments.The bottom line is simple.  Consistent and comprehensive use of any formalism in conceptual model development will be better than the common ad hoc unstructured approach so often used.7.  Conceptual Model DocumentationIt has been recommended that simulation conceptual model documentation employ the scientific paper approach, even if also employing the design accommodation approach by using an implementation oriented descriptive format such as UML [Pace].  Nine items (listed below) are suggested for the description of a portion of the conceptual model (such as a simulation element) or of the entire conceptual model in the scientific paper approach to documenting a simulation conceptual model:  Conceptual Model Portion Identification; Principal Simulation Developer Point(s) of Contact (POCs) for the Conceptual Model (or part of it); Requirements and Purpose; Overview; General Assumptions; Identification of Possible States, Tasks, Actions, and Behaviors, Relationships and Interactions, Events, and Parameters and Factors for Entities and Processes being described; Identification of Algorithms; Simulation Development Plans; and Summary and Synopsis.  This list of items is functionally equivalent to the ten items in the generic content guidelines from IEEE/EIA 12207 for describing a planned or actual function, design, performance or process: -- the ten items are:  date of issue and status, scope, issuing organization, references, context, notation for description, body, summary, glossary, and change history [Sorensen].8.  Comments and ConclusionsThis paper discusses the tenebrous topic of how to represent knowledge so that simulation requirements and specifications, as well the simulation conceptual model connecting them, can be described completely, correctly, consistently, and clearly.  When such occurs, the likelihood that the simulation will function as desired is enhanced.  There are many horror stories from simulation developments which do not have an explicit conceptual model, which have a poorly developed or only partially developed conceptual model, or which have incomplete documentation of the simulation conceptual model.  Wise simulation developers and users do not want their own horror stories – and can avoid such by following the suggestions of this paper.As indicated, much remains to be done before simulation conceptual model development can be routinely done with the rigor, precision, and formalism needed to bring significant increases in assured correctness of conceptual models.  The obstinate complexity of the logical, philosophical, and computational foundations of knowledge representation is not as widely appreciated as it should be [Sowa].  Consequently, one must be very pragmatic in developing a simulation conceptual model.  One must accommodate capabilities (the limitations) of those responsible for simulation requirements and of those who will implement the simulation.  Comprehensive, clear, correct, and consistent descriptions of requirements, specifications, and the conceptual model which can be understood by all are far more important than adherence to a doctrinaire emphasis upon a particular methodology or formalism.  Yet at the same time, it would be prudent for most involved in simulation development and use to move toward increased employment of formal methods to expand mathematically precise descriptions in simulation requirements, specifications, and conceptual models as far as circumstances will permit.Suggestions presented in this paper will help the simulation community move toward more effective methods in developing simulation conceptual models, even though those suggestions are more general and limited than many, including the author, would prefer.  Sources identified may help the reader to broaden appreciation for various aspects of conceptual model development.  The author has observed that few in the simulation community have very broad awareness of the available literature, especially in the arenas of simulation conceptual model development and related topics such as simulation fidelity and VV&A.  This lack of awareness can prevent use of best contemporary practices in simulation development, evaluation, and application. The ideas of this paper are applicable to acquisition, analysis, and training simulation application domains for both unitary and distributed simulations in the Defense community as well as to scientific, technical, economic, social, educational, gaming, etc. simulations, whether unitary or distributed, in non-Defense communities.  There is much room for improvement in development and descriptive methods for requirements, specifications, and conceptual models of simulations in all of these arenas.9.  AcknowledgmentsThis paper draws heavily upon ideas from sources identified in the paper body and in the references cited below, the revised version of the DoD Recommended Practices Guide for Verification, Validation, and Accreditation (VV&A), an unpublished report from the doctoral program of Cüneyd Firat about Conceptual Modeling and Conceptual Analysis in HLA, and work of the 1999 Conceptual Model Tiger Team established by Ms. Simone Youngblood, VV&A Technical Director for the Defense Modeling and Simulation Office (DMSO) and SIW VV&A Forum Chair.  The team included Ms. Candace Conwell (SPWAR), Mr. Robert-Jan Elias (TNO-FEL, the Netherlands), Dr. Reuben Jones (Boeing), Dr. Averill Law (Averill Law & Associates), Mr. Bob Lewis (TecMasters), Dr. Michael Moulding (Cranfield Univeristy, UK), Dr. Dale Pace (JHU/APL), Mr. Terry Prosser (Logicon), Mr. Bob Senko (DMSO), Mr. Jack Shehan (DMSO), Ms. Susan Solick (TRAC), Mr. Dave Thomen (SAIC), and Dr. Bernard Zeigler (Univeresity of Arizona).  Mr. Gary Coe (IDA) also helped.10.  References [Booch, Rumbaugh, and Jacobson] Grady Booch, James Rumbaugh, and Ivar Jacobson, The Unified Modeling Language User Guide, Addison-Wesley, 1999.[Cook]  David Cook, “Evolution of Programming Languages and Why a Language is Not Enough to Solve Our Problems,” CrossTalk:  The Journal of Defense Software EngineeringI, Vol.12 No. 12 (December 1999), pp. 7-12.[Esprit]  Esprit project number: 27169, “An Intelligent Brokering Service for Knowledge-Component Reuse on the World-Wide Web,” available at:  HYPERLINK http://www.swi.psy.uva.nl/projects/IBROW3/home-ibrow.html http://www.swi.psy.uva.nl/projects/IBROW3/home-ibrow.html.[Gaines] B. R. Gaines, “An Overview of Knowledge Acquisition and Transfer,” International Journal of Man-Machine Studies, Vol. 26, No. 4 (April 1987), pp. 453-472.[Gilb] Tom Gilb, “Towards the Engineering of Requirements,” Requirements Engineering  2:165-169, 1997.[Gross] Gross, David C (redactor), “Report from the Fidelity Implementation Study Group”, 99S-SIW-167, 1999 Fall Simulation Interoperability Workshop Papers, March 1999.[Horne and Moulding] G. N. Hone and M. R. Moulding, “Developments in Application Domain Modelling for the Verification and Validation of Synthetic Environments:  Training Process and System Definition.” Proceedings of the Spring 1999 Simulation Interoperability Workshop, March 15-19,1999, Orlando, FL.[Johnson] Thomas H. Johnson, “Mission Space Model Development, Reuse and the Conceptual Models of the Mission Space Toolset” 98 Spring Simulation Interoperability Workshop Papers, March 1998, Vol. 2, pp. 893-900.[Lindland et al]  O. I. Lindland, G. Sindre, and A. Solvberg, “Understanding Quality in Conceptual Modeling,” IEEE Software, Vol. 11, No. 2, (March 1994), pp. 42-49.[Mayhew] Deborah J. Mayhew, Principles and Guidelines in Software User Interface Design, Prentice Hall, 1992.Menzies, Tim, Robert F. Cohen, and Sam Waugh, “Evaluating Conceptual Modeling Languages,”  HYPERLINK http://www.cse.unsw.edu.au/~timm/pub/docs/97evalcon/words.html http://www.cse.unsw.edu.au/~timm/pub/docs/97evalcon/words.html.[Nissanke] Nimal Nissanke, Formal Specification:  Techniques and Applications, Springer-Verlag, 1999.[Moore] James W. Moore, “An Integrated Collection of Software Engineering Standards,” IEEE Software, November/December 1999, pp. 51-57.[NASA] NASA Formal Methods Guidebook: Formal Methods Specification and Verification Guidebook for Software and Computer Systems, Volume I: Planning and Technology Insertion [NASA/TP-98-208193], 1998, and Volume II: A Practitioner's Companion [NASA-GB-001-97], 1997.[Pace] Dale K. Pace, “Development and Documentation of a Simulation Conceptual Model,” 99 Fall Simulation Interoperability Workshop Papers, September 1999.[Pollack and Baker] Anne F. Pollack and John P.Baker, “Federation Decomposition:  HLA Metrics,” 99 Fall Simulation Interoperability Workshop Papers, September 1999.[Roache]  Patrick J. Roache, Verification and Validation in Computational Science and Engineering, Albuquerque, NM:  Hermosa Publishers, 1998.[Sarjoughian and Zeigler] Hessam S. Sarjoughian and Bernard P. Zeigler, “The Role of Collaborative DEVS Modeler in Federation Development,” 99 Fall Simulation Interoperability Workshop Papers, September 1999.[Sawyer et al] Pete Sawyer, Ian Sommerville, and Stephen Viller, “Capturing the Benefits of Requirements Engineering,” IEEE Software, March/April 1999, pp. 78-85.[SEI Interactive, Software Engineering Institute] “Introduction,” Vol. 2, No. 1, March 1999, SEI Interactive:  Software Requirements Engineering	[http://www.sei.cmu.edu/products/sei.interactive].[Sharp] John K. Sharp, “Validating an Object-Oriented Model,” Journal of Conceptual Modeling, Issue No. 6, December 1998 [http://www.inconcept.com/JCM].[Sheehan et al] Jack Sheehan, Terry Prosser, Harry Conley, George Stone, Kevin Yentz, and Janet Morrow, “Conceptual Models of the Mission Space (CMMS):  Basic Concepts, Advanced Techniques, and Pragmatic Examples,” 98 Spring Simulation Interoperability Workshop Papers, March 1998, Vol. 2, pp. 744-751.[Sorensen]  Reed Sorensen, “Software Standards:  Their Evolution and Current State,” CrossTalk:  The Journal of Defense Software Engineering, Vol. 12 No. 12 (December 1999), pp. 21-25.[Sowa] John F. Sowa, Knowledge Representation:  Logical, Philosophical, and Computational Foundations, Brooks/Cole, A Division of Thomas Learning, 2000.[Teeuw and van den Berg]  W. B. Teeuw and H. van den Berg, “On the Quality of Conceptual Models, 16th International Conference on Conceptual Modeling, 3-6 November 1997 (ER’97) in Los Angeles, CA [http://osm7.cs.byu.edu/ER97/workshop4/tvdb.html].[Zeigler et al] B. P. Zeigler, T.G. Kim, and H. Praehofer, Theory of Modeling and Simulation, 2nd ed., 1999, New York, NY:  Academic Press.13.  About the AuthorDale K. Pace, a member of the Principal Professional Staff of The Johns Hopkins University Applied Physics Laboratory, is a specialist in operations research, modeling and simulation, analysis, and wargaming.  An initial member of the Simulation Interoperability Workshop (SIW) Conference Committee, Dr. Pace has been involved in various SIW simulation fidelity endeavors.  He was co-chair of the Military Operations Research Society (MORS) Simulation Validation (SIMVAL) 1999 Workshop and is a member of the Defense Modeling and Simulation Office (DMSO) Verification, Validation, and Accreditation (VV&A) Technical Working Group and its VV&A Technical Support Team, with particular responsibilities in the area of conceptual model development.  He was the initial lead for the validation part of the independent verification and validation (IV&V) team for Wargame 2000.  And he is Simulation’s Associate Editor for Validation. This paper was prepared under sponsorship of the Defense Modeling and Simulation Office (DMSO), with oversight by the DMSO Verification, Validation, and Accreditation (VV&A) Technical Director, Ms. Simone M. Youngblood.  The views of this paper are those of the author and should not be construed to represent views of DMSO or of any other organization or agency, public or private.