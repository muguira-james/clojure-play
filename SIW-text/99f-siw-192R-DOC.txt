Parsing SMART: What Are the Pieces and How Do They Fit Together?Paul H. DeitzU.S. Army Materiel Systems Analysis Activity392 Hopkins RoadAberdeen Proving Ground, MD  21005-5071410-278-6592/98; FAX:  410-278-6584phd@arl.milKeywords:SMART, modeling , simulation, vulnerability, lethality, V/L taxonomy, military utilityABSTRACT:   Simulation and Modeling for Acquisition, Requirements, and Training (SMART) is an Army activity to bring the benefits of computer simulation to all phases of materiel modernization to include both pre-Milestone I and post-Milestone III analyses.  Given the extent and nature of (sub)models somehow enveloped in this totality, key issues are raised concerning disparate versus redundant submodels, compatibility and granularity of metrics, submodel integration, maintenance of proper event causality, and establishment of a rational sequence for model instantiation.  A framework developed previously for vulnerability/lethality (V/L) studies is expanded.  This framework separates metrics according to platform component integrity, platform aggregate capabilities, and platform battlefield utilities.  In addition, a change operator that can modify the status of the platform component capabilities is defined.  This operator can represent not only ballistic live-fire events (as previously exploited), but other causes of component breakage and repair as required.  Also, each (metric) class is connected via an operator that reflects both the intrinsic platform variables as well as external environmental and utility factors.  Using this taxonomy, existing models can be parsed according to the particular function(s) they perform.  The ability of various submodels to work seamlessly can be assessed, and the proper sequencing of events can be assured.  Introduction & BackgroundWhat is SMART?Simulation and Modeling for Acquisition, Requirements, and Training, or SMART, is an Army activity in which Modeling and Simulation (M&S) are used to address the issues of system development and total ownership costs.  This is to be achieved through the collaborative efforts of the communities supporting the three Army functional domains: advanced concepts and requirements (ACR), research, development, and acquisition (RDA), and training, exercises, and military operations (TEMO).  In order to bring the benefits of computer simulation to all phases of materiel modernization, SMART analysis must address two major issues.  The first is the military-analysis (MA) or system architecture part of the problem.  This is a primary objective of SMART analysis.  However, since the context of M&S is supported by executable computer code, SMART analysis must also deal with issues of computer science (CS), or the technical architecture.  Table I lists some of the MA issues that confront SMART.  Table II lists a few of the issues that shape the CS component of SMART.CS standards both enable and constrain.  It is important that the CS tools and methods do justice to the MA requirements that are implicit to SMART analysis.  These issues apply whether the simulation is constructive or interactive.  However, they should follow from the MA requirements, so we will set them aside for now and focus on the MA problem. Historically, all M&S was constructive.  The emphasis was on good engineering fidelity.  The models tended to be narrow in scope, and frequently the CS discipline and computer graphics were poor.In the distributed interactive simulation (DIS) world, CS methods have generally been better, to include interactive graphics.  Often, however, engineering fidelity has been poor and the scope of simulations has been limited. It is important to note that the MA world is changing rapidly.  The number of analytical elements and challenges is growing.  These include the need to support ever-increasing complexity in the area of ballistic mechanisms and interactions, the burgeoning area of communications and sensors, the need to develop truly integrated analyses, and the requirementTable I.  Some Military Analysis (MA) Elements of SMARTPlatform VulnerabilitySystem DesignMission FailureBattlefield EffectivenessCrew CasualtiesProbability of HitTop SpeedLoss of FunctionRate of FireTime to AcquireHit LocationSpall/Fire DamageProbability of DetectionMean Time Between FailuresBattle Damage Repair TimeCost EffectivenessTable II.  Some Computer Science (CS) Elements of SMARTCommunications Protocols      AdaHigh-Level Architecture (HLA)Open Graphics LibraryASCIITCP/IPJMASSCORBAto include cost as an independent variable (CAIV).  The superset need is captured in the DoD focus on the Analysis of Alternatives (AoA) process, which has gained overarching importance.  In addition to the rapid advance in technologies being applied to military platforms, the operational environment is also in rapid change.  New missions for the US are being defined; they are typically characterized by small forces, forward projection, and asymmetric operations.  The need is great for lethality projection and the minimization of friendly casualties.The principal focus of this paper is on SMART from an analytical perspective.  Throughout the proceeding words, read SMART as SMART analysis.Changing TimesA key issue is what constitutes the complete military analysis problem?  Is there an overarching framework or context within which the elements of SMART can be described?  Given the pieces of SMART, how do they fit together?  What levels of resolution are appropriate for the SMART environment, and how can those levels be determined?  What should be the strategies with respect to providing lookup (i.e., table-driven) versus predictive methods?In the next section, we describe an example of SMART analysis   in   order   to provide an overall context  within which to fit the many elements or submodels, identify gaps, duplication, and submodel connectivity.	SMART Example: BallisticVulnerabilityBallistic simulations, both vulnerability as well as lethality, have always been fundamental to the assessment of military fighting vehicles.  We review the process of performing a live-fire test to illustrate how the SMART process can be divided into three major pieces. Figure 1 shows two examples of a live-fire test.  On the left is illustrated a shot performed during the Bradley Fighting Vehicle test program; on the right, a shot made against the Apache-Longbow helicopter.Typically, such shots lead to particular component damage on a platform.  Loss of component function can lead to reduction or loss of certain platform capabilities.  In Fig. 2, a Soviet T-62 tank (on the left) and a U.S. Apache helicopter (on the right) demonstrate climbing and banking maneuvers, respectively.  Understanding both preshot (baseline) and postshot (damage) performance is important to the live-fire process.Whether working at nominal specifications or, due to damage, at some reduced level of capability, a military platform must be viewed in the context of its mission utility (i.e., its ability to achieve battlefield success).  Figure 3 illustrates both ground and air platforms involved in campaign activities.Figure 1.  Ballistic live-fire encounters with Bradley (left) and Apache-Longbow (right).Figure 2.  Capability testing with Soviet T-62 tank climbing wall (left) and Apache helicopter banking (right).Figure 3.  Two kinds of military missions, one involving tanks (left) and the other involving helicopters (right).SMART Analysis FrameworkDuring the mid-1980s, ballistic Live-Fire legislation [2] brought increased attention to the testing and prediction of ballistic live-fire phenomenology.  An outgrowth of the efforts to rationalize the comparison of tests and prediction was the Vulnerability/Lethality (V/L) Taxonomy [3-8].  This framework was developed from a platform-centric perspective (e.g., the numbering system is related to the platform being described).  Later, we assert that this analysis strategy can be applied to multiple platforms engaged both in cooperative and adversarial roles (e.g., groups of communications platforms).3.1 Four Classes of MetricsIn what follows we employ the core levels of the V/L Taxonomy as described Ref. 7.  This concept characterizes a military platform (tank, aircraft, self-propelled gun, etc.) in terms of four classes of metrics. The classes are illustrated in Fig. 4 as four abstract levels or mathematical spaces.  In terms of the preceding ballistic live-fire event, Level 1] represents the complete geometry/material of the striking munition, the complete geometry/material of the platform, and the encounter geometry (e.g., hit location and kinematics); this information can be represented by a vector containing the initial conditions needed to compute the resulting damage to the platform.  Level 1] is shown as an ellipse filled with bullets ((), with each representing one possible encounter vector.The second ellipse, labeled Level 2], represents the space of combinations of working and nonworking components on the platform.  A single vector at this level lists the outcome status of each platform component.The third ellipse, labeled Level 3], represents the measurable capabilities of the platform.  For a fighting vehicle, this would typically consist of its abilities to move, acquire, communicate, and fire its guns.  Again, each bullet represents a vector that describes some particular state of the platform capabilities.  Level 3] metrics can be considered measures-of-performance [MoPs].The fourth ellipse, labeled Level 4], represents the mission utility.  In effect, this metric indicates that the platform is either able or unable to meet the current requirements of the military campaign. Level 4] metrics can be considered measures-of-effectiveness [MoEs].In Table III, we list each level, indicate that a vector associated with that level has a corresponding subscript, and then describe the information typically represented by the vector.  Except for the Level 4] vector, v4, each of the vectors in Levels 1] through 3] is observable, measurable, and testable.  This is important for supporting verification, validation, and accreditation (VV&A) activities that necessarily support these abstractions.  In terms of our ballistic example, a vector at Level 1], v1, defines the threat, the platform under test, and the kinematics of delivery.  The results of the live-fire test generally lead to damage to particular components on the platform.  The complete status of each component (i.e., killed [(], not-killed [(]) would be given by vector v2.  Were the vehicle to be tested with movement, gun firing, etc., the capabilities of the platform would be represented by the vector v3.  Platform utility, typically not measurable, would be represented by the vector v4.  The critical issue of military utility is discussed later.  For more on the properties of these vectors, see Ref. 7.3.2 Three Classes of  OperatorsHow are these vector levels linked?  Mathematically, they can be linked by operators or transformation processes that take a vector at Level n] and map it to a vector at Level n+1].  The mathematical operators described are at the heart of the SMART analysis process.  Alternatively, the operators can be viewed as tests or experiments performed in the field.  The linkage or corroboration between the results of analytical operators and field tests provide the basis for all rigorous VV&A activities.  Figure 4 also shows how the four levels of metrics are linked.The four levels are connected abstractly by operators written as Op,q.  This notation leads to the convention                              vq = Op,q { vp }.                           (1)Since the operators can only connect sequential levels,                           vp+1 = Op,p+1 { vp }.                      (2)The live-fire tests illustrated in Fig. 1 are represented by the O1,2 operator.  The test processes shown in Fig. 2 are represented by the O2,3 operator.  The mission exercises illustrated in Fig. 3 are represented by the O3,4 operator.Figure 4.  V/L Taxonomy illustrated via a mapping abstraction.  The ellipses represent mathematical spaces. The bullets (•) contained within the spaces represent vectors.  The connecting arrows represent operators that map a vector at one level to a vector at the next sequential level.  On the left, the descriptors for the various levels and operators are listed.  On the right, a box labeled Military Operations Context provides descriptors of the external military environment (mission, terrain, threats, etc.) within which the platform must perform.Table III. Composition of Vectors at Each Level or Class                                    Level          Vector                     Vector Descriptor                                      1] 	                v              Platform Geometry & Material, 					       Risk/Repair Geometry & Material,					           Encounter Geometry                                      2] 		 v	    Status of Platform: i.e, Working (()		         and Nonworking (() Components                                      3] 	               v               Platform Capability: e.g. Mobility, Firepower,		                          and Communication Functions                                      4]	               v	     Platform Utility:  e.g. Does Platform Survive?                                                                                  (from Level 2]),                                                                  Can Platform Perform Specific                                                                                                                      Mission Tasks?In order to perform a live-fire shot, all of the target system has to be represented in the test.  To model live-fire accurately, the three-dimensional geometry must be described to high fidelity as well.  Figure 5 shows exterior and interior images [9] of a Soviet T-62 solid geometric model tank built with the Army-generated solid-geometry package BRL-CAD® [10].  This geometry is represented by the vector, v1.To the right of the four ellipses is found a box labeled Military Operations Context (MOC).  This construct has been added to recognize that each of the operators takes input from the operations context in which a platform performs.  The MOC defines the doctrine, tactics, leadership, materiel, scenario, terrain, weather–all of the factors external to the platform itself.  For example, during a live-fire test, the volatility of ammunition that may be ignited is dependent on the ambient temperature for a given day–hence, the context data feed to the O1,2 mapper.  Similarly, with the O2,3 capability mapper, the ability of a platform to move or acquire is a function of the terrain and weather variables– hence the context data connection to this mapper.  Finally, the MOC clearly defines the mission activities or tasks that the platform will have to perform in order to achieve mission success.  The required task levels are fed to the O3,4 operator from the MOC as well.  In effect, a significant portion of the MOC is defined by the Army DTLOMS (doctrine, training, leader development, organization, materiel, and soldier structure).  Importantly, the O3,4 mapping defines mission requirements, for it is the link between platform capabilities and mission utility.  This particular mapping may represent the greatest analytic challenge facing the Army today.  For a notional example on how mission success is determined by a platform current versus required capability, see Ref. 7, Fig. 4.3.3 Extant CodesThe previously listed abstractions are not simply notional.  They have been implemented in code, and used to describe complex ballistic interactions, compute resulting damage and related capability diminishment.  The specific production code consists of a family of BRL-CAD geometry tools [10] and a suite of C-based vulnerability codes supported by a special portable environment.  Called Modular UNIX-Based Vulnerability Estimation Suite (MUVES) [11], this single supporting structure is used to compute tank direct-fire interactions, anti-air encounters, and indirect-fire artillery and bomblet events.  These capabilities are also provided for personnel vulnerability evaluation by a model called Operational Requirements for Casualty Assessment (ORCA) [12], which uses the same taxonomy.3.4 Generalization of the O1,2 OperatorThough originally conceived as a damage operator for representing ballistic phenomenology, the O1,2 Operator has been generalized across a range of damage and risk mechanisms including the description of reliability, availability and maintainability (RAM) [13] as well as electronic warfare and chemical threats [14].  In addition to causing damage, the O1,2 Operator can also represent repair or fix operations and, hence, supply or replenishment.  A partial list of actions that fit the O1,2 framework is given in Table IV.3.5 Event/Time-Stepping the TaxonomyWhen considered in the context of a mission, the Taxonomy framework illustrated in Fig. 4 should be considered dynamic.  That is, as a platform proceeds through time, the MOC a) causes degradation to the system (component) infrastructure and b) challenges the capabilities of the platform with a sequence of tasks.  This notion is a compact, yet detailed way of thinking about mission activities.  The metrics of the four levels and the connecting operators are, in fact, the pieces of SMART analysis and how they fit together.Discussion of FrameworkThree Kinds of OperationsIn the context of SMART analysis, we can see that there are three major classes of modeling and testing that, though different in detail, reoccur in virtually every study.  They are:      1] How risk/repair factors change the platform                     microstructure,      2] How the platform microstructure leads to     platform macroperformance, and,      3] How platform macroperformance leads to	     military success/failure. It is our conjecture that the elements of SMART can be broken down and placed in one of these three categories.  This observation has important ramifications that include global coverage of operations research (OR) studies, the sharing of  various tools/databases, and the achievement of global modeling coverage.  Figure 5.  BRL-CAD® solid geometry target description;outside of foreign tank (left), inside (right) [from Ref. 9].Table IV.  Generalizing the O Ballistic Damage Operator to the O Change Operator(Quasi-) Permanent DamageTemporary DamageComponent Fix/SupplyBallisticElectronic JammingBattle Damage RepairChemicalCosite InterferenceLogistics ResupplyDirected EnergySleep *High-Power LaserNuclearLogistics Burdens (e.g., Fuel, Ammo)ReliabilityPhysics of FailureFair Wear & TearFatigue*Heat Stress*                  * Personnel RelatedTwo Major Links to MAMilitary platforms moving through missions are confronted with two classes of actions:1] Components degraded /upgraded by risk/repair factors, and2] Tests of the platform capabilities for          mission adequacy.As a platform moves through the mission, various risks erode the fidelity of the components.  At each task encounter, the sufficiency of capability is challenged.  As damage increases, capabilities diminish, and continue to be (mission) challenged, etc.4.3 Multivariate, Nonlinear, & Stochastic MappingsThe operators which populate each of the three classes can be extremely complicated.  In general, the mapping operations are multivariate (i.e., depend on many factors), nonlinear, and often stochastic.  For a discussion of these effects on the mapping results, see Ref. 7.4.4 Metric Variations: Level 2]When risks are played at Level 1], three general outcomes can occur at Level 2].  They include:      1] no damage,      2] intermediate damage, and      3] complete/catastrophic damage.Assessment of the last outcome is obvious.  However, there are considerable challenges in dealing with the case of intermediate damage outcome which leads to reduced capability and, as we will discuss later, even defining in a meaningful way what constitutes full mission utility.  The current practice of normalizing full mission utility to a baseline of 1.0 is not particularly illuminating either in an absolute sense or in a comparative one (i.e., one platform utility versus another).With respect to repair activities, they can result in the platform being restored to full or top condition or, in the case of expedient repair, in some, but not all of the components being restored.  This will result in a system with utility that will be mission dependent as well.4.5 Aggregation of DamageTo evaluate properly a particular risk or repair interaction, the result of all prior interactions must be accounted for; thus, the next event sees the platform in its current, not pristine, condition.  The majority of simulations assume risk/repair interactions with “pristine” platforms, whereas in extended missions, this condition represents the exception rather than the rule.  This expediency can lead to erroneous assessments.4.6 Dynamic GeometryIt is important to note that the ability to exercise predictive or physics-based models in this SMART context is based in part on the ability to interrogate, as needed, 3-D solid geometry.  Because of the diverse numbers of risk/repair encounter conditions, it is not possible to precompute all of the outcomes needed a priori.  Nevertheless, it is the practice of the force-on-force community to use table lookup procedures, rather than computing damage or capability metrics as needed, “on the fly.”A yet more advanced capability, and one which will surely be needed in future analysis environments, is dynamic geometry.  This is a feature in which the results of a computation can be used to go back to the geometric data and modify it appropriately.  This is significant for the study of platforms whose utility is to be examined over significant periods of time during which geometry and/or material undergo critical modification.  Such changes alter the character of signatures, performance, and protection, not to mention similar issues with the MOC (e.g., changes in terrain features due to ballistic impacts, vehicle tracks).  BRL-CAD has utilities built in to the environment so that intermediate computations can be used to modify geometric source files.4.7 Metric Variations: Level 3]The effect of a platform with some, but not all, components functional, is that it may lead to reductions in various capabilities at Level 3].  This raises a difficult problem in the assessment of military utility at Level 4].  This problem was recognized in the 1950s by workers at the U.S. Army Ballistic Research Laboratory (BRL). They concluded that the military utility of a platform with degraded capabilities depends on the mission requirements [15]. They developed ballistic metrics that estimated a (normalized) utility averaged over all missions.  For more than 40 years direct-fire ballistic metrics have been computed so as to use (Level 4]) average utility metrics, rather than capability metrics at Level 3].  Exploratory work has been performed for a number of years to understand the benefits and burdens of higher resolution methods [16, 17].4.8 Multi-Platform AnalysesThis process can also be applied to “systems-of-systems.”  For analyses in which multiple platforms are being played, each platform requires its own four-level vector description. This approach is relevant to analyses of C4I systems.  The O1,2 map (evaluating jammers, worms, etc.) is worked for each cooperative platform.  An appropriate O2,3 map is then computed across the gamut of linked systems.  After aggregate MoPs are computed, the O3,4 utility mapping is performed for the joint platforms in mission context.Taking Stock: Current PracticeLooking at today’s SMART simulations, a number of generalizations can be made.5.1 Risk/Repair AnalysesThe models of this class typically demonstrate the best physics and engineering.  Since the scope of the problem is often smaller, limited resources can be concentrated on a more narrow set of issues.  Often, studies of this class will look at failure rates (e.g., due to one or another phenomenology), but to the detriment of overall significance, provide no linkage to platform capability or military utility.5.2 Engineering Performance AnalysesEngineering mappings are among the most sophisticated in the SMART analysis world.  They frequently represent some of the best predictive capabilities.   All too often, however, there is no clear connection between predicted engineering performance at Level 3] and utility at Level 4].  In addition, the O2,3 engineering mappings are frequently performed assuming all components are fully operational.  Thus, there may be no ability to assess capability under arbitrary levels of component damage and, hence, infer the likelihood of graceful degradation for increasing damage.  The link from risk/repair to component status (Level 2]) is seldom made.5.3 OR AnalysesInvestigations into effectiveness are normally supported through wargames.  Historically oriented toward V/L computations, V/L metrics are drawn from table look-up.  In addition to the aforementioned problems of computing V/L estimates for an expected military utility, the kill metrics are further compromised by computing the average mobility and firepower kills at a particular hit location.  These are combined into a single number, weighted by a bivariate hit distribution.  Next, these distributions are weighted by a cardioid hit distribution, weighted to strikes from the front.  There normally is an assumption that the encounter is for a first hit.  Thus, there is no proper aggregation of damage at Level 2], no notion of prior events.As noted in Section 4.7, the methods needed to avoid premature averaging have been established [13, 16, 17].  The tools are in hand to calculate damage, even when prior damage exists.  A significant issue is what are the Level 4] utility metrics or MoEs.In general, the relationship between the platform component status and platform capabilities is not modeled explicitly, and the relationship between the detailed platform capabilities and mission utility is not played.  Since there is no detailed description of platform geometry, it is not possible to perform risk/repair component erosion.Thus, there is no functional thread in the OR assessment process from operational risks/platform repairs (through performance) to OR utility.  This makes the proper assessment of technology tradeoffs and system assessments very problematic and results in a fundamental cognitive disconnect between the U.S. Army Materiel Command (AMC) as a provider of technology (read performance) and the U.S. Training and Doctrine Command (TRADOC) as a customer of military utility.  In other words, in weapons analyses, analysts often fail to establish a clear relationship between MoPs and MoEs.  This reality has not been lost on the Office of Secretary of Defense/Program Analysis and Evaluation (OSD/PA&E).  A number of recent Army systems have fallen victim to the apparent inability to relate system performance to mission effectiveness.5.4 What Are the Right MoEs?Historically, force-on-force modeling has concentrated on loss exchange ratios (LERs), trading projected lethality against received vulnerability.  Current force-on-force modeling practices can be traced to the 1960s with even earlier roots when warfare was structured around massed forces on the European plains.  Central limit tendencies could be invoked given the large numbers at play.  Today the international situation is much changed.  It is widely acknowledged that there will be no more Desert Storms where essentially symmetric (mirror image) forces come into play.  It is unlikely future U.S. opponents will attempt to fight on “common ground.”  Asymmetrical warfare will be more likely.Consider the U.S. actions in Mogadishu.  Well documented in Black Hawk Down [18], on the day in question, U.S. forces killed more than 1000 natives at a cost of 18 U.S. troops– by classic measures, a U.S. numerical victory, but hardly considered a military victory by most Americans. Consider the U.S. actions in Kosovo.  The U.S. Air Force was so preoccupied with pilot casualties that they operated at altitudes for which precise ordnance delivery became an issue.  Beyond the relevancy of LERs, as a primary utility figure of merit, system costs have entered the picture in a significant way.  All of this comes together under DoD’s current thrust toward AoAs. If a set of competing platforms is going to be traded off, what is the appropriate figure-of-merit by which to evaluate the trade?Summary: Current PracticeMany elements of SMART analysis exist within the M&S community.  Currently, however, the elements of the three classes of computation (risk/repair, engineering performance, and mission effectiveness) are typically not knit together so as to enable the desired decision process.6. Rethinking the ProcessSince the introduction of the Taxonomy in Fig. 4, we have been considering the process in a time forward or causal sense.  Such a procedure has the potential for mapping to an outcome space that is both accurate and irrelevant.  This issue is raised when one considers just where the beginning point is when setting up a SMART analysis environment.6.1 Starting at the EndEven if a complete set of computer codes were written and fully integrated, there would still be the task of setting up the key inputs to the codes.  But where to begin?  We suggest beginning with Level 4], Mission Outcome Space, and then working back to Level 1].We turn our attention to Fig. 6 in which the Taxonomy has been reproduced with a few additions.  At Level 4], a number 1 has been placed to the left of the ellipse.  The first step in the instantiation of this process should be a thorough discussion of set of mission outcomes as implied by the vectors of Level 4] followed by a decision of just what subset of those outcomes is acceptable as successful missions.  Notionally, we have sorted the acceptable outcomes of mission space into the circle shown within the ellipse.  Deciding just what constitute the mission MoEs may be one of the most challenging aspects of the SMART analysis process.  However, if this first step is not performed well, then there is no basis for technical performance goals or cost tradeoffs.Given mission success is established at Level 4], the next step is to understand what set of performance metrics at Level 3] will map to the “circle-of-success” at Level 4] in the context of the MOC.  In this process, the nature and height of the mission “hurdles” (as in track & field) are specified and analyzed.  If the mission performance requirements are identified and set properly, in effect the O3,4 mapper for the subject mission is defined. This completes Step 2.  At this point, the nature and level-of-performance metrics at Level 3] needed for success at Level 4] can be defined.  If this process were to be accomplished truly agnostically, there might exist a diverse population of capability vectors that might be reflected in substantially different technical solutions.  Step 3 is complete when the Level 3] “circle-of-success” is defined.  With Step 3 complete, it is only then appropriate to seek specific technical solutions to a military mission need.  If this framework is correct, it says that military mission success depends on capabilities, not technology (which is a Level 2] metric) per se.  Given a performance envelope defined at Level 3], system designers can next postulate materiel solutions at Level 2], in Step 4.  Then through engineering analysis, the O2,3 mapping operation can be estimated via modeling, or, if the platform is built, defined by test. This is Step 5 and the point at which engineering analysis confirms that a potential technical solution exits capable of meeting the performance requirements defined at Level 3].  Also, those components critical to achieving success at Level 3] can be identified and sorted into the “circle-of-success” at Level 2].  If analyses and/or tests confirm that the performance is as expected and adequate, cost figures can be assigned in Step 6.  At this point, an apparent solution has been identified, subject to the assessment of risks.  In Step 7, the MOC is queried to identify all military threats (bullets, high-power microwaves, etc.) as well as the operational context, which speaks to wear and tear on equipment and people.  Once identified, these risk factors are mapped from Level 1] to Level 2] to examine whether any of the critical components shown within the “circle-of-success” is threatened.  If so, platform performance and, hence, mission success are put at risk.  Step 8 is the establishment of the appropriate O1,2 change operators as driven by the MOC and inherent platform susceptibilities to failure/breakage.  Step 9 indicates that this process can be run repetitively as the MOC defines a mission script and drives the four-level state vector represented by the Taxonomy through time from the beginning to the end of the mission.  Given adequate exercise of this framework over reasonable sample sizes, an improved linkage could be established between technology, performance, and utility for a wide variety of scenarios as defined by some set of MOCs.This framework has been used recently to suggest a procedure for planning live-fire test strategies  with a view to cost-effective practices [19].Fig. 6. V/L Taxonomy used to order the process of framework build.  To instantiate the SMART analysis process, the end point is established first; then mapping and metrics worked toward the beginning.  The circles represent the level subspaces which are required to lead, ultimately, to mission success.6.2 Is This Practice New?How new is this practice?  Actually, competent analysts have always known that to construct a satisfactory model, they need to examine the desired end result and infer their way back to the required prior conditions. This approach has, in fact, been practiced by the Air Force analysis community.  In the 1980s, LTG(R) Glenn Kent established a procedure called “Mission-to-Task Process” [20] in which, in effect, a mapping process is established to make clear the relationship between Level 3], performance, and Level 4], military utility.  This is the critical problem of relating MoPs to MoEs.  More recently, a hierarchical decision architecture using fuzzy set theory has been applied to the Mission-to-Task problem [21].The Army actually has an established history of using Mission-to-Task methods to establish a framework within which human performance can be evaluated [22-26].  However, there are apparently few examples within the Army for the application of these techniques to purely nonhuman, materiel analysis.By the Mission-to-Task Process, in effect the O3,4  mapping is generated in the context of a specific mission where the Level 4] utility vector is based on a combination of Level 2] metrics (loss of critical components, including crew), resulting in subthreshold capabilities vis-à-vis task requirements.  The task requirements are defined by the MOC.  Thus, a Level 4] can be written functionally as:                        v4  =   f (v2, v3, MOC) .                    (3)It is probably appropriate that all MoEs be defined in terms of MoPs, which can be either component performance metrics or platform performance metrics.  Although challenging, there is increasing evidence that OR analysts are increasingly moving in this direction, particularly as the AoA process requires nonmetaphysical measures to be demonstrated in the test arena while simultaneously making the case for military worth, which in the main is probably not directly observable!7. Summary & ConclusionsWe have proposed a structure for SMART analysis that:   ( Shows connected performance and operational       pieces,   ( Shows four distinct classes of metrics connected       by three distinct classes of operators,   ( Shows specific connections between the military      operational context and various platform metrics       and operators.This illustrates a widespread class of military OR problems, typified by the following key steps:   1. Define baseline system configuration,   2. Estimate system performance,   3. Estimate system effectiveness,   4. Account for change in system configuration due       to threats, etc., and   5. Go to 2, etc.An adequate SMART analysis requires all pieces to be properly constructed, linked, and integrated.  The corner stone of SMART should be the process of relating platform capabilities to desired mission outcomes.  In SMART, the whole is greater than the sum of its parts when developed with highly resolved operational contexts.  The O3,4 Mission Operator and Level 4] utility status reflect our ability to understand what should become both the beginning and the end of our analysis cycles, operational success. This process says that mission outcomes (including success) are related to platform capabilities, not directly to technology.  Thus, platform capability tradeoffs should be established in the context of mission performance first, and then technology should be perused and exploited for opportunities to meet those capabilities (read requirements), subject to appropriate constraints.We gain insight into: ( The manner in which the Operational, Technical,     and Intelligence communities need to work together,    and  ( The implications for building the Army technology       program.The framework can be used to parse various M&S models to see into what portion of SMART analysis they fit.  It can be used to identify redundancies in some areas and coverage gaps in others.The framework can be used to set interface and architectural standards (per various levels), make clear how algorithms are employed, highlight code gaps, and provide critical global connectivity.  It is at this point that the many computer science metrics and connectivity tools can be best employed.This decomposition shows that a large number of destructive (e.g., ballistic, chemical, nuclear, logistics burden, and reliability) and constructive (e.g., battle damage/expedient repair, and product improvement process) operators require similar inputs, perform similar tasks, and modulate the status of the same components.The framework illustrates that damage and repair/upgrade occur at Level 2].  Multiple instances of damage/repair must aggregate at Level 2], not, as is often done, by manipulating Level 4] metrics.  This approach can be used to analyze “Systems-of-Systems” (e.g., networked communications platforms).Engineering issues focus on the O2,3 operator.  This operator takes Level 2] metrics and operational context data and must compute all capability metrics relevant, ultimately, to mission success.  These include weights, moments-of-inertia, movement, all sensors, communications, guns/missiles capabilities, etc.  A large number of required engineering operators require similar inputs, perform similar tasks, and modulate identical capability status metrics.  This process illustrates the importance of global variables sourced in the MOC and coupled to various levels and operators.Given a prescribed MOC (threats, mission, etc.), an inverse information threading can be established that points through critical performance factors, to supporting critical components and, finally, to any factors that put them at risk.  These relationships can be used to prioritize which information and what strategies of test/model/analyze should be pursued.Finally, costs and benefits can be assigned with clarity to specific metrics and operators to support CAIV and AoA studies.8. ReferencesSee various articles in Army RD&A, May-June 1999, published by HQ, U.S. Army Materiel Command.Live Fire Testing, National Defense Authorization Act for FY 1987, Chapter 139, Section 2366 of Title 10, United  States Code.Paul H. Deitz and Aivars Ozolins, Computer Simulations of the Abrams Live-Fire Field Testing, Proceedings of the XXVII Annual Meeting of the Army Operations Research Symposium, 12-13 October 1988, Fort Lee, VA; also U.S. Army Ballistic Research Laboratory Memorandum Report BRL-MR-3755,  May 1989.Paul H. Deitz, Michael W. Starks, Jill H. Smith, and Aivars Ozolins, Current Simulation Methods in Military Systems Vulnerability Assessment, Proceedings of the XXIX Annual Meeting of the Army Operations Research Symposium, 10-11 October 1990, Fort Lee, VA; also U.S. Army Ballistic Research Laboratory Memorandum Report BRL-MR-3880, November 1990.J. Terrence Klopcic, Michael W. Starks, and James N. Walbert, A Taxonomy for the Vulnerability/ Lethality Analysis Process, U.S. Army Ballistic Research Laboratory Memorandum Report BRL-MR-3972, May 1992.Paul H. Deitz, A V/L Taxonomy for Analyzing Ballistic Live-Fire Events, Proceedings of the 46th Annual Bomb & Warhead Technical Symposium, 13-15 May 1996, Monterey, CA; also Modeling Ballistic Live-Fire Events Trilogy, U.S. Army Research Laboratory Technical Report ARL-TR-1274, December 1996.Paul H. Deitz and Michael W. Starks, The Generation, Use, and Misuse, of “PKs” in Vulnerability/Lethality Analyses, Proceedings of the 8th Annual TARDEC Symposium, 25-27 March 1997, Monterey, CA; also U.S. Army Research Laboratory Technical Report ARL-TR-1640, March 1998; also The Journal of Military Operations Research, Vol. 4, No. 1, pp. 19-33, 1999.J. Terrence Klopcic, The Vulnerability/Lethality Taxonomy as a General Analytical Procedure, U.S. Army Research Laboratory Technical Report ARL-TR-1944, May 1999.Paul H. Deitz and Keith A. Applin, Practices and Standards in the Construction of BRL-CAD Target Descriptions, Proceedings of the BRL-CAD Users Conference, 4-5 November 1992, Baltimore, MD; also U.S. Army Research Laboratory Memorandum Report ARL-MR-103, September 1993.Michael J. Muuss (compiler), Ballistic Research Laboratory CAD Package, A Solid Modeling System and Ray-Tracing Benchmark Distribution Package, Release 1.2, June 1987, Release 3.0, October 1988, and Release 4.5, U.S. Army Ballistic Research Laboratory, February  1998.Phillip J. Hanes, Scott L. Henry, Gary S. Moss, Karen R. Murray, and Wendy A. Winner,  Modular UNIX-Based Vulnerability Estimation Suite (MUVES) Analyst’s Guide, U.S. Army Ballistic Research Laboratory Memorandum Report BRL-MR-3954, U.S. Army Ballistic Research Laboratory, December 1991.Kellye C. Frew and Ellen M. Killion, User's Manual for Operational Requirements-based Casualty Assessments (ORCA) Software System-Alpha+ Version, Applied Research Associates, Inc., Report, July 1996.Lisa K. Roach, Fault Tree Analysis and Extensions of the V/L Process Structure, U.S. Army Research Laboratory Technical Report ARL-TR-149, June 1993.William J. Hughes, A Taxonomy for the Combined Arms Threat, Chemical Biological/Smoke Modeling & Simulation (M&S) Newsletter, Vol. 1, No. 3, Fall 1995.Gerald A. Zeller and Bradford F. Armendt, Update of the Standard Damage Assessment List (SDA) for Tanks, Underlying Philosophy and Final Results, ASI Report 87-02, July 1987.Gary R. Comstock, The Degraded States Weapons Research Simulation (DSWARS): An Investigation of the Degraded States Vulnerability Methodology in a Combat Simulation, U.S. Army Materiel Systems Analysis Activity Technical Report AMSAA-TR-495, February 1991.Beth S. Ward, Mark D. Burdeshaw, Joe L. Aguilar, and David R. Durda, CASTFOREM Combat Simulation Utilizing Degraded States Vulnerability Methodology, In Preparation.Black Hawk Down: A Story of Modern Warfare, Mark Bowden, Grove/Atlantic, Inc., February  1999.Martha K. Nelson, Vulnerability and Lethality Assessment: The Role of Full-Up System-Level Live-Fire Testing and Evaluation, U.S. Army Research Laboratory Technical Report, In Press.Glenn A. Kent and W. E. Simons, A Framework for Enhancing Operational Capabilities, RAND Project Air Force Report, No. R-4043-AF, Santa Monica: RAND, 1991.Suzanne M. Beers, An Intelligent Hierarchical Decision Architecture for Operation Test and Evaluation, Ph.D. Dissertation, Georgia Institute of Technology, May 1996.Laurel Allender, Troy B. Kelley, Sue Archer, and Rich Adkins, IMPRINT: The Transition and Further Development of a Soldier-System Analysis Tool, MANPRINT Quarterly, Vol. V, No. 1, published by the USA Office of the Deputy Chief of Staff for Personnel, Winter 1997.Laurel Allender, Troy B. Kelley, Lucia Salvi, Diane Mitchell, Donald B. Headley, David Promisel, Celine Richer, and Theo Feng, Verification, Validation, and Accreditation of a Soldier-System Modeling Tool, Proceedings of the Human Factors and Ergonomics Society 39th Annual Meeting, October 9-13, 1995, San Diego, pp. 1219-1223.Laurel Allender, Lucia Salvi, and David Promisel, Evaluation of Human Performance under Diverse Conditions via Modeling Technology, Proceedings of Workshop on Emerging Technologies in Human Engineering Testing and Evaluation, NATO Research Study Group 24, Brussels, Belgium, June 1997.Frank Malkin, Laurel Allender, Troy B. Kelley, Pat O'Brien, and Steve Graybill, Joint Base Station Variant 1 MOS-Workload-Skill Requirements Analysis, ARL Technical Report 1441, November 1997.  Richard McMahon, Mike Spencer, and Alvin Thornton, A Quick Response Approach to Assessing the Operational Performance of the XM93E1 NBCRS through Use of Modeling and Validation Testing, Proceedings of the 1995 International Test & Evaluation Association Symposium, Huntsville, AL, pp. 115-117, October 1995.AcknowledgementThe author wishes to recognize the following colleagues for many inciteful discussions: Dr. Michael W. Starks and Ms. Jill H. Smith, ARL/SLAD, Mr. William J. Hughes, OPTEC, Dr. Martha K. Nelson, Franklin & Marshall College, and Mr. Alexander B. H. Wong, AMSAA.Author BiographyDr. PAUL H. DEITZ is the Technical Director of AMSAA.  Employed by the Army for 35 years, he has served in various positions including Branch Chief in, and later the Division Chief of, the Ballistic Vulnerability/Lethality Division in the U.S. Army Research Laboratory.  Dr. Deitz has authored more than 60 technical papers dealing with topics in wave propagation in random media, laser eye damage, smart munitions performance, solid geometric modeling, predictive signatures, and ballistic live-fire simulation.PAGE  PAGE  10