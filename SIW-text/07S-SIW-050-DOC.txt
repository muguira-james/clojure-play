EFFECTS-BASED APPROACH TO OPERATIONS (EBAO) EXPERIMENTATION FOR VALIDATING HUMAN BEHAVIOR REPRESENTATION (HBR) IN SIMULATED ENVIRONMENTS  Randall B. GarrettGeneral DynamicsAdvanced Information Systems115 Lake View ParkwaySuffolk, VA 757-203-3930randall.garrett@je.jfcom.milKeywords:Validation, Agent Objects, Perception, Effects-Based, Experimentation, Code of Best Practices, PmSEII, DIME  . ABSTRACT: There are many challenges associated with the validation of Human Behavior Representation (HBR) agent based simulations. There are limited measurement methods for validating Agent Object perception within an Effects-Based Approach to Operations (EBAO).  Also, within a simulated environment there is no clear understanding of surrogate HBR relationships with their behavioral representation.  These remain as obstacles to advancing our knowledge and understanding of agent model validation.  In many cases, the effects of agent object behavior may be likened to an “experimental frame” with some known potential for accomplishing a task. The effects of these tasks are not always measured beyond face validation. [1] We may not fully understand how synthetic agents - or agents - represent human behavior but we do rely on the fact that they retain a potential for accomplishing specific tasks.  An agent, in this sense, becomes less a matter of size or complexity, but one of 'viewpoint'. [2] This viewpoint infers an agent-oriented approach to the conceptual modeling of HBR agent based simulation, thus implying an agent object relationship. A specific question remains: may human perception in the real world be represented and measured as a correlate to the same perception in an artificial world?1. Experimental Approach:This approach suggests an experimental method to validate, or invalidate, Artificial Perception (AP) in EBAO agent object simulations. Validation results would be achieved by using a significance measure of AP in a simulated HBR agent object environment for a particular human task correlate. [3]  A central premise is that information gathered through eliciting the working knowledge of Subject Matter Experts (SMEs) for a particular critical task “use case”, should have a correlate to an agent’s behavior “seeded'  with values derived from SME test scores.1.1 BackgroundEffects-based approaches are well established in both industrial and defense model simulations.  Effects-based approaches are methodologies for observing and assessing the results of policy, processes or operations with a goal of re-creating the effects needed to achieve a desired outcome while reducing the undesired effects. Generally, EBAO encompasses all levels of knowledge base development, effects-based planning, and effects-based execution and assessment. [4] With EBAO, the focus is in achieving a desired outcome, based on System-of-System Analysis (SoSA) or “linked nodal analysis” rather than through observations of linear solutions.  Through the introduction of Applied Cognitive Task Analysis (ACTA) and critical decision methods, agents seeded with the experimental results from these methods could further validate simulated effects beyond “face validation”. This would result in agent models more accurately replicating a desired outcome.                This technique could easily be integrated into existing effect models such as PmESII (Political, Military, Economic, Social, Information and Infrastructure). PmESII effects of recommended actions DIME (Diplomatic, Informational, Military or Economic) on a simulated entity.  Some examples of these simulated entities are facilities, organizations, military targets, or even information centers. 1.2 Common questions regarding the use of a cognitive approach for agent object Verification, Validation, and Accreditation (VV&A)ACTA and critical decision methods for eliciting knowledge are key cognitive methods that may be employed for use in VV&A of EBAO agent object models. Previous work and specific questions addressed using these techniques include:-  Situational Awareness (SA) [1] (Is what you are seeing real?)   - Perception based on anticipatory response [2] (Can human perception be applied to agent objects in an EBAO environment?) - Treatment of anticipation as an event integral to perception and as a projection of future states [5] (Do we assume that synthetic agents can project future states?)-  VV&A of HBR Agent Behavior [6] (How do we integrate a test method into currently accepted VV&A practices?  What is the “model builder’s truth” for agent object behavior?)- Formalism and logic for critical task scenario simulation development [7] (Is there precedent or formalisms that define human perception correlates to synthetic agents?)- Measurement for perceived HBR Agent response in critical scenarios [8] (Can anticipated results be applied effectively as a “seeded” value to observe agent object behavior?)In the cognitive approach, a precept to synthetic object perception is that there will be anticipation of an object's action.  In this instance, it would be the action resulting from within a critical task scenario.  This also maintains that there is a state space and agent object interaction.2. Cognitive Approach:Applied Cognitive Task Analysis (ACTA) methods may be used to develop critical tasks.  This assumes that mental demands are needed to proficiently perform the task. Simulated scenarios may then be constructed from these critical tasks.2.1 Situational AwarenessGenerally speaking, SA for EBAO is viewed within a context of fully understanding and accurately assessing the cumulative results of sensor detection, messages and reports, or even visual cues. Effects are based on weighted Centers of Gravity (COGs) from nodal analysis. [9] Data results are then normalized and face validation performed by SMEs. Although effective, SA in this instance is not necessarily “ground truth”.  This process may not provide reliable data that could be used as seed values for testing an object or entity’s anticipated response.  What you are actually observing (seeing) in EBAO environments appear “real” within the framework of linked nodes (the effect), but may not be accurate for use as a predictor of anticipated object behavior.2.2 Agent perception and anticipationExamples of agent perception and anticipation may be found in anticipatory systems that use predictions to determine an agent’s behavior, (e.g., let future states affect its present state). [10] These are based on causal reasoning.  Cognitive models have addressed perception in synthetic environments. However, statistics that compare actual perception with simulated perception mainly reside within the fields of psychology and human factors and have not been fully integrated into agent object EBAO simulated environments.Projecting future states as a result of EBAO implies that we are able to use values from weighted COGs to project these states.  Our dilemma is that we are analyzing results based on model effects.  This does not imply that the inputs from a combination of entities involved with SoSA effects are non repeatable and therefore not useful in projecting a future state.   However, this does direct us toward alternative approaches for projecting these states. Using experienced SMEs and applied cognitive techniques to measure predicted results for critical events might provide us with the data that is needed.3. Validation and Verification:One approach toward achieving effective V&V for agent objects is to use the Defense Modeling and Simulation Office (DMSO) reference model as a guideline.  This implies implementing a related process of agent logic, hypothesis, and cognitive results that overlay the five phases of the model. [11] Representative agent objects and synthetic scenario development could then be accomplished through the use of a knowledge audit survey and the three methods associated with Cognitive Task Analysis (CTA). [12] Using the DMSO phased process; an overall approach to EBAO V&V model could then be performed as follows:Phase I:  A critical task scenario and conceptual model for agent behavior associated with EBAO could be validated for its intended use, specific use and the applicable referents identified.Phase II:  Model validation against a SME knowledge base would allow for the assignment of specific critical tasks associated with the EBAO.Phase III: In this phase, cognitive representations could be correlated to the experimental scenario objects for future use as simulation test objects. Phase IV: A simulation using agent logic software representative of the EBAO model would be developed.Phase V: V&V of the EBAO model would be completed by critical task scenario SMEs and through Analysis of Variance (ANOVA) of the simulation test results.  4. Formalisms:EBAO formalisms assume that representations of spatial relationships among objects in a coordinate system are independent from the perceiver. These spatial relationships may be represented by symbols or logic that resemble visual cues for attributing meaning to an object, so that their processing is actually part of a semantic processing of the associated visual information. [13]  This semantic characterization for spatial relationships regards AP as observable by logical methods. Providing a correlation of human visual perception with a surrogate agent object simulated behavior allows us to test the validity of HBR AP from the findings in our CTA human experiments.Formalisms developed in phase IV of the VV&A procedure, allow for the development of an EBAO model and object notation designed to demonstrate the logic representing perception in an HBR agent object.  The following generic notation helps understand the perception, prediction and decomposition dynamics needed for our AP EBAO simulated environment: DM = {M, S, P, F}: Dynamic mental model with perception [14]  M: Mental Model State  S: Sensory Input:P: Perception Input:F: Update / Prediction function 5. Experiment:The experiment would seek a correlate between the agent simulation experimental results and the test scores (likert scale) obtained from SMEs during the ACTA. [15]    Random number generation of state interaction and observance of seeded agent values would provide a measurement suitable for an ANOVA. [16]  We now would have a platform to analyzes how the distribution of a continuous Y variable (Agent A - our simulated autonomous agents) differs across groups defined by a categorical X  (Agent B - our autonomous agents with a “seeded” value obtained from the ACTA scores) variable. Group means would then be calculated.  This procedure would allow us to perform other observations such a best fit model or:	One-way analysis of variance to fit means and to test that they are equal Nonparametric tests Test for homogeneity of variance Comparison tests on meansPower details  Resultant plots could show us the true distribution of values.  By observing these plots, we might conclude that Agent B is performing better than Agent A if its sample mean is sufficiently greater than the sample mean of Agent A’s performance.  (The values of n and the variance are also taken into account.)  If the performance of B is not sufficiently greater than that of A, based on the samples, then we might then conclude that B outperforming A on these samples is likely due to random chance.  Otherwise, we conclude that the comparison is statistically significant.  6. References[1]   Ören, T. I. and Yilmaz, L., “Behavioral                               Anticipation in Agent Simulation”: Proceedings of   the 2004 Winter Simulation Conference, December 2004.[2] 	Riecken, D.: “A Conversation with Marvin Minsky about Agents.”  In the Communications of the ACM Archive Volume 37, Issue 7, ACM Press New York, NY, July 1994.[3] 	Klein, G. A.: “The Development of Knowledge Elicitation Methods for Capturing Military Expertise”, U. S. Army Research Institute for the Behavioral and Social Sciences 1996.[4] 	Davis, Paul K.: Effects-based Operations: A Grand Challenge for the Analytical Community, 2001.[5] 	Bandini, S., Manzoni, S., and Simone, C.: Proceedings of the First International Joint Conference on Autonomous Agents and Multiagent Systems, part 3, pp. 1183-1190, July 2002.[6] 	Balci, O.: "Verification, Validation and Accreditation of Simulation Models": Proceedings of the 29th Winter Simulation Conference, Atlanta, GA, December, pp. 135 -141, Volume 07-10  1997. [7] 	Harmon, S. Y.: “A Taxonomy of Human Behavior Representation Requirements”’ 11th Conference on Computer Generated Forces & Behavior Representation, Paper No. 02-CGF-024, 2002.[8] 	Taylor, R. M.: “Situational awareness rating technique (SART): The development of a tool for aircrew systems design”, Proceedings of the NATO Advisory Group for Aerospace Research and Development (AGARD) Situational Awareness in Aerospace Operations Symposium (AGARD-CP-478) July 1989. [9] 	JWFC Pamphlet 7: Operational Implications of Effects-Based Operations, November 2004.[10] 	Cassimatis, N., J. G. Trafton, M. Bugajska, and A. C. Schultz 2004: “Integrating Cognition, Perception, and Action through Mental Simulation in Robots.” Robotics and Autonomous Systems, 49(1-2), Reed Elsevier, NV November 2004.    [11] 	Defense Modeling and Simulation Office DMSO: “Special Topic - Validation of Human Behavior Representations”, Verification, Validation, and Accreditation (VV&A) Recommended Practices Guide (RPG), U.S. Department of Defense, cited  January 2003.[12] 	Klein, G. A.: Calderwood, R. and MacGregor, D., “Critical Decision Method for Eliciting Knowledge."  IEEE Transactions on Systems, Man, and Cybernetics, Volume 19.3 1989.[13] 	Gaskins, R.C. III, Boone, C.M., Verna, T. M. Jr., Petty, M. D.:" Cognitive and Behavioral Psychological Research for Crowd Modeling," NATO Modeling and Simulation Group Conference, Koblenz, Germany, October 2004.[14]	Johnson-Laird, P.N. : Mental Models. Cambridge, MA: Harvard University Press 1993.[15]	Likert, R.:  A Technique for the Measurement of Attitudes, Archives of Psychology, New York 1932.[16]	Law, A.M., and Kelton, W.D.: Simulation Modeling & Analysis, Third Edition, McGraw Hill 2000.[17]	Sokolowski, J.: "Enhanced Military Decision Modeling Using a MultiAgent System Approach", In Proceedings of the Twelfth Conference on Behavior Representation in Modeling and Simulation, Scottsdale, AZ., pp. 179-186, May 2003. [18]	Joines, J. A., Barton, R.R., Kang, K. and Fishwick: P.A. “Experimental Design for Simulation”, Proceedings of the 2000 Winter Simulation Conference 2000.[19]	Goldstein, E.: Sensation and Perception, Sixth Edition, Wadsworth, Pacific Grove, CA 2002.[20]	Silverman, B. G., R. Might, R. Dubois, H. Shin, M. Johns, and  R. Weaver: “Toward A Human Behavior Models Anthology for Synthetic Agent Development.” In Proceedings of the 10th Annual Conference on Computer Generated Forces and Behavioral Representation, SISO ‘01: Paper 10th-CGF-011.  Norfolk 2001.Author BiographyRANDALL GARRETT is an Analyst and APM at the U.S Joint Forces Command Joint Innovation and Experimentation Directorate (J9) Suffolk, Virginia.