Venator - a Tool for Analysing the Integrity and Correlation of Synthetic Environment Terrain DatabasesPaul Clarke & Paul WonnacottQinetiQE111, St Andrews Rd, MalvernWR14 3PS, UK+044 (0) 1684 896015pdclarke@qinetiq.com, pw@signal.qinetiq.comKeywords:Terrain database, Synthetic Natural Environment, GIS, quality, verification and validation, integrity, correlationABSTRACT: As terrain databases increase in size and complexity so too does the likelihood of errors within them. We define two types of errors which have an adverse affect on quality – integrity errors (anomalies found within databases such as roads blocked by buildings) and correlation errors (differences between databases such as terrain height miscorrelation).  The current technique for identifying these errors, manual inspection, is rarely appropriate and some method for automating the process is required. We therefore introduce Venator, a tool for automatically identifying a range of anomalies for use when generating new terrain databases, reusing legacy databases or interoperating between several SEs. Venator is based on a commercial Geographical Information System and can ingest a wide range of source data and runtime databases. We discuss the four key stages involved: data input; processing; analysis and presentation of results. Three case studies illustrate the application of Venator: verification and validation for the UK Combined Arms Tactical Trainer; testing the output from a commercial terrain database generation tool and assessing an existing Synthetic Environment for reuse potential. Future work will focus on more intelligent tests (relying less on human interpretation) and effective ways of communicating results in a collaborative environment (e.g. interactive maps). Our experiences have proven our hypothesis that quantitative, automated analysis is an essential requirement for producing credible SEs (including reuse and interoperability). Failure to verify and validate terrain databases will see a continuation of the integrity and correlation anomalies affecting so many current systems. This work is sponsored by MOD and is part of the SSS Corporate Research Programme.IntroductionGenerating terrain databases for Synthetic Environments (SE) has historically been a complex and expensive process. Over time, users have demanded increasingly realistic, feature rich and accurate SEs covering larger geographical areas. As the complexity of terrain databases increases, so too does the probability of errors within them [1]. Two types of anomalies can affect terrain database quality: integrity errors and correlation errors. Integrity errors occur within a database when certain features are not represented in a feasible way (i.e. as they would appear in reality). Typical examples include buildings placed on roads, water flowing uphill, holes in the terrain and roads with implausibly steep slopes. Correlation errors occur between different database representations. An SE usually contains a number of subsystems such as the visual out-of-the-window database and the Computer Generated Forces database. If features in one database are represented differently in another (or are missing altogether) then this can have serious implications on the usability of the SE. For example, a road present in a 2D map display but missing from the 3D visuals could impair a user’s ability to navigate effectively. The issue of correlation is further complicated in the case of interoperability between two or more SEs (see Figure 1.1). A recent example of this was the difficulties involved in achieving interoperability between the Close Combat Tactical Trainer (CCTT) and the Aviation Combined Arms Tactical Trainer (AVCATT) [2]. Issues included different fields of view, update rate and polygon priority implementation as well as several aviation models not present in the CCTT. Terrain database quality must also be considered when reusing SE databases or components. Reuse is seen as a way of minimising data redundancy and reducing the costs involved in procuring new simulation systems. However, reusing databases of unknown quality could prove a costly mistake if they are affected by integrity and correlation errors [3]. Clearly a robust verification and validation (V&V) process for assessing the quality of SE databases is required.Until recently, the quality of databases was inspected manually; this is clearly unfeasible for anything but the most simple SE. A highly desirable approach is therefore the automated assessment of integrity and correlation which can provide both exhaustive and quantitative evidence. One such effort is the Synthetic Environment Evaluation – Inspection Tool (SEE-IT) from the Institute of Defense Analyses [4]. This UNIX based application is designed to detect a wide range of errors within a Synthetic Environment Data Representation and Interchange Specification (SEDRIS) transmittal [5]. SEE-IT is a very useful tool for assessing integrity errors on neutral format datasets. However, it cannot detect correlation errors and is limited to a single intermediate format. To our knowledge there are no other tools for the automatic V&V of terrain databases.We have consequently developed a new MOD funded capability, Venator, which can perform both integrity and correlation analysis at all stages of the database generation process, from initial source data through to final runtime databases. This paper provides an overview of Venator and how it is being used to assist database procurement, reuse and interoperability. In section 2, we introduce the tool and outline the four stages involved in analysing a terrain database. A number of case studies are discussed in section 3 whilst section 4 looks at future work and Venator development. Finally, our conclusions to date are given in section 5. VenatorOverviewVenator is a generic solution for assessing the integrity and correlation of source data, intermediate data and runtime databases (see Figure 2.1). It can be used to assess the quality of both new and legacy databases (e.g. when considering reuse of a dataset or interoperability between two SEs). When designing Venator two basic approaches were considered: firstly, generating a bespoke system from the ground up and, secondly, customising an existing commercial Geographical Information System (GIS). We chose the latter as a GIS offered the basic functionality required when processing large geospatial datasets. Key benefits included scalability, a comprehensive visualisation environment,  support for various co-ordinate systems, the ability to import numerous formats (e.g. Digital Terrain Elevation Data and Vector Product Format) and full programmatic access to the underlying object model. Replicating this functionality would have proved very costly. Instead, we were able to concentrate on the important issues of integrity and correlation analysis. Interestingly, commercial GIS are now routinely used as an integral part of many database generation processes [6]. Figure 2.2 shows a typical screen from Venator.Analysing terrain databases using Venator involves a process with four separate stages: data input, correlation and integrity processing of the data, analysing/interpreting the results and finally presentation of the findings. Each of these stages is discussed below.Data inputGiven Venator’s remit for importing everything from source data to runtime databases, a standard file format for storing geospatial data was required. ESRI shapefiles® were chosen as they are an industry standard and open format. If a particular file format is not directly supported by the GIS then it is relatively straight-forward to write an appropriate converter (e.g. SEDRIS to shapefile®). We have successfully imported the following formats: OpenFlight™, SEDRIS, Close Combat Tactical Trainer (CCTT), Combined Arms Tactical Trainer (CATT) and the Compact Terrain Data Base (CTDB).Once imported, input data often requires some pre-processing prior to analysis. One example is that different feature types such as roads and rivers need to be extracted from converted OpenFlight™ data (usually based on texture filename). Another example is the need to extract the ‘edges’ from a polygonal representation of linear networks such as roads and rivers (some of the tests require this edge information). Once pre-processing is complete and the necessary shapefile® layers are loaded into Venator,  integrity and correlation analysis can commence.Integrity and correlation processingA number of integrity and correlation tests which check for specific anomalies and errors have been implemented. Each test is a distinct plug-in with its own graphical wizard that guides the user through the various parameters required for analysis (An example is shown in Figure 	Integrity tests currently supportedTest nameConditions tested forNetwork connectivityGaps, undershoot, overshoot, unconnected segments and unfeasibly sharp bends in a linear network (e.g. roads, rivers, railways).Feature conflictionInvalid intersections between certain feature types (e.g. tree on road, building on railway).Extreme slope3D polygons with unfeasibly large slopes which could indicate a broken feature representation (e.g. vertical barriers in roads).Lateral slopeUnfeasibly large lateral (transverse) slope in linear style networks (e.g. roads, rivers and railways). E.g., rivers should have a lateral slope of 0 degrees.Vertical tearAdjacent 3D polygons that have points with the same (X,Y) co-ordinate but different elevation (Z) values.Terrain hole3D terrain skin such as a Triangulated Irregular Network (TIN) with a non-continuous surface (e.g. missing polygons). Also looks for “T” vertices.Uphill water flowWater flowing uphill (i.e. against gravity). The elevation values of 3D water features should decline sequentially ‘downstream’. If the elevations suddenly increase then the river will appear  to flow upstream.GeometryInvalid feature geometries (e.g. incomplete polygons) and sliver polygons.ProximityFeatures placed either too close or too far from one another (e.g. railway station several miles from nearest railway track).Raster valuesInvalid values in a raster (e.g. missing values or holes in a Digital Elevation Model (DEM)).Correlation tests currently supportedTest nameConditions tested for2D correlationA feature in database A which is has a different position, shape or size in database B. Alternatively a feature in database A which is missing from database B,  or vice versa (e.g. a building in a visuals database is missing from a corresponding CGF database).3D correlationElevation values in database A differ from those in database B (e.g. the terrain skin in a visuals database is inconsistent with the terrain representation in a corresponding CGF database).Vector/DEM correlationElevation values in a vector/polygonal terrain skin differ from those in a corresponding DEM.Attribute correlationThe attributes of features in database A differ from the corresponding features in database B (e.g. a tree-width attribute in feature A is 6.0m but in feature B is 12.0m).2.3). The lateral slope test, for example, allows the user to define a maximum acceptable slope. Any features with a lateral slope that exceeds this value are output as potential anomalies. Appropriate test parameters often vary from database to database and feature type to feature type; an element of ‘trial and error’ is inevitable. This modular plug-in architecture makes it very simple to add new tests as and when needed. For example new, database-specific tests, are sometimes required. Such tests can be constructed rapidly using existing tests and code as templates.The tables above document the currently available integrity tests (top) and correlation tests (below):Performance was found to be a major issue when processing large datasets. Although the GIS was efficient at drawing data, it was less efficient at processing individual features. The problem lies in tests which require spatial querying (e.g. “find all features in layer B that intersect with this feature in layer A”). Spatial querying is very computationally demanding and although the GIS uses spatial indexing we found that executing tests on terrain databases with millions of features was unacceptably slow. Therefore, a dedicated utility was written in C++ to perform rapid spatial querying using shapefiles®. An interface to this utility is available from within the Venator environment to ensure seamless operation. This, combined with other utilities and ‘tweaks’ has drastically reduced the time taken to perform integrity and correlation analyses.For convenience, a batch mode is available so several tests can be set up and run sequentially. This facilitates an efficient use of time as tests can be executed overnight and analysed the following day. The raw data produced by the integrity and correlation tests can be output in a number of user-defined ways: an error layer (which contains all the input features with potential anomalies); an aggregated error layer (where nearby anomalies are aggregated into single features); a summary statistics table, a tabular version of the results (CSV file) and, in the case of correlation tests, a graph. Aggregating anomalies is particularly useful when dealing with an error layer containing thousands of features, so clusters of errors can be identified.Results analysisProducing raw test data is in many ways the easy part – the real challenge is in interpreting the results to extract meaningful conclusions about the ‘quality’ of the database. Although the output from some tests is straightforward (e.g. a feature either conflicts with another feature or it doesn’t) most require an element of human reasoning and intelligence. As it not feasible to manually inspect every potential error a utility, the feature scanner, was created to filter the anomalies based on one or more feature attributes. This ensures the most serious anomalies are dealt with first. For example, a 2D correlation analysis between roads in databases A and B creates a layer of features with an error value equivalent to the distance from a feature in database A to the nearest corresponding point in database B. Large correlation errors are generally more significant than very small correlation errors and should be the main priority when analysing the results. Each feature can be cycled in turn and the view focuses on that particular feature so the potential error can be assessed in the context of surrounding features.Once a potential error has been identified it may be appropriate to verify whether it constitutes a true error or is actually a legitimate representation. When analysing a visual out-of-the-window database it can be difficult to relate a plan view of the area to its appearance in 3D. In these instances it is advisable to navigate to the relevant region in the runtime database (using a stealth viewer) to ascertain the real appearance of the anomaly. For example, a plan view cannot reveal the effects of a vertical tear in an OpenFlight™ terrain skin; there is no substitute for viewing the error in the actual visual database, and then capturing a screenshot as evidence.Given the complexity of terrain databases it is unreasonable to expect that every reported potential anomaly is in fact a true error. Similarly, it should not be expected that the analysis will reveal all errors of a particular type – some might be missed as it is difficult to cover every conceivable scenario. Generally, it is not feasible to manually inspect every potential anomaly, so, it might be the case that an error has been detected but is lost within the ‘noise’ of the data. Presentation of resultsThe final stage of the Venator process is presenting the results of the analysis in a format appropriate to the interested parties. Providing a spreadsheet which documents the location of each and every integrity and correlation error (which could run into the hundreds of thousands) is unlikely to be of interest to a high level manager charged with procuring a new terrain database. However, a filtered version may be of use to the database engineers in isolating certain problems. In our experience it is generally more useful to supply a summary report highlighting the overall quality (in terms of integrity and correlation) of the databases, including quantitative statements such as “39 buildings were found to intersect with roads” and “96% of trees were found to correlate within 0.1m”. This provides an overview of quality and can highlight the key issues that need to be addressed as well as screenshots and maps illustrating certain types of errors. At the highest level, conclusions such as “the anomalies in this databases should not have a significant impact on training effectiveness” or “we do not recommended interoperability between these two SEs” may be drawn.Standard written reports are just one of the ways for distributing results. It is also possible to output labelled error maps (hard and soft copy); a GIS project of the error shapefiles® which can be accessed using a free viewer and spreadsheets of raw or filtered error data. Case StudiesUKCATTThe UK Combined Arms Tactical Trainer (UKCATT) is an advanced virtual training system consisting of two hundred manned simulators split between sites in the UK and Germany. CATT supports large-scale exercises played out on detailed terrain databases typically 100x100 km in size. Each database comprises several representations: visual (out-of-the-window); Semi Automated Forces; Plan View Display and paper maps  which must all be correlated. QinetiQ was tasked by the Defence Procurement Agency (DPA) to support the test & acceptance of CATT and verify that the terrain databases fulfilled the Statement of Requirements. Under this project, Artemis, the Venator capability was utilised to provide automated analysis. In addition to this formal role, several interim analyses were undertaken and a good working relationship established with the contractor Lockheed Martin Information Systems (LMIS). When new versions of the terrain databases were generated, an interim analysis was used to document and verify the improvements made by LMIS. This new approach to test & acceptance, adopted by the DPA, increased confidence in the CATT terrain databases by replacing qualitative judgements with quantifiable evidence and they were successfully accepted in July 2002.Malvern HillsA detailed 8x8km terrain database of the Malvern Hills (UK) was generated using source data of unknown quality for use in chemical/biological modelling. We used Venator to analyse the integrity of the source data, the integrity of the resultant OpenFlight™ database and the correlation between the two (i.e. to measure the success of the terrain database generation tool used). Standard tests (e.g. network connectivity and feature confliction) were run and the study revealed a number of errors in the source data (see Figure 3.1). These included gaps in the road network, buildings blocking roads and trees in the middle of buildings. Integrity errors affecting the OpenFlight™ included holes in the terrain skin (attributed to missing data in the underlying Digital Elevation Model) and vertical barriers in the middle of roads. The correlation analysis detected a large section of missing woodland and a bizarre 80m section of severely broken road which had been erroneously generated in the OpenFlight™ (it was not present in the source data). Another interesting result was a number of missing buildings. We traced this back to incorrect feature attribution affecting a particular building type (which meant the database generation tool did not recognise the features as buildings).  This example nicely illustrates how Venator can be used to identify errors in processes (in this case due to human error) as well as actual databases.Salisbury PlainVenator was also utilised to investigate the reuse potential of an old Salisbury Plain (UK) legacy database. The SE consisted of a 16x12km OpenFlight™ visual database and a 36x35km CTDB representation. Several systematic anomalies were detected including broken bridges in the OpenFlight™, CTDB railways classified as roads, CTDB trees with incorrect foliage radii of just 1.5cm and confliction between roads and wooded areas (roads were not ‘cut’ into the woods as expected) as illustrated in Figure 3.2. Integrity errors included vertical tears, inappropriate lateral slopes and buildings/trees blocking roads. Major correlation errors were a section of roads missing from the OpenFlight™ and a total mis-correlation of soil types between the two databases (which could impact manoeuvrability across the terrain). We concluded that the Salisbury Plain SE was so poorly constructed it could not be recommended for reuse in any form, and highlights the importance of V&V when considering  reuse.Future workIn addition to the creation of new tests and use of Venator to analyse different SEs, future development is currently focused on several key areas: enhanced intelligence; areal to linear conversion; enhanced collaboration and metadata.Enhanced test intelligenceAn interesting conclusion of this work is that fully automated V&V of terrain databases is conceptually simple but extremely difficult in reality. The integrity and correlation tests work in a highly deterministic way with practically no concept of spatial reasoning which is one of the hallmarks of human intelligence. Consequently, there is no easy solution to eliminate the human interpretation of raw results which is the most time intensive and laborious aspect of the Venator process. The tests are simply not ‘clever’ enough to make a judgment about whether a potential anomaly is a true error.In the longer term, some form of artificial intelligence could be integrated within the various tests. One possible approach would be to implement a machine learning algorithm which was trained to recognise errors in a more flexible way. One issue with this approach is that the way the environment is represented varies greatly between different formats and subsystems and the ‘rules’ learnt on certain datasets may not be applicable to other datasets.Ultimately, the ideal situation would be a truly automated process where two or more terrain databases are specified and Venator automatically extracts the relevant feature types, processes them using the various tests, intelligently interprets the results and outputs the findings in a suitable format. Whilst technically feasible, the time and cost involved to achieve this may be greater than the time currently spent on manually inspecting the potential errors.Areal to linear conversionNot every SE analysed will have multiple representations such as CGF and visuals. We are often asked to perform an integrity analysis on a single database (e.g. OpenFlight™). A key problem is that the representation of roads, rivers and railways is areal (polygonal) whereas some of the Venator tests require linear (centreline) data. This means we currently cannot perform network analysis on a visuals database. Some method for extracting/interpreting centrelines from areal data is therefore required. We are currently investigating whether the Automated Linear Feature Identification and Extraction (ALFIE) project [5] offers a solution to this problem. ALFIE has been designed to extract relevant feature data (roads, rivers, railways) from aerial and satellite imagery. It is envisaged that a boolean raster is produced of the relevant areal features (e.g. road polygons) which ALFIE will then process to determine centrelines. These linear features will then be imported back into Venator as a shapefile®, ready for analysis.Enhanced collaboration and interactive mappingAssessing quality is recommended at every stage of the terrain database generation process and prior to any reuse or interoperability. A more collaborative environment for sharing results (e.g. reports and maps) would help integrate V&V within the overall process and facilitate communication between database engineers and the V&V analysts. For example, preliminary results from a Venator analysis might indicate a systematic error in the terrain database (e.g. a certain type of building intersects with a certain type of trees). The database engineers could quickly check their processes and fix the problem at an earlier, rather than later (and more expensive) stage. We therefore plan to investigate peer to peer environments for information sharing and collaboration.As part of this effort, we are developing the ability to publish web-based error maps using Scalable Vector Graphics (SVG) technology. This will allow users to interact with high quality maps and add and exchange suitable annotations in a collaborative manner. We envisage that the error maps will be generated automatically from within the GIS through creation of a new SVG export extension.MetadataVenator could provide invaluable metadata (data about data) to help document the correlation and integrity of a database. Once a Venator analysis is complete, quality metadata could be used to facilitate future reuse and interoperability as it gives a potential user quantitative information to gauge its suitability. The sister project of this work is investigating the concept of a repository of terrain databases and components. A major reason behind this research is to facilitate the reuse of SE terrain databases and reduce data redundancy, and metadata is a key part of that vision [7]. Quality metadata would be an ideal candidate for inclusion in such a repository and could potentially be produced automatically from within Venator.ConclusionTerrain databases will never be perfect, given the complexity of the processes used to generate them. It is therefore vitally important to gain an insight into the types of errors present and how widespread they are. Failure to do so could result in significant problems when the SE is finally deployed which may severely limit its usefulness. Understanding integrity and correlation is equally important when reusing legacy terrain databases or interoperating several SEs. The manual inspection approach is no longer viable and new semi-automated inspection is the only reasonable alternative.Tools such as SEE-IT and Venator are ultimately designed to help improve quality and reduce the cost and time involved in rectifying the types of errors they identify. Utilising GIS technology and industry standard shapefiles® means that a wide range of data can be analysed from source data to runtime databases. Integrity and correlation testing is a technically demanding and non-trivial task which currently augments human intelligence rather than replaces it.All three example case studies highlight the importance of V&V. The experience with UKCATT showed that automated V&V can make a positive contribution to the successful procurement of SEs. The Malvern Hills database revealed problems with source data, the construction process and even the tool used to generate the output database. Finally, the Salisbury Plain case study showed that the integrity and/or correlation of a terrain database might be so poor that reusing it is unadvisable. If the issue of quality is ignored, terrain databases will continue to be affected by broken bridges, flying tanks, trees blocking roads and all the other problems currently encountered in SEs today. We suggest semi-automated V&V approaches are employed as a matter of course in order to replicate the success of systems like UKCATT.ReferencesRichbourg, R. and Stone, S., “Towards the Production of Syntactically and Semantically Correct Synthetic Environment Databases”, In Proceedings of the Spring Simulation Interoperability Workshop, March 2003, 03S-018.Fortin, M., Simons, R. and Butterworth, M. “Database interoperability – The CCTT to AVCATT conversion experience”, Interservice/Industry Training Systems and Education Conference, November 2002. Richbourg, R. and Stone, T., “Automating Error Detection and Correction in Synthetic Environments”, In Proceedings of the Fall Simulation Interoperability Workshop, September 1998, 98F-SIW-087.Synthetic Environment Evaluation – Inspection Tool,  HYPERLINK "http://tools.sedris.org/seeit.htm" http://tools.sedris.org/seeit.htm, accessed August 2003.Donovan, K, Field, M. and Gutowski, G., “An Open Architecture for Synthetic Environment Database Generation”, In Proceedings of the Fall Simulation Interoperability Workshop, September 1999, 99F-SIW-179.Wallace, S., Hatcher, M., Priestnall, G. and Morton, R., “Research into a framework for automatic linear feature identification and extraction, Automatic Extraction of Man-made objects from aerial and space imagery (III), 2001, Baltasavias, E.P., Gruen, A. and Van Gool, L. (eds.), Lisse: Swets & Zeitlinger.Merrifield, B. and Smith, N., “Repositories for Synthetic Natural Environments: Interim Report”, QinetiQ report, October 2001, QINETIQ/S&E/SIP/CR011974/1.0.AcknowledgementsThis work is sponsored by MOD and is part of the SSS Corporate Research Programme.Author BiographiesPAUL CLARKE joined QinetiQ in 2000 as a Research Scientist in the Sensors & Electronics division. He has a BSc (Hons) in Geography and an MSc in Geographical Information Systems (GIS). He has worked on several SE projects including the CRPs "Correlated Synthetic Natural Environments" and "Shared Data Environments for Synthetic Natural Environments" which focused on the creation of consistent and correct SEs to facilitate reuse. This involved the development of a terrain database QA tool, Venator, which is being exploited to assess and improve various SEs. He is the technical lead for enAble3D® which produces real-time 3D visualisations using standard 2D GIS data.PAUL WONNACOTT has been conducting research into the Synthetic Natural Environment (SNE) for six years. He is the Lead Researcher on the TG10 “Consistent SNEs” project and previously for the successful TG10 “Dynamic Synthetic Environments” project. He also worked on the run-time system for the APOSTLE language for parallel discrete event simulation, which gained him a PhD in Computer Science from Exeter University. He was a principal designer of the APOSTLE language itself. Paul also conducted a successful study for the UK OneSAF programme on time management in the SNE. Paul has an MA (Cantab) in Computer Science and has published numerous papers on parallel and distributed simulation and the SNE, including the 'Best Paper' at the 10th Workshop on Parallel and Distributed Simulation. Paul is a member of the British Computer Society and a Chartered Engineer.© Copyright QinetiQ ltd. 2004Figure 1.1 Integrity and correlation within and between two Synthetic EnvironmentsFigure 2.1 Integrity and correlation analysis at each stage of the terrain database generation processFigure 2.2 Venator screenshotFigure 2.3 Example Venator Wizard interface (Vertical Tear integrity test)Figure 3.1 Malvern Hills anomalies: severely broken road (left) and missing buildings (right)Figure 3.2 Salisbury Plain anomalies: church blocking road (left) and broken bridge (right)Missing buildings