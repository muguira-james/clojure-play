Independent Throughput and Latency Benchmarking for the Evaluation of RTI ImplementationsMs. Pamela KnightMr. Aaron CorderMr. Ron LiedelU.S. Army SMDCP.O. Box 1500106 Wynn DriveHuntsville, AL  35807-3801 HYPERLINK mailto:Pamela.knight@smdc.army.mil pamela.knight@smdc.army.mil,  HYPERLINK "aaron.corder@smdc.army.mil" aaron.corder@smdc.army.mil,  HYPERLINK "LiedelR@smdc.army.mil" LiedelR@smdc.army.milMs. Carol JenkinsMr. Ray DrakeMr. Paul AgarwalDr. Edwin NunezCOLSA CorporationAdvanced Research Center6726 Odyssey DriveHuntsville, AL  35806Tel. 256-922-1512  HYPERLINK mailto:cjenkins@colsa.com cjenkins@colsa.com,  HYPERLINK mailto:rdrake@colsa.com rdrake@colsa.com,  HYPERLINK mailto:pagarwal@colsa.com pagarwal@colsa.com,  HYPERLINK mailto:enunez@colsa.com enunez@colsa.comKeywords: Runtime Infrastructure, benchmark, HWIL, latency, throughput, HLA, distributed simulationABSTRACT:  The utilization of Hardware-In-Thethe-Loop (HWIL) processing in a Modeling and Simulation (M&S) environment introduces especially critical latency and throughput requirements.  Data provided by HWIL simulations must be transmitted and processed expeditiously to reduce the risk of information loss.  Of particular interest are issues related to performance – namely, latency and throughput – of the High Level Architecture (HLA) federates interacting by way of a Runtime Infrastructure (RTI).   This paper presents the benchmark design, test approach, and some initial results reported from the evaluationng of four commonly used and readily available HLA-compliant RTI implementations. The evaluated RTIs include the: (1) RTI Next Generation (NG) 1.3v3, (2) MÄK Real-time RT 1.3.3-ngc, (3) Pitch portable RTI (pRTI)1.0r5, and the (4) Georgia Tech Parallel and Distributed Simulation (PADS) Federated Simulations Development Kit (FDK) 3.0 Detailed RTI.The benchmark activity was conducted under the Wide Bandwidth Information Infrastructure (WBII) Program.  The benchmarks employed selected computing resources from the Federation Analysis Support Technology (FAST) Laboratory, a part of the Space and Missile Defense Battle Lab (SMDBL) located at the Advanced Research Center (ARC) in Huntsville, Alabama.  The FAST Lab is a shared Ballistic Missile Defense Organization (BMDO)/SMDBL community asset that assists programs to achieve their M&S goals by leveraging its available high-performance computing (HPC) and High Level Architecture support (HLA) resources to encourage the wider utilization of these distributed simulation technologies.  The benchmark's experimental design evaluates the effect of twelve independent variables on throughput and latency.  The independent variables evaluated include: RTI, number of federates, distribution of federates, Data Distribution Management, network transport mode, objects per federate, attributes per object, interactions per federate, parameters per interaction, attribute buffer size, interaction buffer size, and data bundling. Initial latency and throughput measures were then evaluated to determine if the four RTIs exhibited statistically significant performance differences.  The benchmark results are intended to provide assistance to Modeling and Simulation personnel in HLA federation design and optimization.  This effort was performed under the Congressional Wide Band Information Infrastructure (WBII) initiative. The Space and Missile Defense Command is the executing agent of the WBII and SAIC is the Prime Contractor.DISTRIBUTION A. Approved for public release; distribution unlimited. 1.	IntroductionThe Department of Defense (DOD) High Level Architecture (HLA) has become a standard for models and simulations interacting in a distributed fashion.  It was mandated under DOD directive 5000.59 as the “standard technical architecture for all DOD simulations.”  The main purpose of HLA is to promote reusability and interoperability of simulations.  It was developed to satisfy the requirements of simulations in a wide variety of areas including analysis, testing, training, hardware-in-the-loop (HWIL), and other engineering functions. An HLA federation is primarily comprised of one or more federates, the federation object model and the Runtime Infrastructure (RTI).  HLA is a software architecture that permits objects in one simulation to exchange data with objects in another simulation through services provided by the RTI.  HLA is predicated on the concept of a federation, that is, a composable set of distributed models or simulations interacting with each other.  Each member of the federation is called a federate.  A federate can be any of a wide variety of entities including a computer simulation, an interface to a radar, a data collection utility, or an interface to a live player.  A federate presents (publishes) data for other federates to input (subscribe).  It is within the federates that all objects are represented.  Interactions between federates are not conducted directly, but through the functions provided by the RTI.  In addition, the RTI carries out support services required for federation management.  Thus, the RTI is a distributed run-time interface for the whole federation.The Wide Band Information Infrastructure (WBII) Interoperability Integrated Product Team (IPT) is tasked to investigate issues related to simulation interoperability and systems interoperability.  An important concern is the impact that data exchange overheads has on the performance of simulation systems.  Another consideration is the use of real-time HWIL simulations because they have much more critical latency and throughput requirements.  Data provided by HWIL simulations must be transmitted and processed expeditiously to reduce the risk of information loss.  The Federation Analysis Support Technology (FAST) Laboratory located at the Advanced Research Center (ARC) is chartered to provide assistance and technical support to the modeling and simulation community concerning issues related to distributed simulation technology using HLA.  Problems related to new and legacy simulations desiring HLA compliance, as well as HLA migration analysis, development, and testing are in the direct purview of the FAST Lab.  Several computer platforms are specifically dedicated as FAST Lab assets.  They can provide diverse cross-platform environments for the development, testing, and analysis of distributed simulations.  The ARC also has available a multiplicity of other platforms dedicated to National Missile Defense (NMD) and Space and Missile Defense Command (SMDC) work.2.	PurposeThe purpose of this RTI evaluation activity is to assess the performance of currently available RTI implementations.  Of particular interest are the response variables of throughput and latency.  This is because of their criticality to HWIL participants in HLA exercises. The performance of a federation is affected by many factors.  These include the RTI capabilities, operating system, local and wide-area network (LAN/WAN) environments, federate behavior and characteristics, hardware platforms, and network interface cards, among others.  The RTI is crucial to all interactions between federates and its performance can sharply penalize federation performance. RTI evaluation and selection should be considered during the design phase of a federation because there may be performance, functional and compatibility tradeoffs that impact design.The RTIs under test include:RTI-NG 1.3v3  -   The Run-Time Infrastructure Next Generation 1.3.  This is the reference implementation.  It was sponsored and developed by Defense Modeling and Simulation Organization (DMSO) and is available free of charge to all qualified federation developers.  It corresponds to the HLA Interface Specification version 1.3 ( HYPERLINK http://www.dmso.mil/briefs/msdocs/stand/ie/ifspec-d01-body.pdf http://www.dmso.mil/briefs/msdocs/ stand/ie/ifspec-d01-body.pdf).  It provides a collection of common services that can be accessed through a standard programming language API.  It supports C++, Java, Ada 95, and the CORBA Interface Definition Language.  This RTI is HLA compliant.MÄK Real-time RTI  -  The MÄK Real-time RTI is developed by MÄK Technologies ( HYPERLINK http://www.mak.com/rti.htm http://www.mak.com/rti.htm).  It is currently available free of charge to the simulation community.  It can be configured to use point-to-point, broadcast, or multicast communications for flexibility across different network architectures. In addition, they indicate the MÄK RTI minimizes CPU and memory requirements, simplifying the architecture of HLA-compliant simulations.  However, this RTI does not implement all services.  Thus, all requirements might not be met for some federates.  MÄK Technologies also has developed several tools that can operate with its Real-time RTI.Pitch portable RTI (pRTI)  -  The Pitch portable RTI is developed by Pitch Corporation in Linköping, Sweden ( HYPERLINK http://www.pitch.se/prti http://www.pitch.se/prti).  It is a platform-independent implementation of all services documented in the HLA Interface Specification version 1.3.  It is implemented in Java.  The pRTI runs on Windows NT4/95/98, Sun, SGI, RedHat Linux and other platforms, providing full-scale interoperability on multiple platforms including C++ bindings.  This is also an HLA compliant RTI.RTI-Kit Developed RTIs  -  The RTI-Kit is a collection of libraries designed to support development of RTIs for parallel and distributed simulation systems, especially federated simulation systems running on high performance platforms.  Each library is designed so it can be used separately, or together with other RTI-Kit libraries, depending on the functionality required by the user.  The specific RTI tested will be the Detailed RTI (DRTI) which is a sample TCP/IP implementation of the RTI based on the RTI-Kit.  It is part of the Federated Simulations Development Kit developed at Georgia Technical Institute by Prof. Richard Fujimoto ( HYPERLINK http://www.cc.gatech.edu/computing/pads/tech-highperf.html http://www.cc.gatech.edu/computing/pads/tech-highperf-rti.html).  The developers claim it contains composable modules to build RTIs from which different simulations can be integrated with each other.  It is designed so that RTI developers can pick and choose from the set of FDK modules that are most appropriate for their particular RTI implementation.  Developers can use the ready-made modules, saving the time required to elaborate them on their own.  The RTI-Kit, however, does not implement all HLA services.The availability of several RTIs is especially important for HWIL systems that may find a high performance limited-functionality RTI is essential for its low latency and high bandwidth requirements, even if that RTI does not have full functionality.Table 1 presents the characteristics of the hardware systems that were used in the RTI benchmark testing.  These systems are networked on a LAN and can be connected to other ARC LANs and the Internet.  While under test, these systems are on an isolated network.  Table 1.  Isolated Test AssetsHostnamePlatformOperating SystemCPUsRAMSG 189OCTANEIRIX 6.5(2) IP30 195 MHz704 MBSG 190OCTANEIRIX 6.5(2) IP30 195 MHz704 MBSG 201ONYX 2IRIX 6.5(2) IP27 195 MHz1 GB3.	Experiment DesignA set of tests with various hardware and software configurations have been designed to measure the effects on throughput and latency.  The objectives of these tests include:Establish performance comparisons between RTI interaction parameters and RTI object attributes.Determine the optimal configuration of Reliable/Best Effort and Bundling/No Bundling for various buffer sizes.Compare how well the RTIs scale with respect to the number of federates, number of objects, and number of attributes per object.Compare how well the RTIs scale with respect to the number of federates, number of interactions, and number of parameters per interaction.Examine how well RTIs utilize the network with respect to the network’s theoretical best performance characteristics.Determine the largest object attribute that may be delivered reliably.Independent VariablesThe factors of the experiment include:RTI implementationNumber and distribution of federates (and RTI) within computer systemsData Bundling – on or offNetwork Transport Mode – set as reliable or best effortMethod – varying object attributes or interaction parametersNumber of objects or number of interactions per federateNumber of attributes per object or number of parameters per interactionSize of the attribute or parameter buffer The experiments do not represent a complete block design because some experiment configurations are not applicable for a specific RTI implementation.4.	Test ConfigurationsTable 2 shows the ten configurations of computer systems, RTI execution platform, and federates used during the latency and throughput tests.  As is indicated in the table, one federate is designated a sending federate and the other a receiving federate.Table 2.  Latency and Throughput Test ConfigurationsConfigurationSG 189 (Octane)SG 190 (Octane)SG 201 (Onyx 2)INot usedNot usedRTIf1 (sending)f2 (receiving)IIRTIf1 (sending)f2 (receiving)Not usedIIIRTIf1 (sending)f3 (receiving)f2 (receiving)f4 (sending)Not usedIVRTIf1 (sending)f2 (receiving)f3 (receiving)f4 (sending)Not usedVRTIf1 (sending)f2 (receiving)Not usedVIf1 (sending)f2 (receiving)RTIVIIf1 (sending)f3 (receiving)f2 (receiving)f4 (sending)RTIVIII2 Feds (sending)2 Feds (receiving)2 Feds (receiving)2 Feds (sending)RTIIX4 Feds (sending)4 Feds (receiving)4 Feds (receiving)4 Feds (sending)RTIX8 Feds (sending)8 Feds (receiving)8 Feds (receiving)8 Feds (sending)RTIRTI HostIn order to use the RTI, a federate program must link with the RTI library.  This will provide the federate with the RTI interface.  Every federate has an RTI interface specific to it called the local RTI component (LRC). The LRC resides in the federate’s process space.  In addition, some RTI implementations require extra processes to coordinate communication between federates and the general federation management.  If the federates reside on multiple systems, these additional processes have the potential to either degrade or increase the performance of particular federates.  An example of such a process is the “rtiexec”, RTI executive, required by the DMSO RTI.  If a federation has two federates and each federate is on a different system, the potential for federate bias by the RTI exists.  In that case, the required rtiexec can only reside on one of the two systems running federates, adding computational overhead to the system on which it is running.  The co-located federate’s performance may then be degraded.  It may also be the case that since the rtiexec has some control of the federation, it may favor the co-located federate and increase its performance.  Some of the benchmark configurations are designed to specifically test for this RTI federate bias caused by the location of any required RTI support processes such as the rtiexec. Configuration DescriptionIn the Configurations shown in Table 2, the placement of the RTI is intended to determine the bias resulting from required RTI support processes.  Some RTIs, such as GTRI’s DRTI, may be completely decentralized and not require these extra processes.  For such RTIs, the Configurations that test specifically for RTI federate bias, such as Configurations III through VI, are not relevant and will not be used because there is no potential for RTI process-specific bias.Configurations II through V use two computers of equal capability.  In Configuration II, the RTI and one sending federate (f1) reside in SG 189, while the receiving federate (f2) runs in SG 190.  This configuration serves to determine if the RTI favors the sending federate (f1) that runs with it in the same computer over the receiving federate (f2) residing on the other computer.  Configuration III runs two federates per system.  Computational load balance is attained since each system has one sending and one receiving federate.  Thus, federate f1 in SG 189 sends object updates or interactions to federate f2 in SG 190, while federate f4 does the same with federate f3.  This configuration tests whether the RTI, in the presence of more than one federate, asymmetrically favors the sending of information across platforms for the federate that shares the same computer. In Configuration IV two federates run per system, but the sending and receiving federates are located in the same system.  This configuration is intended to help establish how the RTI might favor the sending-receiving pair co-located with it in the same computer.  It will also be compared with the sending-receiving relation in Configuration III.  Configurations V and VI have the RTI running in a system separate from the other federates.  Configuration V has a two-system architecture, and Configuration VI presents a three-system architecture.  Results from Configuration V will be contrasted with those of Configuration I.  Similarly, the results from Configuration VI will be compared with those of Configuration II.  Through the analysis of the latency and throughput data from these configurations, it is possible to conduct tests to measure the statistical significance of the results. Configurations VI to X use three computers, with the RTI running exclusively on the system with more RAM , SG 201.  Through the whole sequence of tests, the number of federates will be progressively increased until it reaches sixteen per system.  Load balance will be maintained by always having as many sending as receiving federates per system.  This series of configurations test how RTI capabilities scale by increasing the number of federates and also provides information on how communications between federates change with other independent variables.The RTI’s capability to update objects and their attributes under different independent variable combinations is also  examined.  Similar tests are conducted by presenting the RTI with an increasing number of interactions with different parameter buffer sizes. Performance also depends on whether the data is sent using the “bundling” or “no bundling” data transport modes.  In the first case, the RTI waits until the amount of data to send is large enough to fill a buffer or a timeout occurs.  In the “no bundling” case, the data is sent immediately.  While “bundling” ensures that the message traffic in the LAN/WAN is reduced, the “no bundling” case has the advantage of sending data with the least latency.  The RTI’s default value for the “bundling” buffer size and timeout will be used whenever that data transport mode is required.Other choices provided by the RTI are “reliable” or “best-effort” network transport modes.  These services specify the transport mechanism to be used for object attribute updates or for interaction parameter transmission.  For “reliable” transport the TCP protocol is used to guarantee that the underlying network will not discard updates.  The RTI guarantees that data from one federate has reached its destination.  “reliable” transport mode is recommended for essential updates that must be delivered to all subscribers. The “best-effort” mode delivers messages using the UDP multicast protocol to attain efficient update delivery to a large number of subscribers.  However, it presents the possibility that “best-effort” updates may be discarded by the network without getting delivered to all intended recipients.  “Best-effort” delivery is recommended for updates that tolerate some data loss.  By comparison, “reliable” updates are characterized by higher latency and overhead, consuming more network bandwidth.  “Best-effort” service generally has lower latency and bandwidth requirements.Response VariablesThe series of latency and throughput benchmark measurements will reveal the capabilities and limitations of the different RTI and system configurations.  Latency will be measured as round-trip latency.  This is the interval from the time a message is sent by a federate to the time it receives message confirmation.  Halving the round trip latency approximates one-way latency.  Throughput is calculated from the perspective of sending and receiving federates. For sending federates, throughput is determined based on the duration required to send the specified number of updates and the corresponding amount of data. Likewise, for receiving federates throughput is determined based on the duration required to receive the updates and corresponding amount of data.One-Way Object Throughput Test SuitesIn these three test suites, throughput is examined using objects. An example of an object would be a tank.  Objects are made up of attributes that can be updated by federates.  A particular tank is an instance of an object.  For example, an M1A1 tank labeled as tank 128 is an instance of the object tank.  Since objects may remain in existence during the whole execution of a federation, they are said to be persistent.  To eliminate instances of objects a specific deletion instruction must be issued by the  federate owning the object.There will be three object test suites performed to evaluate one-way object throughput:Test Suite I –  Configurations I to VI, “bundling”/”no bundling”, “reliable”/best effort, increasing number of objects with one fixed attribute.Test Suite II –  Configurations I to VI, “bundling”/”no bundling”, “reliable”/best effort, one object with one attribute of increasing size.Test Suite III – Configurations VII to X, “bundling”/”no bundling”, “reliable”/best effort, one object with one attribute of increasing size.One Way Interaction Throughput Test SuitesFederations also have another way for representing data called interactions. The properties of interactions are called parameters.  Interactions are short-lived and have no persistence.  They are typically used to represent transient events.  An example of an interaction is an explosion.  The status of a particular tank object may be changed by the owning federate in response to an interaction representing an explosion if it occurred in close proximity to the tank.  A subscribing federate may request an update for an object such as a tank.  It cannot request an update for an interaction because interactions do not have a persistent state. To test the effect of interactions on throughput under different conditions, three benchmark test suites are conducted.  These test suites are similar to the first three, except that interactions and parameters will be used instead of objects and attributes.Test Suite IV –  Configurations I to VI, “bundling”/”no bundling”, “reliable”/best effort, increasing number of interactions with one fixed parameter.Test Suite V –  Configurations I to VI, “bundling”/”no bundling”, “reliable”/best effort, one interaction with one parameter of increasing size.Test Suite VI – Configurations VII to X, “bundling”/”no bundling”, “reliable”/best effort, one interaction with one parameter of increasing size.Round Trip Object Latency Test SuitesOne-way latency comprises the interval from the moment a sending federate sends an object update to that when a receiving federate accepts the information.  To measure one-way latency, precise time synchronization of all computers is required.  Specialized hardware and software are necessary for that purpose.  An approximation to one-way latency is adopted since specialized hardware and software was not available. Time is registered at the moment an object update or interaction is sent by a federate and upon receipt of an acknowledgement message from the receiving federate.  This time interval is designated as round trip latency.  When divided by two, it provides an estimate of one-way latency.  Such an estimate is predicated on two premises: latency symmetry and unobtrusiveness of the operating system.  The first condition simply assumes the message should take as long to traverse from sender to receiver via the RTI as it does from receiver to sender.  Such a condition is generally true but is not always guaranteed.  The second condition is harder to estimate since the computers in the test suite do not use a real-time operating systems.  Consequently, as updates are transmitted, it is difficult to determine if the operating system preempts message processing to instead dedicate time to perform some “housekeeping chores” or services another user. For the test suites in this section, an isolated lab network is used.  It is not shared by other users and will only have traffic from the federation, federates, or RTI being tested.  Under these conditions interruptions by the operating system are be minimized and are not believed to substantially bias latency measurements. To test how round-trip latency is affected under different data transport conditions, two test suites are performed.  These are:Test Suite VII –  Configurations I to VI, “bundling”/”no bundling”, “reliable”/best effort, increasing number of objects with attributes of increasing size.Test Suite VIII – Configurations VII to X, “bundling”/”no bundling”, “reliable”/best effort, one object with one attribute of minimum size.These test suites have substantial similarity with those used for throughput.  However, some important differences also exist since they will answer somewhat different questions concerning federate and RTI capabilities.Round-Trip Interaction Latency Test SuitesFederations not only use objects, but also work with interactions.  Unlike objects, which have persistence and remain active until specifically eliminated, interactions are short-lived.  Once they are broadcast they cease to exist and cannot be retrieved by any federate.  Interactions are comprised of parameters.Two test suites will be conducted to determine the effect of different configurations, data transport methods, number of interactions, and parameter size on round-trip latency.  These test suites are:Test Suite IX –  Configurations I to VI, “bundling”/”no bundling”, “reliable”/best effort, increasing number of objects with attributes of increasing size.Test Suite X – Configurations VII to X, “bundling”/”no bundling”, “reliable”/best effort, one object with one attribute of minimum size.These test suites are similar to the ones used to test for round-trip object latency.  In these suites, however, federates will send interactions rather than object updates.Benchmark SoftwareThe benchmark software is depicted in Figure 1.  It consists of three components:  Benchmark Manager, Test Sequence Format file, and the Benchmark Federate.  The Benchmark Manager is the driver software.  It is written in PERL. The Benchmark Manager parses Test Sequence Format (TSF) files.  It starts the federates on the proper host/directory with the specified options.  The manager verifies the TSF file before initiating a test.  The manager supports the RTI implementations DMSO RTI-NG1.3v3.2, GTRI FDK 3.0 DRTI, MAK RTI1.3.4-ngc, and the Pitch PRTI (1.0r5).  It conducts  federate  process cleanup following test execution and provides the throughput and latency data for off-line processing.The TSF file allows the full test and system configuration to be defined.  It may contain multiple test sequences.  There may be multiple types of tests specified in the same file but not in the same test sequence.  The test types supported include Round-trip Object Latency (RTOL), One-Way Object Throughput (OWOT), Round-Trip Interaction Latency (RTIL), and One Way Interaction Throughput (OWIT).  The benchmark federate software is written in C++ and reuses many of the design aspects of federates from the DMSO benchmark programs.  It is not intended to replace the DMSO benchmark but to complement the results obtained.  The federate software is capable of performing latency and throughput tests for objects and interactions.  Multiple object or interaction types may be used in a test.  Multiple instances are supported for objects.  The benchmark federate is capable of utilizing dynamic transport modes if this is supported by the RTI implementation.  The federations are comprised of one or more pairs of exclusive send and receive federates.  The master federate propagates test descriptions to subordinate federates.  The master federate synchronizes tests without relying on RTI synchronization points. It does require that the RTI support “reliable” transport for specific RTI interactions. The master federate gathers and stores sample level statistics in “*.sam” files for data analysis following the test. Figure 1.  Benchmark SoftwareFindings and ResultsThe Pitch portable RTI was available for a 60-day evaluation period.  Because of this limitation, benchmark tests were first conducted with the Pitch RTI.  In conjunction with Pitch, enhancements were made to the “tick” function prior to testing.The Benchmark Manager Software was not able to terminate some processes associated with running the benchmark tests using the Pitch RTI.  This resulted in required manual intervention to clean up processes that were not terminated automatically.  This ultimately resulted in a longer than expected time required to conduct the benchmark testing with the Pitch RTI.Automation of the benchmark testing was possible on RTI NG v1.3v3.  This expedited the process and allowed data collection for the entire test suite.  RTI NG v1.3v3 is the most mature RTI and the test team perceived it to be the most stable of the four RTIs.While running the benchmark test suite with the MÄK Real Time RTI, some problems were encountered.  This related to the optimal selection of the RTI “tick” that designates processor time increments and duration allotted to the RTI.  This delayed obtaining a complete benchmark suite for this RTI.Tests that measured RTI bias were not applicable to the FDK.  These benchmark tests were not conducted.Performance of each RTI was sensitive to the selection of the RTI “tick” frequency and duration.  This specifies the processing interrupt permitted for the RTI.Benchmark programs and testing were modified to account for limitations of the RTI implementations.  All of the RTIs were tested using the same benchmark software.The DMSO standard benchmark suite was run on the shared computing assets in the FAST Lab.  Comparing the data collected from the One Way Object Throughput test for varied buffer sizes provided the anchoring data for test validation.  The data collected from the DMSO benchmarks were not statistically different from that collected by running the developed benchmarks using the RTI NG v1.3v3 at the 5% significance level.  A “t” Test for paired observations of the sample means was the statistical method used. EMBED Excel.Chart.8 \s Figure 2.  Throughput Compared For Four RTIs*Note: Modified from Printed ProceedingsBenchmark tests were run for One Way Object Throughput with 10 samples of 100 updates each.  The buffer size was tested at 8, 16, 32, and 64 bytes (corresponding to buffer size 1, 2, 3, and 4).  Initial results from this very specific test are provided in Figure 2.  ConclusionsThe selection and configuration of the RTI and federates effects the performance of the Federation.  Depending on the experiment under test, the number of objects, the number of interactions, the RTI should be selected carefully and tuned for optimal performance.  RecommendationsAdditional benchmark analysis is required to complete the collection of latency and thoughput data outlined in this paper.  This includes statistical analysis to indicate the significance of the results.   Further analysis is an on-going effort.References[1] DMSO, “HLA Hands On Practicum, Programming with the Run Time Infrastructure 1.3”, Course Materials, 1 February 1999[2]  Kuhl, Dr. Fredrick, Weatherly, Dr. Richard, Dahmann, Dr. Judith:  Creating Computer Simulation Systems, An Introduction to the High Level Architecture, Prentice-Hall, Inc., Upper Saddle River, NJ, 2000.[3]  Richardson, Russ, Wuerfel, Roger:  “RTI Performance Testing”, briefing material, 28 September 1998 Author BiographiesPAMELA KNIGHT is the Acting Director for the Space and Missile Defense Command’s Information Technology Directorate.  Ms. Knight has more than 20 years of professional experience in physics, engineering, test and evaluations, "system engineering," information technology and management. AARON CORDER is the Technical Manger for the WBII/WBT contract activity.   He holds a BSEE from the University of Alabama in Huntsville.  Mr. Corder has a wide range of experience in Army and Air Force Systems, semiconductors, electronics/hardware testing, optics, and microelectronics.  He is a member of the Space and Missile Defense Command’s Information Technology Directorate.RON LIEDEL is a senior engineer with the Space and Missile Defense Battle Lab (SMDBL) located in Huntsville, AL, and has authored numerous SMDC and BMDO policy forming software documents.  These include the SMDC Software Development Plan, the SMDBL 10 Year Software Plan, as well as the Command Software Mission and Function Statement.  Mr. Liedel founded and chaired the SMDC/SDIO Computer Resource Working Group and has presented several papers nationally on Software Sizing for Mega Systems.CAROL JENKINS is a Senior Systems Engineer with COLSA Corporation.  She has over 20 years of professional experience in DoD programs including Modeling and Simulation, Design of Experiments, Statistical Data Analysis, Software Development, Independent Verification and Validation, Communication Analysis and Weapon System Test and Evaluation.  She earned her Bachelor of Industrial Engineering degree in 1981 and her MS in Engineering from the University of Alabama in Huntsville.  She is responsible for data analysis following the benchmark testing.RAY DRAKE is a Software Engineer for COLSA Corporation in Huntsville, AL.  He has over 13 years of experience in system administration, optimization, and programming across various UNIX platforms.  He is currently the technical lead for the Federation Analysis Support Technology (FAST) Lab at the Advanced Research Center (ARC) in Huntsville, AL.  He is responsible for developing the benchmark software used to conduct the timing studies. DR. EDWIN NÚÑEZ is a scientist for Research, Development, Test & Evaluation at COLSA Corporation in Huntsville, AL.  He has been involved with WBII and projects requiring the application of innovative technologies to algorithm development.  Dr. Núñez provided the Design of Experiments for the test cases run for the benchmark testing.PAUL AGARWAL is the COLSA Corporation Advanced Research Center (ARC) FAST Lab Program Manager.  He also serves as the COLSA Corporation ARC Program Manager for the ITB, a joint international missile defense simulation testbed project co-sponsored by SMDBL's Testbed Project Office.  He has over 18 years experience in public and private sector computer software and systems analysis, design, development, evaluation, implementation, and program management. EMBED Word.Picture.8  