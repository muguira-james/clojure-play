Optimizing Performance of an Analysis FederationDavid L. ProchnowDavid W. SeidelThe MITRE Corporation1820 Dolley Madison BlvdMcLean, VA  22102prochnow@mitre.orgdseidel@mitre.orgKeywordsHLA, Performance, Pegasus, Management Object Model, MOM, Time Management, RTI Initialization Data, RID, TickAbstractGood performance is essential for a federation used for analysis.  The ability to make numerous runs quickly allows for analytic results with statistical significance and provides more time to test excursions.The Pegasus federation, comprised of three service simulations, will be used for analysis by the Joint Forces Command (JFCOM).  Because the run speed was not adequate to meet JFCOM's required number of runs, the Pegasus team developed methods for measuring and, ultimately, improving performance.  The cumulative effect of the resultant changes was a federation that ran eight times faster than before.This paper will examine the methods employed to measure and improve performance.  Specifically, this document will discuss the use of the HLA Management Object Model, instrumentation of federates for activity and latency data, use of different time management schemes, modification of RTI Initialization Data (RID) parameters, and impact of tick().1. IntroductionPerformance is a rather generic term that means different things to different people.  Similar to the approach that Ferrari takes in his book on computer systems performance [FERR], in this paper, when we discuss performance, we are referring to how fast our federation runs.  We do not include correctness nor do we include system reliability in our assessment, though those aspects would certainly be included with a wider definition of performance.More specifically, this paper focuses on federation performance.  Thus, we take a holistic approach in which we focus on how well the entire federation runs, rather than the performance of the individual components.  This approach considers the interdependencies of many subsystems, each of which may perform far better in isolation.The remainder of this section provides a brief description of our problem space, with an overview of Pegasus and its performance requirements.  Section 2 discusses how we measured performance, and Section 3 describes how we analyzed performance.  Next, Section 4 describes our efforts at improving performance.  Finally, Section 5 lists our conclusions.1.1 PegasusThe Pegasus federation was originally used with a Joint Suppression of Enemy Air Defense (JSEAD) scenario that examined Attack Operations Against Critical Mobile Targets (AOACMT).  It is now being prepared for use by the Joint Forces Command (JFCOM) Joint Experimentation (J9) organization to analyze Rapid Decisive Operations (RDO). EMBED Word.Picture.8  Figure 1.  The Pegasus FederationPegasus consists of three Service-based combat simulations, as well as several HLA tools.  The federation is depicted in Figure 1, and representative objects are illustrated in Figure 2.  The simulations are briefly described below.Figure 2.  The Pegasus BattlespaceEagle: Developed by US Army TRADOC Analysis Center (TRAC), Eagle is a theater-level aggregate simulation that is used in Pegasus to represent Blue and Red ground-based systems and units.Extended Air Defense Simulation (EADSIM): Employed by the Modeling, Analysis, and Simulation Center (MASC) at the USAF Electronic Systems Command, EADSIM represents Blue and Red aircraft as well as the long-range mobile SAMs.Naval Simulation System (NSS): Developed by US Navy Space & Naval Warfare Systems Command (SPAWAR), NSS represents naval strike assets as well as multiple sensor systems in the Pegasus federation.1.2 The Need for SpeedOptimal federation performance is an important element in executing any federation.  For Joint Forces Command (JFCOM), the sponsor of the Pegasus federation, it is crucial that we execute enough runs to produce a meaningful analysis.  Obviously, to maximize our number of runs, we must minimize the time it takes to conduct each individual run.    The time to complete a run is roughly proportional to the amount of game time being simulated, but we can strive to run many times faster than real time.  As a result, for our metric of performance, we used game ratio, which we defined as the elapsed simulation time divided by the elapsed real time.In August 1999, our baseline run, which consisted of about 70 objects in a 4-game-hour scenario, ran at a ratio of about 2:1, or two hours of real time.  The execution phase of this program will employ a 48-game-hour scenario with many more objects.  JFCOM stated a performance goal of running at a 10:1 game ratio.1.3 Earlier Efforts at Performance ImprovementSeidel and Wilson had already made some performance improvements to the Pegasus federation about a year ago.  They made the following changes to increase federation speed:Increase lookaheadReduce federate subscriptionsReduce frequency of attribute updatesFocus object class tree to permit selective subscriptionOptimize federate processing of incoming dataTheir modifications allowed the federation to run three times faster. [SEID]While we were certainly glad that these changes had been made, it meant that any further performance improvements required examining different aspects of the federation.  Fortunately, we were able to find additional changes that had the cumulative effect of increasing our game ratio another eight times, or a total of 24 times faster than before the Seidel/Wilson modifications.1.4 Pegasus Experimentation EnvironmentWe conducted our Pegasus federation experiments at a JFCOM facility. In it, the federation executed on a dedicated LAN using the equipment described in Table 1.  We made every effort to disable all features of the simulating federates that were not strictly relevant to performing their functions; this primarily meant graphics and debugging outputs and resulted in reduced CPU usage by the federates. FederateComputerOSMemoryEADSIMSGI OctaneIRIX 6.4512 MBNSSDell Pentium IINT 4.0256 MBEagleSun Ultra 1Solaris 2.5.1128 MBFMTSun Ultra 2Solaris 2.5.1128 MBTable  SEQ Table \* ARABIC 1. Federate Configurations2.  Measuring PerformanceWhile much improvement to the performance of a federation can be accomplished through analysis and evaluation, it is often necessary to measure and experiment with the federation to fully understand the factors controlling its performance. A large portion of the data necessary to measure federation performance is available from the Management Object Model (MOM) through the Federation Management Tool (FMT). However, critical elements of performance measures cannot be obtained through the MOM, and tools to fully exploit performance data are not available.2.1 Use of Management Object ModelThe Management Object Model (MOM) provides access to operating information that is available to the RTI during federation execution. These MOM facilities can be used by joined federates to gain insight into the operations of the federation execution and to control the functioning of the RTI, the federation execution, and individual joined federates. MOM accomplishes this by utilizing predefined HLA constructs, objects and interactions, in the same way that participating federates exchange information with other federates. The RTI publishes object classes, registers object instances and updates values of attributes of those object instances; subscribes to and receives some interaction classes; and publishes and sends other interaction classes. A joined federate charged with controlling a federation execution (a manager federate) can subscribe to some or all of the object classes, reflect the updates, publish and send some interaction classes, and subscribe to and receive other interaction classes. Thus, while a FOM for a federation includes all elements of the MOM, a federation may choose to use all, part, or none of the MOM standard classes and associated information elements.When analyzing performance of a federation, the MOM is a logical place to start.  The MOM makes some performance-related data readily available.  For instance, by just subscribing to the Manager.Federate object class, one can receive information on object counts (objects owned, objects updated, objects reflected), data throughput (updates sent, reflections received, interactions sent, interactions received), and time management (time, lookahead, lower bound time stamp, queue sizes) for each federate.  Furthermore, one can employ more sophisticated use of the MOM by publishing and sending interactions that query for particular data and subscribing to the interactions that are sent in reply of the queries.2.2 Exploitation of HLA ToolsSeveral HLA tools exploit the capabilities of the MOM to assist developers with testing and executing an HLA federation.  Both the Federation Management Tool (FMT) and Federation Verification Tool (FVT) take advantage of the MOM to give insight on federation operations.  The FMT and FVT are publicly available tools, but there are several commercial tools that also utilize the MOM.We employed the Federation Management Tool (FMT) for most of our performance analysis.  We should point out that we used a modified version of the FMT in order to capture and collect performance statistics, as will be described in Section 2.3.The Federation Verification Tool (FVT) measures, for each federate, the number of invocations of each RTI service, number of updates sent and reflections received, and number of interactions sent and received.  The FVT provides object throughput data at the attribute level and interaction throughput data at the parameter level.  This is a much lower level than what is available in the FMT.  The FVT accomplishes this by making use of service logging, a feature of the RTI in which every service invoked by a federate is echoed and sent as a MOM interaction to a subscribing federate.Pegasus also employs the Data Collection Tool (DCT) in its federation.  At this time, we use the DCT to make all scenario data readily available for subsequent analysis.  In the early releases of DCT, a design decision was made to exclude the collection of MOM data.  Based on Pegasus feedback, the DCT developers are now considering the archival of MOM data in future releases of the DCT to allow for collection of the performance-related data in the MOM.2.3 Instrumentation of Federates Two elements of performance data were not available using the above methodologies and tools: latency and capacity. Latency. To obtain latency, a federate records the time in a message and sends it to another. The receiving federate can note the time of arrival and determine latency. Because we cannot be sure that the clocks of the two federates are synchronized, a round-trip path must be taken. In HLA terms, this takes the form of an interaction (normally called a ping, responded to by a ping acknowledge). When measuring latency in HLA, one must also account for transportation type.  EMBED Word.Picture.8  Figure 3.  Pegasus Interactions to Measure LatencyCapacity. To obtain capacity, a federate records how much of its processing time it spends in each of several categories. To be general, we divided processing time into just three categories:Action. Activities where the federate controls what happens based on its internal algorithms and event queues.Reaction. Activities that the federate takes in response to information received via HLA (a reflection or received interaction, for example).Wait. Time spent waiting for data from the federation.For this effort, we determined these values as follows: All time spent processing callbacks from the RTI was categorized as reaction time. Time spent in calls to the RTI tick() function minus the reaction time was categorized as wait time.  All other time was considered action time. EMBED Word.Picture.8  Figure 4.  Pegasus Interactions to Measure CapacityThe Pegasus team developed a set of special purpose interactions to measure and report latency and activity data.   We modified our simulations to react to the interactions and our manager federate, the FMT, to initiate actions and record results (i.e., to send interactions indicating when to start collecting data, when to send performance reports, and when to stop collecting data).  Figures 3 and 4 illustrate the interaction traffic for measuring latency and capacity, respectively.  Because the federates are entirely reactive to control interactions from the manager federate, these federates perform no extra processing without prompting from the manager federate.We also modified our FMT to write out performance data in a simple comma-delimited text format.  This allows the data to be imported with ease into a spreadsheet program for subsequent analysis.3. Analyzing PerformanceIn August of 1999, to get a handle on Pegasus performance, the systems engineer, sponsor, and federate developers agreed on a set of runs to be made to analyze performance.  Table 2 lists the scenarios used for these runs. SimulationScenario DescriptionEADSIMBaseline Scenario ‚Äì 40 threat objects, 20 friendly objects (6 strike aircraft, 4 mobile SAM batteries, the JAOC, AWACS, JSTARS, Global Hawk, and a few other support objects)3X Scenario ‚Äì 18 strike aircraft, 12 SAM batteries (other counts same as baseline)EagleBaseline Scenario ‚Äì 16 objects3X Scenario ‚Äì Baseline objects plus two more Red motorized rifle battalions, triple the number of SAMs, and an additional Blue battalionNSSBaseline Scenario -- 1 satellite aggregate entity, 4 aircraft3X Scenario -- 1 satellite aggregate entity, 12 aircraft20X Scenario ‚Äì 1 satellite aggregate entity, 80 aircraftTable 2.  Pegasus Model Scenarios Used For Performance Testing3.1 Data CollectionObviously, to be able to analyze data, one must have data to analyze.  We therefore modified the FMT to periodically log data to a text file.  After the FMT user specified a data collection interval, the FMT automatically wrote MOM data to this file at the specified intervals, and requested latency and capacity data from the federates at this same interval.  When the FMT received performance data from the federates through the interactions described in Section 2.3, it logged this data as well.  All told, the log file contained data on federate activity calculations, federate-to-federate latencies, time values, object counts, throughput, and other state data.  Each line of data written to the file was tagged with the recording time and the federate identification.We developed separate programs to parse the log file and produce comma-separated value files of data for each federate.  We then imported this data into spreadsheets for subsequent analysis.  Once in the spreadsheet, it was easy to perform numerical analyses of the data and to graph performance values over time.3.2 Correlation AnalysisIn August, the baseline scenario ran at a ratio of 2.18.  When the number of objects in EADSIM was tripled, we were unable to run at real time (1.0).  Increasing objects in Eagle and NSS also hampered performance, but to a much smaller degree.  Table 3 shows the results of the August runs.EADSIM DB SizeEagle DB SizeNSS DB SizeRatioBaselineBaselineBaseline2.183XBaselineBaseline0.84Baseline3XBaseline2.00BaselineBaseline3X1.973X3XBaseline0.753XBaseline3X0.75Baseline3X3X2.093X3X3X0.74BaselineBaseline20X1.533X3X20X0.67Table 3. Game Ratios Achieved Using Different Scenario Combinations, August 1999To assess the contributions to game ratio caused by increasing the different simulation databases, the database sizes from the runs listed in Table 3 were assigned a numeric value of 1, 3, and 20 for the baseline, 3X, and 20X scenarios, respectively.  When subsequently correlating the database size with ratio, it was shown that federation ratio was highly correlated with EADSIM‚Äôs database size and marginally correlated with Eagle‚Äôs and NSS‚Äô database size.  The August 1999 correlation values with game ratio were as follows:EADSIM	-0.965Eagle	-0.164NSS	-0.208In November, after the improvements made to performance, the game ratio became less correlated with EADSIM database size and slightly more correlated with Eagle and NSS database sizes.EADSIM	-0.911Eagle	-0.329NSS	-0.303The numerical analysis presented this far is mainly a number-crunching exercise.  Assessing causality is much trickier business.  Even something such as determining good candidates for hardware replacement is not trivial.  For instance, we knew that as the number of EADSIM objects increased, game ratio decreased.  However, without additional data, we did not know if this was caused by extra processing in EADSIM, extra processing in the other federates due to the increased numbers of reflections, increased I/O, or other factors.As mentioned earlier, we collected a wide variety of statistics through the FMT.    In the cases that we had a high correlation between game ratio and some factor (absolute value near 1.0), one of the following was probably true:The factor had a significant impact on game ratio, orThe game ratio had a significant impact on the given factor, orSomething else had a significant impact on both game ratio and the given factor.We examined factors that had a high negative correlation with game ratio and then ran experiments to confirm if they were performance bottlenecks.  If so, we experimented with aspects related to the factor to increase game speed.  As we made improvements, different factors would rise to the top of the list of potential bottlenecks, and we would then focus our efforts on these areas.  Through this iterative process, we improved performance dramatically, with our most recent results shown in Table 4.For instance, after our initial set of runs, we observed a high inverse correlation between NSS reflections received and game ratio. We presumed that a large number of reflections in NSS led to poor performance, as suggesting the converse was illogical.  We then concentrated on why the NSS reflections would slow down the federation so much and what we could do to improve performance.EADSIM DB SizeEagle DB SizeNSS DB SizeRatioBaselineBaselineBaseline18.623XBaselineBaseline9.82Baseline3XBaseline15.83BaselineBaseline3X19.253X3XBaseline8.133XBaseline3X9.16Baseline3X3X15.833X3X3X8.45BaselineBaseline20X12.673X3X20X6.92Table 4. Game Ratios Achieved Using Different Scenario Combinations, November 1999At present, when analyzing all combined data from the runs listed in Table 4, the factors appearing to have the largest impact on performance are the number of NSS objects reflected (-0.840 correlation) and the number of EADSIM objects updated (-0.837 correlation).  Within individual runs, when the object counts remain relatively constant, the biggest performance drivers seem to be EADSIM interactions sent followed by Eagle interactions received and NSS interactions received.  After publication of this paper, the Pegasus team will attempt to improve performance further by optimizing software related to these areas.Interestingly, in all our runs, the latency values have almost zero correlation with game ratio.  This indicates that, for the Pegasus federation, network characteristics did not constrain the game speed.  This was later confirmed with empirical evidence, as we ran experiments for persons who insisted that our performance problems were caused by the network.3.3 Using Regression Analysis to Predict PerformanceWhen a number of quantitative factors are available in a system, regression analysis can be employed to develop a model to predict performance.  [FERR]In order to plan for the execution of large sets of Pegasus runs, it was essential that we assess how many runs we can produce in any given time period.  However, similar databases to what we would use for the actual runs were not available, so it was impossible to predict performance using a comparably sized database.  Because the development of large simulation databases is costly, it was not practical to test performance through the development of full-blown scenarios.  Also, design decisions were still being made on the actual scenario to be modeled, so development of the actual databases was not possible yet.  Instead, we used regression analysis of the small- and medium-sized databases listed in Table 2 to estimate federation performance of larger databases.  Using the same scheme of assigning numeric values to the database sizes, based on the relative size of the simulation databases to that of the baseline.  We found that exponential equations produced a much better estimation than did linear equations.  In August of 1999, our analysis produced the following equation to estimate performance.  Ratio = 	3.36 * 0.62EADSIM_DB_SIZE  * 			0.99EAGLE_DB_SIZE   *  0.99NSS_DB_SIZEwhere the ‚ÄúDB_SIZE‚Äù variables represent the database sizes relative to the baseline.  As can be seen from the equation, for each factor by which the EADSIM database size is increased, the ratio is 62% of the starting ratio.  Using the above equation to predict results of runs of various database sizes, we produced the results in Table 5.As can be seen from the table, as the EADSIM database size increased, game ratio became very slow.  Increases to the Eagle and NSS databases produced a much slower decline in performance.It should be noted that because we did not make enough runs to produce results of high statistical significance, the exact numbers above could not be treated as gospel.  Yet, this analysis did highlight some of the performance concerns for Pegasus.  If we were to run with databases 5 to 10 times larger than the baseline, we would not be able to generate enough iterations to produce a good analysis.  Thus, performance became a top concern for Pegasus.EADSIM DB SizeEagle DB SizeNSS DB SizePredicted Game RatioBaselineBaselineBaseline2.062XBaselineBaseline1.283XBaselineBaseline0.805XBaselineBaseline0.3110XBaselineBaseline0.03Baseline2XBaseline2.05Baseline3XBaseline2.05Baseline5XBaseline2.03Baseline10XBaseline2.00BaselineBaseline2X2.03BaselineBaseline3X2.01BaselineBaseline5X1.96BaselineBaseline10X1.842X2X2X1.263X3X3X0.775X5X5X0.2910X10X10X0.02Table 5.  Estimations of Game Ratio Using Different Scenario Sizes, August 1999With the performance improvements described in the remainder of this paper, we have produced the following new predictive equation:Ratio = 	27.08  *  0.73EADSIM_DB_SIZE  * 			0.94EAGLE_DB_SIZE   *  0.98NSS_DB_SIZEAs can be seen by the above coefficients, the baseline runtime is much faster, and the system is less sensitive to changes to EADSIM‚Äôs database size.  Table 6 shows the predicted performance for larger database sizes.  Section 4 describes what was done to achieve this improvement.EADSIM DB SizeEagle DB SizeNSS DB SizePredicted Game RatioBaselineBaselineBaseline18.312XBaselineBaseline13.353XBaselineBaseline9.735XBaselineBaseline5.1710XBaselineBaseline1.06Baseline2XBaseline17.24Baseline3XBaseline16.24Baseline5XBaseline14.40Baseline10XBaseline10.67BaselineBaseline2X18.03BaselineBaseline3X17.76BaselineBaseline5X17.22BaselineBaseline10X15.952X2X2X12.383X3X3X8.375X5X5X3.8210X10X10X0.54Table 6.  Estimations of Game Ratio Using Different Scenario Sizes, November 1999This same type of regression analysis can be conducted to develop predictive equations based on a multitude of detailed factors.  However, one must be careful with multiple regression analysis to not use variables that are highly correlated with other variables in the same equation. [FERR]3.4 Federate Activity AnalysisAs discussed in Section 2.3, we modified the simulations and the FMT to keep track of how much time was spent in various activities.  Interestingly, we discovered that our federates spent the majority of their time waiting for data from the federation.  That is, test results showed that each federate spent most of its time in tick(), waiting for the RTI to return control to the federate.  In the August baseline run, the wait times were as follows:Eagle	97.7%EADSIM	93.2%NSS	73.8%While some wait time should be expected, especially in a time-managed federation in which federates are constrained by the time advancement of other federates, these numbers seemed very high.  This led us to examine the RTI services invoked by each federate to determine which services cause the largest wait times.   After we modified our use of some of the RTI services, we spent significantly less time waiting for the RTI.  The current wait times in the baseline run are as follows:NSS	90.2%Eagle	78.6%EADSIM	55.2%3.5 Effect of Performance Measuring Tools on PerformanceIt should be noted that in implementing a scheme to measure federation performance, there will always be some effect on performance by the measurement mechanism itself.  We assessed the impact of our measurement mechanism by running with this capability turned on and off.  For runs in which we did not employ the FMT or any of the performance interactions, we simply used a stopwatch to measure the overall runtime.  What we found was that our runtime increased by just 0.4% when we captured statistics once per minute.  However, if we captured data more frequently, the impact was more noticeable, with a 4.5% slowdown for 5-second measurement intervals.  For this reason, we used 1-minute measurement intervals for the majority of our runs.4.  Improving Performance (And Failed Attempts at Doing So)Based on our analysis of the data we collected, we attempted to improve Pegasus federation performance through a variety of experiments.  This section describes these attempts at performance improvement, some of which were very successful and others which yielded no improvement whatsoever.4.1 Optimization of Network ThroughputThe Pegasus team tested the modification of several RTI Initialization Data (RID) parameters to speed up the processing of network traffic.  As Black recommended in his Fall 1999 SIW paper, we set the polling_interval parameter for the TCP protocol to zero in order to poll for data more frequently.  [BLAC]  For our federation, this did not improve our run speed; in fact, the federation ran slightly slower than before.  This gave us an indication that our problems did not stem from network overload.In addition, at Architecture Management Group (AMG) Meeting 32, Briggs discussed a mechanism for improving performance by reducing network traffic through use of a single reliable distributor in the RTI [BRIG].  Using the default RID file, each federate acts as its own reliable distributor, meaning that reliable messages are sent from one federate‚Äôs reliable distributor to every other reliable distributor.  By configuring several versions of the RID file, it is possible to route all data through a single reliable distributor.  This has proven effective in other federations, such as the Joint Training Confederation and the Joint Advanced Distributed Simulation. [FURN] [BLAC] Unfortunately, for Pegasus, we saw absolutely no improvement through use of a single reliable distributor.  This reinforced the notion that our performance bottlenecks were not being caused by network traffic.Later, we obtained a network analysis tool to measure network activity.  This revealed that we were using very little of the available bandwidth.  In fact, we saw a few spikes in the graph of network traffic, but nothing to indicate network saturation.  On the contrary, we rarely used more than 15% of available bandwidth.Because our tests to improve performance through optimization of network parameters yielded no improvement in execution time, we did not make any permanent changes based on these findings.  It is important to point out that an analytic federation has much different performance needs than that of other applications. Our attempts to improve performance based on results from other fundamentally different federations failed.4.2 Time ManagementWe found our biggest gains in performance in the area of time management.  The modifications that improved performance involved event ordering, specific time management calls to the RTI, and intervals used for time advance.4.2.1 Event OrderingThe HLA specifies that some data can be sent for immediate delivery while other data is sent in order of specified time stamps.  These two types of data are placed in Receive Order (RO) and Time Stamp Order (TSO) queues, respectively, that the RTI manages.  At first, we sent most of our data, as specified in the Federation Execution Data (FED) file, using timestamp ordering.  However, we later discovered that there was significant overhead when processing the Time Stamp Ordered (TSO) queue.  In one experiment, we turned Eagle‚Äôs time regulation off.  This allowed EADSIM and NSS to run ahead and quickly resign at the completion of the scenario run.  In their wake, these simulations filled Eagle‚Äôs TSO queue with over 60,000 events that they sent as they ran.  Thus, Eagle was left to run without any regulating federates in the federation, but still needing to process all the TSO items.  At this time, because all data that Eagle needed was already resident in its Local RTI Component (LRC), it was not affected by packet delivery time.  Yet, Eagle‚Äôs game speed revealed no significant difference after EADSIM and NSS resigned.  Interestingly, Eagle was only subscribed to about 200 of the items in the TSO queue, so the large majority of the items in the queue never made it out of Eagle‚Äôs Local RTI Component (LRC).  We observed that the LRC was only able to pull off 20 items from the TSO queue per second, even when Eagle was ticking as rapidly as possible.  We achieved a major performance breakthrough when we set our FED file to send all object updates with receive ordering.  Overall, our federation ran four times faster, by this change alone.  We observed that the LRC quickly processed the RO events sent to it, as very little buffering was observed in the queue.  It must be noted that for our federation, we determined that because the updates always occurred for the current game time, this was still valid.  We continued to send our interactions with timestamp ordering because they were frequently scheduled for arrival at a future time.  For performance reasons, an analytic federation should send whatever it can using receive ordering, while ensuring that the validity of the federation is maintained.4.2.2 Methods for Advancing TimeThe HLA provides developers with several mechanisms for evolving time. Time-stepped federates typically use the time advance request services to march forward in fixed intervals of time.  Event-based federates use the next event request services to advance to the next time-stepped event.   Optimistic federates employ the flush queue request service to process all federation events, regardless of time, and then retract messages when necessary.  Some federates use a combination of these time management services.  [DMSO]In order to understand the large wait times that the federates were experiencing (as discussed in Section 2.2), we examined the time management schemes that the federates invoked.  EADSIM employed time advance requests once every 5 seconds of simulation time, while NSS used the next event request service.  Eagle, on the other hand, used a combination of time advance requests and next event requests.  The Eagle time management had originally been designed for another federation, and we determined that for Pegasus, it was not necessary to use the next event requests.  After Eagle was modified to make exclusive use of the timeAdvanceRequest service, our federation speed increased by 20%.  As NSS is a purely event-driven model, it did not make sense to have it convert to using time advance requests. Therefore, NSS continued to use the next event requests.4.2.3 Time IntervalsTime-stepped simulations model all activities for a given interval of simulation time and then advance their internal clock by this time.  The time interval chosen has a significant impact on performance.  When an analytic time-stepped simulation is run standalone, its runtime is usually proportional to the time interval chosen.  For example, if Eagle is run in isolation, then it runs about twice as fast with a 2-minute interval than with a 1-minute interval. When a time-stepped simulation runs as part of a federation, increasing its time interval may speed up runtime, but not to the same degree as when it runs standalone.  However, a bigger concern is that the time intervals of multiple time-stepped simulations should work well together.In Pegasus, the time intervals employed by EADSIM and Eagle differ significantly.  EADSIM advanced time in 5-second intervals, while Eagle used 1-minute intervals.  For EADSIM‚Äôs level of fidelity, advancing time in larger than 5-second intervals is not valid.  We experimented with modifying Eagle‚Äôs time interval.  When Eagle‚Äôs interval was changed to 30 seconds, the federation ran just 2% slower.  However, when its interval was changed to 10 seconds, the federation ran 48% slower.  To get an approximate upper bound on how we could improve performance, we increased Eagle‚Äôs time interval to 10 minutes, but this only yielded a 3% improvement.  In the end, we left Eagle with its original 1-minute interval, but with the knowledge that we could decrease it to 30 seconds with a relatively small cost in performance.Because NSS is an event-driven simulation, it was not part of the time interval experiments.4.2.4 Lookahead ValuesAs Seidel noted in his March 1999 paper, increasing a federate‚Äôs lookahead improves performance by permitting other federates to run in parallel.   [SEID]  This eliminates the leapfrog effect that occurs when each federate takes turns waiting for other federates to advance time, as they leap forward one at a time.We ran some experiments with Eagle lookahead increased from 1 second to 5 seconds, and this showed a 28% improvement in federation performance.  However, after considering the ramifications of this change, we decided that a 5-second lookahead was not valid for Eagle, so we returned to using the 1-second lookahead.  Still, this reinforces the notion that lookahead values should be set to the highest value that is consistent with valid modeling.4.3 Use of tick()Federates must invoke tick() to give processing time to its Local RTI Component (LRC).   The LRC uses this time to accept and process data from other federates, make calls to the federate ambassador to send events back to the federate, coordinate simulation time, and perform internal bookkeeping.There is both a two-argument and zero-argument version of tick().  The two-argument version allows the calling federate to specify a minimum and maximum time to stay in tick().  The zero-argument version returns either after a single callback or after it‚Äôs finished with all its processing, with the behavior dictated by the single_callback_per_tick RID parameter.Unfortunately, there is not clear guidance on the best way to invoke tick().  If tick() is called too rarely or too often, then it is possible that either the LRC or the federate itself will be starved for processing time.  Experimentation is required to get it right.On Pegasus, we achieved a federation speedup of 20% when just one of our federates modified the way that it called tick().  We also learned from an RTI developer a little-known fact that RTI 1.3v6 only processes one packet in the time manager per tick(), so time-managed federates should tick frequently and spend minimal time in the RTI on each tick. 4.4 Other RID File ModificationsWe experimented with bundler timeout periods, which control how often messages are sent and received.  Briggs describes a process for accomplishing this in his performance-tuning document. [BRIG]  The relevant RID parameters that we experimented with were tcp_timeout and tcp_tick_interval.  Unfortunately, our changes did not have a significant impact on Pegasus performance.There were some RID parameters that we modified without a clear knowledge of what the effect would be.  For instance, we experimented with different settings for: rti_core_execute_interval, stream_execute_interval, and stream_manager_queue_alloc.None of these experiments yielded any performance improvement, while in some cases, our changes worsened performance.  We came to the conclusion that arbitrarily experimenting with different RID parameters was not a fruitful effort.4.5 Performance Steps Not AttemptedThere are a few performance improvement mechanisms that we did not attempt.  For one, we did not use Data Distribution Management (DDM).  Because we had limited access to the federate source code and it was believed that the subscription sets of our federates may need to be modified for future analyses, it was not considered worth the investment to implement DDM before knowing the final operational specifications.  Conceivably, DDM could have reduced the buildup of data in the TSO queues by performing sender-side filtering.We also did not try replacing any of the hardware components.  This was mainly a funding issue, as our performance analyses did not reveal any particular resource limitation on any of our machines to justify a new hardware purchase.5. ConclusionsWhile we will not pretend to have a single formula for success for improving the performance of a federation, we believe that we can present some general guidelines.Determine what performance means to your federation.   Different applications have different performance needs.  An analysis federation may seek to maximize the speed at which a run can be completed, while a real-time federation may seek to reduce latency.  What works for one federation will probably not work for another application that is fundamentally different.Devise appropriate metrics for your application.  Before you can improve performance, you should have a mechanism to measure it.  Of course, you cannot measure anything until you decide what you need to measure.Exploit available tools to collect performance-related data.  If a Federation Execution Planner‚Äôs Workbook (FEPW) is filled out, this gives you a start on analyzing performance.  The Federation Management Tool (FMT) and Federation Verification Tool (FVT) can be used to gather insights on performance at runtime.  The FMT provides a wealth of MOM data for analysis, and the FVT quantifies data exchanged and services invoked.  Each of these tools is publicly available.Consider performance ramifications of different time management schemes.  For certain federations that do not run at real time, time management is critical for ensuring causality.  However, along with the benefits of time management, there is also overhead required by the RTI and the federates themselves.  By devising intelligent time management schemes, this overhead can be reduced.Be careful how you invoke tick().  The federates and their Local RTI Components (LRCs) each require processing time.  If tick() is called too rarely or too often, then it is possible that inadequate processing time will be allocated for RTI processing or for federate internal processing, respectively.Try tweaking RID parameters.  Some modifications of RID parameters can lead to performance improvements, but many changes result in insignificant improvements.  Limit changes to what makes sense for your application.Finally, have realistic expectations and beware of the law of diminishing returns.  Realize ahead of time that there will always be some overhead required with any distributed system.  You will always encounter some threshold at which performance cannot be improved further.  As performance is improved incrementally, additional performance improvements become much more difficult.AcronymsAOACMTAttack Operations Against Critical Mobile TargetsDCTData Collection ToolDDMData Distribution ManagementEADSIMExtended Air Defense SimulationFEDFederation Execution DataFMTFederation Management ToolFVTFederation Verification ToolHLAHigh Level ArchitectureJSEADJoint Suppression of Enemy Air DefenseLRCLocal RTI ComponentMOMManagement Object ModelNSSNaval Simulation SystemRDORapid Decisive OperationsRIDRTI Initialization DataROReceive OrderRTIRunTime InfrastructureSIWSimulation Interoperability WorkshopTSOTime Stamp OrderBibliography[BLAC] Black, J., Harris, D., ‚ÄúRTI Recommended Practices,‚Äù Minutes of the Fall 1999 Simulation Interoperability Workshop, Orlando, FL, September 12-17, 1999.[BRIG] Briggs, R., ‚ÄúTools to Plan, Test, and Configure Your Federation For Performance,‚Äù Architecture Management Group (AMG) Meeting 32, August 10-11, 1999.[DMSO] DMSO, MITRE, SAIC, VTC, RTI Programmer‚Äôs Guide, April 2, 1998.[FERR] Ferrari, D., Computer Systems Performance Evaluation, Prentice-Hall, 1978.[FURN] Furness, Z., Robinson, M., Chiang, C., Logsdon, J., ‚ÄúLessons Learned in Integrating the RTI Into the Joint Training Confederation,‚Äù Minutes of the Fall 1999 Simulation Interoperability Workshop, Orlando, FL, September 12-17, 1999.[SEID] Seidel, D., Wilson, A., ‚ÄúEnhancing Performance in Analytic Federations,‚Äù Minutes of the Spring 1999 Simulation Interoperability Workshop, Orlando, FL, March 14-19, 1999.Author BiographiesDAVID L. PROCHNOW is a lead software engineer in the Information Systems and Technology Division at the MITRE Corporation. He currently provides technical expertise to several HLA programs.  He has also chaired the SIW Logistics Forum since 1996.  At MITRE, Mr. Prochnow previously contributed to the ALSP project and to logistics simulation efforts. While at BDM International and Control Data Systems, Mr. Prochnow developed software for various corps-level and theater-level wargames.  He has designed, implemented, integrated, tested, ported, analyzed, documented, and executed modeling and simulation systems throughout his career. Mr. Prochnow received a B.S. in Computer Science from the University of Virginia in 1983.DAVID W. SEIDEL is an associate manager for the Software and Information Architecture Technical Area in the Information Systems and Technology Division at the MITRE Corporation. For DMSO, Mr. Seidel supports several Cadre programs, primarily working with the T&E community.  Mr. Seidel has been active in the refinement of the Management Object Model, the Federation Execution Planner‚Äôs Workbook, and the Object Model Template. Previously, Mr. Seidel supported the JSIMS Program Office, the STOW program, and the ALSP project. He has developed Navy, Army, Air Force, and Joint wargames and Navy command and control systems. Mr. Seidel has a B.S. from Illinois Institute of Technology and an M.B.A. from Bryant College. The Seidel and Wilson experiments were performed using different model databases, so this statement assumes that their changes would have had the same effect on our 70-object scenario. All callbacks occur in the context of a tick() call, necessitating the subtraction. For security reasons, the performance runs did not simulate individual satellites. Correlation values can range from ‚Äì1.0 to 1.0, with values near zero showing no mathematical relationship and absolute values near 1.0 showing a high relationship.  Negative values show an inverse relationship.  In the above analysis, as database sizes increase, game ratio decreases, so the correlation values are negative. For the record, this was not the result of inefficient code in NSS.  NSS just happened to be the federate that received the most reflections, and the underlying problem turned out to be time associated with the LRC taking items off of the TSO queue.  This is discussed later in this paper. All data was being sent with reliable transport.  There also exists a polling_interval parameter for UDP, which would be applicable for federations using ‚Äúbest effort‚Äù transportation. The DMSO RTI, in the absence of Data Distribution Management, performs filtering at the destination, not at the sender. Ticking is discussed in the next section. This may be changed in a future release of the DMSO RTI.	