Investigating HLA Suitability for Real-time Range ApplicationsDaniel C. BissellCharles ConroyCSTE-DTC-WS-TTWhite Sands Missile RangeWSMR, NM  88002505-678-3220, 505-678-4175bisselld@wsmr.army.mil, conroyc@wsmr.army.milKeywords:HLA, RTI 1.3NG, Windows NT, Real-timeABSTRACT: White Sands Missile Range (WSMR) engineers undertook an effort to understand and test HLA in order to determine its suitability in a real-time weapons test and evaluation environment.  Range instrumentation developers must know what impact HLA and the RTI will have on range test support applications before starting HLA integration.  The cost to an application using HLA is the burden added by the Local RTI Component (LRC).  This paper describes the testing done to characterize the delay or latency added by the LRC to a real-time application.  Developers tested using RTI 1.3NG Version 2 and Version 3 on Windows NT systems.  The test federates utilized IRIG timing to ensure accurate time results.  The paper describes the hardware configuration, software design and network components used.  Test results are presented.  Finally, the paper addresses performance optimizing RTI settings.Missile Range TestingWhite Sands Missile Range (WSMR) engineers initiated a testing process in order to investigate HLA and its capabilities.  We familiarized ourselves first by attending a comprehensive introduction of HLA put on by DMSO.  Based on the information presented at this class it was clear that there was at least a possibility that we could use HLA at the Range.  We felt it was necessary to conduct our own detailed investigation in the unique environment of a test range.  Also, while software users are widespread at WSMR, developers are few.  We undertook to gain as much detailed corporate knowledge as we could while at the same time develop software that could be used universally at WSMR.  In the course of the effort we assembled a multi-computer test bed designed to test HLA in the environment of a real-time weapons test and evaluation environment.  The results of our testing lead us to believe that HLA does perform well enough to be used at White Sands.Before we began our local development we attended a HLA Hands-on Practicum.  This proved to be extremely valuable training.  We recommend this course to anyone starting out in HLA.1.1 Real-time asynchronous environmentOne of the primary assets making WSMR a valuable Test and Evaluation resource for the DOD is its vast set of data collection instrumentation.  This instrumentation includes several varieties of radar, telemetry acquisition and relay systems, multiple different optical tracking systems, GPS collection and relay systems and a wide variety of real time display systems.  These systems are located throughout the four thousand square miles of WSMR.These systems are operated as required for any given test in a controlled autonomous mode.  Each system is controlled either locally or remotely based on test parameters.  Those systems needing information as to the location of the test object are provided pointing information from the range control system.  However, the individual data collected by these systems is stored and or transmitted by each system autonomously from all other systems.  In other words, each system collects and relays data asynchronous with any other system whether they share a common data rate or not.Our interest in HLA is in this context.  As such we were not interested in many of the features of HLA that provide for synchronization and time stepping.  We wanted to have similar systems publish similar data to a common subscriber but to be able to do this reliably in an asynchronous environment.  While each system operates with a highly synchronous internal closed loop the systems data relay are not synchronized with one another.  This is the environment that we attempted to create with the HLA test bed.Additionally, the different systems have different data rates and data sizes.  The radars send their small updates to range control at twenty samples per second.  The optics systems run at the video rate of sixty samples per second and relay a larger set of data.  The different various graphics display systems run at whatever rate they can sustain based on the capability of the hardware.  Clearly we needed to test varying data sizes at varying update rates.  This requirement was addressed with the test bed as well.    1.2 Minimal R&D assetsWSMR faces a shrinking research and development capability brought on primarily by loss of personnel. Some software programming expertise exists within each instrumentation commodity area.  These people are the users of existing operational test support systems.  For the most part they do not develop new code; they just modify existing code to apply to whatever test is pending.  There is only one organization at White Sands that has a true R&D mission.  It is this group that is involved in the testing of HLA.  We had several goals for our testing.  First we wanted to gain expertise in writing HLA applications as we attempted to utilize it in our environment.  We wanted to characterize its capability in that environment.  Finally, we wanted to develop code used for our testing that could subsequently be easily integrated with any of the existing operational systems without the users of those systems having to become experts on HLA.Use of IRIG TimingEach federate machine in the test federation is equipped with an IRIG timing board.  These boards receive IRIG timing from various Range timing sources.  IRIG timing is used extensively at WSMR.  All test data collected is time stamped based on IRIG time.  IRIG time is available anywhere on the range via range timing systems or GPS receivers, which generate the same time.  The time available from these boards via IRIG is accurate to 5 microseconds.  The timing boards are used by the test federate to provide time which is included with the data being published and to tag the data being subscribed to.  The time is read at the optimum location in the code to isolate the Local RTI Component (LRC) from the rest of the federate code.  These two times allow a federate to calculate the total one way delivery time for any data sample received from any other federate.  This delivery time includes only the LRC and network portions of the data transfer.One way vs. round tripWe wanted to determine one way delivery time because it includes only two instances of an LRC and only one instance of network propagation and as such is most precise.  More important than this is the fact that data and control relay at WSMR is primarily one way.  As such we wanted to characterize one way data transfer.  Given this design constraint we will be able to configure the test bed to replicate any data relay and related opposite flowing control data and determine performance.Future network test schemeThe design described will allow us to use a simple variant of the software to test unique configurations from any system and location on the range through the range communications network.  This test is helpful for determining update potential and validating an existing test network configuration.Test Federate Software DesignThe software was designed to determine the delivery time of a single attribute update from one federate to another.  We use highly accurate IRIG time to tag the update as it is sent and again when it is received.  The difference in the two times is the delivery time consisting of the LRC and network times.Each test federate runs identical software. The software is version controlled on a central machine.  Having each machine run the same application allows for a single modification to the test parameters that is universally available to all federates. The software was written to allow us to easily change the size of the data being published by modifying a single structure in a universal header file.  Similarly, we can change the update rate of that data by modifying a single dividing variable. Federates pass a single attribute.  The attribute is a structure composed of time sent and message count and an array used to vary the total message size.  The size of the array is set before run time to the desired size for testing.Once the federate has joined the federation it performs two primary tasks.  At the desired rate the federate fills the attribute with its current time (time sent) and message count.  The time is retrieved from the IRIG timing board at the last moment possible before the attribute is updated to the LRC.  When the federate receives an incoming update it immediately reads its local time (time received) and using the time sent located in the received attribute calculates the difference or total delivery time.  This delivery time as well as some local federate information is maintained in shared memory for display and logging.Given the simplicity of our test scheme we use very few federate ambassador services.  In fact, only three of the thirty-nine available callbacks are necessary for our federation.  All that is required is the ability to discover and remove an object instance and to reflect its attribute values.  Additionally, we have all relevance advisories disabled.  Finally, no MOM services are necessary and thus are disabled.The software is composed of two Windows applications that can be executed on as many as ten machines simultaneously.  It was never run on more than three machines at a time due to the limited number of comparable sets of hardware.  The first application is a user interface and the second is the HLA interface.  Windows applications were used because of the ease of producing display information for debugging.  In addition, Windows applications have considerable flexibility in the control of priority and messaging functions through the API.  The applications communicate through a Windows shared memory scheme.  The intention in using two separate applications is to separate the HLA functions from the data logging and screen display functions.  The HLA interface is normally run with REALTIME_PRIORITY_CLASS and the two main threads are run at THREAD_PRIORITY_TIME_CRITICAL.  This allows the application as much processor time as necessary.  In addition, except when debugging, the window for the HLA interface is kept hidden.  This prevents Windows from sending out paint messages.  As a matter of fact, we are so successful at preventing Windows messaging from interfering with the operation of the HLA library calls that even idle time messages are inhibited and a slow timer had to be added to get any messages through at all.User interfaceThe user interface is a dialog-based application that allows the user to control the tests.  The only user functions necessary are the ability to create/join the federation, start/stop the logging of data to disk and resign from/terminate the federation.  The user interface issues commands to the HLA interface through Windows events.  Two events are required, one to start the federate and one to tell the federate to resign.  These are created during the initialization of the instance of the user interface.  The only other function automatically handled is the creation of the named mapping object.  This is used as the shared memory object where the federation data is placed by the HLA interface.  The federation is started/joined upon user command.  This spawns the new process, which is the HLA interface.  When the new process is created, it inherits the environment and security attributes of the parent process.  In addition, this is where the priority class of the process and the thread priority are set.HLA interfaceThe HLA interface as much as possible executes only what is necessary to maintain the federate and to capture delivery time of incoming attribute updates.  When initialized, the software creates and joins the test federation.  It publishes and subscribes to the single attribute as described.  Finally it manages objects by updating its attribute at the prescribed rate and servicing incoming attribute updates.  These object management tasks as well as tick are done in a Windows thread controlled by a high-resolution timer routine.  Inside this thread a divider controls the update of the federate’s own attribute.  We tick at a much higher rate than we update in order to shrink the time that an incoming update sits before the associated reflectAttributeValues callback is serviced.  Tick is performed each time the thread is executed and an attribute update is sent at the predetermined rate as defined by the divider.The reflectAttributeValues callback routine calculates the total delivery time of the incoming update and stores the time as well as the message count.  When the local federate updates its attribute it also stores the incoming update’s information to shared memory.Shared memory schemeThe shared memory object is used by the HLA interface to hold all of the information concerning all joined federates.  The major items of interest are the federate handle, message count, current time and delivery time of each federate.  The shared memory object is structured to contain sixty elements.  Each of the elements contains information from ten federates, with only as many filled as have joined the federation.  The implementation of the structure is a circular queue.  The structure also contains a queue counter to maintain the latest element.  The HLA interface fills the queue as data from the federates is received and increments the counter.  The user interface uses a timer on a 200 ms cycle to read the data from the queue.  If the user wishes to log data, this is also where the data is written to the disk.  Only the major items of interest are saved to the disk.  In addition, the major items of interest are written to the screen so the user can observe the number of federates, their current time and delivery time.  The cycle has to be done often enough to allow the user interface to read the data from the queue before the HLA interface can overwrite samples.  Since the quantity of data written to the disk is small, this does not pose a problem regardless of the rate the HLA interface is filling the queue.Test Federation Hardware EnvironmentEach test federate runs on as close to identical computers as we could get.  Although the federation can be composed of as many federates as necessary, availability of hardware forced us to limit the number of federate machines to three.  Additionally we tested using several different RTIEXEC host machines.  We saw little if any performance impact due to the different RTIEXEC machines and finally arrived at an SGI machine because of availability.Computer systemsWe run the test federate on high end PCs with lots of memory.  We wanted the test results to be as current based on technology as possible and so avoided older, slower machines.  The test federate is run on a Micron Millennia Max GS133 computer.  Each machine has a Pentium III running at least 600Mhz.  Each machine has more than 256MB of RAM.  Each computer runs Windows NT 4.0 and Visual Studio Version 6.We tested with the RTIEXEC running on a Windows NT 4.0 computer, a Linux machine and finally an SGI Indigo II running the latest version of IRIX.  The SGI machine is the machine we currently use and used for most of the testing as it is permanently available to us.  Additionally as was mentioned, we saw no impact on delivery time between federates due to the machine running RTIEXEC.Network configurationThe network configuration is a very simple flat network.  We have the three federate machines, the machine hosting RTIEXEC and the machine hosting our source code all connected to a single hub.  All machines are using 100-Megabit Ethernet.We also tested using two different routers in order to determine what impact they would have on delivery time.  The two routers used were Cisco models 2500 and NGS.Test ResultsThe result of a test is a logged file consisting of information related to each incoming sample.  We are able to analyze the message count to verify that no samples are dropped.  Additionally, we plot message count versus delivery time to show the delivery times over the length of the test. We improved on our initial results as we learned more about optimizing our federation through the RID file.  We tested initially using RTI 1.3NG Version 2 and migrated to Version 3 when it was released.  The results presented span a continuous process of optimizing the RID file.We interpret the results in the context of a time budget.  If we are updating ten times per second then we have a 100-millisecond budget for each sample.  In analyzing the plot we look to see if all of the samples are being delivered and if they are delivered within that 100 milliseconds.Version 2Our initial software development and testing was done using version 2 of the RTI.  Our results showed a relatively flat baseline delivery time for a 1Kbyte update at 10 Hz of approximately 12 milliseconds.  At this time in our testing we had not yet discovered the benefit of turning bundling off.  Given what we learned later we would expect this baseline to decrease between 2 to 9 milliseconds.Within a 100-millisecond budget this kind of performance was encouraging.  However, we saw a performance anomaly that was unacceptable.  Every 69 to 70 seconds we saw a huge spike in our delivery time that lasted approximately 13 seconds.  This was repeatable and did not go away as we made changes to the RID file. A typical test plot that illustrates this condition is shown in Figure 5.1.While system programmers are able to handle an occasional bad sample, data with spikes of this duration would not be usable.  The HLA Help Desk was not able to find a solution for us.  They did indicate that Version 3 might not have the same problem.  Version 3Version 3 was released at about this same time.  We tried our federate with the new RTI and found that the spike had disappeared.  The overall baseline remained the same.  This is shown in Figure 5.2.   EMBED Excel.Sheet.8  Figure 5.1 EMBED Excel.Sheet.8  Figure 5.2We were using a Windows timer routine at this point in our testing.  We could run the timer no faster than 100 Hz.  This allows for 10 milliseconds between ticks.  We allow a very small duration for tick giving the rest of the time to our federate.  We theorized that we were allowing some extra time, approaching 10 milliseconds, which is the time between ticks, to be added to our overall delivery time.  We implemented a higher fidelity multi-media timer routine and bumped up the tick rate to 500 Hz.  This allows for 2 milliseconds between ticks, which is about as small as we dared to go given the amount of time necessary to service a reflectAttributeValues callback.  When tested we found our best case delivery time did indeed decrease by about 8 milliseconds.  Our plot changed using the multimedia timer from a flat baseline to a sawtooth.  This more accurately depicts the real delivery time.  The initial baseline was flattened by the delay between ticks.Using the high rate multi-media timer and with bundling enabled our delivery time was between 4.5 and 14 milliseconds within a 100 millisecond budget.  This is shown in Figure 5.3.At this point in our testing we discovered the bundling parameter in the RID file.  We had this set to its smallest value for all previous testing.  Instead, we disabled bundling entirely.  We saw a marked decrease in delivery time.  The parameter had been set to 5 milliseconds by default.  We saw a decreased delivery time from 13.5 to 3.8 milliseconds on the high end and 4.5 to 2.7 milliseconds on the low end.  This is hard to explain based on a 5 millisecond setting for the bundling parameter.  This improvement is shown in Figure 5.4.   The performance at this stage in our testing is more than adequate for this type of data exchange.  We tested using Best Effort distribution and found the results to be comparable with a little more variation in the plot as shown in Figure 5.5.  Additionally, we tested using these final RID settings at various other rates and data sizes.  As expected we found that as update size increases so does delivery time. The following tests were all conducted with delivery times staying within budget:	1 Kbyte updates at 10 Hz	2 Kbyte updates at 20 Hz	2 Kbyte updates at 60 Hz	5 Kbyte updates at 10 Hz	5 Kbyte updates at 20 Hz	10 Kbyte updates at 10 Hz	20 Kbyte updates at 10 Hz	40 Kbyte updates at 10 Hz EMBED Excel.Sheet.8  Figure 5.3 EMBED Excel.Sheet.8  Figure 5.4 EMBED Excel.Sheet.8  Figure 5.5We tested 5 Kbytes at 60 Hz and began to see too many samples outside of the 16-millisecond budget.  Also, we tested 50 Kbytes at 10 Hz and saw similar late results.The 2 Kbyte tests were done in order to test a specific application configuration.  As was our hope, the testbed is useful for testing any specific configuration to let designers know whether to proceed with that application.  In this case an Optics display federate was developed once we proved the data could be transferred reliably on time.LRC delayFinally, we are able to determine approximately what the delay caused by the LRC is.  The cost of doing business with HLA is the delay that takes place when data is pushed through the LRC before it gets to the network.  In our 1 Kbyte at 10 Hz example this can be calculated.  The minimum delivery time from federate to federate with this configuration is approximately 2.7 milliseconds.  The one way ping time, which approximates the network portion of the delay, is approximately 0.9 milliseconds.  That leaves 1.8 milliseconds of the total attributed to HLA.  There are two instances of an LRC in this sample with one at both the sending and receiving federates.  So each Local RTI Component in this scenario adds approximately 0.9 milliseconds to the total delivery time.  We have not attempted to characterize any general percentages for the LRC over varying data sizes and update rates.  Our concern is mostly whether HLA imparts too large of a delay.  In our testing for most of our reasonable configurations the answer is that HLA does not cause too large of a delay.Performance Optimization LessonsUltimately our testing was an effort to get as much performance out of HLA as we possibly could and then to determine if that was sufficient for our purposes at WSMR.  As such, some of the most valuable lessons were in the performance optimization arena.  We discovered a couple of key things, which enhanced the performance of our federate.  Namely, the tick strategy we used and the elimination of bundling provided us with the most significant performance benefits. TickTick provides processor time to the LRC to handle federation communications.  In our case tick primarily services the reflectAttributeValues callback.  The greater the number of ticks between updates the smaller the delay from the time the update arrives to the time the callback is actually serviced.  We tick the federates at 500 Hz with a 1 millisecond duration.  This gives 2 milliseconds between ticks.  A reflectAttributeValues callback takes less than 2 milliseconds to process an update.  Given this scenario we should be adding something less than 2 milliseconds of delay to each update.6.2 BundlingAs has been described, when bundling was disabled we saw a marked decrease in delay.  This delay is not what we expected given the setting of the bundling parameter MaxTimeBeforeSendInSeconds.  By default it is set to 5 milliseconds.  Given what we know about the network propagation delay based on ping times one would expect the benefit to be from 1 to 6 milliseconds with bundling off.  What we found instead was a benefit from 1.8 to 9.5 milliseconds.  Ultimately, we want to send each update as it is available so bundling is of no interest anyway.6.3 Other RID settingsWe used a polling process model for our federation.  We turned off all relevance advisories to eliminate miscellaneous traffic on the network.  We also turned off all MOM services.Future TestingWe have done some initial tests with different routers to determine how much additional delay is added when we pass data through a router.  We tested using two different models and found varying results.  This is testing that we intend to pursue further.  Additionally, we want to test over the larger WSMR WAN.  The WSMR communications environment includes other devices such as ATM switches.  We will test using these devices as available as well.  The results of this testing with different communications hardware may be published and presented at a future conference.Some of the systems in use at WSMR include VXWorks based computers.  We have attempted to use the HLA software available with our VXWorks systems to test HLA within our test bed.  However, we find an incompatibility with the platform and compiler required by the HLA software and the compiler we are using.  If this incompatibility is resolved we intend to test using our VXWorks system in the future.ConclusionOur understanding of HLA and its capability is greatly increased as a result of our testing.  We have achieved our goals as set forth at the beginning of this effort.  We have gained expertise in writing HLA applications and more important than just getting used to an API we have learned a great deal about optimizing an HLA application for greater performance.  We built our federation with our test range environment in mind.  Based on the results of that extensive and detailed testing we are comfortable making the statement that HLA can be of use in our environment and will help us do our job in the future.  We are in the process of communicating this fact to system programmers at WSMR.Finally, we set out to develop reusable code to facilitate HLA interfaces for Range systems.  Already we have implemented one such interface.  An Optics display federation was developed using this code.  This federation was developed in very short order and we are confident that process can be repeated with any other system at White Sands.  A paper documenting this effort was prepared for the International Test and Evaluation Association (ITEA) Modeling and Simulation Workshop held December 2000. With our insight and knowledge we look to implement HLA capable systems wherever smart at White Sands Missile Range.References[1] Nan Tindal, Charles Conroy, Richard Denman, and    Daniel Bissell: “Real-Time Optical Tracking Visualization”, Sixth Annual ITEA Modeling and Simulation Workshop Proceedings, December 2000.Author BiographiesDaniel C. Bissell is an Electronics Engineer at White Sands Missile Range.  He works in the Technology Development Directorate.  He is responsible for HLA investigation for the Virtual Proving Ground effort at White Sands.  He is doing similar work for the Foundation Initiative 2010 effort at White Sands.Charles Conroy is an Electronics Engineer at White Sands Missile Range.  He works in the Technology Development Directorate.  He is responsible for software development and modernization of key White Sands Optical Instrumentation systems. DISTRIBUTION STATEMENT AApproved for public release, distribution is unlimited.