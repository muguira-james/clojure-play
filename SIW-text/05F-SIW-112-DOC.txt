Evaluating Human System Interfaces in a Game Technology-based Virtual EnvironmentPeter WierzbinskiLockheed Martin STS164 Middlesex TurnpikeBurlington, MA 01803781-505-9532peter.wierzbinski@lmco.com Roy Stripling, Ph.D.U.S. Naval Research Laboratory4555 Overlook Ave. S.W.Washington, DC 20375202-767-3960stripling@itd.nrl.navy.milKeywords:Human-System Interfaces, Virtual Environments, Game Technology. ABSTRACT: The design of a training system often must trade-off fidelity vs. cost.  A higher fidelity system will give the user a more realistic environment in which to train. However, higher fidelity does not necessarily justify the cost in meeting a particular training requirement.In order to evaluate tradeoffs in the area of small team urban combat training, the Naval Research Laboratory (NRL) created the Warfare Human System Interface Lab (WHSIL).  WHSIL hosts a range of different Human System Interfaces (HSIs) connected to a common networked distributed virtual environment.  In this paper we outline the Gamebryo-based virtual environment shared by these systems and describe the HSIs being evaluated.  These HSIs include i) a video game-like joystick and monitor, ii) a projected display with a laser tracked weapon, iii) a stereo Head Mounted Display (HMD) with three degree-of-freedom (3-DOF) tracking of the head and weapon, and iv) a stereo HMD with 6-DOF tracking of head, weapon and torso.   1. The ChallengeVirtual Environments have been widely exploited for training.  However, a challenge in employing immersive virtual environments has been the typically high cost of human system interfaces.   This motivates the need to understand which classes of human system interfaces (HSIs) are most cost effective for various training purposes.  A major element of the Office of Naval Research’s Virtual Technologies and Environments (VIRTE) program is a campaign of experimentation to improve the science of training by addressing these and other questions.  For the portion of this research campaign addressed here, individual combatant training in an urban environment was taken as the reference training task.   The overall VIRTE experimentation program addresses both ecological validity and team training questions.  In this paper we describe the game-technology-based virtual environment and human system interfaces developed and integrated to support this research program.  To support the required experiments, the same virtual environment must work with a wide range of Human System Interfaces.  The virtual environment developed in response to this requirement is referred to as the Human Immersive Training (HIT) Virtual Environment (VE).  Similarly, a range of human system interfaces was required for this research.  Where suitable interfaces were not otherwise available to support the program, they were developed [1].  Five different human system interfaces were initially integrated with the HIT virtual environment.  This effort culminated in the official opening of the Warfighter Human System Interface Laboratory (WHSIL) at the Naval Research Laboratory in August 2004.  In the remainder of this paper, we describe the virtual environment shared across all the human system interfaces and each of the interfaces in turn.  Finally, future plans are described.General setupThe system was setup up with one machine devoted to running the virtual environment program with a certain Human System Interface.  Each of these systems could be run by themselves or in conjunction with any of the other systems.  Other than the different HSIs, each system ran identical Human Immersive Training Virtual Environment software.Another machine was setup up to run Joint Semi-Automated Forces (JSAF), a program developed to create the Computer Generated Forces (CGFs) that the users will engage.An after-action review machine was set up to log the actions of the users.  The logging program was specially developed for the virtual environment program named Ansel.Users would participate in a scenario designed for the WHSIL lab and the results would be logged.  The data would be gathered from the logs.2. Virtual EnvironmentThe program that was used to create the virtual environments is called ManSIM.  The program allows a user to control an avatar around a virtual environment and interact with physical objects and Computer Generated Forces.   It is similar to first-person shooter video games.  The program was designed to be scalable and network with other systems using the High Level Architecture (HLA).  Hence, multiple instances can be run on separate machines allowing different users to work either as a team or independently.The program was built using the Gamebryo game engine [2].  Additional components were included to support realistic physics.  This enabled the networked users to accurately interact with physical objects such as tables, chairs, doors, etc.  Dynamic terrain was also added to the simulation so that a user can shoot a hole in a wall with an M203, walk in a crater created by a bomb, etc.2.1 Gamebryo EngineA commercial graphics engine targeted at the entertainment industry, Gamebryo, by NDL, was selected for this effort.  It is a game engine that has been used successfully in over 70 commercial games and has seen some use in military training applications.  It was selected in part because it includes a well-structured Application Program Interface (API) and its provision, through updates, to track the latest capabilities of graphics hardware and lower level APIs.  This allows developers to focus on application development rather than graphics infrastructure. 2.2 Physics EngineThe Physics engine was created for the program to be a network scalable realistic physics engine.  It allows the networked users to interact with a physical object such as a chair that is being simulated on another machine on the network.  The physics engine also allows for accurate physical response to stimuli.  The physics is more realistic than the standard “bounding-box” method that is frequently used.  More can be read about the physics engine in the Simulation Interoperability Workshop paper “Simulating Rigid Body Physics in a Distributed Environment” [3]. 2.3 Dynamic TerrainDynamic terrain adds another dimension of realism to the simulation.  It allows the terrain to be modified as the program is being run instead of just at load time.  These modifications include wall breaches, craters, fighting positions, and infantry trenches.  Examples can be seen in figure 1 below. EMBED PBrush  Figure 1: Examples of Dynamic Terrain3. Human System InterfacesOnce a virtual environment has been established, different Human System Interfaces can be evaluated.  This constant environment ensures that the only difference between the systems is the interface. These systems vary greatly.  On the low end, the HSI is a standard desktop system configured like most commercially available video game systems.  Additional capabilities were added with additional cost.  Two different tracking systems were used.  The Intersense system tracks three degrees of freedom (DOF) of orientation.  The PhaseSpace works as a six DOF tracker meaning that it can track the orientation of an object as well as its position in three-dimensional space.  Each system will be described in greater detail below.3.1 Desktop SystemThe desktop system is the system that most resembles the common video game environment.  It consists of one desktop machine running as a participant on the network.  The user may choose from several different choices of off-the-shelf input devices.  These devices can include a joystick, a mouse, and/or a keyboard.  One of the first benefits for this setup is that it is readily available and inexpensive.  The majority of first-person shooter video games use this kind of design.  This type of interface is also familiar to the first time user.  Many of the HSIs that will be mentioned later will require varying amounts of time in order for the user to become accustomed to it.  In this case, many of the users are familiar with the first-person shooter and can immediately start using the system effectively.  Even if a user is unfamiliar with first-person shooters, they are still familiar with a keyboard and a mouse and therefore have a very small learning curve to start using the interface.The main limitation of this system is that it removes a lot of the aspects that make the system immersive.  The user must sit in a chair in front of a monitor.  The user also does not get to hold a weapon prop (as will be described later) as he or she would if they were in the any of the other systems.  Even though the system is by far the least expensive of all the systems, it does not create a very effective immersive environment for users to train.3.2 Screen ShooterThe screen shooter system was developed by NAVAIR-Orlando.  The system is set up with a projector system placed in front of the user.  The user holds an Indoor Simulated Marksmanship Trainer (ISMT) weapon, which is a modified M-16 rifle with a laser inserted into the barrel.  It is also hooked up to a carbon dioxide tank, which is used to create realistic recoil when the weapon is fired.  The user navigates by using a wireless joystick that is attached to the barrel of the weapon.  With their thumb on the joystick, they move the avatar forward and backward and rotate it side to side.Figure 2: An ISMT Rifle with JoystickOne of the attributes of this system is that it is still less expensive than the 6-DOF system.  It also incorporates more ways to train the user.  Since they are carrying a weapon prop, proper weapon handling techniques are required.  So it allows the user to be trained in proper weapon handling on top of the training that can be accomplished with the desktop system.Figure 3: The Screen ShooterOne limitation to the system is that it is still not a truly immersive system.  Like the desktop system, the user is placed in front of a screen and tethered to a single location.  Since the user is not wearing a Head Mounted Display (HMD) with a head-tracker, he or she cannot glance side to side in order to get a better sense of their surroundings.  If they want to see, for example, what is to their immediate left, they are required to rotate the body of the avatar and rotate back to return to their original position.  Therefore, sidestepping is not available as a movement option in this system.  That is a limitation because he or she is not allowed to make movements that are common when traversing a combat situation such as pieing a corner (a controlled method of rounding a corner with a weapon).Although this system has improvements over the desktop system, it is also significantly more expensive.  3.3 3-DOF Intersense SystemThe three degree of freedom System is an immersive system that uses two InertiaCube2s for tracking.  InertiaCube2s (IC2s) are inertial trackers that are produced by Intersense of Bedford, MA.  The IC2s use the earth’s magnetic field a method to correct for drift in the inertial sensors.  There is a calibration method that allows the tracker to work well when placed on a steel prop.  This is an orientation tracking system only.The user stands in a Virtual Reality pod and wears an HMD.  In the systems designed at NRL, we used a stereo NVIS nVisor SX.  One of the IC2s is mounted on the HMD.  The other IC2 is placed on the weapon the user will be using to fire and for navigation.  Figure 4: The 3-DOF SystemThe WHSIL Lab utilized two weapons for the user.  The first is the ISMT weapon that is used for the Screen Shooter system.  The other weapon was developed by the Naval Postgraduate School.   This Airsoft based weapon prop is a modified pellet gun similar in size and shape to an M-16.  It was modified by internally mounting a wireless joystick and a rechargeable battery.  The Airsoft weapon was much lighter and easier to use than the ISMT weapon and therefore was more frequently used.Figure 5: The Airsoft Rifle with JoystickNavigating the virtual environment was done by using the joystick that was embedded in the barrel of the gun.  Since the user’s head was tracked, it was no longer necessary to have the rotation of the avatar controlled by the joystick as it was in the Screen Shooter system.  In this HSI, the joystick controlled the user’s front and back motion and side-to-side motion.  Thus allowing the user to sidestep.Since the IC2s can only track orientation it poses many interesting problems when it is used to track a person in a 3-dimensional space.  For example, a given orientation of the weapon is not useful unless we know its location.  Therefore certain assumptions had to be made.  The biggest assumption made was the location of the head and weapon.  If the IC2 reads that the head is tilted at a 30-degree angle, that could mean dramatically different things depending on which part of the body is causing the tilt.  For example, if the user if peering around a corner at an enemy, a 30-degree tilt at the neck would only expose the user eye.  But a 30-degree tilt at the lower back would expose the user’s entire upper body.  Therefore it was decided that the user would remain completely erect through the neck and the head would be placed atop the neck at the orientation dictated by the IC2.The location of the weapon was more straightforward since one of the main traits of properly firing a weapon is that the eye should be lined up through the sight.  Therefore the weapon was placed so that it was directly in front of the eye.  It was oriented about the sight so that no matter how it was oriented, the eye was always looking down the sight.These assumptions made the avatar unable to do certain things that the participant would be able to do in the real world.  For example, because the avatar body is always upright the user is unable to peer around a corner without exposing one side of their body.  Another example is that the user never has to aim.  Since the weapon is placed so that the user is always looking down the sight, he or she never has to line up the front sight with rear sight.  Therefore the user may learn poor technique of pointing the weapon in a general direction and firing and giving inflated accuracy rates.Despite these limitations, there are several advancements over the previously described systems.  This is the first fully immersive system.  The user wears a tracked HMD, which allows the user to look around in a virtual 3-dimensional environment.  They are able to walk independently of the direction they are looking.  This allows for a much more realistic experience for the user giving them a better sense of what it is like to traverse in the virtual environment.  3.4 6-DOF PhaseSpace SystemThe six degree-of-freedom system is very similar to the three degree-of-freedom system.  The user immerses himself or herself using a tracked HMD.  The user also uses a weapon prop that could be either the ISMT weapon or the Airsoft rifle.  As was the case with the Intersense system, we chose to use the Airsoft rifle frequently to make the user more comfortable.Figure 6: The 6-DOF SystemThe main difference between the two systems is that they use a different tracking technology.  This system uses a six degree-of-freedom tracker developed by PhaseSpace.  The PhaseSpace tracker requires LED markers to be placed on the item to be tracked; in this case the head, torso, and weapon.  Each marker flashes a certain code that is picked up by an array of cameras.  The system then triangulates the location of the markers.  When configuring the system, each set of markers is placed onto the tracked item at a fixed position and this is recorded as a rigid body.  When the location of all the markers on the object has been triangulated, the PhaseSpace computer can then determine the location and orientation of the rigid body.In our system we placed six markers on each object.  To track the torso we placed six markers on a small backpack that the user would wear.  This allowed ManSIM to independently track the location and orientation of the head, torso, and weapon, which was not the case in the Intersense system.  The reason it was done in the PhaseSpace system was because the markers are highly scalable and an extra body part could be tracked simply by adding more LEDS when the Intersense system would have required another IC2.  Therefore, adding another tracked body adds ~1% to the cost of the PhaseSpace system but 33% of the Intersense system.The 6-DOF tracking allows for more freedom and realism in the virtual environment.  It allows the user to do things that they would do in a real training environment such as peer around corners and duck for cover.  The 6-DOF tracking on the weapon also makes the weapon handling more realistic.  The user is now required to properly aim the weapon before firing which was not the case in the 3-DOF system.  One attribute that was noted by users was that even though the PhaseSpace system offers very high accuracy, it is still possible to see small tracking errors when lining up the sight of the weapon close to the user’s eye.  The 6-DOF tracking of the weapon also allows the user to take advantage of the physics engine because they can now do things like push a table over or kick a door open.One major difference between the 6-DOF and 3-DOF system is the cost.  The PhaseSpace was roughly an order of magnitude more costly than the Intersense system.  It also requires a second computer to run the algorithms that triangulate the markers’ locations.  The system as implemented also requires a third computer to generate a fifteen-segment avatar.  This system still has the limitation that all of the other systems had: the user still doesn’t move in the real world.  He or she still uses the joystick to navigate because the user is limited to the space inside the VR pod.This system offers higher fidelity at a much greater cost than the 3-DOF tracker system.  The WHSIL lab is running experiments that will help create a fidelity matrix so that future developers can determine which solution is best for them.3.5 Future PlansVariants and extensions to HSIs will continue to be constructed in response to experimenter requirements.  For example, a hybrid a 3-DOF head-only tracked system used in conjunction with a desktop-like (joystick) locomotion control was recently used in a series of experiments.    3.5.1 VirtuSphereA limitation of all the interfaces described above is that they require a joystick to represent locomotion.  A system whose integration has just begun will allow the user to use their legs to control locomotion.  This HSI is based on a device called the VirtuSphere (figure 7).  VirtuSphere is a sphere with an 8.5-foot diameter that is placed on a wheeled platform.  The user enters the sphere through a removable door. Within the sphere, a user is able to walk freely in any direction.  The system monitors the rotation of the sphere and translates that data into the motion of the avatar in the virtual environment.Figure 7: The VirtuSphereThe sphere is made up of a plastic mesh.  Although it is possible to see the user within the mesh of the VirtuSphere, the PhaseSpace optical tracker is unable to track markers through the rotating mesh.  Therefore, the interface will use 3-DOF Intersense trackers.The major benefit added from this system is that it allows the user to use their own movement to control the motion of the avatar instead of relying on a joystick.  This system imposes several constraints.  One is the space requirement.  The VirtuSphere require a room that can hold a sphere with an 8.5-foot diameter plus the height of the supporting platform. The VirtuSphere costs roughly the same order of magnitude as the PhaseSpace system yet it does not provide 6-DOF tracking.  Another limitation is that since the user is inside a large rotating sphere, they must wear the ManSIM computer and tracking system on their back.  This also constrains the head mounted display that can be used.  For example, we intend to use a low cost iGlasses HMD with this system, since it is lightweight and can be battery powered.  The HMD used with the 3 and 6-DOF immersive HSIs has a large associated control box, which makes its use in this system impractical.3.5.2 Training Transfer StudyThe culminating event of this portion of VIRTE research will compare the effectiveness of training transfer between selected human system interfaces and an instrumented live training environment (or “shoothouse”).  The HSIs selected for this experiment will be determined based on an ongoing series of experiments.  The HSIs will be selected as representative in various cost categories.  The instrumented live training environment has been developed by Clemson University.  It includes a reconfigurable shoothouse with video tracking of participants and laser tag-like weapons and hit detectors.7. AcknowledgmentsThe authors would like to thank the Office of Naval Research and its Virtual Technologies and Environments program for sponsoring this work.  We would also like to thank the many members of the VIRTE Government/Industry/Academy team that contributed to the effort.4. References[1]	J. Cohn et. al.: “A Virtual Environment for Urban Combat Training”, Proceedings of the 24th Army Science Conference, November 2004.[2] L. Bishop, et. al.: “Designing a PC Game Engine”, IEEE Computer Graphics and Application, pp. 46-53, January/February 1998. [3]	S. Cullen et. al.: “Simulating Rigid Body Physics in a Distributed Environment”, Simulation Interoperability Workshop, 05S-SIW-079, March 2005.Author BiographiesPETE WIERZBINSKI is a Software Engineer with Lockheed Martin Simulation, Training, and Support.  Pete has worked in the Simulation industry for three years and is currently a developer on the ONR sponsored VIRTE program.ROY STRIPLING is a Research Biologist at the U.S. Naval Research Laboratory in Washington, D.C.  He received his Ph.D. in Neuroscience from the University of Illinois at Urbana-Champaign.  At the Naval Research Lab, Dr. Stripling heads the Warfighter Human System Integration Laboratory, where he seeks to leverage neuroscientific and technological advances to enhance human performance and improve training technologies