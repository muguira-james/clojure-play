Simulated EW threat environment and case study of UAV augmented realityLarge Area, Virtual Training Databases with CTDB-GCSKenneth B. DonovanGregory A. Harrison, Ph.D., Brett Vonsik and Patricia Husband, Michael Longtin, Paul PalmaEytan Pollak, Ph.D., and Brett Vonsik  Lockheed Martin Information Systems 12506 Lake Underhill Road, Orlando, Florida 32825-5002Ph: 407-306-33686580, Fax: 407-306-34724708 HYPERLINK mailto:ken.b.donovan@lmco.com ken.b.donovan@lmco.com,  HYPERLINK "mailto:patricia.husband@lmco.com" patricia.husband@lmco.com,  HYPERLINK "mailto:michael.longtin@lmco.com" michael.longtin@lmco.com,  HYPERLINK "mailto:paul.palma@lmco.com" paul.palma@lmco.comKeywords:Synthetic Natural Environment, Database Generation, SEDRIS, SAF, CTDB, GCSElectronic Warfare, UAV, Augmented RealityABSTRACT: The simulation of electronic warfare emissions can enhance air route planning and tactical employment of UAVs.  Real-time EW simulations including physical beam modeling, antenna patterns, power, free-space transmission, and receiver and sensor modeling can be visualized dynamically to provide the augmented reality needed by UAV operators when employing UAVs in hostile environments.  Radar and other sensor antenna patterns modeled as physical entities in an augmented reality environment allow for operator visualization of encounters with potentially hostile weapon systems and increases UAV survivability while providing enhanced after action analysis and critical pre-mission training.  Using a simulated environment to find minimum risk routes can enhance the security of UAV flight paths.  The risks presented by surface-to-air missiles and radar-controlled antiaircraft artillery may be minimized through pre-tailored flight paths that adapt to current intelligence concerning hostile air defenses.  Real-time changes to the threat environment can be reflected in the virtual environment, and used to adapt the UAV flight plan.  UAV operators can see the instantaneous threat layout of the area of operation and change UAV paths interactively using handheld computers or workstation interfaces.The paper will present a summary of work done at the Lockheed Martin Information Systems R & D lab concerning EW simulation in a virtual environment.  A prototype was created having an emulated UAV in a simulation of a hostile environment.  The paper will describes the architecture for the prototype EW integration with the UAV and the IPAQ wireless control and monitoring. Lockheed Martin Information Systems is producing over one million sq. km of synthetic environment (SE)  databases for virtual flight training applications, with correlated databases in Compact Terrain Database (CTDB) format, visual and radar run-time formats.  The CTDB is produced in the Global Coordinate System (GCS) to provide the necessary correlation with WGS-84 over large areas.  Lockheed Martin has prior experience producing large area CTDB-GCS for use by JointSAF in constructive simulation exercises.  We also have prior experience producing large area databases for virtual flight trainers.  However, this is the first virtual flight trainer database that included CTDB-GCS production. The CTDB-GCS is used pervasively within the simulation functions including semi-automated forces, electronic warfare, communication, ownship vehicle dynamics, training scenario generation and exercise control.  This paper describes the process we used to produce the large area databases, including database re-use from a world-wide simulator library covering over 17 million sq. km. that was built from NIMA sources.   We describe the SE database requirements and tools, the world-wide database library, the use of SEDRIS and CTDB-GCS, and the current database production progress for the flight training system.Unmanned Aerial Vehicles (UAVs) in use by defense organizations today are augmenting manned aircraft missions in a variety of roles from acting as decoy targets in areas with deadly air defenses, to reconnaissance vehicles collecting sensor information and intelligence in close proximity to enemy forces, to weapon delivery platforms employed in direct attack missions in augmentation to or when manned aircraft are not available.  Recent UAV programs have confirmed the present direction of development for these platforms expressing the need for unmanned vehicles that can carry out a wide variety of missions to include intelligence collection, targeting, and suppression of enemy air defenses (SEAD).  The use of UAVs appears to be a growing staple within military forces worldwide.The operational and tactical employment of UAVs continues to be an evolving point of interest at all levels within defense organizations.  Tactical control and the real-time employment of UAVs have followed traditional means where UAV pilots use nose mounted camera video and 2D displays to determine the condition and attitude of their vehicles and 2D map depictions with a UAV icon for situational awareness (SA) in mission environments, many being lethal.  Camera video, when used, suffers the limitation of standard day, low-light, and thermal cameras.  Limited field-of-view and field-of-regard are the most typical of these limitations, but lack of video quality and interference in the video signal returning to the control center can be major issues.  Control of UAVs is accomplished through text readouts or 2D graphic displays of basic vehicle systems status and attitude reference data.  Navigational control of UAVs typically is accomplished with 2D map and icon representations depicting UAV location, waypoints and navigational course lines.  Electronic intelligence and warfare data representations follow traditional display formats similar to manned aircraft, where applicable, or suffer from crude engineering type readouts.  Lack of the SA that manned aircraft enjoy, combined with limitations in displaying sensor data, make UAV operations difficult under the best of conditions.The difficulties faced by UAV crews in controlling their vehicles stems from a lack of SA of the environment they operate in.  Where a pilot flying an aircraft can look around to survey the battlefield and collect enormous amounts of information with a glance, a UAV crew cannot do so without impacting the performance of the UAV by adding cameras in the vehicle, resulting in increased vehicle weight and reduced mission critical performance.   The need for UAV crews to gain SA comparable to manned aircraft is obvious for these weapon systems to reach their full potential.Virtual flight training systems and real-time simulator technologies can provide the augmented reality needed to increase the SA of UAV crews, particularly through real-time 3D visualization of the electromagnetic spectrum in sensor-fused presentations.The fidelity and correlation of synthetic environments (SE) in virtual simulation continues to advance.  Representation of real-world terrains in out-the-windows (OTW), electro-optic (EO) TV, and infrared (IR) displays is rapidly reducing the gap between camera video and virtual scene generation.  Radar representation of real-world terrain by digital radar landmass simulations (DRLMS) is approaching the point that it cannot be distinguished from real sensor video.  Natural environment elements such as clouds, fog, precipitation, sun illumination, moon illumination, temperature, pressure, and humidity all can be simulated in OTW and in sensor models with increasing accuracy and realistic presentation.  The realistic representation of the natural world in virtual simulation has been a goal of the modeling and simulation community for many years and has received considerable attention and funding.  What has interested both aircraft and UAV crews in recent years is the ability to see what previously could not be seen, especially in real-time, such as the electromagnetic spectrum and weapons-related information.  What has kept this technology out of the cockpit is the enormous computational power required to visualize such data and the fusing of data in a useful format.  In short, augmented reality.Virtual simulation technologies and UAVs are a natural fit with augmented reality.  Visualization is performed at the UAV control center instead of on the air vehicle thus eliminating the problem of needing to use mission critical weight allowances and volumes.  Sensor data from the UAV can be fused in the control center and be presented in real-time to UAV crews increasing their situational awareness and potentially surpassing that of manned aircraft.  This new means of augmented reality truly shines where visualization of electronic warfare (EW) and weapons data is needed.This paper describes advances in virtual simulation technologies and their applicability to the operational employment of UAVs, enhancements to operator situational awareness, and improvements to post-mission analysis.Introduction Unmanned Aerial Vehicles Unmanned Air Vehicles (UAVs) in use by defense organizations today are augmenting manned aircraft missions in a variety of rolels from acting as decoy targets in areas with deadly air defenses, to reconnaissance vehicles collecting sensor information at close range to enemy forces, to weapon delivery platforms employed in direct attack missions when manned aircraft are not available.  Recent UAV programs have confirmed the present direction of these platforms by expressing the need for unmanned vehicles that can carry out a wide variety of missions to include intelligence collection, targeting, and suppression of enemy air defenses (SEAD).  The use of UAVs appears to be a growing staple of military forces world-wideworldwide.The operational and tactical employment of UAVs has been and continues to be an evolving point of interest at all levels of defense organizations.  Tactical control and the real-time employment of UAVs have followed traditional means where UAV pilots use nose camera video and 2D map displays to determine the vehicles condition of the vehicles and to direct them in hostile environments.  The difficulties faced by pilots to control airborne UAVs stems from a lack of situational awareness (SA) of the surrounding environment.  Where a pilot flying an aircraft can look around to survey the battlefield and collect enormous amounts of information with a glance, or two a UAV pilot cannot do so without impacting the performance of the UAV by having additional cameras mounted on the vehicle, resulting in increaseding the vehicles weight and reduceding mission critical airframe volume.   The need for UAV crews to gain thea situational awareness comparable to manned aircraft is obvious for these weapon systems to fully reach their full potential.Virtual flight training systems and real-time simulator technologies can providee the augmented reality needed to increase the situational awareness of UAV crews, particularly through real-time 3D visualization of the electromagnetic spectrum in a sensor- fused presentation.continue to advance The in the fidelity and correlation of synthetic environments (SE) in virtual simulation continues to advance.  Representation of real-world terrains in out-the-windows (OTW), electro-optic (EO) TV, and infrared (IR) displays is rapidly reducing the gap between camera video and virtual scene generation.  Radar representation of real-world terrain by digital radar landmass simulations (DRLMS) is approaching the point that it cannot be distinguished from real sensor video.  Natural environment elements such as clouds, fog, precipitation, sun illumination, moon illumination, temperature, pressure, and humidity all can be simulated in OTW and in sensor models with increasing accuracy and realistic presentation.  The realistic representation of the natural world in virtual simulation has been a goal of the modeling and simulation community for many years now and has received considerable attention and funding.  What has peaked the interest of both aircraft and UAV crews in recent years is the  ability to see what previously could not be seen, especially in real-time, such asis the electromagnetic spectrum and weapons -related information.  What has kept this technology out of the cockpit is the enormous computational power required to visualize such data and the fusing of data in a useful format.  In short, augmented reality.Virtual simulation technologies and UAVs are a natural fit withhere augmented reality is concerned.  Visualization is performed at the UAV control center instead of on the air vehicle thus eliminating the problem of needing to use mission critical weight allowances and volumes.  Sensor data from the UAV can be fused in the control center and be presented in real-time to UAV crews increasing their situational awareness and potentially surpassing that of manned aircraft.  This new means of augmented reality truly shines where visualization of electronic warfare (EW) and weapons data is needed.In the past, virtual trainers typically approached SE from the perspective of the simulator visual system, optimizing the SE for the high cost image generator.  The requirements for other simulation functions such as the tactical environment (semi-automated forces, electronic warfare), instructor control, and radar simulation were treated as a secondary consideration.  Requirements for high fidelity tactical environments, coupled with the need for correlated network operation, have challenged this heritage paradigm. This paper describes  three aspects of our solution to these SE requirements for a virtual flight training system.  First, the SE is designed from the start to support the high fidelity dynamic tactical environment in a networked environment, as well as the visual and radar imagery needs of a participant in the vehicle simulator.  Second, the SE system design uses Compact Terrain DataBase (CTDB) to provide the SE representation for the tactical environment.  The CTDB was chosen for compatibility with the several semi-automated forces (SAF) packages, including JointSAF, ModSAF, and SE/SAF.  The basic CTDB “flat earth” representation did not support the required database size of several hundred thousand square kilometers.  One of the enablers for high fidelity tactical environments over large areas was the work performed by Lockheed Martin supporting the Synthetic Theater of War (STOW) in 1997 [1].  For the STOW project, we produced a gaming area of 380,000 sq. km for JointSAF,  with a correlated ModStealth using the CTDB Global Coordinate System (GCS).  For the current large area databases, we  also employ CTDB-GCS to provide WGS-84 accuracy, including earth curvature horizon effects.  A third issue we addressed for the SE system design is database re-use.  The databases include generation of new areas from NIMA sources, however most of the area is re-use from previously developed terrain databases. This paper addresses these issues in the context of building four geographic databases for virtual flight trainer applications – collectively over 1 million sq. km.   Other applications may be interested in this process that efficiently generates large area databases in CTDB format from a worldwide database the advances in virtual simulation technologies and their applicability to the operational employment of UAVs, enhancements to operator situational awareness, and improvements to post-mission analysislibrary.  The inputs to the SE system will be described, as well as internal processing, and system outputs, and potential use of the data is outlined.SE RequirementsSimulated Active Sensors “To see that which is not seeable” has been a desire of military aviators for many years, especially where electronic combat is concerned.  Today, aircrews rely on passive radio frequency (RF), IR, Night-Vision EO, and laser receivers to provide symbolic and pulse or wave pattern displays complete with crude indications for azimuth and elevation.  The point which adversary sensor, surface-to-air missiles (SAM), anti-aircraft artillery (AAA), and fighter aircraft weapons systems with active and passive sensors are able to detect, track and engage aircraft with weapons has been mostly guesswork on the part of aircrews based on available indications.  Sensor envelopes can be further affected by atmospheric conditions including humidity, temperature, time-of-day, fog, scud, and adverse weather.        Three-dimensional visualization of electromagnetic emissions and weapons-engagement envelopes based on the targeted aircraft’s position, attitude, and signature would greatly improve aircrew SA.  This is especially true for UAV aircrews due to their lack of visual and seat-of-the-pants cues that absent them. cues.RadarRadar system simulation in a synthetic environment requires the creationuse  of virtual emitters and receivers, along with real-time simulation of the effect of the electromagnetic beam geometry.  The calculations of the effect of each beam must be determined at each time step for each transmitter/receiver pair.  Envelope information can be used for planning and threat evasion.  The envelopes abstract the beam information to provide a more constant isometric surface representing a given level of radar system sensitivity and power.  EmitterEmitters are placed into the SE when they are discovered, or in advance for anticipated threat localization.  Emitters can be viewed in augmented reality by describing threat envelopes to give a picture of their overall sensing range and lethality concerns when coupled with surface-to-air missiles, or.  The can also be represented using the i as individual radar beam position.  Individual beam positioning can be coupled with auditory information to provide cues that an experienced Electronic Warfare Officer (EWO) can use to judge the threat circumstance.  Information such as the emitter power, beam geometry, scan type, and beam direction are important in determining the correct beam to emulate in the SE.  Once the information is encoded, the beam can be synchronized and represented internally to the SE for use by entities traversing the virtual environment.AntennaThe antenna generates a radiated antenna pattern, which is a 3D moveable entity in the SE.  With the radiated power of the emitter, the antenna pattern determines the possible encounter with the beam.  The aAntenna patterns generally consist of a main lobe and various sidelobes that are useful for jamming.    The antenna pattern is used both for visualization and for internal calculations to ascertain the amount of received power at the UAV receiving antenna at a given moment in time.PolarizationElectromagnetic beams have a polarization property that determines the amount of power that a receiving antenna may capture.  The received power may fall off drastically if an antenna with a different polarization is used to receive as that which is being transmitted.  Polarization refers to an instantaneous geometry in space of the motion of the electric field in a transmitted electromagnetic (EM) beam.  As an entity such as a UAV translates and rotates through the SE, the polarization may change, and hence is a function also of the position of the receiving antenna.Scan pattern evolution in timeThe scan patterns for radar are generally deterministic, but more modern radars use a track-while-scan capability that is contingent upon having received information about a potential target.  Scan patterns have horizontal and vertical displacements, as well as imposed nutations or small rotations about the regular scan progression.  The scan pattern can be abstracted to describe the threat envelope, based on how far in each direction that the beam is scanned.Threat envelopeA threat envelope can be generated for a particular radar, and placed in the SE for use in path planning and threat avoidance.  Different radar/missile installations have different effective distances, which can be described visually in the SE.  These threat envelopes can also be used for automated analysis of potential paths through hostile space.ReceiverThe receiver for transmitted energy in an SE is modeled as a function of its antenna gain pattern and the receiver characteristics.  As in the transmitter, the receiver antenna pattern determines the interaction with the EM environment.  The polarization, the antenna pattern lobes, and the orientation and position of the receiving antenna with respect to the EM energy all affect the received power.  The general unit for received energy is decibels with reference to a milliWatts or dBm.  The UAV radar receiver can register a certain level of received power.  When the receiver is outside the range for which the power may be registered, then further calculations with respect to that radar beam are pointless, and may be filtered out before employing extensive calculations.Threat and Weapons DataThe ability of UAVs to remain outside a hostile weapon system’s tactical and lethal engagement envelopes is critical to its survivability and mission success.  Using real-time 3D visualization to depict to the aircrew a hostile weapon system’s location and its ordinance kinematics range based on altitude and UAV position, velocity, and orientation (PVO) and atmospheric conditions would expand the UAV’s operating envelope while removing the guesswork needed on the part of operators to keep the UAV safe.  Weapon ordinance kinematics envelopes would dynamically updatemove in range and altitude in the 3D visualization display based on known capabilities and target PVO allowing aircrews to pilot their UAVs with greater precision and at less risk of being shot down.  Terrain masking and clutter regions based on weapon seeker limitations can be represented to the UAV aircrew, showing safe zones in which to fly during ingress, egress, and weapons engagement.  Such knowledge can increase SA beyond that present in today’s manned aircraft.Onboard Receivers and Sensor FusionSensor packages incorporated into existing UAVs vary according to mission type.  Fixed and steerable EO and IR sensors are commonly used not only for forward facing video by UAV pilots for orientation, but also for intelligence gathering, target acquisition, and direct attack with a combined laser-target designator.  Electromagnetic sensors, particularly in the radio and radar frequency bands, are being incorporated into UAV sensor packages to support electronic communications surveillance and suppression of enemy air defenses missions.  Much of this data is transmitted back to the UAV control center where the information is fused through human interpretation of data readouts, 2D map displays, and non-fused video streams.  Electromagnetic emissions provide a particularly difficult problem of how to present the data in readouts and displays that turn the data into useful information. ObstructionsAs in the real world, obstructions diminish the amount of directed energy reaching the receiver.  Mountains and the horizon will greatly reduce the energy or stop it completely.  The atmosphere also affects the microwave EM field propagation.  Having a robust synthetic natural environment (SNE) increases the fidelity of the emissions simulation as it interacts with the terrain model.Dynamic and Rapid Sensor Data UpdatesCombining pre-mission intelligence information, real-time data-link intelligence data from remote sensing platforms through national Tactical Data Dissemination System (TDDS) and Tactical Information Broadcast System  (TIBS) intelligence broadcasts with UAV down-link data would provide the best of all worlds to UAV aircrews.  Atmospheric condition updates from remote sensors, weather reports, and UAV onboard sensors can be incorporated into the real-time natural environment simulation and impact sensor and weapon capabilities.  Geo-locating UAV targets and hostile air defense systems on 3D terrain databases in real-time will provide the tactical laydown upon which 3D visualization calculations are to be based.  Incomplete and mismatched data can be quickly identified and corrected for use in the ongoing mission or for after-action and post-mission analysis.  Real-time changes in battlefield conditions and threat system sensor and weapons modes can be represented in numerous ways including multiple color and translucency graphics of threat envelope representations within the 3D display. UAV sensor targeting can also be depicted in the 3D visualization representing field-of-view (FOV) and sensor sensitivity against targets across the electromagnetic spectrum such sensors are capable of detecting.  UAV aircrews would be able to employ their assets with greater effectiveness than previously possible with only camera video and 2D map displays and potentially better than that capable of manned aircraft.ObstructionsAs in the real world, obstructions diminish the amount of directed energy reaching the receiver.  Mountains and the horizon will greatly reduce the energy or stop it completely.  The atmosphere also affects the microwave EM field propagation.  Having a robust synthetic natural environment (SNE) increases the fidelity of the emissions simulation as it interacts with the terrain model.UAV Simulation  The UAV vehicle simulation can be simplistic or as high in fidelity as any manned aircraft simulator today.  Airframe, propulsion, fuel, electrical, avionics, sensor, and weapons simulations are available in numerous legacy aircraft simulations or can be created if existing models prove insufficient.  Data-linked UAV avionics system health, aerodynamics, and payload status can update such models for presentation in the 3D visualization scene and on 2D instrument displays, and incorporate the UAV’s interaction with the synthetic tactical and natural environments.  .  Modeling the UAV signature profiles in the RF, IR, and acoustic spectrums and dynamically updating signature envelopes for display can provide mission critical information affecting the aircrew’s tactical employment decisions for the vehicle.  Three-dimensional visualization of UAV sensor payload FOVs and sensing capabilities, configured weapon envelopes, combined with the tactical and natural environment conditions can increase their effectiveness by the increase in aircrew SA.  Instead of requiring operators to anticipate upcoming targets as they look through a sensor’s narrow FOV in the case of electro-optics and referencing 2D map displays, operators can enjoy a broader view of the battlefield, filled with 3D terrain features complete with imaged texturing.  They can plan, in real-time, the route ahead of the UAV based on the visualized scene, and make minor adjustments to route and attitude that presently are difficult to perform.  The goal of fusing onboard and off-platform data and visualizing the UAV within a 3D environment is to immerse operators into an augmented reality that will rival the experience of manned flight.  A number of high fidelity aircraft simulators, such as the AC-130U presently in use by the US Air Force, immerse aircrews in an environment so realistic that they tend to forget they are not actually flying.  The aircrew’s reactions, employment decisions, and mission tempo reflect that experience in real-world flight missions.  UAV crews can experience this same level of immersion.  In addition to direct control employment in real-time, use of simulation technology allows UAV crews to fly “what-if” scenarios prior to actual mission employment or during enroute cruise phases of the mission to rehearse or determine the best profile to fly.  Augmented Reality ImmersionThe goal of using virtual reality is to allow the user to attain a sense of presence in the simulated environment.  Augmenting this SE with information that allows the user to better understand and utilize the SE representation allows the system to become a tool.  To provide an immersive experience in a representation of the natural world requires that the physics of the world be modeled with a high degree of realism.  It is obvious that in a training scenario, realism is of utmost importance so that the student can carry the skills learned into practice.  In augmented reality, realism is also important, but new features are added to the environment that allow the user to interpret events and situations with more clarity and to interact with the entities in the environment as needed.   Path of least detectionWith the radar threat environment capturedrendered in the SE, intelligent agents can traverse the SE, obeying the behaviors of actual entities, and determine a measure of detectability over the path.  As the path progresses through space and time, the probability of detection is integrated along the path.  Due to the complex environment, this heuristic trial and error method of path planning may be a profitable exercise.  It would be difficult to determine the safety of a path through analysis alone.  The simulation of the path allows analysis to be verified and tested.  Multiple paths may be tried in the safety of an SE, and evaluated to obtain an overall score and a section-by-section detectability score.  Analytic solutions such as augmenting the SE with force field physics to provide resistance against traversing too closely to a dangerous location can be used to help determine the path of least detectability.  A given path can be tested in the SE to determine its rating.  Occulting and radar physical effectsOcculting or terrain masking of radio, radar, and laser emissions can be calculated in the simulation environment by using geo-specific terrain databases created from digital terrain-elevation data (DTED), photo imagery or other sources.  Occulting algorithms and processes are commonly used today in both virtual and constructive simulations.  Line-of-sight (LOS) calculations are used by simulations to mask air defense threats and opposing forces from other entities, including virtual simulator ownships.  These same occulting processes can be used in support of UAV operations by providing crews the ability to anticipate unmasking and masking of target and hostile air defenses prior to the event actually occurring.  Attenuation of EM emissions based on natural environment factors such as water vapor, carbon dioxide other absorptive elements is possible and relatively straightforward in real-time.  Presentation of attenuated emissions in 3D visualized displays adds another layer of increased SA for UAV crews and allows “what-if” mission profile testing by simply filtering such calculations on and off as needed.  Real-time visualization of LOS occulting between UAV and vehicles and point locations in an area of operations (AOA) combined with the impact of atmospheric conditions on EM emissions can further enhance UAV crew SA and improve mission effectiveness.Path dynamic re-planning Evolutionary algorithms such as genetic algorithms can be used to grow an optimized path.  Various requirements can be graded for each path, such as time to target, threat avoidance, and energy consumption.  A human can chose path sections to combine into a final choice, or artificial intelligence can combine paths and test them until a suitable one is determined.  Because of the complex nature of the paths, it would be difficult to employ a gradient search mechanism, unless multiple sections were considered in isolation.  As the vehicle moves through space, different threats become enhanced or decline.  The fitness-driven search through pathspace employed by an evolutionary algorithm can determine more optimum paths.As new information is obtained in real-time and inserted into the SE, the forward path must be re-evaluated for threat level.  The paths traverse geographical space, which has an intrinsic complexity level.  High level rules can apply, consisting of a default hierarchy of rules.  Immediately applicable rules with higher priority can override the planned path.  Paths may then be dynamically altered without extensive replanning of forward paths.  Immediate threat avoidance routines can be executed with respect to the perceived threats.  In the meantime, new forward paths can be calculated, using the new information.Electronic databases and auto route planninDifferent levels of rules can provide a high-level guide for creation of a path.  The decision to use a rule or not would be based upon the specificity of the rule.  Those rules that are more specifically applicable to the current state of path evolution would be more likely to be chosen than a general rule.gThe use of electronic warfare reprogrammable databases such as EWIR allows for rapid updates to the parametric values within the tactical and emission simulations that willhelps ensure the UAV visualization environment remains current with the latest intelligence.  Use of automated and semi-automated loading tools is critical to maintain the currency of sensor and weapons capabilities within the synthetic databases due to the enormous number of data elements available through intelligence and data collection programs today.  Auto route planning for UAV missions based upon current EW data, terrain profiles, environmental conditions, and UAV signatures can augment traditional route planning through the use of 3D visualization of flight profiles.  The same tools, environment, and simulations used to employ actual UAVs can be used in the mission planning, mission rehearsal, “what-if” scenario runs, and post-mission analysis.“What-if” Scenario Testing In the past, simulation of the battlefield was intended to introduce all the obscurants and degeneration of situational awareness that occurs during a battle.  With augmented reality, the emphasis is upon increasing the amount of information available.  No longer is the attempt made to “level the playing field”, providing realistic simulation to all entities participating in the exercise.  Instead, the goal is to highly weight the playing field in the users favor.  As musch situational awareness as possible, as well as rapid use of the information, will provide an informational edge to the warfighter.  Commanders may engage in a what-if type of computer-integrated war planning.  By availing themselves of evolutionary algorithms, effective and safe path planning can become more automated.  Various potential paths can be flown at faster than real-time rates, and fitness points can be garnered along the path.  With advanced planners, sorties can be automatically judged, combined, and evolved into better sorties rapidly using the new informational awareness provided by the immersion of real situations into the synthetic environment.TCP/IP link of distributed intelligence dataThe use of TCP/IP is ubiquitous and reliable.  Simulation standards such as Distributed Interactive Simulation (DIS) and High Level Architecture (HLA) provide a reliable backbone for simultaneous interaction in an SE.  With these standards in place, battlefield information can be reliably disseminated throughout the command centers, and can be simultaneously available for artificial intelligence entities to analyze. TCP/IP is an open standard.  The stovepipe implementations that served to protect earlier information architectures would be gone.  Anyone having access to the information can strive to utilize the data to their advantage.  Thus, it is incumbent upon the simulation community to include encryption in the distributed simulation when it is used for fighting a battle.Field Command and ControlUse of PC-based and Personal Digital Assistants (PDA) computing systems to control UAVs and consume the sensor fused data from their sensor packages is cost effective in the present computing environment.  Commonly available information technology tools, including wireless networking systems, that can be purchased in any consumer electronics store combined with 3D visualization technology make the possibility of controlling UAVs by the average soldier on the battlefield a soon-to-be reality.  Augmented Reality InfrastructureNetworking distributed intelligence dataThe same technology commonly used in computer networking and virtual simulations today applies directly to the augmented reality infrastructure described above.  TCP/IP protocols are to be used to link the various internal components of the visualization system and external interfaces to the UAV system itself.  Data and protocol converters to transform TDDS and TIBS broadcast data to common networking TCP/IP and simulation interoperability protocols is critical.  Semi-automated Force (SAF) platforms, such as Distributed Information Warfare Constructive Environment - 180 (DICE-180), with converted TAB-37 tactical data translated to DIS or HLA entity representations and weapon system laydowns within the synthetic tactical environment.  TDDS and TIBS broadcast data would likewise need a translator tools to convert to the DIS and HLA protocols for consumption by the UAV 3D visualization system.  These translators will be required to work in real-time to support the operational employment of the UAV within the tactical battlefield.  Data elements provided to the UAV’s synthetic environment includes entity representations of ground vehicles, waterborne craft, aircraft, sensors, and weapons.  Entities will represent friendly forces, targets, and hostile Integrated Air Defense System (IADS) weapon platforms.  Dynamic mode and state changes of entities is supported in the DIS and HLA architectures and messaging.  The UAV visualization system can also provide feedback into the TDDS and TIBS networks for targets detected during the UAV mission to be consumed by other military assets through the use of the DICE-360 SAF and Multi-mission Advanced Tactical Terminal / Tactical Radio System (MATT/TRS) terminals.HumanNew human/machine interfaces are always being developed.  Judicious use of the available interface hardware enables a more effective augmented reality session.VisualDisplays can provide an immediate view of the surrounding area from a point of view outside the UAV.  Information obtained from the UAV can be classified and put into the SE to provide encompassing views of the battlefield.Many technologies are available to provide this visual scene, including a PDA, Virtual Reality goggles or a display screen.PDAPersonal Digital Assistants (PDA) such as the Compaq IPAQ have been wirelessly linked to evolving images from a virtual UAV.  Actual UAV camera information can also be transmitted to numerous participants through the wireless architecture to obtain greater analysis capability of the battlefield.GogglesVirtual reality goggles have grbeen employedresearched to allow a natural 3D interface to a human warfighter for instant analysis of threats in the SE.Display screenA display screen is ubiquitous.  Whether implemented using a Microsoft Windows machine, Linux, Unix, or Microsoft Windows CE on a handheld machine, visual displays of the augmented SE can be driven from a common database.  For full appreciation of the detail and complexity of the SE, a virtual simulator such as Lockheed Martin ReadiSim should be used to translate the SE into an image, as seen in Figure 1.HapticHaptic control refers to the kinematical or touch capability that we possess.  Haptic control, when immersed into an SE, provides instant ability to interact with controls in the SE.  This makes the UAV control experience more intuitive.  Less dependence upon computer formatted data, such as numbers and GUIs is required.GlovesVirtual reality gloves provide an effective method to interact with the SE. Motion commands and other control can be encoded into the use of the gloves, and objects may be placed into the SE for direct interaction with the glove wearer.UAV flight controlsThe UAV may be directed using an immersive virtual reality control system.  Both current UAV camera information, and the SE overview can be simultaneously presented to the UAV controller. SpaceballA spaceball interface allows six degree of freedom control for entities in the SE.  Acceleration, velocity, translation and orientation are all provided by a spaceball.  The spaceball consists of a spherical ball that is monitored electronically to determine motions imposed upon it by the user.  These motions are encoded and digital translated by the SE host to allow various interactions to occur.RobotArtificial Intelligence, (AI), implemented on a UAV or at the UAV base of operations can perform automated UAV control.  It can perform route planning, and adapt to changing priorities.AgentIntelligent agent techniques provide a means to include artificial intelligence technology in the operation and analysis of the UAV flight path.  These agents can simulate a UAV and fly potential paths in faster-than-real-time, to determine the suitability of prospective path choices. EMBED Visio.Drawing.6  Figure 1.  UAV in Augmented SE  (BV) (GH) rms?, s AntennaDistance from sourceTarget Radar cross sectionJammersThexxxMathematics and Physicsxxx primary SE requirement for the application is to support a wide range of flight training tasks, such as: takeoff/landing; formation flight with other networked simulators; terrain following; and tactical engagements with computer generated entities and electronic warfare.  A top level view of a typical networked simulation system is shown in Figure 2-1.  Figure 2-1 Typical Virtual Simulation SystemEach vehicle simulator (only one shown in figure) provides the virtual environment for one aircraft crew.  The tactical environment is provided, via the network, from other networked vehicle simulators and the tactical environment applications such as SAF entities and electronic warfare (EW).  Various support applications allow control and monitoring, such as the Instructor / Operator (IOS) and after action review (AAR).Figure 2-1 also highlights that there are several “presentations” of the SE.  For example, the tactical environment applications query the SE to govern entity movement (trafficability, collision) and to determine line of sight (LOS) occlusion with other entities.  Within each vehicle simulator,  the ownship uses SE queries such as height above terrain (HAT) and line of sight occlusion (LOS).  The SE must also support generation of radar and visual images (infrared, night vision goggles, and unaided out-the-window).  Each of these SE presentations must be correlated, even where different run-time formats are used to support the different presentations.  For the subject implementation, three run-time database formats are used.  CTDB is used to support the tactical environment, support applications and vehicle simulator SE queries.  Correlated radar and visual run-time databases are used for each vehicle simulator radar simulation and visual simulation.  The SE is required to be geo-specific, generally at a 1:250,000 scale with 1:50,000 scale in areas of interest and specific detail to support mission operations.  The flight training application requires the ability to support operating areas covering dozens of geo-cells.  The largest database exceeds one half million sq. km.  One immediate implication of the large area requirement is that each run-time representation must provide correlation with WGS-84, including position, range, bearing and earth curvature.  In addition, terrain paging must be supported.  To support these requirements, CTDB with GCS is used.Another key SE requirement for our training applications is database re-use. The SE approach was based on database re-use from a SE library developed over several programs that covers over 17 million sq. km.  A portion of the coverage of this library is illustrated in Figure 2-2.  The SE Library is built from geo-specific source data.  The resolution of the library is generally at a 1:250,000 scale, with 1:50,000 scale in areas of interest.  The fidelity of the SE Library supports a range of training operations, from airfield operations to ground target engagement.  The SE Library is attributed with feature types and materials that support SAF, radar and visual presentations.  Figure 2-3 illustrates a sample of the library coverage from Alaska. (Reference [2] for additional information regarding this library.)Emitters and EW DataXxxxThreat and Weapons DataThe ability of UAVs to remain outside a hostile weapon system’s tactical and lethal engagement envelops is critical to it’s survivability and mission success.  Using real-time 3D visualization to depict to aircrew a hostile weapon system’s location and its ordinance kinematics range based on altitude and UAV position, velocity, and orientation (PVO) and atmospheric conditions would expand the UAV’s operating envelop while removing the guesswork needed on the part of operators to keep the UAV safe.  Weapon ordinance kinematics envelops would dynamically update in range and altitude in the 3D visualization display based on known capabilities and target PVO allowing aircrews to pilot their UAVs with greater precision and at less risk of being shot down.  Terrain masking and clutter regions based on weapon seeker limitations can be represented to the UAV aircrew showing safe zones in which to fly during ingress, egress, and weapons engagement.  Such knowledge can increase SA beyond that present in today’s manned aircraft.XxxxOnboard Receivers and Sensor FusionxxxSensor packages incorporated into existing UAVs vary according to mission type.  Fixed and steerable EO and IR sensors are commonly used not only for forward facing video by UAV pilots for orientation, but also for intelligence gathering, target acquisition, and direct attack with a combined laser-target-designator.  Electromagnetic sensors, particularly in the radio and radar frequency bands, are being incorporated into UAV sensor packages to support electronic communications surveillance and suppression of enemy air defenses (SEAD) missions.  Much of this data is transmitted back to the UAV control center where the information is fused through human interpretation of data readouts, 2D map displays, and non-fused video streams.  Electromagnetic emissions provide a particularly difficult problem of how to present the data in readouts and displays that turn the data into useful information.Dynamic and Rapid Sensor Data UpdatesCombining pre-mission intelligence information, real-time data-link intelligence data from remote sensing platforms through Link16, TAB-37national Tactical Data Dissemination System (TDDS) and Tactical Information Broadcast System  (TIBS) intelligence broadcasts, and TIBS broadcasts with UAV down-link data would provide the best of all world to UAV aircrews.  Atmospheric condition updates from remote sensors, weather reports, and UAV onboard sensors can be incorporated into the real-time natural environment simulation and impact sensor and weapon capabilities.  Geo-locating UAV targets and hostile air defense systems on 3D terrain databases in real-time will provide the tactical laydown that 3D visualization calculations are to be based.  Incomplete and mismatched data can be quickly identified and corrected for use in the ongoing mission or for after-action and post-mission analysis.  Real-time changes in battlefield conditions and threat system sensor and weapons modes can be represented in numerous ways including multiple color and translucency of threat envelops representations within the 3D display. UAV sensor targeting can also be depicted in the 3D visualization representing field-of-view (FOV) and sensor sensitivity against targets across the electromagnetic spectrum such sensors are capable of detecting.   UAV aircrews would be able to employ their assets with greater effectiveness than previously possible with only camera video and 2D map displays and potentially better than that capable of manned aircraft.XxxObstructionsxxxxFigure 2-3 Sample Area of  SE Library Contents emissions  as it interacts with the terrain modelCTDB and GCS BackgroundUAV Simulation CTDB is the runtime representation of the tactical environment used by several SAF systems, including JointSAF, ModSAF, and ExportSAF.  The CTDB storage representation was designed with compactness as a primary driving factor so that file sizes may be kept to a minimum.  Libctdb refers to the software library that provides an API for the SAF system, allowing it to extract information from CTDBs.  Runtime efficiency of the feature lookup algorithms is a major design emphasis of libctdb.CTDB Coordinate SystemsTwo coordinate systems are supported by CTDB: topocentric coordinates (TCC) and the Global Coordinate System (GCS).  The following provides a brief overview of these systems. The UAV vehicle simulation can be simplistic or as high in fidelity as any manned aircraft simulator today.  Airframe, propulsion, fuel, electrical, avionics, sensor, and weapons simulations are available in numerous legacy aircraft simulations or can be created if existing models prove insufficient.  Data-linked UAV avionics systems health, aerodynamics, and payload status can update such models for presentation in the 3D visualization scene and on 2D instrument displays and incorporate the UAV’s interaction with the synthetic tactical and natural environments.  Modeling the UAV’s signature profiles in the RF, IR, and acoustic spectrums and dynamically updating signature envelops for display can provide mission critical information affecting the aircrew’s tactical employment decisions for the vehicle.  Three-dimensional visualization of UAV sensor payload FOVs and sensing capabilities, configured weapon envelops combined with the tactical and natural environment conditions can increase their effectiveness by the increase in aircrew SA.  Instead of requiring operators to anticipate upcoming targets as they look through a sensor’s narrow FOV in the case of electro-optics and referencing 2D map displays, operators can enjoy a broader view of the battlefield filled with 3D terrain features complete with imaged texturing and plan in real-time the route ahead of the UAV based on the visualized scene and make minor adjustments to route and attitude that presently are difficult to perform.  The goal of fusing onboard and off-platform data and visualizing the UAV within a 3D environment is to immerse operators into an augmented reality that will rival the experience of manned flight.  A number of high fidelity aircraft simulators, such as the AC-130U presently in use by the US Air Force, immerse aircrews in an environment so realistic that they tend to forget they are not actually flying.  The aircrew’s reactions, employment decisions, and mission tempo reflect that experienced in real-world flight missions.  UAV crews can experience this same level of immersion.  In addition to direct control employment in real-time, use of simulation technology allows UAV crews to fly “what-if” scenarios prior to actual mission employment or during enroute cruise phases of the mission to rehearse or determine the best profile to fly.  xxxAugmented Reality Topocentric Coordinate SysteImmersion (GH)The goal of using virtual reality is to allow the user to attain a sense of presence in the simulated environment.  Augmenting this SE with information that allows the user to better understand and utilize the SE representation allows the system to become a tool.  To provide an immersive experience in a representation of the natural world requires that the physics of the world be modeled with a high degree of realism.  It is obvious that in a training scenario, realism is of utmost importance so that the student can carry the skills learned into practice.  In augmented reality, realism is also important, but new features are added to the environment that allow the user to interpret events and situations with more clarity and to interact with the entities in the environment as needed.  xxxxmA TCC system is a Cartesian system whose origin is on the earth's surface.  The system can either be derived from UTM data (as SIMNET terrain databases have been; often called the flat-earth approach) or from GCC data (often called the curved-earth drop-off approach). UTM-derived TCC systems use a Transverse Mercator projection of the elevation data onto the XY plane of the Cartesian frame.  TCC systems have the run-time computational advantages that the X,Y,Z axes represent East, North, Up and the XY-plane represents an elevation of zero with  respect to the reference ellipsoid.  Many standard ellipsoids  may be used.  These characteristics facilitate simulation programming because the  numerical values of locations and vectors described in TCC systems  have an easily understood and intuitive physical significance.  However CTDB-TCC has several key disadvantages.  The gaming area size for CTDB-TCC is generally limited to a few geo-cells (~40,000 sq. km).  One limiting factor is that standard libctdb, such as used by ModSAF and JointSAF, has a simplistic database paging scheme so the gaming area must be relatively small to maintain efficient run-time performance.  Another limiting factor is that projection distortion effects such as earth curvature and latitudinal convergence increase with gaming area size.  For example, ships may be able to detect entities  that are over the horizon in the CTDB-TCC world, whereas they  would not be able to do so in the real world.  Because of these disadvantages, we determined that CTDB-TCC was not suitable for large area flight databases.With the radar threat environment captured in the SE, intelligent agents can traverse the SE, obeying the behaviors of actual entities, and determine a measure of detectability over the path.  As the path progresses through space and time, the probability of detection is integrated along the path.  Due to the complex environment, this heuristic trial and error method of path planning may be a profitable exercise.  It would be difficult to determine the safety of a path through analysis alone.  The simulation of the path allows analysis to be verified and tested.  Multiple paths may be tried in the safety of an SE, and evaluated to obtain an overall score and a section-by-section detectability score.  Analytic solutions such as augmenting the SE with force field physics to provide resistance against traversing too closely to a dangerous location can be used to help determine the path of least detectability.  A given path can be tested in the SE to determine its rating.  xxxx and radar physical effectsOcculting or terrain masking of radio, radar, and laser emissions can be calculated in the simulation environment by using geo-specific terrain databases created from digital terrain-elevation data (DTED), photo imagery or other sources.  Occulting algorithms and processes are commonly used today in both virtual and constructive simulations.  Line-of-sight (LOS) calculation are used by simulations to mask air defense threats and opposing forces from other entities including virtual simulator ownships.  These same occulting processes can be used in support of UAV operations by providing crews the ability to anticipate unmasking and masking of target and hostile air defenses prior to the event actually occurring.  Attenuation of EM emissions based on natural environment factors such as water vapor, carbon dioxide other absorptive elements is possible and relatively straight forward in real-time.  Presentation of attenuated emissions in 3D visualized displays adds another layer of increased SA for UAV crews and allows “what-if” mission profile testing by simply filtering such calculations on and off as needed.  Real-time visualization of LOS occulting between UAV and vehicles and point locations in an area of operations (AOA) combined with the impact of atmospheric conditions on EM emissions can further enhance UAV crew SA and improve mission effectiveness.Radar physical effects(GH)xxxxxEvolutionary algorithms such as genetic algorithms can be used to grow an optimized path.  Various requirements can be graded for each path, such as time to target, threat avoidance, and energy consumption.  A human can chose path sections to combine into a final choice, or artificial intelligence can combine paths and test them until a suitable one is determined.  Because of the complex nature of the paths, it would be difficult to employ a gradient search mechanism, unless multiple sections were considered in isolation.  As the vehicle moves through space, different threats become enhanced or decline.  The search through pathspace employed by an evolutionary algorithm can determine more optimum paths.As new information is obtained in real-time and inserted into the SE, the forward path must be re-evaluated for threat level.  The paths traverse geographical space, which has an intrinsic complexity level.  High level rules can apply, consisting of a default hierarchy of rules.  Immediately applicable rules with higher priority can override the planned path.  Paths may then be dynamically altered without extensive replanning of forward paths.  Immediate threat avoidance routines can be executed with respect to the perceived threats.  In the meantime, new forward paths can be calculated, using the new information.Electronic databases and auto route planningThe use of electronic warfare reprogrammable databases such as EWIR allows for rapid updates to the parametric values within the tactical and emission simulations that will ensure the UAV visualization environment remains current with the latest intelligence.  Use of automated and semi-automated loading tools is critical to maintain the currency of sensor and weapons capabilities within the synthetic databases due to the enormous number of data elements available through intelligence and data collection programs today.  Auto route planning for UAV missions based upon current EW data, terrain profiles, environmental conditions, and UAV signatures is can augment traditional route planning through the use of 3D visualization of flight profiles.  The same tools, environment, and simulations used to employ actual UAVs can be used in the mission planning, mission rehearsal, “what-if” scenario runs, and post-mission analysis.xxxCTDB-GCS for Large Areas“What-if” Scenario Testing Fly path in simulated worldxxxxFaster than real-timexxxxGather ‘hit points’xxxx  Assess current and planned flight pathxxxxFind better pathsxxxxThe global coordinate system was designed in an attempt to eliminate the disadvantages of projected systems while retaining the advantages. For information on CTDB-GCS, refer to [3]. A brief background is provided here, including extracts from the referenced paper.  According to the GCS definition, the earth is divided into geo-cells, as shown in Figure 3-1. Predominantly, each geo-cell covers one degree latitude by one degree longitude, although there are some exceptions near the poles, where lines of longitude converge.      Figure 3-1: A GCS cell on the earth's surface. The GCS cell origin is the center of the cell. (Latitude curvature is exaggerated.)Within each GCS cell, a local Cartesian coordinate system is defined.  Each system's XY plane is tangent to the WGS84 ellipsoid at the center of its cell.  Passing the GCC frame through a linear transformation derives each cell's frame.  Every point on the earth can be identified by four values, three Cartesian coordinates (x,y,z) and a GCS cell id. See Figure 3-2.Figure 3-2: The GCS coordinate system for one cell. The Z coordinate comes up out of the page. Latitude curvature is exaggerated.One key advantage of the CTDB-GCS approach is that it supports large gaming areas – potentially worldwide.  As mentioned earlier, CTDB-GCS has been used for large areas for use in constructive simulation for STOW.  Because each Cartesian frame covers a limited geographic extent,  GCS can be used with gaming areas of arbitrary size, including  the entire earth.  With CTDB-GCS, each geo-cell can be paged as required for run-time access.  There is no distortion  associated with GCS frames, since no projection is taking place.  Thus, a straight line in GCS space  maps to a straight line in the real world.  Ships can no longer  "cheat" by being able to detect entities that are over the  horizon.  .  Another advantage of CTDB-GCS is that  converting between GCS and GCC is computationally cheap,  requiring only a few multiplications and additions.   Each cell's frame is a linear transformation of the GCC  frame.  There are a few issues with CTDB-GCS that must be considered.  Because lines of longitude are not parallel, the Y-axis of a GCS  frame does not exactly point north, but it comes very close.  The deviation is limited within a geo-cell, and has a negligible  effect for most simulation applications.  Similarly, because the XY-plane of the GCS Cartesian frame is  not projected onto the ellipsoid, the Z-axis is not  exactly the gravitational vector.  Again, this deviation is limited within a geo-cell, and has a negligible effect for most  applications.  More exacting models can be implemented on top of libctdb to compute the gravity  vector given a GCS point for those few instances when it is  required.  Considering the benefits of CTDB-GCS, these GCS issues are relatively minor, provided the developer keeps them in mind.xxx  Representation of the Geoid in CTDBAugmented Reality InfrastructureFor flight simulator applications, elevations are typically expressed as elevation above sea level (Zmsl).  Sea level does not occur at a constant vertical GCS coordinate value, nor does it exactly follow the ellipsoid. The actual sea level of the earth is modeled by the geoid, as shown in Figure 3-3. The WGS-84 geoid is an empirically determined surface that can differ from the WGS84 ellipsoid by as much as 100 meters.   (See http://www.nima.mil/GandG/wgs-84/egm96.html)Since the actual shape of the earth (the geoid) is irregular, it can't be adequately described by a closed-form mathematical representation.  Rather, it makes sense to represent it via data tables.  The CTDB-GCS geoid representation is based on 15-minute geoidal separation data from NIMA. Known geoid points typically differ from each other in offset from the ellipsoid by less than two meters; for a 5000-meter view range, the geoid changes by less than half a meter.Figure 3-3.   The ellipsoid, geoid, and one GCS cell coordinate plane.CTDB-GCS provides the capability to include the geoid grid at evenly spaced points within the cell coverage area by interpolation using the nearest known 15-minute data points.   The geoid correction was significant for our application since the elevation above mean sea level is required.xxxTCP/IP link of distributed intelligence dataCTDB Feature Representation CTDB includes both physical and abstract representations of features.    These CTDB representations are summarized below, with more detailed information available elsewhere, for example reference [4].xxxCTDB Physical Features Field Command and ControlUse of PC-based and Personal Digital Assistants (PDA) computing systems Physical xxxfeatures in CTDB are those that have a physical significance, particularly the physical features effect queries for  intervisibility, elevation, or soil type.  Currently, the following categories of physical features are supported by CTDB:  Terrain Skin Polygonsmicroterraincraters , ditches, bermsfighting positionsbridgesrubblesnow  Volumes  (Buildings and other structures that block intervisibility)buildingsrock drops  Linear Features  (Features described by a series of line segments plus additional feature-specific information)tree linestreesconcertina wireminefield fencessingle fenceswire road blockdragons teeth  Canopiestree canopies, camouflage canopies  Laid Linear FeaturesRoads, riversInteraction with augmented reality infrastructureAbstract Features  to control UAVs and consume the sensor fused data from their sensor packages is cost effective in the present computing environment.  Commonly available information technology tools, including wireless networking systems, that can be purchased in any consumer electronics store combined with 3D visualization technology make the possibility of controlling UAVs by the average soldier on the battlefield a soon-to-be reality.  Abstract features are those that have no physical significance, but may be useful to the simulation.  One purpose would be for reasoning or human readable displays of non-physical information, such as political boundaries and town names.  Another purpose of abstract features is for storing abstractions of physical features.  The abstractions require less run-time computation to process than the corresponding physical features. For example, the tactical map of the tree canopies would be time consuming if drawn from the large number of fragments stored as physical features.  Instead, the map is drawn from the simpler 2D footprint stored as an abstract feature.   Another example of a feature that has both a physical and abstract representation is soil defrag areas.  The actual soil types of the terrain skin polygons are stored in the physical features (for microterrain polygons), the grid, and/or the TIN section.  The perimeter for the aggregation of these polygons are stored as an abstract feature for quick, convenient retrieval and map drawing.  The types of abstract features supported by CTDB format are:  Point Abstract Features:  LABEL, TACTICAL_SIGNLinear Abstract Features: PIPELINE, POWERLINE, OFFROAD_SEGMENT, RAILROAD, DRAGONS_TEETH, POLITICAL_BOUNDARY, MICLICNetworking distributed intelligence dataThe same technology commonly used in computer networking and virtual simulations today applies directly to the augmented reality infrastructure described above.  TCP/IP protocols are to be used to link the various internal components of the visualization system and external interfaces to the UAV system itself.  Data and protocol converters to transform Link16, TAB-37TDDS, and TIBS broadcast data data to common networking TCP/IP and simulation interoperability protocolsformats  is critical.  Semi-automated Force (SAF) platforms, such as Distributed Information Warfare Constructive Environment - 180 (DICE-180), cwith converted TAB-37 tactical data translated to DIS or HLA entity representations  andto weapon system laydowns within the synthetic tactical environment.  Link16TDDS and TIBS broadcast data would likewise need a translator tools to convert to the DIS and HLA protocols for consumption by the UAV 3D visualization system.  These translators will be required to work in real-time to support the operational employment of the UAV within the tactical battlefield.  Data elements provided to the UAV’s synthetic environment includes entity representations of ground vehicles, waterborne craft, aircraft, sensors, and weapons.  Entities will represent friendly forces, targets, and hostile IADS weapon platforms.  Dynamic mode and state changes of entities is supported in the DIS and HLA architectures and messaging.  The UAV visualization system can also provide feedback into the TAB-37TDDS and TIBS networks for targets detected during the UAV mission to be consumed by other military assets through the use of the DICE-360 SAF and Multi-mission Advanced Tactical Terminal / Tactical Radio System (MATT/TRS) terminals.New human/machine interfaces are always being developed.  Judicious use of the available interface hardware enables a more effective augmented reality session.AssitantsAssistantsgbeengreen (GH)xxxxHapticHaptic control refers to the kinematical or touch capability that we posses.  Haptic control, then immersed into an SE, provides instant ability to interact with controls in the SE.  This makes the UAV control experience more intuitive.  Less dependence upon computer formatted data, such as numbers and GUIS is required.GlovesVirtual reality gloves provide one effective method to interact with the SE. Motion commands and other control can be encoded into the use of the gloves, and objects may be placed into the SE for direct interaction with the glove wearer.UAV flight controlsThe UAV may be directed using an immersive virtual reality control system.  Both current UAV camera information, and the SE overview can be simultaneously presented to the UAV controller. SpaceballA spaceball interface allows six degree of freedom control for of entities in the SE.  Acceleration, velocity, translation and orientation are all provided by a spaceball.  The spaceball consists of a spherical ball that is monitored electronically to determine motions imposed upon it by the user.  These motions are encoded and digital translated by the SE host to allow various interactions to occur.RobotSoftwareSimulation software providing robust synthetic natural and tactical environments complete with the capability to interface with UAV aero and control systems, real-time processes, emissions modeling, mission functions including LOS calculations, DIS and HLA interoperability functionality, powerful subsystem interface modules, and interfaces with OTW, EO, and IR image generators is required to achieve the rich 3D visualization necessary to provide enhanced SA to UAV crews.  Such software must be adaptable, stable and expandable to support the evolving mission of UAVs now and into the future.Agentnpaths in faster-than-real-time, to determine the suitability of prospective path choices. SoftwareSimulation software providing robust synthetic natural and tactical environments complete with the capability to interface with UAV aero and control systems, real-time processes, emissions modeling, mission functions including LOS calculations, DIS and HLA interoperability functionality, powerful subsystem interface modules, and interfaces with OTW, EO, and IR image generators is required to achieve the rich 3D visualization necessary to provide enhanced SA to UAV crews.  Such software must be adaptable, stable and expandable to support the evolving mission of UAVs now and into the future.Hardware (GH)The distributed nature of the UAV system relies upon dependable communication links and powerful computers to process the virtual environment.  Full military specification machines are expensive and hard to find.  Redundant commercial machines that have been tested for the conditions should be acceptable.  They will need to be hardened to perform in the field.  A higher level of machine reliability and robustness needs to be employed to ensure long-term operation.  This extends from the SE processor, to the display and human interface devices, to the cables and connectors used in the system. xxxxUAV (GH)xxxxx A primary concern with the UAV is the radio system, and the transmission of control information.  Telemetered commands to the UAV need to follow the correct protocol.  As each UAV is different, a UavAV Battlefield Interface should be designed to supply a common interface to a UAV from the augmented reality controller, so that different UAVs can be controlled without having to perform extensive rewrites of the SE code.UCAVAreal Abstract Features: BOTTOM_MATERIAL, CANOPY, HYDROGEOLOGICAL, HYDRO_SURFACE, HYDRO_SUBSURFACE, SOIL_DEFRAG, STEEP_SLOPE, SNOW, URBAN_AREA, BRIDGE, BERM, CAMOUFLAGE, CRATER, DITCH, FIGHTING_POS, RUBBLE, CONTAMINANT  xxxPhysical Features vs. Abstract Features  Physical features and abstract features are each stored in their own sections of a CTDB file.  Physical features are stored on a patch-by-patch basis (typically 500 m X 500 m).  Abstract features can be arbitrarily large and are stored in a quad tree for the geo-cell. When considering how to represent a new feature in CTDB, one must decide if it should be represented as a physical feature, an abstract feature, or both.  In general, a feature should be encoded as a physical feature if it has an effect on intervisibility, or if it represents a change to soil types (like a road or a river).  Features that do not affect intervisibility or represent a soil-type change, but need to have their presence acknowledged in the database are put into the abstract features.  Both the physical and abstract features may be used for some functions.  For example, JointSAF has three major movement planners: movemap, routemap, and the road-route planner.  Routemap deals with gross-level planning and ignores small obstacles such as buildings, individual trees, and craters.  Routemap obtains all of its obstacle information from the abstract features and the linear topology.  The road-route planner is used to plan a route that stays on roads, so it obtains all of its information from the linear feature topology.  Movemap provides near-term vehicle-level navigation so it must take all obstacles into consideration.  Movemap obtains information from the physical features (for trees, tree lines and buildings), the abstract features (for tree canopies, steep slopes, and soil defrag areas), and the linear topology (for road and river networks).  Since the CTDB we generate can be used to support multiple  movement algorithms, we provide both the abstract and physical representations of the SE.System Database Generation Approach Lockheed Martin has developed a system-level approach for the SE.  This approach is  illustrated in Figure 4-1.  We build a Common SE that contains the complete SE definition as needed by all the simulation functions for our application. The Common SE is then compiled to run-time formats and distributed to the simulation. Common SE RepresentationThe Common SE provides both geometry (position, shape, size, height, width, etc.). and attribution to support the various simulation functions.  The principle elements of the Common SE are the terrain skin, imagery blanket, surface features, and surface models.  The Common SE represents the terrain surface as an Integrated Triangulated Irregular Network (ITIN).  The ITIN precisely captures topographic vector features with known elevations, (such as airfield runways or ocean shorelines) as well representing the elevation grid – typically from NIMA DTED™ Level 1 or map contours. The Common SE includes a geo-referenced imagery blanket that can be at different resolutions in different areas.  The imagery blanket is used to supplement or correct surface feature locations.   The imagery blanket it also provided for visualization purposes by texturing the visual polygons with the imagery information.The Common SE surface features include areal, lineal and points, such as hydrography, vegetation, transportation, and built-up areas.  The surface feature attribution includes feature type (classification), surface material, trafficability type, and visual appearance (color, textures, light parameters).  The Common SE surface models are all contained in a model library.  Library entries include both simple models (tree, building) as well as complex models (airport with runways and multiple buildings).The Common SE provides both abstract and concrete, or physical,  representations.  For example, the road representation includes the abstract centerline, the area of the road bed, and the many road polygons that are incorporated into the terrain surface.  Similarly, the models in the Common SE model library for a building include the wall and roof polygons, as well as the roofline “footprint” and roofline height. The Common SE attribute table provides a flexible mechanism for feature attribution of the landmass and models using external attribute tables (that is, attribution stored externally from the landmass and model data).  The SE attribution table can support different environmental data models.  (See [5] for a discussion of  environmental data models.)  The SE Library utilized the NIMA Feature Identification Code (FIC) and material (MAT) for the feature classification/material combination.  Current databases are processed with the Environmental Data Coding Standard (EDCS) Classification Codes and material codes. The SE attribute table accommodates both the NIMA attribution type and the EDCS attribution, allowing the heritage database to be easily upgraded with the  EDCS.  This capability allows use of heritage and new databases together.  SE Database Generation Process An analysis of requirements was performed and a requirements traceability matrix established. The capabilities and limits for each function were identified.  The requirements traceability and function definition were used to identify the database design criteria to meet each requirement.  The database design includes feature attribution definitions required to support the various simulation functions to insure the Common SE is efficiently processed for each function.The Common SE is produced by an enhanced Synthetic Environment Database Generation System (SE/DBGS), called TARGET3™.   TARGET3 leverages COTS tools into a simulation framework (or “toolbox”) that addresses simulation application requirements. [6]  The TARGET3 toolbox provides the  framework to integrate the COTS tools and interface with SEDRIS, OpenFlight™, various NIMA data products, and run-time formats.  For this application, the COTS tools include: Kodak ENVI™ for image processing;  MultiGen Creator™ for 3d modeling; and Adobe Photoshop™ for object texturing.  A key factor of our process is leveraging database re-use from the SE Library.  The entire geographic area is based on re-use from the SE Library that covers over 17 million sq. km.  Our process could regenerate these areas from source data, but we have seen several advantages to re-using databases that have been field-proven in other trainers.  The life cycle production cost for database conversion is significantly lower, particularly for the areas that were enhanced with multiple sources and user feedback.    In addition, we are able to provide correlation with previously fielded trainers by sharing a common SE definition. The SE attribute table was updated to include the appropriate EDCS coding, and the SE model library was updated to provide the necessary CTDB representations.  Some enhancements were also made to the SE Library landmass area for the current application areas of interest; using various source data such as overhead imagery.  The Common SE is then used to compile the CTDB, radar and visual run-time formats.  The Common SE can be incrementally built, and small areas can be enhanced without re-compiling unaffected areas.  While the immediate discussion is related to virtual flight trainer applications, we have also successfully applied this Common SE approach to tactical ground trainers, as discussed in [7].  For the tactical ground trainer application, the SEDRIS was used to compile correlated databases for tactical environment application based on the Close Combat Tactical Trainer (CCTT) SAF.   This same SEDRIS has also been used to compile CTDB.SE Export and CompilationFigure 5-1 shows the export data flow to produce the CTDB, radar and visual run-time formats from the Common SE.  The radar and visual compilers operate directly on the Common SE.  For the CTDB, the Common SE is first exported into SEDRIS, and the SEDRIS is then compiled to produce the CTDB.  The SEDRIS is also available for interoperability with other simulations. Each of these export steps is discussed in the following paragraphs.  SEDRIS Transmittal RequirementsThe CTDB representation of the SE is developed through the use of  SEDRIS.  SEDRIS provides a flexible format that allows data interchange between producers and consumers.  SEDRIS provides a variety of ways to represent and organize data.  This flexibility can also present problems since the producer may represent information in SEDRIS in a way the consumer software is not designed to handle efficiently (or at all).  The producer may also produce SEDRIS that is “valid”, but does not contain the feature content needed by a consumer.  For example, a SEDRIS transmittal could be produced with road polygons, but no road centerline.  A compiler for a SAF that requires a road centerline for route planning would be forced to derive the centerline.We used a SEDRIS Transmittal Content Requirements Specification (TCRS) to document the structure and content details for the SEDRIS transmittal.  The TCRS provides the detail that allows consumers to effectively consume the transmittal. The TCRS aided the front end design process by: 1) ensure ensuring the SE contains all the information needed in the CTDB and 2) provide providing a requirement specification for the SEDRIS export software and CTDB compiler designs.  Reference [67] for additional information regarding our use of a TCRS in the context of a tactical ground trainer program.  A brief description of the TCRS for this flight applications follows.The SEDRIS transmittal is in geodetic coordinates relative to the WGS-84 with the geoid correction (lat, long, Zmsl).   The SEDRIS transmittal structure is partitioned into one or more geocells.  Within a geocell, the terrain is represented as a spatially indexed hierarchy of geometry.  Areals, lineals and points are represented as spatially indexed feature topology.  A union of features is created to contain each feature type under the spatially indexed feature topology.  Areal and lineal data is placed into the union of feature object in layer order where the first feature is closest to the terrain skin and the last feature in the list is fully visible, or “on top”.  We use the model library for all models, stored in local space rectangular coordinates.  These models are then placed and oriented through the use of feature model instances stored in the union of features.  This allows the consumer to parse the library once to retrieve all the models that are used in the transmittal.  The model is placed into the SEDRIS Model Library as a Feature_Model. The polygons, which represent either model footprints or parts of the visual representation, are created as areal features.  The span and ramp polygons that represent a bridge are placed in the Feature_Model object as the SAF representation of the model.SEDRIS  Export The SEDRIS Export software operates from the Common SE representation, which is the editable source created and maintained by the DBGS tools.  The Common SE representation includes landmass source, an OpenFlight model library and an SE attribute table. Landmass source contains both the 2D culture features as points, lines and areals and the 3D terrain polygons. The Common SE landmass includes multiple levels of detail, however only the finest level of detail for the database is exported for the CTDB.  In addition, only selected representations of the Common SE are exported to SEDRIS. For lineal features (roads, streams), only the centerline with a width is exported.  For models, much of the model polygon detail is required for the visual representation only, so it is not exported.  Most models are provided with a simple footprint, such as a building roofline.  Other models may contain more information, such as bridge span and ramp polygons.  These are exported with appropriate attribution so the CTDB compiler has an explicit definition of spans or ramps.The SEDRIS export application processes the “feature side” of the SEDRIS transmittal first. The source data is read and the areal, lineal and points are processed in that order.  Elevation data (Zmsl) is provided for each feature location vertex based on the elevation of the Common SE terrain skin polygons.  The SEDRIS model library is constructed as the landmass data is parsed so that only those models used in the transmittal are included.The “geometry side” of the SEDRIS is processed next.  The terrain skin polygons are processed into the geometry side of the SEDRIS transmittal.  Feature areal data is integrated into the terrain skin as required for the CTDB where the terrain skin polygons encapsulate areal classification and attribution as well as elevation data.  The feature areals that represent forests are stored in the SEDRIS geometry as both forest floors, at the surface of the terrain skin,  and as raised polygons to represent the forest canopy. The height is included in the elevation component of the location object. A SEDRIS  attribute set table is used to provide a common attribution set for all feature and geometry data.  The SEDRIS attribute set table is created from the SE attribute table.  The SE attribute table also contains the unit and type for each SEDRIS Attribution Code and allows customization of SEDRIS data on a per database basis.  SEDRIS features and geometries are assigned an attribute set index which points into the attribute set table.  Some attribution that is generally unique to a feature, such as the width of a lineal (road or river), is provided with an inline property value.We incorporate several of tools provided by the SEDRIS team to aid our development and quality check the SEDRIS data.  These tools include depth, syntax checker, rules checker, FOCUS, and SEE-IT.  These tools provide a significant benefit.CTDB Compilation When we began the project in 2001, we assessed the SEDRIS_2_CTDB compiler available through the SEDRIS team, however it did not support multi-cell GCS with geoid correction. [4]  Therefore, we elected to enhance the  TARGET3 CTDB compiler to consume SEDRIS.   The TARGET3 CTDB compiler has options to produce CTDB3 and CTDB7; supports big and little endian; and supports GCS with geoid correction.The TARGET3 SEDRIS-to-CTDB compiler consists of two fundamental components: the front end and the back end.  The front end uses the SEDRIS API to traverse the source transmittal, extract the feature data, perform relevance filtering, map it to suitable CTDB features, and write the resulting data to intermediate files.  The front end permits the gaming area to be divided into multiple SEDRIS transmittals, and small portions of the gaming area can be updated via additional transmittals.  Once the intermediate files have been completed, the feature data in the intermediate files are passed to the back end that outputs the feature data into CTDB format.  The backend formatter works on a geo-cell basis, so the gaming area can be incrementally built up during database production.  Selected geo-cells can also be re-formatted from the intermediate files if there is a database update.In the interest of efficiency, the ultimate goal of compiling the SEDRIS data into CTDB format is to extract only the data that is necessary for use by the SAF.  There needs to be an easy way to configure the mapping between the SEDRIS features and their corresponding CTDB instantiations.  This is achieved by putting the mapping into text files that are read by the compiler.  The mappings can thus be changed by the user without having to rebuild the compiler.  This is desirable since there are many ways to describe features and attributes in SEDRIS transmittals. This customization capability enables the compiler to consume many types of SEDRIS transmittals with varying content specifications. Each type of transmittal can have its corresponding mapping files.  There are two mapping files:features.rdr maps SEDRIS features to their corresponding CTDB  instantiationsattributes.rdr maps SEDRIS property values to their  corresponding CTDB feature attributes.Feature Mapping File  Features found in the source SEDRIS transmittal are mapped based on their SEDRIS classification codes (according to the Environmental Data Coding Standard) and their SEDRIS data types.  The following data types are supported:  SE_POINT_FEATURE_TOKEN  (POINT) SE_LINEAR_FEATURE_TOKEN  (LINEAR) SE_AREAL_FEATURE_TOKEN  (AREAL) SE_UNION_OF_PRIMITIVE_GEOMETRY_TOKEN   (POLYGON)  These primitive SEDRIS types can be mapped to any of the CTDB physical or abstract feature classes.  Each entry in the feature-mapping file has the following format:  (<facc> <physical feature class> <abstract feature class> <soil type>)  <facc> consists of the SEDRIS classification code immediately followed by AREAL, LINEAR, POINT, or POLYGON.<physical feature class> specifies one of the following:NONEBUILDINGLINEARTREETREE_CANOPYTREE_CANOPY_EDGEMICROTERRAINML_MICROTERRAINDEFAULT_MICROTERRAIN  Any given feature in the SEDRIS transmittal file may have a physical instantiation, an abstract instantiation, or both.  <abstract feature class> specifies any of the abstract feature classes listed in Section 3.2.2.  <soil type> specifies one of the standard fifteen SIMNET soil types (i.e. SOIL_DEEP_WATER and SOIL_ROAD).  The following example is an excerpt of a feature mapping file.  (The line numbering is provided for reference in the description that follows.)Line 1;; House(AI070AREAL BUILDING NO_ABSTRACT SOIL_DEFAULT)  Line 2;; School Building(AL351AREAL BUILDING NO_ABSTRACT SOIL_DEFAULT)  Line 3;; Bridge Span (areal representation PVD display)(AQ045AREAL NONE CTDB_ABSTRACT_SOIL_DEFRAG SOIL_ROCKY)  Line 4;; Bridge Span (polygonal representation of bridge deck)(AQ045POLYGON ML_MICROTERRAIN NO_ABSTRACT SOIL_ROAD)  Line 5;; Runway(GB055LINEAR LINEAR NO_ABSTRACT SOIL_ROAD)Line 1 specifies that any feature in the SEDRIS transmittal of type SE_AREAL_FEATURE_TOKEN with a classification code of AI070 will be instantiated as a building feature in CTDB.  Since CTDB doesn't distinguish between houses and school buildings, both are mapped to the same feature type, as shown in Line 2.  Lines 3 and 4 illustrate the case where a single feature has multiple representations in both SEDRIS and CTDB.  For instance, a bridge feature in the SEDRIS transmittal has both a geometry component and a feature component.  The geometry component describes the bridge deck as a series of polygons that are instantiated as multi-level terrain in CTDB.  The feature component describes the outline of the bridge deck used by the SAF for map drawing.  Line 5 specifies that any features of type SE_LINEAR_FEATURE_TOKEN with a classification code of GB055 (airport runway) are mapped to linear features in CTDB.  Attribute Mapping File  Feature attributes specified as property values in the SEDRIS transmittal may also be mapped to feature attributes in CTDB with the format (<ctdb attribute> <SEDRIS tag>).  An example excerpt is given below: (height  HGT_)   (width  WID_ LEN_)  The “height” line specifies that any property values in the SEDRIS transmittal with a tag of HGT_ will describe the height attribute of its corresponding CTDB feature.  The “width” line illustrates the case where multiple SEDRIS property value tags may be mapped to a single CTDB attribute.  For example, some transmittals may use the WID_ tag to describe some widths and LEN_ to describe others.  Thus, both tags are mapped to the CTDB "width" attribute.  Visual and Radar Compilation The SE/View PC-based IG software requires a run-time format to provide the visual representations of the Common SE.  The visual run-time uses geocentric coordinates.  These The visual representations include the out-the-window (OTW), sensor infra-red (IR) and NVG simulations.  Each polygon stored in the Common SE database has four fundamental attributes: feature classification, material, color, and texture. These attributes in conjunction with data tables are used during the formatting process to generate the run-time data for SE/View. For each surface (or light), separate representations are provided for the unique characteristics of the OTW, NVG and infra-red images.  For OTW and NVG, each surface is assigned a color and texture to be used for the respective image type.  For the IR simulation, the color and texture is based on the thermal characteristics of the surface that changes during run-time.  So the formatted database does not provide an IR “color”, but rather encodes the feature classification and material for each surface.  This allows the  run-time thermal model to accurately represent the thermal characteristics of the surface color and texture due to the varying conditions in the simulation environment, as well as the characteristics of the feature classification and material.   The SE/DRLMS radar simulation software requires a separate run-time format to provide the radar representations of the Common SE.  The radar run-time data is provided in geodetic coordinates (lat, long Zmsl) using a 3D gridded structure.  Within the Common SE each feature that is radar significant contains attribution needed for the compiler to determine the radar cross section. Some features in the Common SE may only be represented for the radar, such as a radar reflector marking a runway.  The radar feature representation is comprised of the radar cross-section, or reflectivity for the feature. These reflectivity tables have been established for various radar systems, and allow the Common SE to be re-formatted for multiple radar types.  Radar reflectivity is based upon the same feature classification and material code pair as is used by the IR simulation. Production Experience The current database covers over 500,000 sq. km (50 geo-cells), including the geocell border that surrounds the movement area in order to support the radar sensor and occulting range. High resolution enhanced areas such as airfields and target ranges are included in the database. The database contains geo-specific imagery with resolutions from 1.25 meters to 10 meters per pixel.  We used an incremental production approach for the database currently in production.  Once the design criteria was established, a representative area of the database was identified to be used for verification of the design.  The sample area process permitted the database procedures and design to be verified prior to full scale production.  The process included extracting the Common SE from the SE Library, updating the attribution as required, and verifying the operation on each simulation (OTW, IR, radar and CTDB).  The OTW simulation was formatted first to support a visual review during a program review.  Once the OTW simulation was verified, the other simulations were formatted and verified as well.  Each simulation was verified independently using test points from the Common SE.  Landmark quality control (QC) points were recorded from the Common SE (lat, long Zmsl), and those QC points were checked in each simulation.   We then verified correlation between simulations. Correlation verification for the OTW, IR and NVG simulations includes side-by-side evaluation of database features and entities on the visual system. This side-by-side comparison verified the positional accuracy of features within each environment. For instance the position of a vehicle entity  placed in the environment was verified to be properly located in each simulation. Verification of color and texture for these simulations was also performed during the side-by-side evaluation. Since each database is formatted from the Common SE, the correlation between simulations is maintained. Database correlation with the CTDB-GCS simulation involved a series of test points with known locations (latitude, longitude and elevation) and features. The vehicle simulation software positioned an entity at a given test point location in the environment, emulating a remote entity. The vehicle simulation software uses the CTDB-GCS data to determine the elevation information for the test point. The OTW visual system was used to verify the proper position and elevation of the entity. An additional part of this correlation test was to view the entity position on a JointSAF plan view display. This process was repeated at test points throughout the gaming area.  The end result being verification that the CTDB-GCS database correlated with the visual simulations.The sample area verification effort produces  a documented set of procedures for production of the remaining geo-cells of the database. This database production process is currently near completion, with the first database area scheduled for release in the summer of 2002.   Additional databases are scheduled for production about every 6 months.The sample area is also provided to support trainer system integration and test.  This includes an evaluation of all simulations in the correlated run-time environment.  The verification utilized preset flight path information and entities located within the environment.  The flight path is enabled and each simulation verified for correlation throughout the flight path test.  This testing is planned for summer of 2002.   Summary and Conclusions Reliable SE systems must be utilized in the real-time application of augmented reality to UAV threat avoidance.  There is no time for machine rebooting.  All processing must occur in an online fashion.  All systems must be tested and proven to be applicable for a time duration and a data capacity compatible with the anticipated missions.  There are large quantities of data to be anticipated, and intensive ongoing computations to be performed.We have discussed our experience in generating SE databases for virtual flight applications that incorporate CTDB-GCS for large areas.   We described our process for generating a Common SE that leverages SE Library re-use, as well as COTS tools to enhance the database or generate new areas.  The Common SE provides a complete definition of the environment that supports all the simulation functions, and is used as the common source to compiling all run-time formats and to produce SEDRIS.   We also described our use of a SEDRIS Transmittal Content Requirement Specification (TCRS) to explicitly define the SE content and structure.This process is being applied successfully to support our flight applications on multiple programs.  The initial database currently in construction has been verified with a sample area and the full area is undergoing verification this summer.  We believe that this is the largest CTDB ever generated, and supports fully correlated run-time visual and radar simulations.   Three additionalAdditional flight application databases are scheduled for production over the next twelve months, allowing continuing refinement of the process for flight applications.  System Concept GHLeverage to warfighter by incorporating real-time simulation and battlespace visualization of the electromagnetic environment  BVLeveraging virtual simulation technologies for use in operational UAV missions provides the warfighter an added dimension in visualizing the battlefield.  The complexity and lethality of today’s tactical and electromagnetic environments that UAVs traverse demands increased SA to effectively carry out mission objectives.  Immersion of UAV crews within an augmented reality, including visualization of electromagnetic and weapons envelopes, allows them “to see that which is not seeable,” increasing their SA and providing the UAV system the potential of matching and possibly surpassing the effectiveness of manned aircraft.This process can also  be used to support other applications that require efficient generation of large area CTDB databases.xxx   ReferencesDr. J. Hogan, Dr. E. Pollak, M. Falash: ``Enhanced Situational Awareness for UAV Operations Over Hostile Terrain”. American Institute ofd Aeronaurics and Astronautics, May 2002.B. Vonsik, G. Mikkelson: ``Common Battlespace Architecture''. Interservice/Industry Training Simulation and Education Conference, November 2001.[1] D. Miller, T. Miller, C. Cornish, J. Rourke, K. Cauble: “STOW Southwest Asia Terrain Data BaseElectronic Warfare and radar Systems Engineering Handbook,”.  Naval Air Systems Command and Naval Air Warfare Center, 4th EditionSimulation Interoperability Workshop, March 19981993.John H. Holland. Adaptation in Natural and Artificial systems: An Introductory Analysis with Applications to Biology, Control, and Artificial Intelligence.  MIT Press, 1994.Journal of Electronic Defense, EW 101: EW Simulation, Part 10 – Threat Antenna-Pattern Emulation, January 2000.Journal of Electronic Defense, “What’s the plan: Integrated mission planning helps modern forces do more with less”, February 2001.Journal of Electronic Defense, EC Monitor: US Army Kicking Off TUAV SIGINT Program, March 2001.Journal of Electronic Defense, “A Sampling of Unmanned Aerial Vehicles”, March 2001. [2] K. Donovan, J. B. Howie, W. Wojcieshowski: ``A Multi-Resolution, Integrated Terrain (MRITIN) Database''. Simulation Interoperability WorkshopJournal of Electronic Defense,, “New thinking in air defense networks puts SEAD in check”, September 1997May 2001.Journal of Electronic Defense, Washington Report: Congress Supports Global Hawk Block 10, February 2002.Journal of Electronic Defense, EW 101: EW Threats, Part 5 – Communications Signals, February 2002. HYPERLINK "http://www.fas.org/irp/doddir/wg2000/part03.htm" www.fas.org/irp/doddir/wg2000/part03.htm, National/Organic Support – Warfighter Guide to Intelligence 2000. [3] H. Lind, D. Gafford, D. Bakeman: ``Generation and Simulation of Synthetic Environments Using the Global Coordinate System''. Simulation Interoperability Workshop, September 1997.[4] F. Mamaghani, K. Wertman, A. Tosh, “SEDRIS to CTDB and CTDB to SEDRIS Conversions Tutorial”.  SEDRIS Technology Conference, June 2001[5] A. Janett, D. Miller, J. Nordstrom, P. Birkel: “Extending the JSIMS Terrain Common Data Model to Support Virtual Simulations”.  Simulation Interoperability Workshop, March 2000. Dr.[6] K. Donovan, P. Husband, M. LongtinField, P. PalmaG. Gutowski: `` Large Area, Virtual Training Databases with CTDB-GCSAn Open Architecture for Synthetic Environment Database Creation''. Simulation Interoperability Workshopxxxxxxxxxxxxxxxxxxx, xxxxSIW FallSeptember 1999 2002. [7] K Donovan, K. Stump, J. Watkins: ``Virtual Database Production with SEDRIS''. Simulation Interoperability Workshop, September 2001.Author’s BiographiesKENNETH B. DONOVANEYTAN POLLAK is a xxxxx technology manager of synthetic environment products at Lockheed Martin Information Systems.  He has 20 years experience in the simulation industry, and has numerous publications and patents in the field.  Ken led the company’s development of the TARGET DBGS systems and participated in standards development for SIF and SEDRIS. PATRICIA HUSBANDGREGORY HARRISON is a  staff software engineer in synthetic environment databases at Lockheed Martin Information Systems.  Pat has over 20 years as a software engineer and twelve years in the simulation field.  Pat spend five years at the National Test Bed producing real-time wargaming simulations to evaluate concepts for military operations.  Pat also has several years experience in the development of software used to generate synthetic environment databases, including consuming SIF databases for CCTT/UKCATT and generating SEDRIS data for ground and flight SAF.staff software engineer at Lockheed Martin Information Systems.  He works in SE system host technologies, including EW, SNE, and math modeling.  He has written numerous papers, and patents granted and in process for artificial intelligence.  He earned the Ph.D. at the University of Florida, and is on the Graduate Faculty at the Florida Institute of Technology.MICHAEL J. LONGTINBRETT VONSIK is a staff systems engineer at Lockheed Martin Information Systems software engineer at Lockheed Martin Information Systems Advanced Simulation Center.  Michael has contributed to numerous projects involving terrain reasoning, route planning, movement, Stingray, and many behaviors including ground vehicle tactics, RWA behaviors, and smoke-related behaviors.  He was also involved in the Dynamic Terrain and Objects (DTO) development, and extended ModSAF's terrain database representation to support additional DTO features.  He has contributed in the field of route planning, having developed the concealed route planner in ModSAF, and making significant enhancements to the high-level cross-country route planner as well as the road network route planner in ModSAF.  He currently is an applications engineer in the Read-Sim( technology group and product lead for the SECore software PAUL PALMA is the lead project engineer in synthetic environment databases at Lockheed Martin.  Paul has over 20 years experience in the simulation and commercial graphics field.  Paul manages database modeling teams for both ground and flight trainers.tool.  He is a former Air Force Special Operations EWO with over 2,000 hours of flight experience including combat flight time in Panama and the Middle East.  He is a graduate of the United States Air Force Academy and earned his Masters in Computer Resources and Information Systems from Webster University.EYTAN POLLAK is the Research and Development Director for Training and Simulation Solutions at Lockheed Martin Information Systems in Orlando, Florida.    He has published papers dealing with distributed simulation, embedded simulation, reconfigurable simulators, and common architecture for simulation systems.  Dr. Pollak serves as adjunct professor of Electrical Engineering at the University of Central Florida.  He received his Ph.D. from Purdue University.Notice:  All trademarks are property of their respective owners.© 2002 Lockheed Martin CorporationFigure 2-2 Portion of Extensive SE Library CoverageFigure 4-1  SE Flow for Virtual Training SystemFigure 5-1 SE Export and Compilation