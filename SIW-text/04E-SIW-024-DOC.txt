The Referent in Simulation Development and Assessment*Dale K. PaceThe Johns Hopkins University Applied Physics Laboratory11100 Johns Hopkins RoadLaurel, MD 20723-6099240-228-5650, 240-228-5910 (FAX) HYPERLINK "mailto:Dale.Pace@jhuapl.edu" Dale.Pace@jhuapl.eduKeywords:Simulation, validation, accreditation, referentABSTRACT:  The subject of the referent, the information used as the standard for modeling and simulation (M&S) validation and accreditation assessments, has not been addressed extensively in M&S or verification, validation, and accreditation (VV&A) literature. Much improvement is needed in the way referents are used if M&S validation and accreditation assessments are to be as effective and productive as they should be. A short-term international study of the referent was conducted in the first part of 2004. The purpose of the study was to develop guidance about what information should be used as a referent, how the referent should be selected, how the referent should be described, etc. This paper describes the referent study and reports its findings.  (Paper 04E-SIW-024) 1.  IntroductionThe importance of reliable and complete information about what a model or simulation (M&S) is to represent and about what should be the standard that M&S results will be compared with has long been recognized.  In spite of such recognition, the M&S literature does not address extensively how to identify, select, or describe the information about what a M&S is to represent for use in validation and accreditation assessment.  This report calls that information the referent (a formal definition is presented later). This lack of extensive treatment of the referent in the M&S literature has hindered development of best practices and contributed to inconsistent and sometimes inept performance by M&S communities.  The U.S. Defense Modeling and Simulation Office (DMSO) chartered a study of the validation and accreditation referent [1].  This study is available from the verification, validation, and accreditation (VV&A) page of DMSO‚Äôs website ( HYPERLINK "http://www.dmso.mil" http://www.dmso.mil).  Objectives of The Referent Study are shown in Table 1.  While this study has not produced the final word on the referent, it is a substantial contribution to the M&S literature about the referent. ____________* This paper was prepared under sponsorship of the Defense Modeling and Simulation Office (DMSO), but its views are those of the author and should not be construed to represent views of DMSO or of any other organization or agency, public or private.Table 1.  Referent Study ObjectivesTo articulate a definition and connotation for referent which is suitable for VV&A applied to the full spectrum of models and simulations (M&S).To identify and articulate desired referent characteristics.To develop guidelines (based upon current state of the M&S art and best VV&A praxis) for selecting and describing a referent, and as needed to make such guidelines a function of M&S characteristics and applications.To develop guidelines (based upon current state of the M&S art and best VV&A praxis) for referent use in validation and accreditation assessments, and to include suggestions for what can be done when the referent is inadequate, poorly identified, or unknown.To identify research needs for advancing referent identification/selection, articulation, and use in validation and accreditation assessments.Nineteen people from four countries (China, France, the United Kingdom, and the United States) contributed to or reviewed The Referent Study.  They are identified in Table 2.  The study‚Äôs depth and breadth benefited from the varied backgrounds of the individuals who comprised the study team.Table 2.  Referent Study ParticipantsStudy Lead: Dale Pace (JHU/APL)Richard Bernstein (JHU/APL)Zhang Bing (Harbin Institute of Technology, China)Mark Dumble (Aegis Simulation Technologies UK Ltd)Daniel Girardot (Centre d‚ÄôAnalyse de D√©fense, France)COL Kevin Greaney (Missile Defense Agency)Robert Hamber (Naval Facilities Engineering Command)Hanae Hara (Space and Naval Warfare Systems Command)Scott Harmon (Zetetix)Sam Johnson (USAF Modeling and Simulation)Mike Leite (SETA support to DMSO, SAIC)Mike Metz (IMC)Stuart Randlett (SAIC)Bob Senko (DMSO Consultant)Susan Solick (Army Training & Doctrine Analysis Center)Marcy Stutzman (Northrop Grumman)James Wallace (Army Modeling & Simulation Office, Alion)Scott Weidman (National Academy of Sciences)Simone Youngblood (DMSO VV&A Technical Director)This paper presents insights from The Referent Study, and uses a great deal of material verbatim from the Study‚Äôs Final Report without additional attribution.  This paper begins with a definition of the term referent as it pertains to M&S validation and accreditation assessments.  It then addresses how to identify and select an appropriate referent.  Next it discusses how to specify or describe a referent, followed by discussion of how to use a referent in validation and accreditation assessments.  The paper concludes with identification of referent research needs.2.  Referent DefinitionThe various definitions for ‚Äúreferent‚Äù that may be found in dictionaries and other sources are correct, but they do not fully meet the need of referent connotation for use in validation and accreditation assessment.  For that reason, The Referent Study suggests the following definition be used for referent:The referent is the best or most appropriate codified body of information available that describes characteristics and behavior of the reality represented in the simulation from the perspective of validation assessment for intended use of the simulation.   Various words and phrases from this definition are discussed below to ensure that it is fully understood.  The information used as the referent may consist of: ‚Äúdata‚Äù (observations of the simuland, either under controlled circumstances as in tests and experiments, or under natural circumstances), theories as expressed in algorithms that describe characteristics, behaviors and relationships (preferably theories validated against observations of the simuland), simulation results (preferably from simulations that have been objectively and quantitatively validated), expert human knowledge, or combinations of these.  In cases where explicit observations and theories do not provide a comprehensive and sufficiently reliable description of the reality represented in the M&S, information from subject matter experts (SMEs) may have to serve as the referent or part of it.  Such SME information may not be explicitly articulated and systematically organized since it is a form of qualitative assessment, and the basis for qualitative assessments usually are not as explicitly described as in quantitative assessments.  However, it is recognized that M&S communities that deal mostly with physics-based models that may in principle be compared against quantitative data tend to be less willing to accept SME information and other qualitative assessments as part of the referent than are other M&S communities.  The computational science and engineering community, which has major verification and validation (V&V) concerns in applications of computational fluid dynamics (CFD) and computational solid mechanics simulations, is an example of that kind of M&S community.The information used as a referent is a codified body of information.  Codify has several connotations.  It implies system and organization, both useful aspects of information to be used as a referent for M&S validation.  Codified often implies authority and is frequently associated with laws.  Wherever appropriate and possible, the referent should be information drawn from credible authoritative data sources.The definition above deals with the ‚Äúreality represented in the simulation.‚Äù  Typically that reality includes actors/systems/entities interacting with other actors/systems/entities by various processes through or in one or more environments.  The referent pertains to all of these:  actors/systems/entities, processes, and environments.  Connotations associated with these may vary by M&S type and by the kind of application.  For example, the simulation operator would not be part of the referent for a batch-run constructive simulation, but the simulation operator might need to be part of the referent in an interactive simulation (such as a game or war game).  Three challenges arise from the referent as the best or most appropriate information available that describes characteristics and behavior of the reality represented in the simulation from the perspective of validation assessment for intended use of the simulation.   Development and organization of the best information possible may be too costly and/or take too long for schedule and resource constraints of a particular M&S application.  This leads to use of an adequate referent; one that is not the ‚Äúbest‚Äù information possible but one that is adequate as the basis for validation assessment within the context of the M&S intended use.  Such an adequate referent may be the best information available with the time and resource constraints for a particular M&S application. The idea of an ‚Äúadequate referent‚Äù increases the importance of exact and precise specification of the M&S intended use.  A poorly specified intended use increases the risk of decision error, i.e., the risk either that a simulation will be judged acceptable (valid) for the intended use when it is in fact not acceptable, or that an acceptable M&S will be judge unacceptable.  Consequences of such mistakes depend upon the impact of the M&S application.  There may be little consequence from such a decision error for a simulation that only produces background information about a subject, but catastrophe could result from such a decision error about a simulation used as part of a real-time decision aid for a safety critical system.  The ultimate authority for the acceptability of a referent is the accrediting activity (or accreditation authority), the person or organization responsible for the decision that the M&S is appropriate for the intended use.  This report develops rationale for identification, selection, and description of a referent by the V&V agent (or others responsible for such) and a basis for its acceptance by the accrediting activity.The most appropriate information has adequate fidelity (accuracy, scope, resolution, context) to serve as the basis for comparison in validation assessment of the M&S for its intended use.  It is a truism that one cannot demonstrate greater fidelity for M&S results than the fidelity of the referent to which those results are compared.  Consequently, the referent must have greater fidelity (i.e., more closely represent the reality addressed by the M&S) than required for M&S results to be valid; otherwise the body of information selected is inadequate as a referent for intended M&S use.  Vague specification of M&S intended use can make it difficult to determine required fidelity for an adequate referent.  This leads to increased decision risk and makes it more likely that an inadequate M&S will be considered acceptable for the intended use.  As a general rule, the most appropriate collection of information to serve as the referent is the least expensive set of information that has adequate fidelity to support M&S intended use.  Otherwise, the referent becomes gold plated and wastes resources.  For example, if M&S results are to provide ballpark estimates of performance as background information, SME judgment may be an acceptable referent for such ‚Äúback of the envelope‚Äù fidelity.  Spending time and effort collecting data and manipulating the data into an acceptable format to serve as a referent for such an M&S could be a waste of resources if the SME judgment could provide an adequate assessment of M&S fidelity for its intended use.  Because the referent describes characteristics and behavior of the reality represented in the simulation, specification of the referent is particularly difficult for those M&S which generate new knowledge or additional knowledge about the reality represented in the M&S (vice the M&S simply being a reliable representation of that reality).  This could be the case with a simulation demonstrating emergent behavior.  This also could be the case if the simulation employs some form of judgment in uncertain conditions (such as representation of human decision making) or if the simulation portrays a future reality that is difficult to predict (such as social structure, military posture, etc.).  There are two implications here for referent specification.  One is that iteration of the referent specification may be necessary as new knowledge is generated.  The hazards of such should be obvious.  The other is that intended use of M&S with such uncertainties in the reality represented implies that the referent specification will have low fidelity.  It is not the length of time projected into the future that is the issue; astrophysical simulations project conditions billions of years in the future with high fidelity.  It is the uncertainty in the representation that determines the fidelity needed in the referent specification.2.1  ConnotationsThe study examined ten varieties of M&S and M&S application domiains specifically to give the study reasonable breadth and depth in terms of connotation variation for referents and to ensure that the definition proposed can apply to all varieties of M&S and their full range of potential applications.Five varieties of M&S were considered:Extensive use of adaptive programmingExtensive use of human behavior representation (HBR)Distributed simulationExtensive use of aggregationSystem/hardware/software/human in the loopFive M&S application domains were considered:Computational science and engineering applications, such as computational fluid dynamics (CFD) M&SEngineering level applicationsGame/training applicationsMilitary theater-level/campaign applicationsNon-physical applications, such as economic M&SThe study focused on the role of the referent in validation and accreditation assessments.  It did not address how information used to develop a model or simulation is identified or described nor how one selects data that is appropriate to use as simulation inputs, whether hardwired in the simulation or fed into the simulation as needed to run it.  However, the study included issues related to statistical independence of information used for simulation development and information used as the referent in validation and accreditation assessments when simulation results are to be used to predict performance or behavior. Many M&S communities, especially those in which models often have heuristic factors that are adjusted to make simulation results fit experimental data (as is the case, for example, in computational solid mechanics) stress the importance of separate data for results used in such model calibration from the data used in M&S validation assessments.  The problem is similar to that encountered in clinical trials in the health field, in which randomization of patient involvement in a clinical trial (with its associated unpredictability) is used to ‚Äúprotect against the unpredictability of the extent of bias in the results of non-randomized clinical trials‚Äù [2].  Many do not appreciate the issues associated with using the same information for M&S validation assessment as used for M&S development.For simplicity, validation assessment is used to encompass all referent aspects of both validation and accreditation assessment.  It is important to appreciate the difference between validation assessments and accreditation assessments.  The validation assessment determines whether the M&S has adequate fidelity to support intended uses, and perhaps can even quantify the likelihood that the M&S has the required fidelity with a statement like ‚Äúwe have 90% confidence that M&S results will differ from the referent by less than 5% for any conditions within the specified application region.‚Äù  Such a quantitative statement is the most precise kind of validation assessment, and depends upon a very well defined intended use, an explicitly well defined referent, and a precise measurement capability for the M&S.  Few validation assessments have such quantitative precision.Accreditation assessment uses the validation assessment, but supplements it with risk, programmatic and other considerations to determine if the M&S is appropriate for use in a particular situation.  It is important to remember that validation assessment is always in the context of intended M&S use, and the accreditation activity (or accreditation authority) is the one who determines the acceptability criteria and in essence is the one to approve the referent.3.  When Should the Referent Be Specified?A referent must have adequate fidelity to support validation assessment.  Referents may be selected by direction, convenience, economics, or proxy (using information about something similar).  The M&S lifecycle can be broken into eight phases, which may be passed through repetitively and several of which may be concurrent.  These phases occur regardless of the M&S development paradigm employed, whether a serial like the waterfall paradigm or an iterative one.  These eight phases are:Requirements (expression of need or desired M&S capabilities)	Planning	Conceptual Model  ( recommended location for referent identificationDesign	Implement	Test	Use	Modify	The referent may be specified at any time during the M&S lifecycle (from the earliest ideas about the M&S to after its development is completed and the M&S is in use), but the best place to specify the referent is in the simulation conceptual model.  The simulation conceptual model, as specified by the DoD M&S VV&A Recommended Practices Guide (RPG) [3], consists of simulation context, mission space (representational aspects of the simulation), and simulation space (simulation implementation aspects).  The referent should be identified and specified as part of the simulation context.  However, it is more important that the referent be specifically identified and described than that it be identified in a particular phase of M&S life cycle, even though the simulation conceptual model is the best place for specification of the referent.  Referent specification in the simulation conceptual model ensures that it is available for both conceptual validation and results validation, and also ensures the information included in the referent is available as needed for M&S development.4.  How Should the Referent Be Described?While the information content of the referent is depended upon the kind of M&S and its application domain as well as intended use, identification and specification of the referent should be definite and unambiguous in all cases.  The referent description should specify the referent context, its domain coverage, and pertinent actor/system/entity/process/environment attributes.  Parameter uncertainties should be quantified and how information from various sources is combined to form the referent should be explained.Referents have five fundamental attributes:  context, domain coverage, attribute distinctiveness, parameter uncertainty quantification, and information coherence.  Each of these is discussed below.  Some general characteristics should apply to all referent specifications.  Identification and specification of a referent should be definite and unambiguous.  Just doing this consistently will bring improvement over current M&S practice.  If some information in the referent is to use only part of the information in a particular source (such as results from a test or set of tests), what is not to be used should be clearly and specifically delineated.  For example, if the flight profile of a missile is to come from a particular test, but the radar signatures of the missile in that test are not appropriate for the missile referent, that should be noted explicitly and clearly in the referent identification and description. This is to eliminate any possible confusion about what is to be used as the referent.  4.1  Context It is always important for a referent to specify its context.  The context will range from information about conditions under which human performance information is collected to physical conditions (such as temperature, pressure, radiation, etc.) that might influence measured parameters in the referent which are not addressed specifically by information in the referent.  Care must be taken to avoid specifying the referent too narrowly.  For instance, if an M&S relies on physical characteristics of water, the referent should be as broad as possible so that it catches erroneous situations such as water being in the wrong phase during an explosion or other event, which while rare, might be exactly the sort of unusual condition for which the model‚Äôs prediction is most useful.  4.2  Domain CoverageA simulation has a specified application domain.  Its intended use determines the appropriate application domain.  The application domain is always multi-dimensional.  Referent description should indicate what portion of that application domain is addressed by the referent.  For example, test data used as a referent may reflect steady state, smooth, undisturbed flow conditions for a parameter (such as fluid volume passing through a pipe, traffic on a road, time delay waiting for a technician in on-line support, etc.) but is not appropriate for use as a referent in transition, turbulence, disturbed flow conditions (such as might be experienced if an obstacle were in the pipe or on the road, or the on-line support shift is short handed).Limitations in referent domain coverage forces one to consider inference.  Figure 4.2-1 illustrates three possible relationships between data for a referent (the circles) and intended use (boxes) in the application domain.  If the referent is restricted to data, only when the data completely overlap the application area can there be high confidence in a quantitative assessment of the relationship between simulation results and the referent.  In the other two situations (partial overlap and no overlap), indeterminate uncertainty is present for applications outside the data region.  If reliable theory exists, then either of these cases might be reduced to the equivalent of the complete overlap case. EMBED PowerPoint.Slide.8  Figure 4.2-1  Possible Relationships of Validation Referent Data and Application DomainGeneral parameter regions are fairly easy to understand.  For example, if a simulation application is to address what happens when bodies collide (as with a missile defense kill vehicle hitting its target), the range of interest for collusion velocities may vary from nearly zero (as could happen in a scenario in which the kill vehicle approaches from behind the target) to very fast (10 km/s or more for a fast interceptor against an ICBM in a head-on encounter).  Data from full-scale tests may be very limited, not only in the number of tests, but also in the portion of the speed regime for which there are tests and in the availability of precise information from the tests.  Data from surrogate tests (such as sled-tests or light gas gun experiments) may supplement the full-scale test data, but they introduce uncertainties into the data because of test artifacts (such as need to scale results, differences between the surrogate and the real object, etc.) and such tests may not fully cover the parameter regime of interest.  Then theory or perhaps very high fidelity simulations may be used for ‚Äúdata‚Äù for parts of the parameter regime that testing (either full-scale or surrogate) cannot address.  And finally, expert opinion may be used to fill in any remaining information gaps in the domain (and to reconcile any discrepancies among the various kinds of information mentioned).Description of the referent will indicate how information about the M&S actors/systems/entities, processes, and environments throughout domains of interest are addressed.  What information sources are to be used for which portion of the domain, where gaps may exist, and how information from different sources may be reconciled and are to be combined. 4.3  Attribute DistinctivenessThe referent is concerned with actors/systems/entities, their interactions, processes, and environment(s) of the reality represented in the simulation.  As illustrated earlier, not every possible attribute of these is significant for the referent.  All attributes needed to satisfy M&S requirements should be specified for the referent, and described fully for the domain coverage required with all pertinent variations indicated.  For example, if the object size or color changes with temperature, etc. and the object‚Äôs size or color is important for intended M&S use, that attribute trait should be specifically noted.4.4  Parameter Uncertainty QuantificationThere are two uncertainty dimensions in fidelity and validation assessments.  One dimension is M&S uncertainty.  These arise from imperfect algorithms, computation characteristics (such as table look-up errors), input errors, etc.  Often when simulation results are compared with a standard (theoretical curve, test data, etc.), it is assumed that all error or uncertainty is a result of M&S uncertainties.  This usually is not the case.  The other uncertainty dimension in fidelity and validation assessment is uncertainty in the referent.  Only when referent uncertainties are fully characterized can M&S uncertainties be fully characterized.  When both uncertainty dimensions are fully characterized, then fidelity and validation assessments can be performed rigorously and fully characterized. Some feel the term ‚Äúfully characterized‚Äù is redundant, that it means no more than ‚Äúcharacterized.‚Äù  Others worry that addition of the term ‚Äúfully‚Äù provides an excuse for people not to try since it is too hard or impossible to ‚Äúfully‚Äù characterize the uncertainties in an absolute sense.  What is intended by ‚Äúfully characterized‚Äù is that adequate information is provided so that meaningful quantitative statistical statements can be made about the relationship between the referent and M&S results with high confidence. In the computational science and engineering arena, a great deal of attention has been given to quantifying uncertainties, both M&S uncertainties and referent uncertainties, in papers [e.g., 4], short courses [e.g., 5], conferences [e.g., 6], and workshops [e.g., 7].  Various validation metrics have been suggested in this arena, guidance in regard to validation experiments has been developed so that referent information with characterized uncertain may be known, etc.  A proper referent description will include specification of parameter uncertainties in referent information.4.5  Information Coherence  Information for the referent may come from multiple sources.  Some will be redundant (same parameters for the same part of the same domain), some will be supplementary (different parameters for the same part of the same domain, or same parameters for different parts of the same domain), and disjoint (different parameters and different domains).  Some information will have parameter uncertainties quantified, and other information will not.  Information coherence is concerned with how information is combined so that information about a particular aspect of the referent (parameter, actor, system, entity, process, environment, etc.) makes sense.  Reference description should explain how information combination is done so that information coherence is achieved.  The two examples below illustrate some of the ways this can be done.  Neither example tries to show a best or preferred way for combining information.Case 1.  Information about parameter x will be taken from three tests, all of which address how the parameter varies with respect to y over the same set of values for y.  In Test 1, uncertainties in the measurement of x are fully characterized.  Tests 2 & 3, which were done at the same test facilities using the same equipment and test personnel, do not characterize x uncertainties nor describe environmental conditions fully.  Options for referent information about x include:Use all data from Tests 1-3 and ignore uncertainties about xUse only data from Test 1 and fully characterize x uncertaintiesUse a plot of x from Test 1 with uncertainties indicated, overlaid with data from Tests 2 & 3, as a basis for an equation to characterize x vs yCase 2.  Information about parameter x is available from Test 1 for a portion of the domain of interest.  Information about a parameter y, which it is believed that x is proportional to, is available for a portion of the domain of interest not covered by Test 1 (but without any information for the part of the domain that is addressed in Test 1).  Two SMEs gave widely divergent opinions about what multiplication factor should be assumed in the proportional relationship between x and y.  Options for referent information about x include:Use only data from Test 1Use both data from Test 1 and a band for x determined by proportionality to y (with the two SME estimations setting the edges of the band)Declare that x is unknownA proper description of the referent will explain how information from different sources is to be combined coherently.  Obviously general guidance for how information should be combined is beyond the scope of this report; but it is clear that explicit description of how such information will be combined is an important part of a referent‚Äôs description.5. Referent Use in Validation and Accreditation AssessmentsThis section is different from the preceding ones.  It is concerned with referent use instead of referent identification and description.  It is included here because it emphasizes the importance of proper identification and description of the referent.  Three basic situations are discussed.  In the first, referents are data-rich (the information about the referent is abundant).  In the second, referents are data-poor (there is little information about the referent).  And in the third, the referent is poorly defined, poorly identified, or otherwise inadequate.This section also discusses the important issue of the relationship of the referent used for validation and accreditation assessment to information used for M&S development and to information used as inputs in M&S runs.5.1  Data-rich ReferentsA data-rich referent is always the most desirable situation, especially if uncertainties about parameters of the referent are well specified.  Such referents provide the basis for the most reliable and most credible fidelity and validation assessments of simulation results since they provide an objective and factual basis for statistical comparison of results.  However, even in situations with data-rich referents, care must be taken to ensure that the information used for the referent is truly pertinent for M&S intended use.  For example, if human size is part of the referent (as might be pertinent for M&S concerned with passenger movement in a new vehicle design), one must be sure that the information about human size is recent since (at least in America) humans are bigger now than they were a couple of generations ago.In a data-rich environment, it should be possible to separate the information used for the referent from information used to designed and develop the simulation so that M&S validity and capability for reliable predictions can be assessed more robustly.  The need for statistical independence between information used for M&S development and information used for validation assessment was mentioned earlier in the report and is discussed in Section 5.4 below.5.2  Data-poor ReferentsIn a data-poor situation, the referent is mainly theoretical by necessity.  The theory may be well articulated and explicitly formulated, as in an astrophysical simulation of processes insides stars [8] for which we have only inferred data, no in situ observations.  On the other hand, the theory may be unarticulated, as is often the case when SMEs are used as part of the referent.  The SME may not make explicit how a judgment was reached or upon what the assessment is based.  The credibility of fidelity and validation assessments based upon data-poor referents is always severely limited, but a thoughtful probing of SMEs (and good documentation of their insights) makes the best of this situation.In data-poor environments, it is seldom possible to separate information used for M&S development from information used for validation assessment because all information has to be used in M&S development.  Some, especially M&S personnel within the computation science and engineering community, would claim this limits M&S assessment to calibration and prohibits validation of M&S predictive capabilities.  It is recommended that the referent description note that referent information was also used for M&S development so that appropriate caveats may be associated with the validation assessments.  This report does not attempt to prescribe how such caveats are determined, only that the lack of statistical independence between information used for M&S development and assessment be noted so that caveats can be indicated with the assessment as appropriate.5.3  Inadequate ReferentsIn the third case, the referent is not clearly identified or has recognized inadequacies (such as contradictory information, so that if simulation results agree with some of the ‚Äúreferent‚Äù data, they will not agree with other parts of the ‚Äúreferent‚Äù data).  If there is no clear way to sort the referent information so that it is coherent and non-contradictory, then it is necessary to declare that fidelity and validity assessments are not possible for the simulation because of referent inadequacies.  However, as Hodges and Dewar noted a decade ago, even a model which cannot be validated can have utility and value [9].  In the data-poor situation, use of theory or SMEs as the referent allows one to make fidelity and validation assessments, even though the credibility of such may be low; but when the referent is contradictory, one cannot even make a low credibility assessment.  Inadequate referents challenge the courage and professionalism of V&V personnel since they force the V&V personnel (if competent and honest) to declare to M&S sponsors, users, and others that validation assessment is not possible because of referent inadequacies.  Most V&V personnel are acutely aware of how often the messenger bearing bad news has been shot; many have scars that demonstrate the hazard of bearing bad news.5.4  Referent Relation to M&S Development and Run InformationInformation about the reality represented in the M&S is used to develop the M&S, in running the M&S,  and as the referent in validation and accreditation assessments.  It is reasonable to inquire about the relationships among these three sets of information since they all deal with the same reality.In a data-rich environment, it is most desirable that the set of information used for M&S development, the set of information used as inputs for running the M&S, and the set of information used as validation referent be statistically independent.  This is particularly important when M&S results are used to predict how things will be in regions for which data are sparse or absent.  The rationale for this is simple.  This approach provides the highest likelihood that M&S results will be most representative of the reality represented in the M&S.The medical community has found that conclusions from clinical trials can be vary significantly from what is believed to be more correct if appropriate care is not taken in control of such statistical issues [10].  Similar concern is needed in M&S assessments.  Many do not appreciate the issues associated with using the same information for M&S validation assessment as used for M&S development.Concern about this general problem, and its specific related problem of not letting a modeler know experimental outcomes before the model describing the same situation is run, is abundant in the computational science and engineering community.  The need for the modeler to know the exact conditions of the experiment precisely before running the model is understood.  Numerous guidelines are presented for the ways that ‚Äúempirical adjustable parameters‚Äù [11], knobs, dials, fudge factors in more colloquial terminology, must be treated for M&S results to be acceptable in peer-reviewed circumstances.In data-poor environments, statistical independence among the three sets of data may be impossible.  There simply may not be enough data.  The three data sets may even have to be identical.  This condition limits what can be claimed about the validity of simulation results.  One can describe with quantitative precision how well the M&S reproduces its input data, but one cannot make very meaningful comments about M&S predictive capabilities.  In that regard, one may be unable to make a stronger assessment than one can with an inadequate referent.  Candor about limitations of validation assessments in such circumstances is an important aspect of V&V professionalism.  The 1979 words of statistician George Box [12], who at the time was the Past President of the American Statistical Association and President of the Institute of Mathematical Statistics, should not be forgotten in this respect:  ‚Äúall models are wrong, but some are useful.‚Äù6.  Referent Research NeedsThere are three primary research needs relative to referents.  The first is concerned with how to consolidate specified referent information so that it is coherent and so that its uncertainties are appropriately specified.  Uncertainty quantification efforts within the computational science and engineering community are basically addressing this problem within their domain, and many of the mathematical techniques may have some applicability elsewhere too.  It is unlikely that the approaches identified for the computational science and engineering community will deal with all issues of concern in other M&S communities.  For instance, some communities rely more on proxy information, and there is a need to develop methodologies for assessing the adequacy of proxies in referents.  There is also a need to develop concepts for evaluating the overall adequacy of a referent, where ‚Äúadequacy‚Äù can be a very slippery and subjective notion.More generally, there is a need for codifying best practices for the use of SMEs and other sources of qualitative information so as to achieve the maximum rigor in referents that are based on such information.  Good practices from the social sciences might be adapted to regularize the use of such inputs.  It is also necessary to develop methodologies for good sensitivity testing of referent data so as to understand which aspects require the strongest data in order to properly illuminate M&S quality, and which have lesser affects on the VV&A.  Part of this effort will require research into appropriate ways to combine descriptions of variations (uncertainties) in referent characteristics for the different actors/systems/entities, processes, and environments that comprise the referent.The second primary research need is concerned with dynamic referents.  If a simulation is to represent something that changes with time or other parameters, how can a referent at a previous state/condition (which may be the only situation for which data are available) be used with simulation results for a subsequent situation, especially if emergent behavior or adaptive processing is used in the simulation?  This issue will be an increasingly important problem for simulations in the future.  For example, behavior in many biological systems and in crowd/traffic situations varies with population density in ways that are not easily predictable by algorithms at the macro-level, though behaviors can be reasonably described at micro-levels.  Should the referent be restricted to micro-level behavior?  Or can there also be some kind of useful referent at macro-levels?A third area of research addresses the method for documenting the use of referents in the V&V of models and simulations.  Specifically, how is the referent documented so that it is traceable and reproducible?  The key concern is the reuse of simulations or the incorporation of simulation results into subsequent simulation events. How can we achieve a better understanding of how to use results of one simulation to inform another‚Äîe.g., when and how to integrate fine-scale models into coarse-grained ones?  In a particular case, if we want to use an accepted simulation as a referent for other simulations, one also needs to know how differences in scale, fidelity, and assumptions of a ‚Äúreferent simulation‚Äù should be interpreted as part of a VV&A process for the simulation being validated.7.  ConclusionsThis paper indicates the importance and complexity of referents for validation assessment.  Its treatment of this subject should stimulate fuller consideration of this subject by others, especially from the perspective of their individual M&S communities.A workable referent definition for validation and accreditation assessments has been suggested that should be appropriate for all M&S varieties and applications.The importance of explicit and thorough referent specification, especially in regard to M&S intended use, information sources used for the referent and how they are combined, and how the referent is to be used in validation and accreditation assessments, has been stressed.  Ways that the referent should be described has been presented with consideration for how M&S variety and application may impact referent description.Three areas of important research that should be pursued to improve our capabilities for validation assessment referent identification and specification are: how to consolidate information from various sources for use in a referent, how to specify dynamic referents, and how to document referents properly in M&S VV&A.It is hoped and expected that others in M&S communities will also give serious attention to referent identification, selection, description, and use, and that they will publish their ideas so that we all may improve.8.  References[1]  	The Referent Study Final Report, May 2004.  The Enclosure to Johns Hopkins University Applied Physics Laboratory Report JWR-04-010, Defense Modeling and Simulation Office (DMSO) Referent Study, May 19, 2004.  Available from the VV&A page of the DMSO website:   HYPERLINK "http://www.dmso.mil" http://www.dmso.mil. [2]	Regina Kunz and Andrew D. Oxman, ‚ÄúThe Unpredictability Paradox:  Review of Empirical Comparisons of Randomized and Non-randomized Clinical Trials,‚Äù British Medical Journal, October 31, 1998, pp. 1185-1190.  Available at  HYPERLINK "http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&list_uids=9794851&dopt=Abstract" http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&list_uids=9794851&dopt=Abstract (accessed May 2004).[3] 	Special Topic on Simulation Conceptual Models in DoD Recommended Practices Guide (RPG) for Modeling and Simulation VV&A, Millennium Edition ( HYPERLINK "http://www.msiac.dmso.mil/vva/default.htm" http://www.msiac.dmso.mil/vva/default.htm).[4] 	William L. Oberkampf, Sharon M. DeLand, Brian M. Rutherford, Kathleen V. Diegert, and Kenneth F. Alvin, Estimation of Total Uncertainty in Modeling and Simulation, Sandia National Laboratories Report SAND2000-0824, April 2000.[5]	Roger Ghanem and Steve Wojtkiewicz, Uncertainty Quantification Short Course (February 9, 2003) at Society for Industrial and Applied Mathematics (SIAM) 2003 Conference on Computational Science and Engineering,  HYPERLINK "http://www.siam.org/meetings/cse03/UQShortcourse.htm" http://www.siam.org/meetings/cse03/UQShortcourse.htm (accessed February 2004).[6]	SAMO 2004, 4th International Conference on Sensitivity Analysis of Model Output, SAMO 2004,March 8-11, 2004,  HYPERLINK "http://www.floodrisknet.org.uk/events/SAMO%202004" http://www.floodrisknet.org.uk/events/SAMO%202004 (accessed February 2004).[7]	Center for Integrative Multiscale Modeling and Simulation (CIMMS) Focused Workshop on Uncertainty in Engineering Design, May 23, 2002, Caltech,  HYPERLINK "http://www.cimms.caltech.edu/workshops_dir/w-focused/abstracts/all_abstracts.pdf" http://www.cimms.caltech.edu/workshops_dir/w-focused/abstracts/all_abstracts.pdf (accessed February 2004).[8] 	A. C. Calder et al, ‚ÄúA Case Study of Verifying and Validating an Astrophysical Code,‚Äù in V&V State of the Art:  Proceedings of Foundations ‚Äô02, a Workshop on Model and Simulation Verification and Validation for the 21st Century, October 22-24, 2002, Laurel, MD.  CD published by The Society for Modeling and Simulation, 2002.  [9]	J. S. Hodges and J. A. Dewar, Is It You or Your Model Talking? A Framework for Model Validation, RAND Report, R-4114-AF/A/OSD, 1992.[10]	Regina Kunz and Andrew D. Oxman, ‚ÄúThe Unpredictability Paradox:  Review of Empirical Comparisons of Randomized and Non-randomized Clinical Trials,‚Äù British Medical Journal, October 31, 1998, pp. 1185-1190.  Available at  HYPERLINK "http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&list_uids=9794851&dopt=Abstract" http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&list_uids=9794851&dopt=Abstract (accessed May 2004).[11]	Patrick J. Roache, Verification and Validation in Computational Science and Engineering, Hermosa Publishers (Albuquerque, NM), 1998.  ISBN 0-913478-08-3.[12]	G. E. P. Box, "Robustness is the Strategy of Scientific Model Building" in Robustness in Statistics, eds., R.L. Launer and G.N. Wilkinson,  Academic Press, 1979, p. 202.About the AuthorDale K. Pace, a member of the Principal Professional Staff of the Johns Hopkins University Applied Physics Laboratory, has been extensively involved in M&S VV&A since the mid-1980s: contributing to VV&A process development, teaching and publishing about VV&A, and leading M&S VV&A reviews.  He has been part of the DoD VV&A Technical Working Group (TWG) since it began.  He was an initial co-chair of the Distributed Interactive Simulation (DIS) VV&A Subgroup, part of the initial SIW Program Committee, and a leader of the SIW Research Development and Engineering (RDE) User Forum for several years.   He was co-chair of the 1999 MORS/SCS SIMVAL workshop and program co-chair for the Foundations ‚Äô02 V&V workshop.  He was the Study Lead for the Referent Study.