True Model Reuse, A Challenge For The M&S CommunityDavid R. McKeebyIIT Research InstituteAlexandria, VA 22203703-933-3366dmckeeby@iitri.orgKeywordsGrand Challenges, Architecture, VV&A, repositoriesABSTRACTRealizing of the dream of reuse is a grand challenge for the M&S world as well as the military establishment in general.  Not the reuse of the simulations, because that is on a small scale.  Reuse of the models.  Too often, every simulator rebuilds the models from scratch because the existing ones "don't meet the requirements".  This means that money is being spent to "reinvent the wheel" instead of advancing the state of the art.  Think of how many different or proprietary cloud models, smoke models, tank models, propagation models, clutter models, etc. there are (basically, one for each R&D lab plus one for each major contractor (see a pattern here?)).  Now, how many simulators use any of these models?  Most of them.  Now, how often is JSAF used?  It's one of the more reused simulations around.  The numbers don't compare.  Where could M&S go if we were able to roll say 50% of the money spent on regenerating models into refining these same models, or, heaven forbid, modeling human behavior... There are several facets to a solution for this challenge.  The hurdles that must be overcome are cultural, architectural, VV&A, and technological. An architecture needs to be developed that would support a building block approach to simulation development through the assembly of off-the-shelf models.  A culture that fosters the reuse of models must be cultivated.  This may include model repositories and rewarding those who contribute their models to them also to those that use these models.  VV&A programs must be established to accredit these model components and defining the valid range of applications for their use.  And incentives for program managers and design agents to mandate the use of these components in their models and to fold any improvements back into the repositories.   Investments will also be required to generate well-documented APIs to support the use of the models and in many cases, porting the models to languages that support the APIs and architectures.  This paper will discuss each of the technological, cultural, and programmatic initiatives required to make reuse of simulation components a viable effort.  BackgroundIn order to have a meaningful discussion on model reuse, several terms need to be defined to prevent any misconceptions.  A model, strictly defined, is a set of mathematical expressions that attempt to describe a “real-world” event or process.  In the context of this paper, these will be referred to as math models.  In the simulation community, more often, a numerical model, or a software implementation of the math model is called a model.  It is these numerical models that are the models that this paper promotes the reuse of.  Finally, numerical models are assembled as a system to satisfy a set of needs or requirements are a simulation. There are models for such disparate things as motion, signal propagation, system transfer functions, terrain, radiation, reflectivity, and human behavior.  Simulations may be used for training, testing of other systems, concept analysis, research, et cetera.  Depending on the purpose of the simulation, different levels of precision (typically referred to as fidelity) may be used.Typically the math models for many processes are quite well known and precise a notable exception being models for human behavior.  In many cases, these math models may be simplified for the convenience of the implementor.  For example, the Helmholtz equation describes the behavior of a wave in free-space.  In most cases, closed form, or exact solutions of this second-order differential equation are not possible.  So, approximations are made.  Depending on the assumptions, implementation by numerical modelers, especially in the case of more complex processes, such as wave propagation, can be as numerous as there are modelers, with each implementation being unique.  Due to the complexity of the problems, the size of some of these models can be non-trivial, consisting of many thousands of lines of code and taking quite a few years of development and refinement before they are widely accepted.  Developers gather up a set of models and data, assemble them into a simulation to meet a declared purpose.  Typically, each developer will use a set of in-house models as the building blocks.  These models are usually assembled within a proprietary architecture that may have been used by the developer for a whole series of simulator devices in order to take advantage of economies inherent in “tried and true” designs and to recycle old components.  A side effect of this activity, beyond reducing development risk may also be reduced cost, resulting in improved competitiveness and/or profitability in a cost sensitive procurement.  Because of the investment in the legacy architecture and components and the marketed/perceived advantages of the proprietary legacy approach, the developers are reluctant to migrate to an open, community architecture that reduces simulation components to commodity items.  Except in the case of large, complex models, few are shared community wide, especially when there are more than one model of the same phenomenon.  DiscussionA discussion was held on the Simulation Interoperability Standards Organization (SISO) Grand Challenges e-mail reflector in early January 2001 where Chuck Walters proposed that a grand challenge for the Modeling & Simulation community is to develop a “Star Trek Holodeck” capability.  In this concept, simulation developers would just grab simulation components and quickly assemble them into a simulation.  This capability requires an ability to reuse models.  Imagine the savings in labor if all simulations of a similar type can use the same models.  For the sake of argument, if it takes 5 kSLOC (a conservative estimate) to implement a complex model and the typical simulation uses, say, 3 of them, which would result in potential savings of around $1.5M per system.  Now this may be construed as a loss by the developer, but with enlightened management, a portion of the savings can be used to further refine these models or to develop new ones where none exist and thus, extend the capabilities of simulation.  This now prompts the question of how can models be made so that they are reusable.  In the background section it was stated that developers currently reuse simulation components and models in their product lines already. This paradigm is not unique to simulation, but is also used in the development of any software intensive system such as combat and weapons systems.  This is possible because they use similar architectures and infrastructures throughout their product lines, with well-defined interfaces to these components.  Furthermore, the developers have members of their technical staff that are familiar with these components.  If we can emulate these processes on a community wide scale, community-wide reuse would then be possible.What must happen in order to enable this?  First, there must be some level of architectural commonality to permit reuse.  Models would require APIs that enable simulation construction without rework.  Second, an infrastructure would be necessary to ease this assembly.  Third, the models themselves will require validation and accreditation to certify the application domain of a particular model so that it may be used only in applications where its results are valid.  Finally, the acquisition culture must be changed to favor a composable simulation environment.  This culture shift may be the true challenge, because it will generate the greatest distress.  Initial steps towards a common architecture and simulation infrastructure have begun.  The DOD’s High Level Architecture for simulation (HLA) provides a framework for the integration of simulations into a larger federation.  The architecture is basically an object oriented systems engineering process which defines the format for data interchange requirements documentation in the Object Modeling Template (OMT) specification, and a Run Time Infrastructure (RTI) to interchange the data.  Similarly, the Joint Modeling and Simulation System  (JMASS) is undertaking a similar task by developing a simulation infrastructure where simulations can be built out of models.  Once models are developed, they must undergo a strict VV&A process to provide the model users with confidence in the products.  The validation portion of the process is to define the domains of application for the models.  If we go back to the wave propagation model illustration.  There are some implementations developed for high-frequency cases.  Therefore, if this particular model is applied to a low-frequency problem, the application violates the assumptions used to develop the model thus, it may not provide accurate or meaningful results.  It is limitations such as this that must be documented during the validation phase of the VV&A process.  Finally, someone must take the authority to accredit the models as suitable for use over these domains.  Unless models are accredited, they cannot be treated as authoritative.  Finally, the cultural aspect of the problem must be addressed.  Developers and procurers must change to facilitate a reuse environment.  The developers will resist this effort because it will force them to do business differently.  Many of the barriers to entry for new activities will be lowered because now the component base is available community-wide.  Procuring activities will resist this effort for several reasons as well.  Doing business this way requires the procuring agency to advocate using an existing suite of models.  The activity that proposes use of particular models, infrastructures, and architectures assume the risk that these components may not perform as advertised.  Historically, the procuring agent has placed this performance risk on the developer by letting the developer propose the components of their design.  If the procuring agent specifies that the reusable components are to be used, they now assume much of this performance risk which they will probably be wont to do.  By nature, the procurement authorities are risk adverse and will push the risk off to the developer whenever possible.  Next, because the procurer knows of the savings resulting from this type of development, it is likely to budget or offer to pay less for the simulation thus, restricting the advancement of the state-of-the-art of the component models.  SummaryIn conclusion, there were several things discussed in this paper.  First, there is a need for the reuse of models on a global scale.  The reuse is to reduce the costs associated with simulation development and to apply the savings to improving the quality of these models.  Given that this is desirable, there are three enabling events that are required to achieve this.  They are: A common simulation architecture that supports the use of reusable numerical models.A global VV&A program for these models, andA cultural shift in the procurement/development of simulations that accepts the implications of this new paradigm.The efforts that are required to bring these events into being are going to be a grand challenge indeed.Author BiographyDavid McKeeby is a Science Advisor with IIT Research Institute’s Modeling and Simulation Information Analysis Center (MSIAC) and is currently the Systems engineer for the Weapons of Mass Destruction Civil Support Information System and is supporting DMSO in the JSIMS T&E program.  He has worked in modeling and simulation since 1985 performing IV&V, numerical modeling, and development of simulation systems for the US Navy and the Federal Aviation Administration and has authored several papers on networked simulation and protocols during that time.  He received a Bachelor or Electrical Engineering from the University of Delaware in 1984 and a Master of Science in Engineering Management from the Catholic University of America in 1988.  