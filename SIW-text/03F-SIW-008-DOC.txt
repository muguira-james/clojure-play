Improvements in Qualitative Assessment for SimulationsDr. Dale K. PaceThe Johns Hopkins University Applied Physics Laboratory11100 Johns Hopkins RoadLaurel, Maryland 20723-6099(240) 228-5650 / (240) 228-5910 (FAX) /  HYPERLINK "mailto:dale.pace@jhuapl.edu" dale.pace@jhuapl.eduKeywords:verification, validation, SME, qualitative assessmentAbstract.  Qualitative assessments are used extensively in simulation verification and validation.  Such assessments come in various forms:  inspections, walkthroughs, peer reviews, subject matter experts (SMEs), “face validation,” etc.  Qualitative assessment was identified by the Foundations ’02 V&V Workshop as one of three areas in which the modeling and simulation community consistently fails to  perform as well as it knows how to do.  This paper identifies areas in which qualitative assessments often fail to be performed properly and suggests how such problems can be avoided.  Particular attention is given to selection and use of personnel for qualitative assessments, to ways that such assessments can be properly combined and related to quantitative assessments, and to qualitative assessment resource implications.  Ideas in this paper are pertinent to both individual simulations and multiple simulations in a distributed simulation.  (Paper: 03F-SIW-008)1.  IntroductionModel and simulation (M&S) validation assessments are either quantitative, qualitative, or a combination of both.  Quantitative validation assessments compare simulation results with data, theory, or results from other simulations.  Credibility for the simulation derived from such comparisons is often limited by uncertainties associated with the data, theory, etc. and with the coverage of the intended application domain for which such quantitative referent information exists.  Consequently, for many situations, quantitative assessments alone cannot provide adequate guidance about simulation potential appropriateness for intended applications.Qualitative assessments of M&S validity involve people who express opinions about the correctness of simulation structure, design and construction, or results.  They may also make predictions to serve as a surrogate for data, theory, or other referent material for use in validation assessments.  In the past, even though qualitative assessment has been the most common kind of validation technique employed in many areas of Defense simulation applications, qualitative assessments usually have not been done as well as they could have been done.  “Best practices” have not been routinely employed.  Foundations ’02 identified this area as one of three basic management challenges for the M&S community (the challenge is to consistently employ best practices so that what should be done is done, vice a research issue where what should be done has yet to be determined before best practices can be identified): “Qualitative assessment involves human judgment in assessment: “peer review,” “subject matter expert (SME)” evaluation, face validation, etc. Often people involved in qualitative assessments are selected and perform their assessments without appropriate credentials and/or formal processes. Methods exist, which if used, can increase qualitative assessment objectivity and consistency.” [1] There are two basic reasons for this undesirable situation.  First, due process has not always been exercised in restricting personnel used in qualitative simulation assessment to those with appropriate qualifications.  Second, too little formality and rigor have been employed in processes used for qualitative assessments.  This unnecessarily limits their reliability and credibility.This paper examines ways that M&S validation qualitative assessments can be improved.  Suggested improvements draw heaving upon three sources:  1) material about subject matter expert (SME) use in the DoD Recommended Practice Guide (RPG) for M&S Verification, Validation, and Accreditation (VV&A) [2]; 2) the A4 paper on SME use in the proceedings of the Foundations ’02 Verification and Validation (V&V) Workshop [3]; and 3) the way that the legal community screens/qualifies and uses technical experts.2.  Varieties of SMEsMany different terms are used for personnel involved in qualitative assessment:  technical expert, peer review, SME, etc.  While there are important distinctions associated with the various kinds of personnel and the terms describing them [3], this paper groups them all together under the rubric of SME since the ideas presented in this paper pertain to all varieties of such assessment personnel.  SMEs come in two basic varieties:  1) those used for advice, and 2) those used for assessment.  SMEs may be asked to provide advice about many M&S areas:  the application domain (its characteristics and functions, principles and algorithms for describing its behavior, data and information sources, etc.), M&S design and development (tools, techniques, resources, etc.), M&S management and use, M&S VV&A (tools, techniques, resources, management, etc.), and more.  This paper does not address issues associated with SME advice, but focuses on SME use in M&S validation assessment.  This is not to suggest that there is no need for improvement in using SMEs for advice; that area also needs improvement.  For example, the field of knowledge engineering has valuable information and insights about ways to properly extract knowledge and insights from experts that are not as widely used in M&S VV&A as they should be.  This is particularly pertinent in using SMEs as a source of advice.SMEs used in qualitative validation assessment serve two basic functions; some act in only one of the two areas, and others act in both areas.  First, they express opinion about the correctness of simulation structure, processes, behavior, results, etc. (e.g.,  a SME may indicate whether simulation responses agree or not with expectations of knowledgeable persons – often this process is labeled “face validation”).  Second, the other function that SMEs serve in qualitative assessment is to predict characteristics of the reality represented in the simulation to provide the basis for comparison with simulation results (i.e., to provide estimations that serve as the validation referent in the absence of real-world data, accepted theory, etc. or to supplement such).3.  SME Qualification and SelectionThe legal community has long grappled with how to sort fact from fiction and how to use expert witnesses.  The American legal system has developed elaborate procedures (sometimes called “rules of evidence”) that help to separate facts from opinion in court proceedings.  Most witnesses are only allowed to testify about facts.  This process is designed to facilitate decisions based upon facts (vice undue influence from opinion) by judge or jury.  M&S VV&A has a similar objective:  to accumulate adequate factual and logical evidence for a sound decision about appropriateness of simulation use in particular applications.Both ordinary (“lay”) witnesses and expert witnesses can testify to facts in court when they have direct and personal knowledge of the facts.  Opinion in testimony from lay witnesses is severely restricted by the rules of evidence.  However, expert witnesses have much more opportunity to express opinion in their testimony.An expert witness is allowed to express opinion in testimony because the expert is “an individual who, by virtue of his or her specialized knowledge and experience, can explain, through competent testimony, a technical matter that lies outside the understanding of the average lay person . . . An expert may base his opinion on facts and documents not in evidence, as long as those facts and documents are reasonably relied upon by experts in his field” [4].  “The factors the Supreme Court listed as relevant to the inquiry [for use in assessing the admissibility of scientific expert testimony] are: 1) whether the expert’s technique can be tested through the scientific method; 2) whether the technique was subject to peer review and publication; 3) the known or potential rate of error of the technique; 4) the existence and maintenance of standards controlling the technique’s operation; and 5) whether the technique had gained general acceptance in the relevant scientific community.”  [5]  Extensive discussion of the rules of evidence regarding expert testimony may be found in legal literature such as the Federal Rules of Evidence [6].The reason that the legal system has so restricted opinion in testimony is the desire to ensure that testimony used for judgment by a jury or judge is as reliable as possible.  This attitude recognizes that opinion from ordinary people can be very unreliable.  The training and experience of an expert witness in a court case is always specified so that that person’s qualifications are known to all, and the expert’s testimony is subject to cross-examination which provides opportunity to identify suspect elements in it.The DoD M&S VV&A RPG [2] defines SME in a manner that is similar to expert witness in court proceedings:  a SME is “an individual who, by virtue of position, education, training, or experience, is expected to have greater-than-normal expertise or insight relative to a particular technical or operational discipline, system, or process, and who has been selected or appointed to participate in development, verification, validation, accreditation or use of a model or simulation.”  Unfortunately, SME selection for M&S VV&A in Defense applications has not always enjoyed the formal qualification requirements imposed on expert witnesses in court; with the predictable consequence that SME qualitative assessment (which is often just opinion) for Defense simulation applications has been notoriously unreliable.Thus, an important way to improve M&S validation qualitative assessment is to improve SME selection processes so that only qualified SMEs are used.  A nomination form can be an effective way to accomplish this because it provides documented rationale for SME selection by making SME qualifications explicit.  This simple process has proven to be quite valuable.  The RPG [2] identifies information that is appropriate in a SME nomination form:  Contact Information:  SME name, organization and position, address, phone/FAX numbers, email, etc.SME Qualifications: education, experience, positions, etc. relating to potential areas of SME useSME Simulation Knowledge:  of the simulation in question and of simulation in generalSME Availability:  for reviews, advice, etc.Additional Information: Other pertinent info4.  Formalize SME Assessment ProcessesThree basic steps to improve SME assessments have been suggested [3]:  1) appropriate orientation of SMEs so that their assessments employ appropriate perspective and context, 2) specification of assessment processes and information sources before assessment begins, and 3) review of the assessments by others prior to finalization of the assessments.Orientation of SMEs is essential.  Regardless of SME technical expertise, the SME needs to fully understand the perspective and context for the validation assessment so that the SME will use appropriate criteria in his or her judgment.  Without such orientation, a SME is likely to simply employ perspectives and contexts with which he or she is most familiar and comfortable, even when those may not be the most appropriate ones for the simulation application at hand.Specification of assessment processes and information sources before the assessment starts is a key factor in assessment reliability and its general acceptance within the concerned community.  Establishment of evaluation criteria, identification of analytic techniques to be used, data sources, etc. help to ensure that the assessment plan is robust enough to ensure that a reliable assessment conclusion will be reached if possible.  Specification of assessment processes may include techniques which examine SME consistency so that when part of an assessment by a particular SME seems out of line (either with assessments by other SMEs or with other aspects of the assessment by that particular SME) it can be given additional consideration to determine how much credence should be given to it.  Various techniques (e.g., [7]) have been developed for evaluation of SME consistency.The most common method for qualitative assessment of simulation results has been to run the simulation and then have SMEs examine the results to see if they look appropriate.  When this has been done, SMEs may have a few data points (from tests, other simulation results, etc.) which they specifically check, but in general the SMEs will not have specifically identified what they expect the results to be before they examine them.  Some [8] have begun to experiment with using SME predictions as a surrogate for data or information that can be used as the validation referent in the qualitative assessment.  In doing so, more formal processes for collecting and organizing SME judgments have been used than is often done.  This has been deemed essential to make the SME estimations more reliable.  Such formal approaches to SME evaluations would probably enhance the reliability of SME assessments in all situations.It has been recommended (e.g., [2] & [3]) that SME assessments be subject to review by M&S developers, users, and others prior to finalization and publication to ensure that they do not include misconceptions or errors, and to provide opportunity for a contrary perspective in the final version of the assessment.  This kind of process has proven useful in preventing avoidable assessment problems and in enhancing acceptance of assessment conclusions.5.  Combining Qualitative AssessmentsFirst, this section provides guidance for combining multiple qualitative assessments.  Then the section addresses combining qualitative assessments with quantitative information.  5.1  Multiple Qualitative Assessments.  For small simulations (or for those for which V&V resources are very limited), only a single SME may be involved in simulation review.  In which case, the guidance of this section is not needed.  For larger simulations, and for those whose V&V resources allow more than one SME to be involved, guidance is needed for how to combine inputs from various SMEs – especially when the assessments by SMEs contradict one another.  The material below is mainly taken from [3].The first principle is to give priority (i. e., more consideration) to the assessment of a SME who is expert in the particular topic of the assessment than to other SMEs.  For example, if two SMEs are reviewing representation of a military radar system in a simulation and one SME is a radar engineer and the other SME is a military officer with experience in using that kind of radar, whose assessment should be given priority would depend upon the specific aspect of the radar representation being addressed.The second principle is to give priority to the SME assessment which has more substantial factual and logical evidence in it.  For example, if one SME showed that the derivation of an algorithm used in the simulation omitted a factor, had errors, or used low-fidelity approximations in it which could cause significant problems under some conditions and the other SME simply showed that the algorithm produced acceptable results for simple conditions, the first SME is the one whose assessment should be given the most weight.The third principle is to make sure that SME assessments are actually addressing the same situation if some form of arbitration among them seems to be required.  In many cases, what seems to be contradictory or inconsistent assessments by SMEs are actually supplementary assessments because the SMEs have focused on slightly different situations.  In such cases, the SME assessments can simply be combined to generate a more complete assessment of the simulation.The fourth principle is to make sure that all SME assessment perspectives are included in V&V reports about the simulation, and explain why more emphasis is placed upon some SME assessments than on those of others.  This kind of candor can be embarrassing at times, but it always enhances simulation credibility and confidence in its capabilities when it is clear that all assessments were given due consideration.Finally, when SME assessments agree, it adds credibility to their conclusions – especially if the SMEs come from different communities.  Credibility of the simulation based upon SME assessments can be enhanced if the diverse background of SMEs is described along with their conclusions about the simulation.5.2  Combining Qualitative and Quantitative Assessments.   For many simulations, quantitative information exists for portions of the application domain addressed by the simulation but is inadequate for a full quantitative validation assessment.  In that kind of situation, it is helpful if quantitative and qualitative assessments can be combined in a meaningful way.  Issues in this area are similar to the validation challenges faced for models of different levels of resolution addressing the same application domain ([10], basically updated and expanded by [11], identifies and discusses these issues).The key to combining assessments from varied sources is coming to grips with the uncertainties associated with each of the assessments.  This is essential for meaningful combination of the assessments.  As noted in [12], there is much room for improvement in this area, even in the kind of simulations that are more inclined to scientific rigor and correlation with experimental data than many other simulation applications – those simulations being the ones used in computational science and engineering, such as computational fluid dynamics (CFD) simulations. Quantifying uncertainty for qualitative assessment is largely virgin territory.  Some aspects of it are addressed by [7] & [8]; but much remains to be done to develop approaches that are generally applicable for broad areas of application.  However one does it, it has to be done if assessments are to be combined in a meaningful way.  Obviously, it would be wonderful if all assessments had only small uncertainties (a few percent, for example); but some may have large uncertainties (orders of magnitude, for example).  Once the uncertainties are specified (even if they can only be estimated very crudely), then assessments can be combined meaningfully.  At this point in time, the key is to be sure that the rationale for uncertainty estimation and for how assessments are combined is fully described and documented.  Such will help the community to better understand how to combine such assessments.6.  A Road AheadThe Foundations ’02 V&V Workshop identified qualitative assessment as an area in which the M&S V&V community does not perform as well as it knows how to do [1].  The suggestions above may appear intuitively obvious, but they are often not followed, and consequently many qualitative assessments do not have as much reliability and robustness as possible.  The issue is, how to cause selection of only qualified SMEs, appropriate orientation, formal assessment processes, and pre-publication review of SME evaluations to become common practice.6.1  Culture Transformation.  Transformation of simulation development and use culture within the Defense community is required.  The early 2003 disaster of NASA’s space shuttle Columbia provides a cautionary tale for Defense M&S VV&A.  NASA had accomplished great things, but had developed a culture that did not ensure it had all needed information for assured success for its missions, did not appreciate the potential significance of events as they occurred, and did not take actions that might have avoided calamity.  Conclusions from investigation into causes of the shuttle disaster are likely to stimulate changes in NASA and its current culture.  Like NASA, Defense simulation has achieved great things; but also like NASA, Defense simulation (especially VV&A) also needs a culture change to ensure that future disasters are avoided.Who can change Defense simulation VV&A culture?  It will take the combined efforts of two groups:  VV&A practitioners and M&S managers.  Neither alone is likely to cause the comprehensive change in culture needed; but working together, they can make it happen.What must they do?  First, all must insist upon consistent employment of current best practices.  The RPG and other sources (e.g., [9]) provide the Defense M&S community with adequate guidance so that both M&S management and VV&A practitioners can understand what current best practices are and insist upon their use.  Those best practices include the things identified in this paper as essential for improving qualitative assessments in simulation validation:  selection of only qualified SMEs, orientation of SMEs, specification of assessment processes, and pre-publication review pf SME evaluation.  No longer should excuses for not employing best practices be acceptable, not even when the excuses are based upon resource limitations or expediency.  Second, more formal assessment processes are required.  Clear and explicit identification of the validation referent is being emphasized more in new versions of V&V-related guidance documents.  This principle also applies to SME qualitative assessments as well as to quantitative evaluations.  Making SMEs be explicit about what they expect before they see simulation results helps to make SME evaluations more reliable because SMEs have to go on record about what is expected and why – this brings more consistency to their assessments because of increased diligence that is used when such exposure exists.Third, emphasizing both pre-publication review of SME evaluations (to eliminate avoidable mistakes in the assessment) and inclusion of contrary views (where such exist) also ensures that qualitative assessments have better reliability than they would otherwise.  The scientific community has used this kind of approach (mainly through peer-reviewed publication) to create a body of reliable knowledge that facilitates continued scientific advance.  The Defense M&S VV&A community would benefit from following this pattern.6.2  Qualitative Assessment Resources Requirements.  It would be wonderful if validation assessments could be done without consuming time or any other resources.  Until warp drives (or other sci-fi fantasies) become reality, such is very unlikely.  If use of a simulation is important, then the level of acceptable risk associated with possibly bum information from it determines the level of resources that can reasonably be expended to ensure that the correctness of simulation results are appropriately understood.  Employing the best practices for qualitative validation assessments described above is unlikely to significantly change the level of validation expenditures.  Using SME nomination forms, orienting SMEs, requiring SMEs to employ formal assessment processes, making SME evaluations available for pre-publication reviews should impose little additional cost on qualitative assessments.  It only requires the will and discipline to do things right!7.  ConclusionQualitative validation assessment is extremely important in M&S VV&A because it is essential for appropriate decisions about simulation use.  There is much room for improvement in qualitative assessment.  Comprehensive and consistent employment of current best practices in qualitative assessment will bring great improvement.  Changing the M&S culture so that such best practices will be routinely employed requires both M&S managers and V&V practitioners to work together toward that objective.8.  References[1]  Dale K. Pace, D. E. (Steve) Stevenson, and Simone M. Youngblood, “Executive Summary,” V&V State of the Art:  Proceedings of Foundations ’02, a Workshop on Model and Simulation Verification and Validation for the 21st Century, October 22-24, 2002, Laurel, MD.  CD published by The Society for Modeling and Simulation, 2002.[2] 	DoD RPG.  2000.  Special Topic on Subject “Matter Experts and VVA&” in DoD Recommended Practices Guide (RPG) for Modeling and Simulation VV&A, Millennium Edition [ HYPERLINK "http://vva.dmso.mil/" http://vva.dmso.mil/].[3]  	D. K. Pace and J. Sheehan.  2002.  “Subject Matter Expert (SME) / Peer Use in M&S V&V,” V&V State of the Art:  Proceedings of Foundations ’02, a Workshop on Model and Simulation Verification and Validation for the 21st Century, October 22-24, 2002, Laurel, MD.  CD published by The Society for Modeling and Simulation.[4]  Marc S. Friedman & Stanley H. Kremen, “Technical Experts in Computer-Related Performance Litigation,” Computer Forensics Online, HYPERLINK "http://www.shk-dplc.com/cfo/articles/perform.htm"  Vol. 1, No. 1, Dec. 1997,  HYPERLINK "http://www.shk-dplc.com/cfo/issue%201/expert.html" http://www.shk-dplc.com/cfo/issue%201/expert.html.[5]  New Jersey Judiciary: njcourtsonline.com, “Notice to the Bar:  2000 – 2002 Report of the Supreme Court Committee on the Rules of Evidence,”  HYPERLINK "http://www.judiciary.state.nj.us/reports/indexsupp.htm" http://www.judiciary.state.nj.us/reports/indexsupp.htm, February 8, 2002.[6]  Federal Rules of Evidence (2003), Legal Information Institute (LII) website,  HYPERLINK "http://www.law.cornell.edu/rules/fre/overview.html#article%20vii" http://www.law.cornell.edu/rules/fre/overview.html#article%20vii. [7]  Osman Balci, “A Methodology for Certification of Modeling and Simulation Applications,” ACM Transactions on Modeling and Computer Simulation, Vol. 11, No. 4 (Oct. 2001), 352-377.[8]  Michael Metz and Scott Harmon, “Using Subject Matter Experts for Results Validation - A Progress Report, paper 02S-SIW-095, Spring 2002 Simulation Interoperability Workshop (SIW),  HYPERLINK "http://www.sisostds.org/conference/View_Public_Number.cfm?Phase_ID=2" http://www.sisostds.org/conference/View_Public_Number.cfm?Phase_ID=2.[9]  American Institute for Aeronautics and Astronautics (AIAA) Guide G-077-1998, Guide for the Verification and Validation of Computational Fluid Dynamics Simulations,  January 14, 1998.[10]  James H. Bigelow and Paul K. Davis, “Implications of Multi-Resolution Modeling (MRM) and Exploratory Analysis for Validation,” in Dale K. Pace (editor), V&V State of the Art:  Proceedings of Foundations ’02, a Workshop on Model and Simulation Verification and Validation for the 21st Century, October 22-24, 2002, Laurel, MD.  CD published by The Society for Modeling and Simulation, 2002.  [11]  James H. Bigelow and Paul K. Davis, Implications for Model Validation of Multi-Resolution Modeling (MRM) and Exploratory Analysis, Rand Report DRR-3045-AF, 2003.[12] William L. Oberkampf, Timothy G. Trucano, and Charles Hirsch, “Verification, Validation, and Predictive Capability in Computational Engineering and Physics,” V&V State of the Art:  Proceedings of Foundations ’02, a Workshop on Model and Simulation Verification and Validation for the 21st Century, October 22-24, 2002, Laurel, MD.  CD published by The Society for Modeling and Simulation, 2002.  About the AuthorDale K. Pace is a member of the Principal Professional Staff of the Johns Hopkins University Applied Physics Laboratory.  Dr. Pace has been extensively involved in modeling and simulation VV&A since the mid-1980s, contributing to development of VV&A processes and technology, teaching VV&A, and being involved in modeling and simulation VV&A reviews and other activities.  He has been part of the DoD VV&A Technical Working Group (TWG) since it began and supports DMSO VV&A endeavors in a number of ways.  He was an initial co-chair of the Distributed Interactive Simulation (DIS) VV&A Subgroup and a member of the initial Simulation Interactive Workshop (SIW) Program Committee as well as being a leader of the SIW Research Development and Engineering (RDE) User Forum for several years.   He also was co-chair of the 1999 SIMVAL workshop sponsored by MORS and SCS and program co-chair for the Foundations ’02 V&V workshop sponsored by more than two dozen organizations. The work reported in this paper was performed under sponsorship of the Defense Modeling and Simulation Office (DMSO), but its views are those of the author and should not be construed to represent views of DMSO or of any other organization or agency, public or private.