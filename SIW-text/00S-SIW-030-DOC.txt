Interest Management Using an Active Networks ApproachDr. Stephen ZabeleThomas StanzioneLitton-TASC Inc.55 Walkers Brook DriveReading, MA 01867(781) 205-7340, (781) 205-7281gszabele@tasc.com, tstanzione@tasc.com Keywords:Active Networks, Interest Management, Declaration Management, DM, Data Dissemination Management, DDMABSTRACT: While distributed simulation infrastructures have evolved dramatically over the past several years to provide ever increasing levels of flexibility, abstraction, and interoperability, the scalability and performance of the simulation infrastructure continues to be a critical limiting factor. In particular, it is now becoming apparent that the limitations of the supporting networking technologies are a significant impediment to achieving the needed scalability and performance. Advancing the state-of-the-art for large-scale distributed simulations therefore requires significant advances both in the underlying network technologies and in the ability of simulations to exploit these new networking capabilities. Towards achieving scalable, high-performance distributed simulations, Litton-TASC and the University of Massachusetts at Amherst, are designing and developing a suite of enhanced network protocols and services that exploit the highly programmable and dynamic Active Networking environment. 1. Active NetworksActive Networks [1] are being touted by many in the industry as the next major advance in computer networking technologies. The core idea behind Active Networks is that placing computational resources directly within the network specifically to support end user needs will not only provide higher performing traditional service offerings, but will allow faster deployment of new service offerings as well as enable a whole new class of service capabilities.1.1 Advantages of Active NetworksFrom a performance perspective, the design of networking protocols and services is fundamentally one of balancing scarce resources (e.g., buffer space, computational resources and bandwidth) in order to optimize performance (e.g., throughput, latency, robustness, and resource sharing). Traditionally, network design efforts have quite reasonably assumed severe limitations on network buffer and computational resources and, as such, have pushed the burden to the end hosts whenever possible. However, since the cost of processing and memory continues to drop at a far faster rate than the cost of link capacity, alternate design points are becoming not only feasible but also increasingly attractive. For example, our work in the reliable multicast area [2] has shown that the placement of small amounts of session-specific computation at key routing nodes can significantly increase throughput, decrease latency, and reduce congestion to make more bandwidth available for other sessions. From a deployment perspective, developing and fielding new network protocols and services has also been a significant undertaking. The current painstaking Internet Engineering Task Force (IETF) standardization process is both slow and cumbersome, and increasingly lags technology development cycles where a "net year" can be measured in terms of a few months. Moreover, when a "best guess" design proves erroneous, correcting and updating the implementation presents a major problem. The Herculean effort by DARPA to get Jacobson’s enhancements into TCP before widespread release of the protocol is the rare exception: the IETF adoption process is the rule. The development and deployment problem is compounded by the unfortunate reality that newer, more complex protocols and services necessarily require testing on larger scale networks with large numbers of users to truly prove a design. Network simulations, while invaluable, are inherently limited because of the all-too-necessary simplifications that must be made in modeling network behavior. Furthermore, laboratory experiments with limited network topologies are generally too small to really understand the full impact and dynamical range inherent in a given approach. Active Networks promises to provide a highly flexible mechanism for deploying and testing new concepts and ideas by allowing their direct insertion into the internetwork fabric itself.1.2 Active Networks and Distributed SimulationActive networks can be used to increase the scalability and performance of distributed simulations in numerous ways. Opportunities to leverage active networks include:Enhanced Network Transport – Our work on the use of active networks for reliable and semi-reliable multicast transport protocols has demonstrated that AN-based implementations can provide substantially reduced latency and significantly improved throughput and scalability over traditional end-host approaches. Since multicast protocols include unicast protocols as a proper subset (i.e., a unicast session is equivalent to a multicast session with only two participants), this work also shows that the performance of traditional protocols such as TCP can be significantly enhanced using active networks.Priority-based congestion management – Routing points within the network are the primary sources of data loss during times of congestion, which can occur when the aggregate traffic from multiple in-bound links exceeds the capacity of a target outbound link for a period of time. The ability to collocate processing at these congestion points provides an opportunity to preferentially discard lower priority packets in favor of delivering higher priority packets (with priority assigned by end users) so that the value of delivered information is optimized and overall simulation fidelity is improved. This contrasts sharply with current networking approaches, where packets are discarded (more or less) indiscriminately when queue overflows occur.Data caching – Caching commonly accessed data sets, such as terrain and weather information, near those entities needing the information can provide substantial performance benefits both in terms of decreased network traffic and in terms of reduced access latencies. Historically this has been done quite successfully using hosts at the edges of the network; however, the ability to physically locate caches along the data paths within the network enables even greater performance gains [3]. Moreover, having direct, detailed knowledge of network topology, latencies, per-link network loading, and the location of information sources (e.g., databases) and sinks (i.e., entities) with respect to the topology greatly facilitates identifying the “best” location for data caches within the network. Computational Caching – The idea of caching information within the network can be extended to include “caching” of those computational entities responsible for generating the information. By moving the information providers closer to information consumers, further reductions in delivery latencies and in overall network traffic can be achieved. Although this notion of entity migration as a means for improving simulation performance has existed within the simulation community for some time, differences in simulation codebases, operating systems, and other implementation-specific components have been major impediments to this approach. In contrast, a key concept associated with active networks is the notion of portable, mobile code that can move through the network and embed itself for execution within network elements. If the mobile code is configured to perform computation on behalf of a simulated entity (e.g,, a missile) then computational caching becomes a real possibility.Consistency maintenance – Simulation fidelity requires that multiple entities relying on a common data set have a consistent, logically coherent view of the data, even when the individual entities are viewing the data at different resolutions. Currently, simulations must perform consistency checking at edge hosts located outside of the network, which imposes additional network overhead as well as processing delay. Active networks enable the placement of consistency checking mechanisms at more strategic, centrally located positions within the network, and thereby provide improved latency and throughput characteristics.Clock synchronization – Simulation-wide clock synchronization is critical for maintaining the accuracy and efficiency of time managed distributed simulations. Currently, all applications must participate in clock synchronization, which in practice is quite difficult to achieve. This difficulty is primarily a result of the inability to accurately estimate the (potentially widely varying) queuing delays within network routers. Using an Active Networks approach, clock synchronization primitives can be located within the network routers where queuing delays can be obtained directly rather than through an estimation process. Hence simulation-wide clock synchronization becomes more accurate, with the positive side effect of further reduced computational load.  Sequence management – Consistent, simulation-wide ordering and presentation of events is also critical for the repeatability and accuracy of time managed, event-based distributed simulations. Currently, all applications must participate in the event management process to ensure that events arriving from multiple sources are processed in the same order by each simulation. This is particularly problematic, since events sent at the same time from two different simulations can arrive in quite different orders at different receivers due to both reordering within the network as well as the more benign differences in delivery latencies between sender and receiver pairs. The ability to perform the needed sequence enforcement once within the network, rather than redundantly across all end hosts, promises to provide more accurate results with less network overhead and with far less aggregate computational load.Interest Filtering - An analysis of packet traces from a Sim2 STOW scenario emulation [4] revealed that overall network traffic could easily be cut by over 60% using otherwise simple  partitioning methods to reduce the amount of irrelevant data transmitted and received. However, achieving this straightforward partitioning  requires a network infrastructure that can support large numbers of interest groups. Unfortunately, this number is substantially larger than the number of groups that can be supported efficiently by the current suite of Run Time Infrastructure (RTI) implementations -- a shortcoming that is first and foremost a limitation imposed by current generation network infrastructures. As such, an active networks-based approach that allows the network to support large numbers of interest groups offers tremendous value. The area of Interest Filtering is of particular interest due to the substantial, quantifiable performance gains that can potentially be achieved. As such, the primary focus of our work (and the focus of this paper) is on developing techniques that will allow network infrastructures to efficiently support large numbers of interest groups using active network technologies.   1.3 Advantages of Active Interest FilteringTo illustrate the potential power of using active networks to provide an active interest filtering capability, consider a simulation scenario of a fast flying aircraft at low altitudes with limited sensor range. With a large number of ground vehicles sparsely distributed over a correspondingly larger geographical region, the aircraft simulation is interested in relatively small numbers of ground vehicles at any given time, but relatively large numbers of ground vehicles over time. If communication grouping is based on a spatial partition of the battle space (as in [4]), the region specification process must weigh the disadvantages of a few large groupings (which will likely include too many ground vehicles) against a larger number of smaller groupings (requiring more frequent maintenance of group membership). In contrast, consider a customized dynamic interest filter in the form of agents injected into selected network routers on behalf of the aircraft simulation. From their locations within the network, these agents can easily examine position information in state update messages from ground vehicles and forward only those within a given distance of the aircraft’s current position. Active interest filters can easily be configured to examine more than simple geographic proximity in making forwarding decisions. For example, consider a JSTARS simulation, which has a large field of view of the battlefield, but is only sensing moving vehicles. The active interest filters can filter static vehicle data before it reaches the JSTARS simulation, so that the simulation need not spend time weeding out non-moving vehicle data. An active interest filtering approach is conceptually flexible enough to implement several forms of multi-resolution processing as well. For example, filters could be used to forward update messages for entities further away on a less frequent basis. Similarly, it could also be used to implement a priority-based congestion management mechanism. For example, state data from SAMs and enemy aircraft would have higher priority over data from lesser ground-based threats due to their substantially higher lethality potential. When faced with a choice of which packets to drop during heavy network congestion, the active interest filter could selectively forward the higher priority packets.Finally, from the perspective of current generation RTIs which rely on IP/Multicast as a convenient and network-efficient means for delivering event data to  interest groups, the only manner in which an entity can control the data it receives is through the joining and leaving of multicast groups.  Technically, this is accomplished through the Internet Group Management Protocol (IGMP) and network multicast routing protocols (e.g., Distance Vector Multicast Routing Protocol (DVMRP), Protocol Independent Multicast (PIM)) which  provide only coarse control over data delivery at relatively long time scales (e.g., 100s of milliseconds to join a group, and tens of seconds to leave a group).  Here, active interest filtering offers a means to introduce fine-time-scale, flexible (programmable) per-entity information filters into the routers’ forwarding function that can react to changes in receiver interests nearly instantaneously. 2. Interest Management within the HLA Within the HLA [5], interest management is achieved through the combined use of the HLA Declaration Manager (DM) services and the HLA Data Distribution Manager (DDM) services. The DM is used by a federate to inform other federates of the types of data it can provide (i.e., publish) and the types of data it needs from other federates (i.e., subscribe). As such, the DM provides class-based filtering to ensure that only data from compatible sources and types are forwarded. For example, the DM provides the mechanisms that allow the JSTARS simulation to enable delivery of data from relevant sources such as ground vehicles while preventing delivery of data from irrelevant sources such as dismounted infantry. The DDM is used by federates to further reduce the delivery of irrelevant data by providing value-based filtering. Here, delivery of data from a publisher to a subscriber requires not only that the publisher and subscriber have compatible data types (per the DM) but also that the publisher’s data values fall within a range that is of interest to the subscriber. For example, the DDM provides the mechanisms that allow the JSTARS simulation to keep from receiving data from ground vehicles that are not moving (based on the value of their motion vectors), even though the source type (i.e., ground vehicle) has been declared to be of interest through the DM.From an HLA perspective, a multidimensional space of attributes used by the DM and DDM defines a Routing Space, and the ranges of attribute values in which a subscriber is currently interested (similarly, for which a publisher is currently providing) defines a volume within the routing space. Volumes defined by publishers are known as Update Regions, and volumes defined by subscribers are known as Subscription Regions. DDM operation is notionally illustrated in Figure 2.1, where an update region U0 has been defined by publisher P0, and two subscription regions R0 and R1 have been defined by subscribers S0 and S1, respectively. Based on these region descriptions, DDM determines that state update information from P0 is of interest to S0 but not S1 and makes this information available to the RTI. The RTI then attempts to ensure that updates from P0 are only delivered to S0, saving network bandwidth as well as processing by S1. Ideally, the RTI takes this a step further to ensure that the only updates from P0 delivered to S0 are identically those that fall within S0’s subscription region rather than the full set of updates that P0 can provide. This can be expressed as the intersection of U0 with R0, as indicated by the shaded region.  Figure 2.1  DDM Publish-subscribe ProcessingNote that region definitions need not be static, and in fact are likely to change over time. For the low flying aircraft example, the aircraft’s subscription region might be expressed as a moving geographic region that overlaps different ground vehicle update regions at different points during the course of a simulation run.2.1 Use of Multicast within HLA-compliant RTIsOften information from a publisher will be of interest to more than one subscriber. For example, a particular moving ground vehicle may be of interest to both the low flying aircraft and the JSTARS simulation. For these situations, current generation RTIs are attempting to leverage multicast technologies. With multicast, messages intended for two or more recipients can be replicated within the network at the point where delivery paths diverge. This contrasts sharply with a repeated unicast approach where senders transmit each message multiple times, once for each interested receiver. Clearly, use of multicast can improve network efficiency as well as eliminate the host processing otherwise required for message replication. It also has the positive side effect of eliminating the need for senders to keep explicit lists of interested receivers, which provides increased scalability. Finally, use of multicast reduces latency and also eliminates a source of latency variability that occurs between release of the first copy of a message and the last copy of a message using repeated unicast approaches. The DDM naturally provides the mechanisms for  identifying multicast opportunities. As shown in Figure 2.2, when two or more subscription regions overlap, the intersection of the overlap area with an update region defines a set of updates that can be delivered more efficiently using multicast.Figure 2.2  DDM Derivation of Multicast RegionsAn interesting insight arises from the notion that update and subscription region definitions can change, and in fact are likely to change, over the course of a simulation. Since the overlap processing that is needed to assess whether a message should be unicast or multicast can add significant computational load, it may in fact be beneficial to deliver all needed messages using multicast, even when the interest group (currently) has only a single member. The motivation for this approach is developed more fully in the following section.2.2 Mapping Interest Regions to Multicast GroupsThe goal of DDM may be thought of as reducing or eliminating delivery of unneeded data to uninterested federates in order to reduce both network bandwidth requirements and receiver processing. With the introduction of multicast considerations, this goal is expanded to include the reduction or elimination of redundant transmissions in order to further reduce network bandwidth requirements as well as to reduce sender processing.Intertwined with the use of multicast is the pragmatic problem of mapping interest groups to multicast groups. This mapping problem carries with it several requirements that stem from robustness considerations. Specifically:No false exclusion - The mapping must be such that all data needed by a federate is mapped to one or more multicast groups to which the federate is allowed to subscribe. Minimal false inclusion – The mapping should be such that the amount of unneeded data a federate receives by joining a multicast group carrying needed data is minimized. Additionally, the mapping must accommodate any network constraints, such as limitations in the number of multicast addresses that can be (efficiently) supported by the network infrastructure. This latter, apparently simple, additional constraint gives rise to considerable computational complexity, and is the key motivation for our work.2.3 Mapping Approaches At least two classes of mapping approaches have been proposed [4], described here as grid-based filtering and object-based filtering. With a grid-based filtering approach, the routing space is divided into a set of non-overlapping cells independent of any update or subscription region definitions. Typically the cells are rectangular in shape and organized in the form of a rectilinear grid as shown in Figure 2.3 (hence the name.) With a grid-based filtering approach, multicast addresses from the available set are assigned to cells a priori. Each publisher then simply transmits messages on the multicast addresses assigned to those grid cells having non-null intersections with the publisher’s update region. Similarly, subscribers simply join the multicast groups assigned to cells having non-null intersections with the subscriber’s subscription region.Figure 2.3 Cell organization for grid -based approachNote that if each of the N dimensions in the routing space is divided into (roughly) L levels, then the total number of grid cells is O(LN). Increasing L provides finer-grained quantization of the routing space, resulting in an improved ability to limit the amount of irrelevant data that might be delivered to a given receiver. The penalty for finer grained quantization, however, is a corresponding increase in the number of multicast groups that must be supported by the network infrastructure. Conversely, decreasing L requires fewer multicast groups, at the cost of correspondingly poorer control over the amount of irrelevant data that might be delivered to any given receiver. The limiting case here is when L is identically 1, corresponding to a pure broadcast scenario where all messages are delivered to all receivers. Trades involving grid size and delivery of excess data are illustrated in Figure 2.4. First, in order to insure that no needed data is excluded, a receiver must subscribe to all interest groups associated with any grid cell overlapping its subscription region. Then, since a receiver can only subscribe to an interest group in its entirety (at least with conventional networking technologies), any areas within a grid cell that are not within the subscription region represent unneeded data that may be delivered to that receiver. These areas of unneeded data are shown as shaded areas In Figure 2.4. Here, the smaller grid spacing (shown on the right) is seen to be better able to approximate the subscription region as it results in a smaller shaded area. In general, a smaller shaded area implies a reduction in the amount of unneeded data delivered.It is worth noting, however, that an accurate analysis of the amount of unneeded data delivered using a gridded approach must consider the amount of traffic that will likely be generated for any given cell over the course of the simulation run. For example, the shaded area on the left of Figure 2.4 may actually yield less unneeded data in areas with little or no traffic than the shaded area on the right of Figure 2.4 in areas with higher traffic density. For this reason, recent use of grid-based approaches within the HLA (e.g., United Endeavor) have employed a non-uniform quantization of the routing space in order to better match grid cells to subscription regions in areas of higher traffic density.Figure 2.4 Trades between grid size and excess dataIn comparison, object-based filtering approaches make extensive use of update and subscription regions to construct (more) precisely tailored interest group definitions. Here, intersections between the various subscription regions are used to identify the set of potential interest groups. Multicast address assignments are then made once the interest groups have been defined. An example of how an object-based filtering approach can use the subscription regions to define interest groups is shown in Figure 2.5 for a three-subscriber case. First, a complete decomposition of the various possible overlaps between three subscription regions R0, R1, and R2 yields a total of 7 possible subregions, labeled r1 through r7. Each subregion then defines an interest group that is assigned a unique multicast address. A subscriber needing data from a given subscription region then simply subscribes to those interest subregions derived from the subscription region. For example, the subscriber having subscription region R0 would then join interest groups r1, r3, r5, and r7 in this example. Similarly, the subscriber having subscription region R1 would then join interest groups r2, r3, r6, and r7, and the subscriber having subscription region R2 would join interest groups r4, r5, r6, and r7.An object-based filtering approach has the clear advantage that each subscriber receives precisely what is needed and nothing more. Additionally, it has the added advantage that no multicast addresses are “wasted”, as addresses are assigned only to those regions of the routing space having actual subscribers.One of the disadvantages of an object-based approach is the same as the grid-based approach in its need for a potentially extremely large multicast address space. In fact, it is straightforward to show that in the worst case J subscribers can yield 2J-1 unique subregions, potentially requiring the network infrastructure to support 2J-1 multicast groups. This clearly presents a scalability problem, since some of the larger simulation runs have included on the order of 400 end hosts (i.e., J>400).  Moreover, the host processing required to perform the region decomposition requires global knowledge of all subscriber regions, and has O(2J) computational complexity worst case. Finally, since subscription regions can change over time, the process of providing subscription region updates, calculating updated subregion definitions, and distributing the updated interest group mappings is ongoing and potentially adds significant overhead in terms of bandwidth and host computation.Figure 2.5 Interest group construction using an object-based approach for J subscribers (J =3)Although the grid-based filtering approach gives rise to situations where a given multicast group may in fact have only one (or even no) subscriber(s), it has several distinct advantages. First, the attendant intersection processing requires only knowledge of its own interests and knowledge of the grid, and as such is intrinsically more scalable than the object-based approach which potentially requires knowledge of all subscription regions in order to function. Second, since the multicast addresses are assigned a priori, it does not incur the run-time overhead of dynamic assignment, dissemination, and recovery of multicast addresses that is required for the object-based approach. Since region definitions can change over the course of a simulation run, both interest group definitions and multicast address assignments are necessarily dynamic using an object-based approach.2.4 Network Implications of Mapping ApproachesThe two approaches share the common problem of potentially requiring a tremendously greater number of multicast groups than can realistically be supported by the network infrastructure. One solution to this problem is to merge the groups into some smaller number of aggregate groups that can be supported, with the goal of minimizing the amount of irrelevant data delivered to the end hosts (and possibly within the network). An initial analysis of an aggregation approach shows that the aggregation operation itself is potentially quite expensive computationally. For example, the number of ways that M groups can be aggregated to form K groups (K<M) where each of the M groups is assigned to one and only one of the K aggregate groups is given by the Stirling number of the second kind, given by:The last term in the expansion of the summation is given by:such that the number of such partitions is approximately:A brute force approach for constructing an optimal aggregation then requires computing and comparing the amount of irrelevant data generated by each of the possible aggregations. Clearly a brute force approach is intractable for most problems of interest: for perspective, there are 1,203,163,392,175,387,500 ways that each of M=25 groups can be assigned to one of K=10 aggregate groups. While “fast” computation of an optimal aggregation may be possible, for example, using linear or dynamic programming techniques (although no workable approach is yet known, due to subtleties in calculating the amount of irrelevant data), the problem is compounded by the fact that an optimal aggregation may actually involving assigning a group to more than one aggregate group – meaning that the Stirling formula may actually provide a lower bound on the computational complexity of the aggregation problem.For illustration, consider the problem where an initial set of three groups (r3, r12, and r15) with a total of four subscribers (S0, S1, S2, and S3) must be aggregated to form two aggregate groups. An example set of  regions, the associated traffic, and the associated subscriber requirements are given in Table 2.1. Note that a value of 1 in a subscriber’s column for a given row indicates that the subscriber needs information from the region associated with that row, whereas a value of 0 indicates that the subscriber does not need information from that region. Also, note that: the “Region Traffic” column indicates the amount of traffic originating from the various regions; the “Number of Subscribers” column indicates the number of subscribers that must obtain information from the various regions (i.e., the sum of the subscriber column values); and the “Required Subscriber Traffic” column provides the product of the “Region Traffic” values and the “Number of Subscribers” values.Table 2.1 Example Group Aggregation ProblemRegionRegionTrafficSubscriber RequirementsNumber ofSubscribersRequired Subscriber TrafficS3S2S1S0r3100011220r12101100220r151111144From the table, each subscriber requires information from exactly two regions, which includes r15 and either r3 or r12, exclusively. This implies that each subscriber has a required traffic value of 11, and that the total required subscriber traffic is 44. The set of all possible, non-trivial, assignments of the three regions to two aggregate groups, where each region is assigned to at least one group, is shown in Table 2.2. Here, values in the “Traffic” column for each aggregate group are calculated as the product of the number of subscribers and the sum of the Region Traffic values for the constituent regions. Values in the “Total Traffic” column are simply the some of the values from the two “Traffic” columns.For this example, the “optimal” configuration providing the smallest total traffic value occurs when region r15 is assigned to both groups, as shown by the shaded row in Table 2.2. The surprising implication here is that the way to minimize traffic for this example requires transmitting data from region r15 twice. Moreover, an equally surprising result is that the total traffic value resulting from aggregation has not increased beyond the total required subscriber traffic value before aggregation – no irrelevant data is being delivered to the subscribers.Table 2.1 Example Group Aggregation AlternativesAggregate Group 1Aggregate Group 2Total TrfficRegionsSubscrs.TrafficRegions Subscrs.Traffic r3S1+S02X10r15+r12S3+S2+S1+S04X1164r3-0X10r15+r12+r3S3+S2+S1+S04X2184r12S3+S22X10r15+r3S3+S2+S1+S04X1164r12-0X10r15+r12+r3S3+S2+S1+S04X2184r12+r3S3+S2+S1+S04X20r15S3+S2+S1+S04X184r12+r3S3+S22X20r15+r3S3+S2+S1+S04X1184r12+r3S1+S02X10r15+r12S3+S2+S1+S04X1164r12+r3-0X20r15+r12+r3S3+S2+S1+S04X2184r15-0X1r15+r12+r3S3+S2+S1+S04X2184r15+r3S1+S02X11r15+r12S3+S22X1144r15+r3S1+S02X11r15+r12+r3S3+S22X2164r15+r12S3+S22X11r15+r12+r3S1+S02X2164It is interesting to note that for some configurations (e.g., the second row entry) the first aggregate group has no subscribers. For these cases, each subscriber must at least subscribe to the second group in order to obtain required information that is not available from the first group. Then, since the second group carries all information required by each subscriber, subscription to the second group is both necessary and sufficient to meet each subscriber’s delivery requirements. Hence, subscription to the first group is unnecessary and redundant.3. Interest Filtering Using Active NetworksEven from the simple preceding example (where aggregating three regions into two involves evaluating 12 distinct alternatives), it becomes clear that aggregation processing itself potentially presents a huge problem. This is particularly problematic if aggregation must be repeated relatively frequently, as may be required with object-based approaches. An obvious alternative to aggregation for resolving the mismatch between the number of groups needed and the number of groups that can be supported is simply to increase the numbers of groups the network can support. A straightforward mechanism for achieving this is simply to “extend the usable address space” by adding sufficient resources at the hosts and within the routers to support the needed number of multicast groups. The mapping problem is then trivial: each grid cell or region is assigned a unique multicast address.Some practical considerations make a straightforward mechanism somewhat undesirable. For example, consider the partitioning of a 700km x 500km x 20km play box (typical of some recent STOW exercises) into 500m x 500m x 500m grid cells. This partitioning yields 56 million grid cells in total, and requires 24 bits of addressing capability. If the gridding also provides separation by other attributes (e.g., vehicle speed, EM emitter frequency, etc.) the addressing requirements easily exceed the 28-bit multicast address limit of IPv4.  Moreover, if a subscriber wants information from all EM emitters over the entire battlefield, this might require joining (at least!) 56 million groups, with serious implications on the number of data structures and associated memory requirements that must be maintained by the host and network routers.3.1 Virtual Extensions to the Usable Address Space The alternative, active-networks based approach we are developing can be thought of a providing a virtual extension to the usable address space. Our approach combines native network support for multicast (to provide efficient delivery to multiple subscribers) with active network support for installing and modifying customizable interest filters within the network routers (to selectively prune irrelevant data from the multicast streams).  The core approach embodies the following key aspects:Coarse-grain Multicast Grouping – The routing space is first coarsely divided into a number of groups that can be reasonably supported by the network infrastructure. This is entirely consistent with interest group management approaches historically used within the HLA/RTI.Embedded Interest Headers – Methods for encoding and organizing information relevant for interest management (e.g., latitude, longitude, altitude, etc) in a simulation-wide common data construct are defined and agreed to by all federates. Then, for each state update sent by a publisher, the data construct is filled out according to its associated interest information and subsequently embedded as header information within the corresponding state update message. Note that a perfectly acceptable mechanism here is to define the interest header as being comprised of state update information already embedded within the messages. In this situation, the header is really a virtual entity that imposes no additional processing or message overhead.Multicast Group Subscription – During the course of the simulation exercise, subscribers and publishers join and/or leave the coarse-grain multicast groups as necessary to assure delivery of needed information. Again, this is consistent with historical interest group management approaches used within the HLA/RTI.Interest Filter Specification – During the course of the simulation exercise, subscribers submit, modify, and/or retract fine-grained interest filters to active nodes within the network. Interest filters are specified as a set of operations performed on interest headers to determine which update messages on a specific multicast group should be delivered to the subscriber. Interest Filter Distribution – Active routers propagate interest filter specifications from subscribers (receivers) to publishers (senders) in the reverse direction along the multicast path from sender to receiver. In general, the active routers merge interest filters from downstream entities (subscribers or other active routers) before sending filter specifications upstream both to reduce the overall number of filters needed as well as to reduce the complexity of the filtering expressions. Multicast Routing – Routers that are not also active routers provide standard support for setting up and maintaining multicast routes in response to join and leave requests from subscribers and publishers. These routers also provide the standard best-effort services for routing multicast packets between interfaces according to its current multicast routing tables. This is also consistent with historical interest group management approaches used within the HLA/RTI.Active Filtering – In addition to standard multicast routing decisions, routers that are also active routers will perform an additional step of running any relevant interest filters to determine whether a packet should be replicated onto a given outbound interface. Hence message replication onto an outbound interface requires both an admissible multicast route through that interface for the multicast group and that the message contain interest information that is admissible by the associated interest filter for that interface.Note that the collection of bits constituting the interest header in update messages can well be interpreted as address information since the bits are used to decide which of a given set of end hosts should be sent the message. Hence, the approach can be viewed as providing a virtual extension to the usable address space. The notional operation of the interest filter specification and distribution functions are illustrated in Figure 2.6. Here, the routing space is represented as larger squares, and needed data regions are represented as colored cells within the squares. Interest filters from each subscriber (receiver) are forwarded upstream towards the publisher (sender). Active routers along the path intercept interest filters from both subscribers and downstream active routers and merge the filters into a single, more general interest filter that is subsequently forwarded upstream. The corresponding operation of the active interest filtering function is illustrated in Figure 2.7. Here, the sender transmits a message with an embedded interest header to the multicast group. (Note: it is assumed that all subscribers shown are members of the group). Upon receipt of the message, each active router compares the information in the interest header with the interest filter associated with each outbound interface. The message is replicated only onto those interfaces with interest filters that will admit the message.Note that in this example, each of the active routers has forwarded an aggregate filter specification that is exact in the sense that no simplifications were made by any of the active routers (e.g., as needed to meet performance goals.) Hence, even though all subscribers are in fact members of the single multicast group, only those subscribers needing the data actually receive the message. As such, no irrelevant data is received, and the system behaves as if a larger number of more precisely specified groups are in use.Figure 2.6 Interest filter specification and distribution3.2 Key Design ConsiderationsKey considerations driving the design of our active networks based interest management approach are now  presented. Issues influencing the design include:Operation with Limited Active Networks Availability – The developed system should work in the absence of any active router support without degrading the nominal level of simulation performance. Our approach meets this need by relying on the existing HLA/RTI multicast delivery mechanisms to ensure that all needed data can be received by each subscriber, and uses active filters only to enhance baseline performance by eliminating unneeded data. Hence the system can operate in the absence of active routers, yet provide enhanced performance even when only a limited number of active routers are available.Graceful Service Degradation - The developed system should continue to function and provide some benefit (albeit degraded) even when active routers are heavily loaded or otherwise have insufficient resources to fully support all requested services. With our design, each active router has the option to “punt” during overload conditions by simply bypassing the filtering operations and routing messages in the usual way. In this situation, all downstream entities are still provided all needed data, with the downside that they may receive an increased level of irrelevant data – but not more than they would have without the active services. Moreover, with this design it is possible to configure filters so that if filtering operations can only be partially completed (e.g, as might be the case within a deadline scheduled environment), the filtering still provides some benefit. Finally, with this design it is possible to construct filters that are inexact but otherwise correct by allowing (slightly) larger “pass” regions in exchange for simpler filter specifications and therefore faster execution.Figure 2.7 Active filtering operations Scalability – The system should scale to support large numbers of users. A key mechanism for achieving scalability (as well as ensuring more robust operation) involves eliminating to the extent possible any need for direct, global exchange of dynamic information. Towards this goal, our design maintains subscriber state information only at the nearest active node, and propagates only merged filter information between active nodes. As such, all, interest filter management is performed locally between neighboring pairs (i.e., between receiver-node pairs and between node-node pairs). This is also desirable in that receivers act directly in their own self-interest to block unneeded data rather than relying on a remote, centralized authority (e.g., a sender) which may become overloaded when the number of receivers becomes too great. Robustness – The system should continue to function correctly in the presence of lost messages (e.g, lost filter specifications), and be able to recover from temporary or permanent host and/or network outages. To address this problem, we are making extensive use of soft-state techniques which require routers and hosts to periodically refresh their filter specifications. Periodic updating assures (at least probabilistically) that any needed state information that is lost due to outages or dropped messages is (eventually) recovered. In addition, we require that in the absence of a required refresh from a downstream entity, active nodes respond by assuming that the entity has removed its interest filter and that all data for the multicast group should be forwarded on the appropriate interface.Router Performance – The developed system should not impose significant performance penalties when active services are invoked. Since the base mode of operations (i.e., operations without active networks support) leverages native multicast routing with small number of groups, routing operations stay on the fast-path (i.e., message processing is performed in hardware). In order to maintain this level of throughput, active filtering operations should also be sufficiently lightweight to allow sustained operations at line speeds. Towards this goal, we have explicitly separated filter specification and distribution functions (which can be performed less frequently and at much lower data rates) from filtering operations (which must be performed on a per-packet basis.) Moreover, we are focusing on filtering mechanisms that are amenable to hardware implementation, and in particular can be performed using programmable hardware (e.g., using the ASICS on Nortel’s Accelar, which is a gigabit-speed router with active network support). By limiting filtering operations to simple bit-field comparisons on predefined interest headers, we expect to keep interest filtering operations on the fast-path with negligible, if any, reduction in throughput.4. Summary We are leveraging the capabilities of Active Networks to provide enhanced HLA interest management services. For distributed simulation applications, an Active Networks approach for interest management offers several important benefits: Because data forwarding is decoupled from multicast route setup, an entity can rapidly and dynamically select which data a router will forward. This selection can be done at a sub-millisecond timescale, as compared to the multiple-second timescale imposed by the existing multicast group join/leave mechanisms. Because each entity can install its own filters, information filtering is accomplished in a “receiver-driven” manner, allowing each entity to customize its filters according to its own need. This decentralized approach allows active filtering to scale well as the number of entities grows large. Because active filtering is performed at a routing point, filtering can also be dependent on the state (e.g., congestion-level) at that router. In particular, this allows both entities and network routers to determine which data should be shed in times of congestion. Active interest filtering also enables an effective means for routers to mediate among conflicting demands.Although we are currently still in the development stages, our end goal is to demonstrate that increased distributed simulation scalability and performance can be achieved with these techniques using an approach that is fully HLA-compliant. Towards this end, we intend to integrate the developed capabilities with an HLA-compliant RTI and demonstrate its improved performance in the context of a meaningful, realistic set of simulation scenarios. 5. References[1]	D. Tennenhouse, J. Smith, W. Sincoskie, D. Wetherall, and G. Minden, “A Survey of Active Network Research”, IEEE Communications Magazine, Vol. 35, No. 1, pp80-86. January 1997.[2]	S. Kasera, S. Bhattacharyya, M Keaton, D. Kiwior, S. Zabele, J. Kurose, and D. Towsley, “Scalable Fair Reliable Multicast Using Active Services”, IEEE Network Magazine, Vol. 14, No. 1., January-February 2000.[3]	S. Bhattacharjee, K. Calvert and E. Zegura., “Self-organizing Wide-area Network Caches”, IEEE Infocom'98, San Francisco, CA, March 1998.HYPERLINK "ftp://ftp.cc.gatech.edu/pub/people/bobby/an/publications/talks/infocom98.ps.gz" [4]	D. Van Hook, S. Rak, and J. Calvin, “Approaches to Relevance Filtering”, 94-11-144, Eleventh Workshop on Standards for the Interoperability of Distributed Simulations, September 26-30, 1994.[5]	Department of Defense High Level Architecture Interface Specification, Version 1.3, DMSO, April 1998Author BiographiesDR. STEPHEN ZABELE is Manager of Advanced Networking and Information Systems at Litton-TASC, Inc., in Reading, MA, where he is Principal Investigator on a number of DARPA projects in the advanced networking area. His current research activities include the design and analysis of scalable reliable and semi-reliable multicast transport protocols, the design and analysis of composable protocols and services for specialized applications using Active Networking technologies, and the design and analysis of techniques for secure group communications in wireless networking environments.THOMAS STANZIONE is the manager of the Simulation Technology Section at Litton TASC, Reading, MA.  He manages a staff of 11 engineers investigating several simulation technology areas including computer generated forces, synthetic environments, software engineering, simulation infrastructure development, and simulation networking.  Mr. Stanzione has a Master of Science degree in Photographic Science from the Rochester Institute of Technology. This work is supported by the Defense Advanced Research Projects Agency (DARPA) under contract N66001-9117-411V. Controlled coarsening of the quantization grid to reduce the number of grid cells, and therefore the number of groups required, can be cast as a form of aggregation.Information needed by subscriber S0 from publisher P0Subscription region R1 from S1Subscription region R0 from S0Update region U0 from P0Routing SpaceData needed by both S1 and S2  (multicast with group of size 2)Data needed only by S1 (unicast or multicast with group of size 1)Data needed only by S2 (unicast or multicast with group of size 1)R1R0U0AltitudeLongitudeLatitudeR1R2Finely Gridded Routing Spacer7r6r5r4r3r2r1R0Subscription Region (Needed Data)Area(s) of Potential Excess Data Delivery (Unneeded Data)Coarsely Gridded Routing Space EMBED Equation.3   EMBED Equation.3   EMBED Equation.3  2) Aggregated into a single interest filter specification by the Active Nodes, and...1) Interest filter specifications from downstream interfaces are...3) Pushed upstream towards the sender.        KeyS    - Sender                 AR - Active Router R    - ReceiverRRRRRRARARARSARR2) Filter on outbound interface evaluates header, allows message through3) Filter on outbound interface evaluates header, discards message1) Message with interest header sent to multicast groupRRRRRRARARARSARR