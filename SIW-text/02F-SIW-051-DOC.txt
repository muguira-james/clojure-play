Approaches to V&V on Millennium Challenge 2002Randy SaundersJohns Hopkins UniversityApplied Physics LaboratoryLaurel, Maryland 20723Randy.Saunders@jhuapl.eduKeywords:Federation, FOM, VV&AABSTRACT:    The Millennium Challenge 2002 (MC 02) Joint Experiment Federation (JEF) is a large collection of dissimilar models.  The federation was built and integrated, as the federates were evolving, over a series of brief integration/test events.  The MC 02 Experiment is a single opportunity to use the JEF to stimulate hundreds of current and experimental C4I workstations.  With no planned scenario, the JEF must simulate whatever course of action the commander can specify through these C4I devices.  V&V of the MC 02 JEF raises a number of unique concerns.  Central to all of them is the need to test a large scope of simulation functionality without a large stand-alone V&V test program.  Typical simplifications based on a specific scenario or statistical measures across a number of executions are not applicable in the MC 02 situation.  This paper examines the issues of V&V for MC 02 JEF, the approaches taken, and the results of these approaches.  Future programs, even smaller federations, can benefit from the processes applied to MC 02 and the perspectives of JEF V&V.Introduction – What is MC 02Millennium Challenge 2002 (MC 02) is this nation's premier joint integrating event, bringing together both live field exercises and computer simulation July 24-Aug. 15, 2002. Sponsored by U.S. Joint Forces Command (USJFCOM), MC 02 focuses on how a U.S. explores the military's ability to conduct Rapid Decisive Operations (RDO) against a determined adversary.MC 02 will incorporate elements of all military services, most functional/regional commands and many DoD organizations and federal agencies. The Secretary of Defense has directed that participants involve elements representative of their future force concepts such as the Air Force's Expeditionary Aerospace Force, the Army's medium-weight brigades and the Navy's "Forward From the Sea" vision.[1]The Role of the M&S Federation in MC 02The M&S Federation, called the Joint Experiment Federation (JEF), is not used directly to produce the results of the experiment.  MC 02 is a live experiment, not a virtual one.  The results of the experiment are produced from the actions of the military personnel involved.  The goal for the JEF is to "Simulate the Joint battle space for the purposes of presenting a realistic operational picture to stimulate the experimental audience, via their native C4I devices, and the white cell players."[2]To accomplish that goal, four top level M&S requirements were identified: stimulate any/all Service C4ISR systems with >30,000 entities; use an evolving version of the DoD High Level Architecture Run-Time Infrastructure (HLA RTI) to link greater than 44 simulations/simulators/tools, distributed among 17 sites; continuous JEF execution 24 hours a day for three weeks; model entities which can be represented in the simulation as a constructive model or as a virtual model of the entity, depending on the desired level of fidelity at a particular point in the scenario.Figure 1 shows the design concept to surround the live audience with a rich simulated environment. EMBED Word.Picture.8  Figure 1 - Simulations from Service Analysis, Training, & Experimentation Communities with Live forces creating a requirements-driven, Joint synthetic battle space.These four requirements drove a system design for a very large scale federation.  A few statistics:Number of participants/workstations: 2950Simulated battle space: 2,400,000 sq. milesMaximum simulated entities: >33,000Different types of entities modeled by all federates:          675 platform types and 450 munition types.Peak RTI Update Rate: >73,000/minConceptual Approach to V&V of the MC 02 JEFThe traditional approach to Verification and Validation (V&V) bases the verification of requirements on the execution scenario. [3] The design of the federation is verified to accommodate the scope of the scenario and then the integrated federation is validated to meet that design. The scenario defined for the MC 02 event is completely free play; thus the scope is bounded only by the imagination of the commanders.  Testing such a broad scope requires the use of judgment to identify detailed requirements for V&V.Since no one can predict what events will occur during the execution of MC02, the accreditation will be based on tests involving “a similar class of situations”. The presumption is that proper model performance in testing situations implies proper results will be produced during the single execution run. Situations are systematically tested and the results validated by subject matter experts (SMEs). Caveats document situations where performance is invalid, and work-around techniques are developed and tested to handle these situations when they come up.In MC 02, the Service proponents selected which models would go into the JEF. Though a program management challenge, this approach streamlined some V&V activities. Each Service was asked to provide V&V information for the models (federates) it brought to the JEF. The Service SMEs were expected to be familiar with the model, and many had direct experience with using their models in other applications. The Service could then be expected to handle V&V of the internal mechanisms of their federates, and federation V&V could be focused on the results of information exchanges between federates.V&V Testing examined how well the federates work together to stimulate a Joint Task Force Headquarters (JTF HQ) through traditional C4I and new experimental C4I systems. The key testing considerations were:Infrastructure - Is data distributed to federates as necessary?Terrain - Does the virtual world match between federates?Representation (enumerations) - Do federates agree on what they are representing, including platforms, sensors, and munitions?Detection - Do federates perceive the situation realistically?Engagement - Do weapons and other actions cause realistic effects?As previously described, the MC 02 JEF interacts directly with hundreds of simulation controllers. With only a single full-scale run planned, testing had to be done under modified conditions/configurations. It would not have been affordable to conduct even a single V&V test run of the whole execution configuration and timeline. The MC 02 program addressed this challenge by taking a continuous approach to V&V testing.  Integration and development for MC 02 was done in a series of "spiral" events.  Each event brought more of the JEF online and tested operations in a distributed configuration more like that planned for execution.V&V testing is done as part of functional tests, critical event tests, and vignettes to maximize applicability to MC 02 Execution. Every day the federation was running for integration, testing, or training purposes was considered a V&V testing day.  Every RTI update or interaction was logged for verification.  The result was over 100GB of data available for analysis. Clearly this volume of data was not examined by hand. Early in the MC 02 development process a detailed Federation Agreements Document (FAD) was written to guide developers in adapting their software to work in MC 02. Verification tools were built following the FAD and used to find exceptions to the requirements.  These exceptions were considered and turned over to developers or controllers as appropriate to be corrected.  This continuous V&V technique proved to be the key conceptual breakthrough in making MC 02 JEF V&V possible with reasonable resources.Limitations to VV&A of MC 02There are limitations that could be covered by VV&A during MC 02.  These limitations were defined by examining the priorities of accreditation. Accreditation of MC 02 relies on much more than just V&V of the JEF. Forces must be credible and effective on both sides. The C4I systems must provide the commander with a realistic view of the battle space.  The JEF is a means to this end, only because it is more cost effective than additional controllers. Controller work-arounds are used when the JEF does not handle a situation correctly to provide realistic results. With 3000 people in the loop at controller and player workstations, the accreditation must cover the net result of the JEF and it's controllers. This is a significant difference from a federation of reference models where the models themselves are accredited to produce valid results autonomously.V&V of JEF is not seeking to show that this combination of models can correctly simulate every aspect of warfare. Rather the goal is to show that as the MC 02 controllers plan to operate it JEF will provide suitable and reasonable stimulus to the task force commander.V&V Testing of MC 02 JEFFigure 2 shows the overall V&V testing solution for MC 02. SMEs document V&V Results when they observed the results they expected during testing. Technical tests are designed from the JEF FAD. Both are used to extract data from a database of archived RTI data made by hlaResults for examination to test the technical and operational validity of the simulation results. EMBED Word.Picture.8  Figure 2 – MC 02 V&V testing analyzes the results of federation operations. EMBED Word.Picture.8  Figure 3 – MC 02 V&V Testing consists of technical tests (blue) and SME observations (gold).Figure 3 shows how the MC 02 accreditation criteria have been allocated to these two V&V testing strategies.Technical TestingA series of 32 technical tests were developed to verify that federates were adhering to the Federation Agreements in terms of the data they sent out and accepted. The technical tests of sent data considered only the form and format of the data sent, not its military correctness. Platforms were checked to see that they put out valid locations, and timely updates/heartbeats. The speed and agility of the platforms were not evaluated in technical tests. These 32 tests were considered across all 18 core simulations, but some tests were not applicable (for example, where an air platform model did not put out sea platforms).  Overall, 94% of the applicable tests were passed. Only 1% of all tests could not be fixed, resulting in a controller work-around.RTI performance tests were run to assess how well the data updates were being distributed across the nationwide, secure, multicast wide-area network.  Archives collected in San Diego, CA were compared with archives collected in Suffolk, VA to measure what percentage of the information was lost to collisions or other risks of unreliable transport.Validation of SME ObservationsSMEs were instructed to observe tests and evaluate the military correctness of the combatant interactions. Where the results observed were correct they filled out a V&V Report (VVR) form.  These VVRs were used to direct analysis of the archive data to confirm that what the SME thought they had observed was actually what was reflected in RTI data.  When SMEs, or controllers, observed an incorrect result from the JEF they filled out an online Trouble Report (TR).  Throughout integration and testing of MC 02 a total of 470 TRs were generated.  The archive data was frequently used to identify which model had made the mistake noted on a TR involving several federates.  When these were cleared, the archive was examined to see that the correct data was present.  For the 2% of TRs that could not be fixed by MC 02 Execution, for schedule or resource reasons, each had a controller work-around documented and tested.Data Validation for MC 02Almost all federates are significantly dependent on input data to configure/parameterize/control their behavior. This reflects general trends in software engineering to improve the configuration stability of source code by giving users the ability to vary conditions, characteristics, and behaviors by modifying data external to the software.Data Validation for MC 02 was done in four areas: forces, the military units available to commanders; targets, the fixed facilities of military significance; terrain, the shape of the virtual battlespace; and enumerations, the lowest level representation identifiers used in MC 02.Data Validation of the ForcesExercise Directives provide a list of the forces that will be available for use in MC 02, at an operational level like an armor battalion. The JEF FAD requires federates to simulate these forces at a lower level of aggregation, typically individual platform entities. In the land combat domain, forces are simulated as individual soldiers. SMEs work with JFCOM to identify their Service’s force at the individual or entity level used by the Service model. The resulting force is realized in the Service federates via initialization file/database/etc. Similar processes are used with the JTASC world-class OPFOR to validate the enemy forces.  Once validated, these forces become the only military objects used for further testing.  Terrain Data ValidationThe MC 02 Experiment takes place on a large virtual battlefield.  This battlefield avoids the use of a real-world place, in political consideration for the governments involved.  As a result, custom data products were built for MC 02 on a large scale (24° x 24°). Both land and sea representations were developed, tested and distributed in both map formats and terrain databases for the simulations. Analytic investigation of the databases identified problems where different formats of the terrain data needed repair to correlate. These corrections were incorporated into the final databases used. Functional Tests were done to confirm that federates use terrain data correctly, matching map locations with simulation coordinates.Target Data ValidationBecause MC 02 is much larger than just the JEF, some data must be validated beyond its use in M&S.  Target data for fixed infrastructure such as bridges is one such case.  This data is used in database form within C4I and experimental C4I systems.  The same data must be used in M&S where the bridges must line up with the terrain (rivers and roads). If the data does not match, simulated bombers targeting a LatLon provided from the C4I system will miss the bridge as seen in the UAV scene and simulated enemy troops will continue to move across it.Representation Data ValidationMC 02 took advantage of Service familiarity with the wide variety of models to benefit from Service V&V.  This advantage came at the price of extensive validation testing of representations across the variety of models.  The JEF used the hierarchical DIS Enumeration system to identify the type of each platform, system, and munition.  This widely accepted scheme requires testing to assure that all the representative types planned for the federation are enumerated with the same values.  MC 02 developed a process to validate all the enumeration data:Platform entities were organized by battlefield function and compared.Weapon fire enumeration lists were built for each function and consolidated.Target entities validated weapon damage effects on an enumeration by enumeration basis (>50,000 individual cases). Once the federation baseline of enumerations was established, an automated tool based on JSAF-SNN was used to scan all the enumerations as they passed through the RTI. Usage statistics were provided to the Federation Controllers. When invalid enumerations were used during any testing, they were reported to the developers or controllers and subsequently corrected.V&V Philosophy from MC 02The MC 02 team applied an incremental philosophy to development, integration, and test that resulted in an opportunity to use an incremental philosophy to V&V.  It was not so much a different process as a spiral methodology that built upon incomplete results.  During the course of MC 02 development some federates were converted from DIS interfaces through gateways to direct HLA connections, and some were not.  The result is a large, loosely coupled federation with lots of flexibility for controllers to respond to an unfolding battle scenario.V&V for MC 02 had a similar spiral path.  There was no V&V Test Script, and no days allocated for V&V testing.  Rather each development or test spiral was an opportunity to collect V&V data.  In early stages of the project these opportunities were often failures.  Sometimes the simulations didn't produce the correct results.  Sometimes the proper data collection and analysis tools weren't available. Results from the first tests were available weeks after the event, whereas the final spiral tests were producing results continuously as the JEF was being tested.  V&V test results that are 15 minutes old are much more actionable to a developer or controller than a report that something went wrong a week ago.ConclusionThis shift in perspective represents one of the most general lessons learned from MC 02. V&V was not an event or milestone in MC 02, but rather a continuous part of the federation engineering process.  While much room for improvement exists in V&V test tools, applying them throughout the program life cycle clearly improved the quality of the V&V assessment of MC 02.References[cit] Author. (year). Title of Citation. Publisher. City.[1] Joint Forces Command, (2002) Millennium Challenge 2002, USJFCOM. Suffolk, VA http://www.jfcom.mil/main/about/experiments/mc02.htm[2]Joint Forces Command, (2002) MC 02 Exercise Directive Message #9, USJFCOM. Suffolk, VA[3] Simone Youngblood, (1999) A VV&A Process for the HLA FEDEP, SISO 99F-SIW. Orlando, FL http://www.sisostds.org/doclib/doclib.cfm?SISO_FID_3638Author BiographyRANDY SAUNDERS is a Senior Staff Engineer at the Johns Hopkins University Applied Physics Laboratory.  He has over 20 years of experience in the design, implementation, and integration of high fidelity simulations for military and business customers.  He received his M.S. degrees in Engineering from Harvey Mudd College in 1980 and in Computer Science from the University of Southern California in 1985.  Mr. Saunders has been involved in distributed simulation standardization since the first DIS Workshop, both DIS and HLA standards committees, and as the initial Technical Area Director for the Real-Time Platform Reference Federation Object Model (RPR FOM).  Simulation Interoperability Workshop – Fall 200202F-SIW-051		