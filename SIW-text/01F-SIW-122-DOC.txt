A Technology roadmap for computer generated actorsSheila B. Banks, Ph.D.Martin R. Stytz, Ph.D.Calculated InsightAir Force Research LaboratoryOrlando, Fl  32828Wright-Patterson AFB, OH(407) 353-0566(937) 255-2811 x4380sbanks@calculated-insight.commstytz@worldnet.att.net, mstytz@acm.orgAbstractThe operation of high fidelity and reasonably priced computer-generated synthetic environments relies heavily upon computer-generated actors (CGAs).  Unfortunately, the pace of technological development and the level of expected performance continue to rise at a much faster rate than the capability improvements.  This ever-increasing expectation for fidelity and realism in computer-generated actors is fostered by the growing role that modeling and simulation plays in a variety of areas.  These areas include training, analysis, procurement, mission rehearsal, doctrine development, training, force structure analysis, and logistics analysis.  In these and other uses of synthetic environments, computer-generated actors play a central role because they can significantly increase the realism of the environment while also substantially reducing the cost of operating the environment.  The progress made in addressing computer-generated actors technical challenges and the technical areas that should be the focus of future work are the subject of this paper.1.  IntroductionDistributed virtual environment (DVE) or distributed simulation technology [Sty96] enables real-time human interaction with complex synthetic environments.  The models used to represent individuals, systems, and the environment in these virtual worlds interact and move using computable models of real world physical constraints such as gravity, solid matter, friction, and aerodynamics.  When considering virtual environments, it is useful to consider the major functional components of a distributed virtual environment technologies.  In general practice, the major technical components are the following: 1) virtual environments technology, 2) software architecture, 3) traditional simulation, 4) networking, 5) phenomenology, 6) real world activity, and 7) computer-generated actors.  While all of these technologies have a significant role to play in assembling and executing a simulation environment, the focus of the work reported in this paper is the crucial technology of computer-generated actors.In this paper, we review and examine the technical progress that has been achieved in computer-generated actors (CGAs) and make considered recommendations for further focused research.  Computer generated actors (also called semi-autonomous forces or computer-generated forces) are computer controlled actors in the DVE that respond with human-like behaviors to activities in the distributed virtual environment.  We focus on CGAs because they are crucial to developing realistic simulation environments and because CGAs are crucial to achieving a sense of presence  ([Fla98], [Fre99], [Hee92], [Zah98]).  CGAs provide the bulk of the actors in the DVE and their behaviors largely determine if the DVE will achieve its objectives.  CGA behaviors are founded upon knowledge acquisition activities related to the environment to be simulated.  Artificial intelligence techniques are used in conjunction with the knowledge to analyze the situation that confronts a CGA and to select an appropriate response to the situation in light of the CGA’s role, goals, mission, and skills.  To attain the best possible CGAs, they should be composed of models for physical motion, phenomenology, sensors, and human behavior.  These criteria insure that the CGA portrays the actual operation of its real-world counterpart as closely as possible and operates within a virtual environment that accurately portrays the real world.  The need for correct human behavior modeling insures that the CGA’s operation is indistinguishable from the operation of a human-controlled actor.  Consideration of these criteria points to a broad-based set of requirements for CGA systems and their capabilities.  The requirements are the following: 1) modifiability, 2) high fidelity representations, 3) adaptive decision making, 4) automated incorporation of past reasoning into the decision process, and 5) human behavior modeling.  These requirements indicate that flexible software systems, advanced reasoning capabilities, and comprehensive human behavior models are crucial to current and future successful operations of CGAs and the environments they populate.Before turning to a review of technical achievements, permit us to briefly discuss the key element of any successful CGA, its behavior.  Correct behavior is achieved using adaptive decision mechanisms and comprehensive, expandable human behavior models.  In a CGA, the decision mechanism must be structured so that the amount of information considered when making the decision can be adaptively varied and so that additional behavioral possibilities can be considered as time and circumstances permit.  However, the most difficult and expensive challenge to be addressed is modeling human behavior.  Our progress and achievements in these areas have been far from adequate.  Human behavior modeling addresses the need for making the behavior and reactions of a CGA seem realistic by developing models that yield a reasonable analog of the output of the human decision-making process.  A human behavior model, or representation, is a computer-based, computable model whose outputs faithfully mimic the behavior of a single human or a group of humans executing a task or set of tasks.  Human behavior modeling requires the broad-based acquisition of domain-specific knowledge that humans use in the decision-making process.  To be effective and computationally efficient, human behavior models for CGAs are required at several levels of fidelity and aggregation.  The models are needed for individuals, small organizations (flight, platoon, etc.), individual vehicles, groups, command and control elements, and very large organizations.  Finally, acceptable fidelity for a human behavior model can only be attained by incorporating skill level representations, stress factor representations, and situational psychological representations at some level of fidelity.  The resulting comprehensive models can be used for all types of actors, from the individual to the large aggregate.  Incorporation of rich, flexible, and comprehensive human behavior models into CGAs will lead to better behaviors for CGAs and result in a more accurate performance on the part of the CGAs.In light of the challenges facing the distributed simulation and CGA development communities, we concluded that we should take stock of accomplishments and reckon the most significant challenges that yet remain.  The purpose of this paper is to help organize and review the components of CGA technology and to suggest some fruitful future directions for research.  We will not rank research contributions or present a criticism of results, but rather to provide a sense of the accomplishments that have been achieved and of the challenges that remain before us.  We have included as comprehensive a set of references at the end of this paper as space permits.  This paper is organized as follows.  Section Two contains a review of reasoning system techniques and architectures.  Section Three contains our review of human behavior modeling approaches.  Section Four contains our conclusions and recommendations for future research.2.	Reasoning System Techniques and ArchitecturesIn this section we will examine a few of the most significant CGA reasoning system architectures.  A reasoning system architecture describes the type of reasoning methodology employed, how information is presented to the decision mechanisms, knowledge storage, planning, learning and how the actions to be performed as a result of decisions are distributed to the rest of the application.  In this sub-section, we will examine a few reasoning system architectures and focus upon the decision mechanisms that each is designed to employ.TacAir-Soar ([Tam95b], [Jon99]), is currently the dominant aircraft/pilot CGA.  TacAir-SOAR is a symbol-processing, rule-based system constructed upon the SOAR architecture. The overall goal for the TacAir-SOAR project is to develop humanlike behaviors in pilot CGAs that operate within military DVEs.  SOAR uses production rules to store long-term knowledge.  Rules are also used to propose, select, and apply operators.  All tasks in SOAR ([Lai87]) are attempts to achieve goals in a problem space(s).  Each problem space consists of a set of states and a set of operators. States represent situations.  Operators act as goals, which are decomposed by SOAR’s rules into primitive operations that execute as soon as they are discovered.  Operators perform the basic acts in the system, such as modifying internal state, generating primitive external actions, or performing complex actions.  Whenever operator-selection knowledge is insufficient to permit the selection of an action, an impasse occurs.  The impasse leads to the creation of a subgoal to determine the next operator.  Additionally, if an operator is too complex, an impasse also occurs, which leads to the creation of a subgoal to resolve the complex operator.  All non-decision capabilities for TacAir SOAR are provided by ModSAF.One of the earliest CGA reasoning system architectures is the hybrid system described by Chaib-draa, Paquet, and Lamontagne, [Cha93].  Their architecture, called RPD, allows an individual agent to combine reactive, planning, and deliberative (RPD) decision-making.  The RPD architecture has five principal components; 1) perception-action, 2) recognition and identification, 3) decision making, 4) planning, and 5) knowledge base.  A procedural reasoning system determines the procedures to run.  A rule based reasoning component is used to recognize situations and classify them in terms of the CGA’s goals.  A case-based reasoning component is used to adapt old situations to meet new demands.  Decision making using cognitive maps is used to reflect decision maker beliefs.  Case-based reasoning is used for re-planning.  The architecture has three layers, one layer each for reactive, planning, and deliberative reasoning.  The reactive level is used to tie perception directly to action and to tie perception to identification to action.  The planning level is used to mediate the connection between perception and action and supports handling familiar situations.The architecture presented by Becket and Badler, [Bec93], uses a four level hierarchy of asynchronous control levels to model behavior.  Symbolic reasoning is used at the top of the hierarchy to perform abstract reasoning and planning and operates as an incremental planner.  Behavioral control is used at the bottom of the hierarchy to provide motion control and to perform precise tasks.  The hierarchy enables the system to address the individual shortcomings of the two approaches.  Below the symbolic reasoning level in the hierarchy is the object-specific reasoning level.  The object-specific reasoner is used to resolve issues of intention that arise when applying an action to a situation.  The next two levels of the architecture perform non-symbolic reasoning.  The third level executes reasoning about behavioral abstraction and couples the higher levels and their planning functions to the lower levels wherein actions are performed.  The fourth, and lowest level, performs perceptual, control and motor behaviors.  The levels of the hierarchy operate asynchronously but are only partially de-coupled, each level is required to monitor the output of the levels below it.Ge, James, and Nerode, [Ge95], introduced a multiple-agent hierarchical command architecture (MAHCA) for instantiating computer-generated forces.  Their architecture assigns a software agent to each component (fire team, squad, platoon) of a complex environment.  To aid in planning, each agent has its own mathematical model of the local unit and its own cost function for the unit’s state.  Each agent is responsible for developing a plan based upon its cost model and activities in the environment.  During operation, each agent observes its unit and environment and changes its state as a function of its observations of the environment and the messages from other agents.  The agent then uses this data to develop a plan that minimizes its cost function.  The communication between agents contains adjustment terms for each agent’s cost function and constraints issued by agents higher in the command hierarchy.  Plans are developed based upon three classes of variables; sensor system variables, message content variables, and orders variables.  In their system, cooperation is achieved through information passing.McIntyre and Middleton, [McI95], argue that CGAs must provide correct behavioral cues to the participants in a DVE.  Therefore, they assemble scenarios by using decision trees to represent the options available to each CGA in response to the actions of other actors.  Their work was performed within the context of the Integrated Unit Simulation System, IUSS.  In IUSS, the common basis for all effects is the psycho-physiological state of the individual and how that state relates to the soldier’s performance.  In IUSS, a mission is represented as a task network, where the tasks follow the form defined in the Battlefield Operating Systems/Tasks in the Army Training and Evaluation Program (BOS-T/ARTEP).  This form of task representation permits the system to dynamically adjust task performance throughout the execution of a scenario, which can be exploited to improve CGA behaviors.  Because IUSS has no man-in-the-loop capability, behavioral fidelity chiefly addresses the decisions made and the behaviors exhibited.  In IUSS, all the decisions and behaviors of the simulated soldiers reduce to a set of yes/no decisions concerning environmental state and appropriate response.  These decision processes are arranged in a decision tree that provides successively finer resolution of decision/classification choices.  In the tree, the factors that describe the environment are cues to the soldier’s behaviors and the behaviors are specified by the mission task structure.  In IUSS, the decision trees hold scenario mission descriptions that provide behavioral detail down to the level of individual taskings, taskings are supplemented/augmented by first level (leaf) decision trees that define individual behaviors within a task.The AISim CGA system, described by Kocabas and Oztemel ([Koc98]), simulates an F-16 performing combat air patrol and escort and is based on ITEMS and InterSIM.  AISim consists of two main modules, Situation Assessment and Action Management.  The Action Management module has eleven operators arranged in a hierarchy, at the top of the hierarchy is the Task Control Operator, which implements a hierarchical control structure.  The task structure captures four goals; mission, task, subtask, and activity.  The Situation Assessment module collects relevant information about all of the state information for entities in the distributed virtual environment.  The state of the DVE is compiled ten times per second and sent to the Task Control Operator as a message list.  The Task Control Operator examines the list and determines the task operator that should activate.Howard and Lee, [How98], discuss the architectural features of the Canonical Commander Model (CCM) that they developed.  The CCM extends ModSAF and inserts CGA commanders into military simulation environments.  The architectural features of the CCM system include a modular design, parameterized software modules, an embedded expert system, and an execution-time coordination table.  Their cognitive architecture has three levels, perception, reasoning, and action.  At the perception level, sensor input and messages are used in conjunction with prior commands to perform analysis.  The reasoning level performs planning and action selection based upon perceptions and prior planning using their fuzzy logic server.  The action level uses the action selection and planning outputs to generate commands and operations orders for subordinate units.  Planning is performed using a hierarchical task network representation that can plan to achieve multiple goals with multiple constraints against an ongoing adversary that can force re-planning.  Re-planning can take the form of either adopting standard operating procedures to address a situation or in actual re-planning of a portion of the mission.  The hierarchical task planner contains a set of rules that specify how to perform task reduction; the planner always starts its computations with a plan composed of a set of non-primitive actions and primitive actions.  Re-planning is accomplished using the same techniques as used for initial plan formulation.  Knowledge is represented within the system using fuzzy logic tables, with each table corresponding to a single decision.The expense and difficulty encountered in developing and extending knowledge bases led to the concept of composable behaviors for CGAs, as discussed by von der Lippe and Courtemanche, [Von98].  Their project, called Composable Behavioral Technologies (CBT), allows users to compose custom behaviors for CGAs and to develop a repository for behaviors.  Their system consists of five components; 1) Behavior Repository, 2) Behavior Engine, 3) Semi-Automated Forces (SAF) Specific Services, 4) Execution Engine; and 5) Execution Tool.  These components exist and execute independently of the CGA so that they can provide greater re-usability, the only architectural location for CGA dependencies is in the SAF Specific Services component.  The Behavior Repository provides persistent storage for behaviors.  The Behavior Editor allows the user to construct a BRG graphically using a logic diagram approach.  The Execution Tool allows a user to create a mission and schedules the execution of behaviors.  The SAF Specific Services component provides synchronous and asynchronous communication between the CBT and the CGA system that it is supporting.  The foundation for their work is the Behavior Representation Grammar (BRG), which contains the instructions for executing composite behaviors and implements a control-flow behavior model.The quest for realism in the behavioral modeling of command entities is the focus of the work by Gillis, [Gil98].  Gillis describes the Command, Control and Communications Simulation (C3SIM) system and a human behavior model hosted within this constructive simulation.  Gillis argues that any simulation representation of unit behaviors must consider the stress of battle, fatigue, time pressure, danger, individual differences, and level of training and must also represent superior performance brought about by individual differences and superior training.  Therefore, the behavior model used by Gillis is based on a cognitive reservoir theory that uses an effectiveness ratio computed using the product of interactions between battlefield stressors, circadian rhythm, performance use, confidence building events, training, and experience.  The effectiveness ratio is then divided by the number of potential courses of action, which yields a probability of a correct decision.  A timing variable is used in conjunction with the effectiveness ratio to determine the time lag for situation awareness activities and for course of action selection.  The model also incorporates physical stressors and task stressors.  The stressors are moderated by training, personality, unit cohesion, leadership, and social supports.  In C3SIM, situational awareness and assessment is accomplished using fuzzy sets.Busetta, Ronnquist, Hodgson, and Lucas, [Bus99], present the JACK Intelligent Agent framework.  The Jack architecture is based upon the Belief-Desire-Intention (BDI) architecture for human behavior modeling.  The JACK agents are not bound to a specific communications language, instead they use object-oriented middleware, such as CORBA, and message-passing structures.  The JACK architecture consists of architecture-independent facilities and extensions.  The Jack extensions include a number of keywords to identify the main components of an agent, statements for the declaration of attributes and characteristics, statements for the declaration of relationships, and statements to manipulate an agent’s state.  The authors also defined a set of classes to manage concurrency between agent tasks, support multiple agents, enable specification of default behaviors, and permit specification of a communication infrastructure.  The BDI architecture used by JACK employs a model of human behavior that considers humans to have beliefs about the world, desires to satisfy, and intentions to act.  BDI systems differ from purely deductive systems and other traditional reasoning systems in that they model intentionality, which allows it to effectively model behaviors centered on standard operating procedures.  In JACK, the mental state is defined by determining the external events that drive the agent, the goals that the agent can set for itself, the beliefs that influence the adoption of plans, and the procedures required to accomplish tasks.The Synthetic Forces Behavioral Architecture described by Page and Widdowson, [Pag99], has standard interfaces between components so that behavioral models can be re-used between applications.  They define a means for supporting a distributed behavioral model and providing a capability for communicating detailed goals and world views between CGAs.  Within the architecture application locations are not fixed on the network and instead applications are given the tools they need to find services that they need.  In general, in their approach every communication between two models requires two translations, one from the first model’s representation to the standard format and another from the standard format to the second model’s representation.  The standard format is based upon the author’s unsubstantiated claim that only a few aspects of a model must be considered; 1) sensors and effectors, 2) goals, world-view and decision-making, and 3) communicators.  The sensors and effectors allow a behavioral model to control the physical aspect(s) of a simulation.  The goals and world-view portion contains the specification of the state of the beliefs about the world held by the model and the objectives for the model’s actions.  To provide the mechanisms to move the goals and world-view into and out of a model, the architecture uses an interface component to handle communications.  In their model, goals drive the decision-making process and the selections made by the decision-making processes are informed by the world-view.As the preceding summaries demonstrate, reasoning system architectures have been developed with the objectives of helping to limit the decision space and providing timely information flows for the reasoning system, supporting a single decision-making approach, and, most recently, enabling composability of behaviors.  The more popular/widespread approaches for reasoning system architectures are ones based on rules and task frames and those using production systems coupled with a hierarchy of operators.  Decision trees and case-based reasoning have also been investigated, usually in conjunction with task frames.  Few of these architectures support learning by the reasoning system.  Agent architectures have been investigated from the point of view of a complex agents with multiple capabilities as well as simple agents with limited capabilities.  In the effort to achieve composition of behaviors and to control the complexity of decision systems, some have explored the use of primitive behaviors that are then scaled or combined to achieve complex behaviors.  Other approaches to composability have included separating the behavior mechanisms from the remainder of the CGA’s decision mechanisms and then using a grammar to structure the composition.3.	Human behavioral/ Cognitive ModelsHuman behavior modeling addresses the problem of computing realistic behavior for a CGA.  The objective for the model and the computation is to produce a reasonable analog of the output of the human decision-making process.  Human behavior modeling requires the acquisition of knowledge about the domain models and information that humans use in the decision-making process, such as doctrine, tactics, and knowledge models.  In this section we briefly examine a few human behavior models.The Action/Cognition Behavior Model (ACBM) described by Landweer, [Lan93], performs three main functions within a DVE; scenario control, force simulation, and scenario development.  What Landweer calls generic functions are used to represent human functions; a generic function allows a CGA to perform a simple activity, complex behaviors are composed from the simple functions.  To increase the realism of the behaviors of ACBM CGAs, the actors do not have direct access to the ground truth (actual state) of the simulation world.  Instead, each actor has its own representation of the DVE that is based upon its defined perceptual capabilities.  The cognitive functions available to an ACBM CGA are 1) notice, 2) digest, 3) react, 4) ponder, and 5) review.  To store information and associate the information with a time frame, iconic, short-term, medium-term, and long-term memory are used.  Within the ACBM, thinker objects associated with each cognitive function are used to move information from one stage of memory to another.LaVine, Laughery, Young, and Kehlet, [LaV93], address the issue of modeling performance degradation caused by environmental stressors.  The methodology that they developed to assemble a model has three major parts; 1) a task classification taxonomy, 2) a questionnaire to determine the relationship between performance and physiological response based on stressor and time since exposure, and 3) a task network model.  The task taxonomy consists of five skills, which are attention, perception, psychomotor, physical, and cognitive skills.  The ability to perform each skill is rated on a seven-point scale by a subject matter expert using a questionnaire.  The data from the questionnaires are used to develop functions that describe degradations in performance of specific tasks over time.Weaver and Muller [Wea94] developed on approach to decision-making modeling that is composed of four parts:  the characteristics of the decision, the perceived environment, a neural network for decision making, and three output variables that scale the decision.  The external inputs into the decision are the intrinsic classification characteristics of the decision and the environment for the decision.  These data are then combined using a model of human decision making to characterize the decision, in their system they use a neural network.  They chose neural networks for decision making because they claim that the neural networks’ non-linear characteristics match the characteristics of decision making.  The system has three output variables, the neural network can provide either a specific outcome value or a probability distribution for each of the three variables.  The first of the variables is the timeliness of the variables, the second is the quality of the decision, and the third output is the quantitative aspect of the decision.Parsons, [Par94], uses fuzzy logic control technology to simulate operational command decision making in the Maneuver-Warfare Analytical and Research System (MWARS).  In Parson’s approach, fuzzy logic is used to perform analysis on intelligence data and situation reports.  A fuzzy logic controller is used to correlate and to store incoming intelligence data and to convert these inputs into a value called combat power.  Combat power indicates what the CGA commander believes to be true about the distribution and effectiveness of enemy forces.  Using the combat power information, the CGA commander computes route assessment values and area assessment values to assess routes and enemy strength.  Another fuzzy logic controller computes updated assessments from the situation reports.  The situation state fuzzy controller computes fuzzy values for mission status and psychological status using values for the current intelligence assessment and the unit status values.  The psychological status variable is especially important to Parson’s work since it provides insight into the effectiveness of the maneuver warfare operations being conducted by the CGA commander.  The psychological status variable is used compute a fuzzy value for a unit’s willingness-to-fight.  The outputs of the two fuzzy logic controllers are used by the decision engine fuzzy logic controller to simulate the commander’s decision making function.  Non-fuzzy rules are used to de-fuzzify the assessments and convert the assessments into mission orders, location assignments, and air/fire support allocations.Fogel, Porto, and Owen, [Fog96], discuss the use of evolutionary programming for human behavior modeling in CGAs.  They employed a Valuated State Space matrix to score potential actions at each time step and to represent the enemy’s mission.  They use the matrix in conjunction with evolutionary programming to generate alternative courses of action.  The mission is represented as a hierarchy of measurable parameters and a normalizing function that operates across the levels of the hierarchy.  The Valuated State Space and the normalizing function together indicate what to measure, how accurate the measurement must be, and how to combine the measurements into a single assessment of a situation.  The approach also identifies open problems for the CGA and assigns values to prospective solutions.  To solve problems, they use evolutionary programming.Rajput and Karr, [Raj96], investigated achieving cooperative behavior between entities using a Decentralized Control Architecture (DCA).  In their approach, entities cooperate directly without direct supervision by a commander.  The commander CGA makes the behavioral decisions for its unit and conveys decisions to subordinates, then each subordinate executes the decision independently.  In this approach, the authors claim that unit behaviors emerge as a result of direct cooperation.  Their approach to achieving cooperative behavior is based upon finite state machines coupled with the use of events within simulated radio messages and observation to enable cooperation.  To minimize synchronization problems that can occur when internal and external events are interleaved for execution, they maintain separate queues for each type of event, with execution priority given to internal events.  They can achieve complex behaviors by assembling a hierarchy of finite state machines that each has their own set of responsibilities and that cooperate and communicate with each other to achieve the objectives and tasks assigned to the commander.Zachary, Ryder, Hicinbothom, Santarelli, Scolaro, Szczepkowski, and Cannon-Bowers, [Zac98], describe the behavioral simulation of a team of Naval watchstanders for the purpose of providing a real-time evaluation environment for trainee activities.  They use the COGNET representational system coupled with the GINA simulation environment as the basis for their system.  COGNET is a computational cognitive framework that has four parts, a theoretical part, a description language, knowledge elicitation methods, and behavioral/cognitive simulation tools.  The COGNET theory proposes that cognitive processes are the result of the operation of a computational mechanism on a set of symbols that represent a portion of the world.  In COGNET, human information processing is modeled in two parts, a cognitive architecture and a set of symbols operated upon by the architecture.  The cognitive architecture separates human information processing into three components; perception, cognition, and motor action.  The perceptual process receives information from the environment, turns it into internal symbolic information, and places the symbolic information into the information store.  The cognitive process manipulates the symbolic representation using internal knowledge.  The internal knowledge indicates how to “chunk” the environment, how to achieve task goals within different contexts, and how to recognize situations when a task goal is relevant.  The pieces of procedural knowledge are unified by a common representation of the environment, which they call the mental model.  The mental model is implemented as a series of blackboard panels.Booker, Burke, and Hughes, ([Boo99]), present a decision-making framework for simple combat behaviors for CGAs based upon the concept of motivational control.  In human behavior modeling, motivational control is based upon the observation that internal influences are powerful guides to how action selection mechanisms operate.  Their work is based upon the d-r-k model, in their implementation of the d-r-k model the strength of a motivation is indicated by the deviation of some internal variables from user-defined preferences and the actor is motivated to reduce deficits by taking action.  In the model, given a set of deficits, d, availability parameters, r, and accessibility parameters, k, the optimal solution to reducing the deficits is to formulate the problem as a control problem and compute the d-r-k products for each possible combination of parameters.  The optimal solution is the d-r-k product with the largest value.  In moving the d-r-k model into the CGA arena, they focused on characterizing deficits as doctrinal factors that influence tactical operations.  They treat the repertiore of tactical behaviors as resources and threats as stimuli.The Land Formation Model (LFM) described by Grant, Fisher, and Pearce, [Gra99], models decision-making behavior, physical behavior, communication, and interaction between individuals, between organizations and between constituent parts of the organizations.  The LFM is based upon CABLE, a distributed agent architecture that uses multiple, co-operating software agents to represent human interaction.  The LFM architecture uses functional unit agents (FUNs) as a coordinator to manage the passage of time, and a presenter to manage the user interface.  A FUN is a special type of agent used to represent an organization.  All interactions are specified using an Interaction Component, which is a text description of an interaction and all the data associated with the interaction.  In the LFM model, real-world roles are modeled by an Intelligent Behavior modeler.  The Intelligent Behavior modeler uses a role-specific description of behavior functionality, the Interaction Component it received, and the domain objects that constitute its perception to determine actions and decisions.  To interact with the DVE, an actor creates one or more actions for physical behavior or for organization behavior.  Actions are sent to the agent assigned the role(s) called for by the action.Heinze, Lloyd, Goss, and Pearce, [Hei99], discuss a model of human cognition that separates the skill-based from the knowledge-based components of human behavior.  To represent the knowledge-based component they use the BDI model and for the skill-based component they use machine learning.  In the BDI architecture, agent operation is defined by four processes: 1) processing incoming data to develop situation awareness, 2) assessing the situation, 3) selecting a tactic based upon the assessment, and 4) employment of standard operating procedures to realize the tactic.  The BDI module is responsible for realizing cognition related to plans, beliefs, goals, intentions, and other cognitive processing.  They supplemented the BDI model by adding skill-based perception and skill-based action to the model.  Skill-based perception allows the agent to recognize situations based upon previous experience.  Skill-based action gives the agent the ability to perform complex sequences of behaviors with little cognitive effort.  The skill module is responsible for sub-cognitive processes such as experience, memory, recognition, and learning.  Because skill and knowledge based activities may have to be executed simultaneously, their architecture directs all incoming information to both the BDI module and the skill module to permit each module to respond to the world state.Hudlicka and Billingsley, [Hud99], present a methodology for modeling individual differences in human behavior, called the Methodology for Analysis and Modeling of Individual Differences (MAMID).  The MAMID approach consists of: 1) identification of the cognitive structures and processes that mediate performance, 2) design of a parameterized model, 3) identification of the cognitive, affective, and personality factors that mediate performance, and 4) encoding of the parameters and knowledge bases for the behavior model.  They argue that a methodology like theirs is needed because existing models and simulations generally only portray normative behaviors and so do not express the full range of variability of normal human behaviors.  The authors contend that the subtle differences between people that arise out of skills, personal history, training, education, and leadership style should be modeled so that the human behavior models used in simulations have greater utility.  The MAMID methodology attempts to capture the factors that lead to differences in behavior and not just the effects of environmental stressors.  To demonstrate the methodology, the authors developed the MAMID cognitive architecture, which includes an attention process that controls cue detection, a situation assessment process that mediates information integration, a decision-making and action selection process, an action execution process, and an action monitoring process.  Within this architecture, two techniques are used to reflect individual behavior.  The techniques are the manipulation of model content to reflect desired characteristics and the manipulation of model processing parameters to reflect a particular style.The importance of training as a determinant of performance in a military environment led to the performance modeling research reported by Archer, Walters, Yow, Carolan, Laughery, and Gillis, [Arc99].  They contend that models of humans in combat should reflect the training levels of each human, but the difficulty and complexity of modeling training led them to develop a simpler model that they contend is adequate for a first effort.  For their first effort they identified eight skill categories (taxons) that form their skill taxonomy; 1) planning, 2) maneuvering, 3) gunnery/weapons employment, 4) communications, 5) emplacement/displacement, 6) command and control, 7) doctrine and tactics, and 8) navigation.  To model the affect of training on performance, they consider the peak performance achievable for a task, the contribution toward peak performance made by the type and quality of each type of training, the minimum amount of each type of training required to attain peak performance, the task difficulty, and the elapsed time since training occurred.  Their model reflects the fact that greater performance improvement occurs during the initial training on a task than occur in later training.  To compute the effect of training on the performance of a complicated task, the process initially determines each skill relevant to a task and then computes each component skill’s value (as a percentage of peak performance) based upon the number of hours of training received in each type of training venue (classroom, field, simulator, etc.) for the skill.  This computation yields the percentage of peak performance achieved for a skill.  Then, to determine task performance, each skill value is multiplied by the percentage that the skill contributes to task performance.  The resulting weighted skills contributions are summed to indicate the percentage of peak performance achieved on a task.Powell, [Pow98], investigated plan development in the face of incomplete information and poorly-defined goals and objectives.  Powell notes that humans generally operate successfully within this type of environment, but all current CGA systems can not.  The approach that he advocates interleaves planning activities and includes activities for sub-problem decomposition, plan critiquing, and plan repair to permit the CGA to partially plan based upon incomplete information and fuzzy goals.In conclusion, the area of human behavior modeling has witnessed an even wider variety of approaches than the areas of software or reasoning system architectures.  In general, a model of human behavior should account for attention, intent, situation assessment, decision making, skill, training, fatigue, environmental stressors, education, experience, and motivations.  To date, very few systems have attempted to model more than a few of these factors and most models are generally assembled using an ad hoc approach instead of employing a strong methodology.  Early efforts at human behavior modeling used finite state machines, recently these efforts have been coupled with agents to try to improve the fidelity of the modeled behaviors and robustness of the response.  Concurrent control and cognition in conjunction with arbitration as well as case-based reasoning coupled with machine learning have also been investigated.  Recent work in human behavior modeling for CGAs has addressed situation awareness, teamwork, skill level, training, learning, and psychological factors to a limited degree.  Modeling of the effects of fatigue has been undertaken, but again most of these efforts have been ad hoc and have not produced a methodology or structure for the model.  Planning has been modeled as a set of competing processes that gradually progress toward a final solution as additional information becomes available.  The cognitive models use a variety of techniques such as motivational control, beliefs-desires-intentions, and traits and states.   The level of training for a CGA has been modeled by determining the skills relevant to a task and then assigning the CGA an appropriate level of skill given the level of training that is to portrayed.An examination of the human behavior modeling research reported in the literature reveals that the work is fragmented and that research results are difficult to integrate and validate.  Currently, there is no comprehensive theory or modeling approach for integrating the various efforts or that can serve as a common platform.  Also, unfortunately, the systems are difficult, costly, and time consuming to assemble.4.	Conclusions and Future WorkIn this paper, we have tried to present a roadmap of the progress in the development of CGAs that highlights some of the developments that have occurred.  The remainder of this paper contains recommendations for future work that, in our opinion, flow from the technical advances that have been obtained.  The current state of CGA technologies is one where our ability to compute has far outstripped our ability to model; it is much easier to increase the computational power at a host than it is to improve any aspect of a human behavior model or other component of a CGA.  The most profound criticism of the current CGAs is their brittleness and rigidity, ie. they are unable to respond in a realistic manner to situations that were not explicitely programmed or entered into their knowledge base.  Therefore, an important objective is to develop CGAs that are adaptable to a variety of circumstances and robust in their response to situations.  The new class of CGAs must exhibit realistic unpredictability and variability in behaviors, and there is a need for realistic behavior of adversaries and teammates, which requires representations for intent, deception, and adaptability in adversarial CGAs.  However, addressing the knowledge acquisition issue is just the first step.  Extensive knowledge bases must be coupled with adaptable decision making and behaviors that overcome the brittleness and rigidity of current systems.  The objective of this new class of CGAs is improved CGA fidelity and performance coupled with a reduction in development and maintenance costs.  The creation of a comprehensive learning capability that supports all types of decision making systems is also needed. Autonomy and fidelity remain important objectives for all CGA systems, and human behavior modeling is an important technology for achieving both of these objectives.  However, human behavior models and CGAs in general can not address the need for autonomy and fidelity in a timely manner for a number of reasons.  Chief among these reasons is that the current approach to developing human behavior models does not produce robust, scaleable models.  Instead, these models generally have a narrow focus and it is difficult to apply a model across a broad array of uses.The first necessary step in improving human behavior models is the development of an ontology and research methodology for the categorization and acquisition of the knowledge needed to develop human behavior models and to combine multiple behavior models into a single system (as discussed in [Ban98a,b], [Ban99a,b,c,d]).  Ontology development requires a precise definition for all of the terms and phrases used to define a knowledge base and human behavior model.  In addition to the development of the ontology and methodology, there is a parallel need for software tools and intelligent agents to support variable resolution modeling, interoperability, and behavioral representation.  The human behavior models that are needed must incorporate behavior moderators, including intent, skills, fatigue, emotion, experience, training, and shared goals.  As a necessary first step in developing the higher quality human behavior models that are required, two taxonomies are needed, one for human behavior models and one for decision systems.  For the human behaviors taxonomy effort, it is especially important to classify/evaluate the various behavioral and cognitive models to determine the military tasks that are best handled by cognitive models and those that are best handled by behavioral models.  The results of this taxonomy development work will guide further model development efforts, classify and correlate our current understanding, and guide in developing hybrid cognitive models that exploit the best aspects of current and future work.  The second taxonomy development effort should create a decision mechanisms taxonomy and develop a correlation between decision mechanisms and types of problems and tasks each supports best.  In addition to the development of these taxonomies, there is a need to develop a precise definition of fidelity and realism for CGAs and human behavior models.  The definition for these four terms should specify a measurement scale, a means of evaluation, and a methodology to determine which level/degree of fidelity and realism is needed for a particular application or exercise.To produce the human behavior models in a timely manner, enable verification, and achieve the requisite quality and robustness, a human behavior model development environment is needed (as discussed in [Ban98a,b], [Ban99a,b,c,d]).  The general development environment would not be a CGA or human behavior modeling solution, rather it is a technology that can be used to develop better human behavior models and CGAs and enable them to be widely used because they are not tied to a specific, specialized development or application environment.  The development environment should support knowledge acquisition as well as system creation, evaluation, experimentation, and development activities.  Because the size and complexity of the knowledge bases will greatly increase, composability coupled with a learning capability for CGAs will be required as part of the environmentThe needs for composability, flexibility, rapid construction, verification and validation of systems, coupled with the desire for high fidelity CGAs that can be assembled by warfighters who are not experts in computer science, points to the need for a generalized CGA software and knowledge base architecture (as introduced and discussed in [Sty98a,b], [Sty99a,b]).  This architecture should support a variety of decision-making systems, enable assembly and composition of knowledge bases, and support experimentation, evaluation and development of human behavior models, decision support systems, information distribution systems, simulations, and CGA systems.  This software architecture would be the software skeleton for all of the software components and system capabilities.  The software architecture must support a variety of decision architectures, including blackboard systems, distributed agents, and hybrids.  The architecture should provide intelligent agents and software gauges to aid the user in assembling a human behavior model or a system. Future CGAs require a number of additional capabilities.  First, and foremost, the CGAs must be able to operate autonomously and independently adapt to new situations and to changes in  mission.  These new CGAs must possess information fidelity, therefore they must accurately model information flows and information quality within their software so that the decision mechanisms operate upon data whose volume, quality, timeliness, and comprehensiveness matches the characteristics of the data available in the real world to the modeled actor.  The CGAs must be able to exhibit a natural variability in their performance and behavior that arises from the nature of the mission, skill, level of training, and experience on the part of the actor and due to the characteristics of its environment.  CGAs must be able to engage in cooperative planning in a manner that reflects skill, experience, and position in the command hierarchy.  Finally, CGAs must be able to automatically detect critical or existence threatening situation(s), automatically respond to the situation(s), and disseminate a warning about the situation(s) to the rest of its organization. Because there is a need for more comprehensive CGAs, human behavior models that all operate and reason at the strategic level are needed.  To achieve this capability, modeling of the political decision making process and strategic reasoning must be performed and the results integrated into upper command level human behavior models.  A final consideration for human behavior models is the reduction of costs via re-use.  An effective means for enabling re-us is the development of repositories for CGAs, models, human behavior models, scenarios, information distribution, decision support systems, simulations, and training results.  To aid users and developers in locating information in the repositories, intelligent agents will be necessary.In conclusion, we believe that a long-term commitment to developing a consistent, significant research and development thrust for human behavior models and the systems that use them.  The vision calls for a broad suite of human behavior models, information distribution systems, decision support systems, simulations, and CGA capabilities.  The suite of capabilities is sufficient to enable rapid and frequent access to high fidelity training, analysis, rehearsal, and planning for all types of operations, under all conditions, and against any red force.  The simulations and CGAs would be robust enough and have sufficient software tools so that any scenario can be developed and evaluated on short notice.  Because of the duration of the research and development effort that we propose, joint service research teams and joint service technology demonstrations will be required to prove the benefits of the research and development at major milestones.bibliography and References[Arc99]	Archer, R.; Walters, B.; Yow, A.; Carolan, T.; Laughery, R.; & Gillis, P.  (1999) “Training as a Performance Shaping Factor in Computer Generated Forces,” Proceedings of the 8th Conference on Computer Generated Forces and Behavioral Representations, Orlando, FL, 11-13 May, pp. 505-513.[Ban98a]	Banks, S.B.; Stytz, M.R.; Hutson, L.J.; & Silver, S.M. (1998) “A Computable Combat Psychology Model for Computer Generated Forces,” The 1998 Fall Simulation Interoperability Workshop, Orlando, FL., 13-18 Sep., pp. 35-45.[Ban98b]	Banks, S.B.; Hutson, L.J.; Stytz, M.R.; & Santos, E. Jr.  (1998) “Incorporation of Multiple Skill Levels into a Domain-independent Computer Generated Force Architecture,” 7th Conference on Computer Generated Forces & Behavioral Representation, Orlando, FL, 12 - 14 May, pp. 497-508.[Ban99a]	Banks, S.B. & Stytz, M.R. (1999) “Considerations for the Next Generation of Air Force Computer-Generated Actors,” Proceedings of the 4th International SIMTECT Conference, Melbourne, Australia, 29 Mar – 1 Apr, pp. 175-180.[Ban99b]	Banks, S.B. & Stytz, M.R. (1999) “An Approach to Enhance Human Behavior Modeling for Computer-Generated Actors,” Proceedings of the 4th International SIMTECT Conference, Melbourne, Australia, 29 Mar – 1 Apr, pp. 199-204.[Ban99c]	Banks, S.B. & Stytz, M.R. (1999) “User Modeling for Distributed Virtual Environment Intelligent Agents,” SPIE 13th Annual International Symposium on Aerospace/Defense Sensing & Controls:  AeroSense ‘99, Modeling & Simulating Sensory Response for Real & Virtual Environments, vol. 3694, Orlando, FL, 4-9 April, pp. 118-127.[Ban99d]	Banks, S.B. & Stytz, M.R. (1999) “Inclusion of User Modeling Techniques for Human Behavior Representation in a Domain-Independent Computer Generated Force Architecture,” 8th Conference on Computer Generated Forces & Behavioral Representation, Orlando, FL, 12 - 14 May, 457-468.[Bax99]	Baxter, J.W. & Horn, G.S. (1999) “A Model for Co-ordination and Co-operation Between CGF Agents,” Proceedings of the 8th Conference on Computer Generated Forces & Behavioral Representations, Orlando, FL, 11-13 May, pp. 101-111.[Bec93]	Becket, W. & Badler, N.I. (1993) “Integrated Behavioral Agent Architecture,” Proceedings of the 3rd Conference on Computer Generated Forces & Behavioral Representation,  Orlando, FL, 17-19 March, pp. 57-68.[Bel99]	Belyavin, A. (1999) “Modeling the Effect of Stress on Human Behavior,” Proceedings of the 8th Conference on Computer Generated Forces & Behavioral Representations, Orlando, FL, 11-13 May, pp. 481-487.[Boo99]	Booker, L.B.; Burke, C.D.; & Hughes, J.D. (1999) “Motivational Control of Tactical Behaviors:  Interim Results,” Proceedings of the 8th Conference on Computer Generated Forces & Behavioral Representations, Orlando, FL, 11-13 May, pp. 17-24.[Bou99]	Bouche, J-P; Floch, J-P; & Michel, M. (1999) “Using Command Agents for Military Planning,” The 1999 Spring Simulation Interoperability Workshop, Orlando, FL., Sept. 12-17, pp. 503-510.[Bra93]	Braudaway, W. (1993) “A Blackboard Approach to Computer Generated Forces,” Proceedings of the 3rd Conference on Computer Generated Forces & Behavioral Representation,  Orlando, FL, 17-19 Mar, pp. 11-20.[Bus99]	Busetta, P.; Ronnquist, R.; Hodgson, A.; & Lucas, A. (1999) “Light-Weight Intelligent Software Agents in Simulation,” Proceedings of the 4th International SIMTECT Conference, Melbourne, Australia, 29 Mar – 1 Apr, pp. 169-174.[Cal99a]	Calder, R.B. & Drummey, J. (1999) “Definition of a Military Intelligent Agent Architecture,” Proceedings of the 8th Conference on Computer Generated Forces & Behavioral Representations, Orlando, FL, 11-13 May, pp. 551-562.[Cal99b]	Calder, R.B.; Kleiner, M.S.; & Carey, S.A. (1999) “From Domain Knowledge to Behavior Representation,” The 1999 Spring Simulation Interoperability Workshop, Orlando, FL., Sept. 12-17, pp. 363-372.[Care98]	Carey, S.A. & Kleiner, M.S. (1998) “Multi-Echelon Mission-to-Task Decomposition,” Proceedings of the 7th Conference on Computer Generated Forces & Behavior Representation, Orlando, FL, 12-14 May, pp. 67-73.[Cer94]	Ceranowicz, A.  (1994) “ModSAF Capabilities,” Proceedings of the 4th Conference on Computer Generated Forces & Behavioral Representation,  Orlando, FL, 4-6 May, pp. 3-8.[Cha93]	Chaib-draa, B. & Paquet, E. (1993) “Integrating Reaction, Planning, and Deliberation in Architecture for Multiagent Environment,” Proceedings of the 3rd Conference on Computer Generated Forces & Behavioral Representation,  Orlando, FL, 17-19 March, pp. 45-55.[Cha97]	Chandra, A. (1997) “A Computational Architecture to Model Human Emotions,” Proceedings of Intelligent Information Systems (IIS’97), Grand Bahama Island, Bahamas, 8-10 December, pp. 86-89.[Cha99]	Chandrasekaran, B. & Josephson, J.R. (1999) “Cognitive Modeling for Simulation Goals: A Research Strategy for Computer-Generated Forces,” Proceedings of the 8th Conference on Computer Generated Forces & Behavioral Representations, Orlando, FL, 11-13 May, pp. 117-126.[Cor96]	Coradeschi, S.; Karlsson, L.; & Torne, A.  (1996) “Intelligent Agents for Aircraft Combat Simulation,” Proceedings of the 6th Conference on Computer Generated Forces & Behavioral Representation,  Orlando, FL, 23-25 July, pp. 93-99.[Cou94]	Courtemanche, A.J. & Monday, P. (1994) “The Incorporation of Validated Combat Models into ModSAF,” Proceedings of the 4th Conference on Computer Generated Forces & Behavioral Representation,  Orlando, FL, 4-6 May, pp. 129-140.[Cou99]	Courtemanche, A.J. (1999) “Design Patterns for Computer Generated Forces,” Proceedings of the 8th Conference on Computer Generated Forces & Behavioral Representations, Orlando, FL, 11-13 May, pp. 25-36.[Cox99]	Cox, M. & Flaglien, B.A. (1999) “Agent-Based Automation of CGF Operator Tasks in a Combined Simulation Environment,” Proceedings of the 8th Conference on Computer Generated Forces & Behavioral Representations, Orlando, FL, 11-13 May, pp. 203-209.[Dav95]	Davies, M. & Gabrisch, C.  (1995) “The Distributed Interactive C3I Effectiveness (DICE) Simulation Project:  An Overview,” Proceedings of the 5th Conference on Computer Generated Forces & Behavioral Representation, Orlando, FL, 9-11 May, pp. 15-27.[Dom99]	Dompke, U.K.J.; Baeyer, A.V.; & Heineken, E.  (1999) “Problems of Validating Human Behavior Models,” The 1999 Fall Simulation Interoperability Workshop, Orlando, FL., Sept. 12-17, pp. 151-153. [Fla98]	Flach, J.M. & Holden, J.G. (1998) “The Reality of Experience:  Gibson’s Way,” Presence:  Teleoperators & Virtual Environments, vol. 7, pp. 90-95.[Fog96]	Fogel, L.J. V; Porto, W.; & Owen, M.  (1996) “An Intelligently Interactive Non-Rule-Based Computer Generated Force,” Proceedings of the 6th Conference on Computer Generated Forces & Behavioral Representation,  Orlando, FL, 23-25 July, pp. 265-271.[Fra99]	Franceschini, D.; Zimmerman, J.; & McCulley, G. (1999) “CGF System Composability through Dynamically Loadable Modules,” Proceedings of the 8th Conference on Computer Generated Forces & Behavioral Representations, Orlando, FL, 11-13 May, pp. 341-347.[Fre99]	Freeman, J.; Avons, S.E.; Pearson, D.E.; & Ijsselsteijn, W.A. (1999) “Effects of Sensory Information and Prior Experience on Direct Subjective Ratings of Presence,” Presence:  Teleoperators & Virtual Environments, vol. 8, no. 1, February, pp. 1-13.[Fun99]	Funge, J. (1999) “Cognitive Modeling for Computer Generated Forces,” Proceedings of the 8th Conference on Computer Generated Forces & Behavioral Representations, Orlando, FL, 11-13 May, pp. 491-497.[Gag95]	Gagne, D. (1995) “Realistic Doctrinal Behaviors in Computer Generated Forces Through Plurality,” Proceedings of the 5th Conference on Computer Generated Forces & Behavioral Representation,  Orlando, FL, 9-11 May, pp. 521-527.[Gal97]	Galanis, G; Jennings, A.; & Beckett, P.  (1997) “Towards a General Model of Perception for Simulation,” Proceedings of the 2nd International SIMTECT Conference, Canberra, Australia, 17-20 Mar, pp. 21-26.[Ge95]	Ge, Z.; James, J.; & Nerode, A.  (1995) “A Multiple Agent Hybrid Control Architecture for Automated Forces:  Design and Software Implementation,” Proceedings of the 5th Conference on Computer Generated Forces & Behavioral Representation,  Orlando, FL, 9-11 May, pp. 45-52.[Geo99]	George, G.R.; Breitbach, R.A.; Brooks, R.B.; Steffes, R.; Bell, H.H.; & Bruhl, C.C. (1999) “Synthetic Forces in the Air Force Command and Control Distributed Mission Training Environment, Current and Future,” Proceedings of the 8th Conference on Computer Generated Forces & Behavioral Representations, Orlando, FL, 11-13 May, pp. 319-331.[Gil98]	Gillis, P.D. (1998) “Realism in Computer Generated Forces Command Entity Behaviors,” Proceedings of the 7th Conference on Computer Generated Forces & Behavior Representation, Orlando, FL, 12-14 May, pp. 455-464.[Gil99]	Gillis, P.D. & Hursh, S.R. (1999) “Using Behavior Moderators to Influence CGF Command Entity Effectiveness and Performance,” Proceedings of the 8th Conference on Computer Generated Forces & Behavioral Representations, Orlando, FL, 11-13 May, pp. 237-251.[Gle98]	Glenn, F.; Stokes, J.; Wolf, J.J. & Scolaro, D. (1998) “A Simulation Model of the Operator of an Air Defense Weapons Systems,” Proceedings of the 7th Conference on Computer Generated Forces & Behavior Representation, Orlando, FL, 12-14 May, pp. 357-363.[Gon95]	Gonzalez, A.J. & Ahlers, R.  (1995) “Context-based Representation of Intelligent Behavior in Simulated Opponents,” Proceedings of the 5th Conference on Computer Generated Forces & Behavioral Representation,  Orlando, FL, 9-11 May, pp. 53-62.[Gon99]	Gonzalez, A.J. & Murillo, M. (1999) “Validation of Human Behavior Models,” The 1999 Spring Simulation Interoperability Workshop, Orlando, FL., Sept. 12-17, pp. 50-57.[Gra98]	Grama, C.; Gonzalez, A.; Pollak, E.; Brasch, R.; & Wartski, J. (1998) “Automated Plan Generation for Mid-Echelons in the Military Command Decision Process,” The 1998 Spring Simulation Interoperability Workshop, Orlando, FL., 9-13 March., pp.636-645.[Gra99]	Grant, S.; Fisher, M.; & Pearce, P.V. (1999) “The LFM Organisation Model,” Proceedings of the 8th Conference on Computer Generated Forces & Behavioral Representations, Orlando, FL, 11-13 May, pp. 155-166.[Hee92]	Heeter, C. (1992)  “Being There:  The Subjective Experience of Presence,” Presence:  Teleoperators & Virtual Environments, vol. 1, pp. 262-271.[Hei99]	Heinze, C.; Lloyd, I.; Goss, S.; & Pearce, A. (1999) “Collaborating Cognitive and Sub-Cognitive Processes for the Simulation of Human Decision Making,” Proceedings of the 4th International SIMTECT Conference, Melbourne, Australia, 29 Mar – 1 Apr, pp. 239-246.[Hep96]	Hepplewhite, R.T. & Baxter, J.W. (1996) “Broad Agents for Intelligent Battlefield Simulation,” Proceedings of the 1st International SIMTECT Conference, Melbourne, Australia, 25-26 Mar, pp. 107-112.[Hic99]	Hicinbothom, J.H. & Lyons, D.M. (1999) “Synthetic Forces for Team Training,” Proceedings of the 8th Conference on Computer Generated Forces & Behavioral Representations, Orlando, FL, 11-13 May, pp. 333-338.[Hie95]	Hieb, M.R.; Tecuci, G.; Pullen, J.M.; Ceranowicz, A.; & Hille, D.  (1995) “A Methodology and Tool for Constructing Adaptive Command Agents for Computer Generated Forces,” Proceedings of the 5th Conference on Computer Generated Forces & Behavioral Representation,  Orlando, FL, 9-11 May, pp. 135-146.[How98]	Howard, M. & Lee, C. (1998) “Architecture of a Generic Command Entity,” Proceedings of the 7th Conference on Computer Generated Forces & Behavior Representation, Orlando, FL, 12-14 May, pp.569-579.[Hud99]	Hudlicka, E. & Billingsley, J. (1999) “Representing Behavior Moderators in Military Human Performance Models,” Proceedings of the 8th Conference on Computer Generated Forces & Behavioral Representations, Orlando, FL, 11-13 May, pp. 423-433.[Jon93]	Jones, R.M.; Tambe, M.; Laird, J.E.; & Rosenbloom, P.S. (1993) “Intelligent Automated Agents for Flight Training Simulators,” Proceedings of the 3rd Conference on Computer Generated Forces & Behavioral Representation, Orlando, FL, 17-19 March, pp. 33-42.[Jon99]	Jones, R.M.; Laird, J.M.; Nielsen, P.E.; Coulter, K.J.; Kenny, P.; & Voss, F.V. (1999) “Automated Intelligent Pilots for Combat Flight Simulation,” AI Magazine, vol. 20, no. 1, Spring, pp. 27-42.[Kar95]	Karr, C.R; Rajput, S.; Cisneros, J.E.; & Lee, H.L.  (1995) “Automated Mission Planning in ModSAF,” Proceedings of the 5th Conference on Computer Generated Forces & Behavioral Representation,  Orlando, FL, 9-11 May, pp. 159-168.[Kei94]	Keirsey, D.; Krozel, J.; Payton, D.; & Tseng, D.  (1994) “Case-Based Computer Generated Forces,” Proceedings of the 4th Conference on Computer Generated Forces & Behavioral Representation,  Orlando, FL, 4-6 May, pp. 307-316.[Ken96]	Kenny, P.G.; Durfee, E.H.; & Kluge, K.C.  (1996) “Mission Planning and Coordinated Execution for Unmanned Vehicles,” Proceedings of the 6th Conference on Computer Generated Forces & Behavior Representation, Orlando, FL, 23-25 July, pp. 329-336.[Koc98]	Kocabas, S. & Oztemel, E. (1998) “AISim:  An Intelligent Agent for Distributed Interactive Simulation,” Proceedings of the 7th Conference on Computer Generated Forces & Behavior Representation, Orlando, FL, 12-14 May, pp. 257-261.[Lai87]	Laird, J. E., Newell, A., & Rosenbloom, P.S. (1987) “SOAR:  An Architecture for General Intelligence,” Artificial Intelligence, vol. 33, no. 1, pp. 1-64.[Lai95]	Laird, J.E.; Johnson, W.L.; Jones, R.M.; Koss, F.; Lehman, J.F.; Nielsen, P.E.; Rosenbloom, P.S.; Rubinoff, R.; Schwamb, K.; Tambe, M.; vanDyke, J.; vanLent, M.; & Wray, R.E. III.  (1995) “Simulated Intelligent Forces for Air:  The SOAR/IFOR Project 1995,” Proceedings of the 5th Conference on Computer Generated Forces & Behavioral Representation,  Orlando, FL, 9-11 May, pp. 27-36.[Lai98]	Laird, J.E.; Coulter, K.J.; Jones, R.M.; Kenny, P.G.; Koss, F.V.; & Nielsen, P.E. (1998) “Integrating Intelligent Computer Generated Forces in Distributed Simulations: TacAir Soar in STOW-97,” The 1998 Spring Simulation Interoperability Workshop, Orlando, FL., 9-13 March., pp.1195-1201.[Lan93]	Landweer, P. (1993) “Action/Cognition Behavior Model for Computer Generated Forces,” Proceedings of the 3rd Conference on Computer Generated Forces & Behavioral Representation,  Orlando, FL, 17-19 March, pp. 335-345.[LaV93]	LaVine, N.D.; Laughery, R.K.; Young, B.; & Kehlet, R. (1993) “Semi-Automated Forces (SAFOR) Crew Performance Degradation,” Proceedings of the 3rd Conference on Computer Generated Forces & Behavioral Representation,  Orlando, FL, 17-19 March, pp. 405-415.[LaV99]	LaVine, N.D.; Kehlet, R.; & Peters, S.D. (1999) “A Client-Server Approach to CGF Behavioral Representation,” Proceedings of the 8th Conference on Computer Generated Forces & Behavioral Representations, Orlando, FL, 11-13 May, pp. 411-421.[Mal95]	Mall, H.; Bimson, K.; McCormack, J.; & Ourston, D.  (1995) “Command Entity Cognitive Behaviors for SAF and CGF,” Proceedings of the 5th Conference on Computer Generated Forces & Behavioral Representation,  Orlando, FL, 9-11 May, pp. 203-209.[McI97]	McIlroy, D.; Heinze, C.; Appla, D.; Busetta, P.; Tidhar, G.; & Rao, A.  (1997) “Towards Credible Computer-Generated Forces,” Proceedings of the 2nd International SIMTECT Conference, Canberra, Australia, 17-20 Mar, pp. 79-84[McI95]	McIntyre, R.T. III & Middleton, V.E.  (1995) “A Behavioral Approach to Fidelity Requirements for Simulation of Dismounted Combatants,” Proceedings of the 5th Conference on Computer Generated Forces & Behavioral Representation,  Orlando, FL, 9-11 May, pp. 495-499.[Pag99]	Page, I & Widdowson, M.  (1999) “Synthetic Forces Behavioral Architecture,” Proceedings of the 8th Conference on Computer Generated Forces & Behavioral Representations, Orlando, FL, 11-13 May, pp. 89-100.[Par94]	Parsons, J.D.  (1994) “Using Fuzzy Logic Control Technology to Simulate Human Decision-Making in Warfare Models,” Proceedings of the 4th Conference on Computer Generated Forces & Behavioral Representation,  Orlando, FL, 4-6 May, pp. 519-529.[Pow98]	Powell, G.M. (1998) “A Cognitive Task Analysis of Army Corps-level Planning,” Proceedings of the 7th Conference on Computer Generated Forces & Behavior Representation, Orlando, FL, 12-14 May, pp. 47-58.[Pra96]	Pratt, D.  (1996)  “Next Generation Computer Generated Forces,” Proceedings of the 6th Conference on Computer Generated Forces & Behavior Representation, Orlando, FL, 23-25 July, pp. 3-8.[Pso98]	Psotka, J. (1998) “Technologies for Representing Cognition and Emotion in CGF,” The 1998 Spring Simulation Interoperability Workshop, Orlando, FL., 9-13 March., pp.1154-1158.[Raj96]	Rajput, S, & Karr, C.R.  (1996) “A New Mechanism for Cooperative Behavior in ModSAF,” Proceedings of the 6th Conference on Computer Generated Forces & Behavior Representation, Orlando, FL, 23-25 July, pp. 189-199.[Ree96]	Reece, D.A. & Wirthlin, R.  (1996) “Detection Models for Computer Generated Individual Combatants,” Proceedings of the 6th Conference on Computer Generated Forces & Behavior Representation, Orlando, FL, 23-25 July, pp. 409-416.[Rob99]	Roberts, D.W.; Mitchell, C.M.; Thurman, D.A.; & Chappell, A.R. (1999) “Computational Models of Team Decision-Making,” The 1999 Fall Simulation Interoperability Workshop, Orlando, FL., 12-17 Sept., pp. 993-1000.[Sap96]	Sapaty, P.S. (1996) “Distributed Modeling of Cooperative Behavior by Mobile Agents,” Proceedings of the 6th Conference on Computer Generated Forces & Behavioral Representation,  Orlando, FL, 23-25 July, pp. 599-613.[Sev98]	Severinghaus, R.; Williams, K.; & Clare, T. (1998) “Behavior Representation using Cognitive Task Analysis Modeling – Bridging the Gap Between Decision Processes and Model Development Requirements,” The 1998 Fall Simulation Interoperability Workshop, Orlando, FL., 13-18 Sep., pp.296-304.[Sto98]	Stone, G. F. & Pettitt, B.L. (1998) “Future Battlespace Implications to Computer-Generated Forces,” Proceedings of the 7th Conference on Computer Generated Forces & Behavior Representation, Orlando, FL, 12-14 May, pp. 389-392.[Sty96]	Stytz, M.R. (1996) “Distributed Virtual Environments,” IEEE Computer Graphics & Applications, vol. 16, no. 3, pp. 19-31.[Sty98a]	Stytz, M.R.; Banks, S.B. & Santos, E. Jr. (1998) “The Distributed Mission Training Integrated Threat Environment Project:  Goals, Architecture, and Current Status,” 7th Conference on Computer Generated Forces & Behavioral Representation, Orlando, FL, 12 - 14 May, pp. 155-168.[Sty98b]	Stytz, M.R. & Banks, S.B. (1998) “Requirements for the Next Generation of Air Force Models,” The 1998 Fall Simulation Interoperability Workshop, Orlando, FL., 13-18 Sep., pp. 46-54.[Sty99a]	Stytz, M.R. & Banks, S.B. (1999) “The Distributed Mission Training Integrated Threat Environment System Architecture, Rules, and Design Overview,” The Spring Simulation Interoperability Workshop, Orlando, FL, 14-19 March, pp. 858-867.[Sty99b]	Stytz, M.R. & Banks, S.B.  (1999) "Requirements and HLA Considerations for Computer-Generated Actors in Distributed Mission Training," The SCS 1999 Advanced Simulation Technology Conference: 1999 Military, Government, & Aerospace Simulation Conference, San Diego, CA,  11-15 April, pp. 33-38.[Tam94]	Tambe, M. & Rosenbloom, P.S. (1994) “Event Tracking in Complex Multi-agent Environments,” Proceedings of the 4th Conference on Computer Generated Forces & Behavioral Representation,  Orlando, FL, 4-6 May, pp. 473-484.[Tam95]	Tambe, M., Johnson, W. L., Jones, R.M., Koss, F., Laird, J. E., Rosenbloom, P. S., & Schwamb, K. (Spring 1995)  “Intelligent Agents for Interactive Simulation Environments,” AI Magazine, vol. 16, no. 1, 15-40.[Von98]	VonderLippe, S.R. & Courtemanche, A.J. (1998) “Interim Results in the Development of User Composable Behaviors,” Proceedings of the 7th Conference on Computer Generated Forces & Behavior Representation, Orlando, FL, 12-14 May, pp. 545-553.[Vra98]	Vrablik, R.; Carey, S.A.; Chamberlain, F.; Longfritz, M.J.; & Kleiner, M. (1998) “Advanced Synthetic Command Forces Architecture,” The 1998 Spring Simulation Interoperability Workshop, Orlando, FL., 9-13 March., pp.176-184.[Wea94]	Weaver, W.B. & Mullen, W.J. (1994) “Simulating Generic Military Decision Making with an Empirically-Trained Neural Network,” Proceedings of the 4th Conference on Computer Generated Forces & Behavioral Representation,  Orlando, FL, 4-6 May, pp. 531-540.[Wil98a]	Williams, K.; Severinghaus, R.; & Clare, T. (1998) “A Modified GOMS Cognitive Task Analysis Technique for Creating Computational Models of Adversary Behavior,” Proceedings of the 7th Conference on Computer Generated Forces & Behavior Representation, Orlando, FL, 12-14 May, pp. 75-83.[Wil98b]	Williams, K.; Severinghaus, R.; & Clare, T. (1998) “Behavior Representation Using Cognitive Task Analysis Modeling – Bridging the ap Between Decision Processes and Model Development Requirements,” The 1998 Fall Simulation Interoperability Workshop, Orlando, FL, 13-18 Sep., pp. 296-304.[Yuf97]	Yufik, Y.M.; Beadling, R.G.; & Hu, T.H.G. (1997) “Operator Modeling in Complex Large-Scale Systems,” Proceedings of the Military, Government, & Aerospace Simulation Conference, Atlanta, GA, 6-10 April, pp. 231-237.[Zac98]	Zachary, W.; Ryder, J.; Hicinbothom, J.; Santarelli, T.; Scolaro, J.; Szczepkowski, M; & Cannon-Bowers, J.  (1998) “Simulating Behavior of Tactical Operators and Teams Using COGNET/GINA,” Proceedings of the 7th Conference on Computer Generated Forces & Behavior Representation, Orlando, FL, 12-14 May, pp. 365-376.[Zah98]	Zahorik, P. & Jenison, R.L. (1998) “Presence as Being-in-the World,” Presence:  Teleoperators & Virtual Environments, vol. 7, pp. 78-89.[Zim99]	Zimm, A.D. (1999) “Modeling Maneuver Warfare: Incorporating Human Factors and Decisionmaking in Combat Modeling,” Proceedings of the 8th Conference on Computer Generated Forces & Behavioral Representations, Orlando, FL, 11-13 May, pp. 225-236.	 PAGE 18	