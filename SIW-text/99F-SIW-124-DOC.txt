The Remotely Locating of Simulator Display and Controls through High Speed Video Transfer and HLA/RTIEugene A. StoudenmireTrang H. EscobarMike CarusoMike Baldwin SAIC4001 N. Fairfax Drive, Suite 250Arlington VA 22203703-704-3698/2725Eugene.a.stoudenmire.ii@cpmx.saic.com, trang.h.escobar@cpmx.saic.com, mcaruso@nvl.army.mil, mbaldwin@nvl.army.milMax LorenzoNight Vision Electronic Sensors DirectorateU.S. Army Communications and Electronics CommandFt. Belvoir, VA 22060-5806703-704-3185mlorenzo@nvl.army.milMike Muuss Army Research LaboratoryAberdeen Proving Grounds MDmike@arl.milKenny ShafferSignal Corp.Fort Belvoir VA(703) 704-1639kshaffer@nvl.army.milKeywords:RTI, Video, Latency, Controls, ATM, Remote, Simulator ABSTRACT: This paper evaluates the feasibility of remotely controlling a simulator through high-speed video transfer and an HLA/RTI simulator control federate. Currently, the video displays and physical controls of simulators are tightly coupled to the simulation engines, many times with proprietary formats.  As a result, the simulation engine must be physically located in close proximity to the simulator itself.  As fidelity requirements increase, so does the cost of the hardware to support it.  If the display and controls could be separated from the expensive simulation engines, the hardware could be centrally located and shared among remotely located facilities.  To do so efficiently would require high speed, low-latency transfer of video for the display as well as high speed, low-latency transfer of the control data, preferably with a standard protocol such as HLA/RTI.  This paper evaluates the result of such a project using the CECOM/Night Vision Electronic Sensor Directorate’s (NVESD) Paint the Night (PTN) simulation at Fort Belvoir, VA.  This simulation will be connected to the Mounted Maneuver Battlelab at Fort Knox, KY via the Defense Research and Engineering Network (DREN).  Each site is connected to the DREN with an Asynchronous Transfer Mode (ATM) OC-3 link providing 155 mbits/sec of bandwidth point-to-point.BackgroundThe Paint the Night (PTN) simulation at Fort Belvoir, VA is a 3-D infrared scene simulation.  It was designed for many purposes including virtual prototyping of infrared sensors and the rendering of realistic 3-D images for training and testing Aided/Automatic Target Recognizers (ATRs).   The simulation includes high-resolution terrain, features such as deciduous trees, gravel roads, paved roads, tank trails, rocks, and urban buildings.  Various atmospheric models and high-resolution targets are also supported.In many simulations, including PTN, the simulator display and controls are tightly coupled to the simulation they relate to, necessitating the display and controls being in the same physical location as the image generator.  Also, high fidelity scene simulators tend to need expensive hardware to render images at an acceptable rate making it expensive and difficult to move the simulation to a different location.  The controls also usually have a unique and proprietary interface, limiting their usability with other simulators.  This paper explores an alternative method of running a simulation—a method that would (1) provide users with high fidelity sensor representation in simulations, (2) lower costs through cost-sharing the hardware and software, (3) simplify upgrades and maintenance, and (4) establish better relationships between customer and developer.Limitations of Current Simulator ApproachHigh fidelity simulators are currently driven by costly high-end scene generators and are limited by bandwidth, speed, and latency concerns in their ability to take advantage of complex simulation models that are not in the immediate vicinity of the simulator.CostsNot every organization can afford high-end computers for image generators for simulators.  These computers have an initial cost of from $500K and once procured, continue to require expensive maintenance and upgrades.  Both hardware and software maintenance presents an expensive and time-consuming burden.  To deal with this, many users opt for a low cost, hence low fidelity solution.  While these low cost solutions can provide capabilities to meet the needs of some users, too often they just don’t have sufficient horsepower to support the fidelity necessary to adequately represent the capabilities of sensors.  The reduced fidelity results in inaccurate representation of detection, recognition, and identification of vehicles and false alarms.Ability to take advantage of expertise at other sites  Another limitation of current simulator concepts stems from the fact that no single organization is an expert in all domains of a simulation.  A simulator normally takes the most advantage of expertise available in its immediate proximity.  For instance, if a simulator is in an organization concerned with vehicle vulnerability, it might very well have a dynamite vehicle vulnerability model, but could be lacking in some other critical field such as atmospheric effects.Requirements of an Optimal ApproachA viable solution would have to successfully tackle the above limitations.  The solution would have to be affordable, be able to take advantage of the expertise available at remote locations, and, in communicating with the remote locations, meet any requirements the simulation might have for latency, bandwidth and speed.We propose a solution in which the simulator controls and display are divorced from and remotely located from the remainder of the simulation.  If this capability could in fact be achieved, the most expensive portion of a simulator, the image generating hardware and software could be cost-shared—many sites could pool together and gain the advantages of having the tremendously powerful compute capacity, and the high fidelity terrains and features it could support.  Furthermore, if remote connectivity can be demonstrated to be feasible at high bandwidth, high speeds, and low latency, then the entire simulation could be distributed, taking advantage of complex models at other sites, thus solving the second problem.To accomplish this smartly would require three things.  First there must be a modular simulator design.  Second, there needs to be a distributed simulation execution capability.  Third, the solution should be open architecture to the maximum extent feasible.Modular Design  The modular simulator design is essential because we are not the “experts” in all fields.  Other authors have recognized the benefits of modularity as well [2,4].  We need the capability to bring together those who are the “experts” from the appropriate places.  We also need to be able to easily implement different complexities/fidelities of components into the simulation.  We anticipate the ability to incorporate, at a particular user’s complexity needs, appropriate models for atmospherics, vehicle dynamics, vehicle vulnerability, meteorlogics, and environmental effects.  Otherwise, if each model were at the high fidelity/complexity required by that field’s experts, the simulation would quickly become bogged down.  For any given image generation capability, increasing complexity in one field, reduces the power available to support complexity in the other fields resulting in a tradeoff between number of models and the complexity of each.  The simulation can be viewed as being able to support an aggregate amount of complexity from which each model simulated draws.The experts in particular fields need to be able to use the simulation at sufficient complexity in the areas that most affect their field, yet be able to use models of lesser complexity for those fields that matter less. With modular design, a simulation with scalable models can be developed that would allow each model being simulated to have the desired complexity assigned at simulation execution time.  Each “expert” using the simulation could assign complexities to each model to meet his/her needs.Distributed Simulation Execution CapabilityNot only are the controls and display to be remotely located but also these “experts” and their resultant simulation models can be located at geographically dispersed sites.  To implement them together requires a distributed simulation, and again, bandwidth, speed, and low latency are key.Open ArchitectureFor a simulator to take maximum advantage of simulation models from other locations, an open architecture approach is essential.  Any proprietary system would severely limit the attractability of other simulations to adapt to the architecture, as their return on investment would be lessened [3].Initial Application:  Long Range Advanced Scout Surveillance System (LRAS3) SimulatorNVESD has undertaken a project to construct an LRAS3 simulator for the Mounted Maneuvers Battlelab at Fort Knox, KY.  LRAS3 is a surveillance system mounted atop a HWMMV and operated by a scout who searches for targets by looking through a monocular sight at a small monitor displaying the output of a Forward Looking Infrared (FLIR) sensor or monochrome camera (switchable between the two).  The system also has a laser and two Global Positioning Systems (GPSs) used to determine the location of acquired targets.  The laser is beamed in the direction the sensor is pointing in order to measure the distance of the target.  The GPSs are used to determine the absolute location of the LRAS3 and the azimuth and elevation of the line of sight of the sensor (and laser).  Various switches are operated by the scout to accomplish such functions as changing the field of view, switching between the FLIR and the camera, and adjusting the gain and contrast of the sensor.  The operational concept has a platoon consisting of three pair of these systems, each pair providing overwatch capability to an area.  A functional architecture applying Paint the Night (PTN) to the LRAS3 simulator is depicted in figure 1.   The reconfigurable design of PTN is explained in more depth in another paper submitted to this conference [4]. ImplementationFor an infrastructure we considered three alternatives: HLA and its RTI, Joint Modeling and Simulation System (JMASS) and a uniquely designed architecture of our own.  Since HLA supports a modular approach and open architecture and has increasingly widespread use, we opted to investigate the DMSO RTI and determine if could support the bandwidth, speed, and low latency requirements of an interactive, high fidelity, 3D simulation.  In particular, could it keep up with the requirements to quickly move the near-continuous data emanating from the controls.  We ran some preliminary tests and assured ourselves that it was as a minimum a viable choice with which to proceed for transfer of the control data but not for the video data.Distributive RequirementCreating a federate to accomplish the tasks of each of the major functional nodes of figure 1 would facilitate the remote implementation/execution of the functions at various locations.  The simulation would then consist of three types of federates: those which take user inputs in real time (e.g. control grips), those which are involved in the production and processing of data for image generation (e.g. vehicle dynamics) and those which are involved in pure scene generation (e.g. real-time image generators and ray tracers).  Such an architecture is depicted in figure 2.  Federates which process inputs are depicted on the far left of the RTI, those involved in production/processing are on the immediate left of the RTI and those involved in pure scene generation (or other outputs) are on the right of the RTI.In terms of distributive capability required, the ability to remotely locate the LRAS3 hardware and hardware federate from the output of the scene generation federates would provide the capability to remotely locate an actual simulator from the remainder of the simulation.   The supporting federates could live at whatever location modules with the appropriate functionality were being developed.  For our LRAS3 application, we located the hardware (control grips and display) and hardware federate at Fort Knox  KY and the remaining federates at Fort Belvoir VA.Network Design  Asynchronous Transfer Mode (ATM) was selected for transfer of the control and video data for many reasons, including its ability to handle high bandwidth with low loss and low latency.  With fiber optics and ATM, a pure optical path can be established, creating a direct virtual connection whereas ethernet is prone to retransmissions initiated by collisions and late packets.The Defense Research and Engineering Network (DREN) is available to Ft. Knox and Ft. Belvoir at OC3 rates of (155 mbits/s) and will be available in the near term at many other military installations.For the remote connection linking the control federate, a point-to-point connection had to be established instead of using multipass.  While multipass was the preferred method, it was not feasible because of security concerns and the lack of our ability to control the configuration of remote networks.The bandwidth required for the control data was minimal compared to that required for the video.  The control data consisted of updates in position, velocity, and time of day.  The video consisted of 640 columns x 480 rows x 8 bits at 30 frames/sec for a bandwidth of approximately 74 mbits/sec, exclusive of socket header data and acknowledgement data (both minimal when compared to the total size of the data).LatencyThere is an issue of how much latency is involved from beginning to end—from the simulator controls to the video display. To avoid excessive lag time between control movement and the resulting display update, a latency of ¼ sec, or 7.5 video frames is the estimated maximum allowable.Our expectations are that approximately 5.5 frames would be involved.  The image generator has 3 frames, the post-processing to add sensor effects has 1 frame (to provide blurring and sensor noise emulation), the optical path adds .5 frames at most, and the other electronics uses one more frame.When tested locally, the optical path introduced minimal delay.  We used TCP/IP because of its built-in reliability.  With a test between Fort Belvoir VA and Aberdeen Proving Grounds MD, the latency increased to 5 ms. With TCP/IP and the wait incurred with its inherent acknowledgements, the frame rate slowed from 30 Hz to 18 Hz.  This led to investigation of UDP, then on to the direct use of lower ATM layers, with XDI being selected as the protocol of choice. In addition to the sending of the uncompressed data, we are also investigating the feasibility of compressing the video.  If compression is used, a significant amount of additional time is required for the compression and decompression, adding to the latency.  We used a Textronic codec, which inputs analog or serial digital (D1) video, compresses it, and transmits the data via ATM.  Even with this broadcast quality hardware, and the configuration set to minimize latency (at the cost of additional bandwidth), an additional 150-250 ms of delay was incurred.  In terms of frames, this amounts to 5-7 additional frames.Demonstration of ConceptA proof of concept demonstration was accomplished July 1999.  We located a generic set of controls (a flybox from BG Systems) and a display at Fort Knox KY and connected to the PTN simulation being run on an image generator at Fort Belvoir VA (see figure 3). The demonstration included a flybox federate executing at Fort Knox and a flybox mapping federate and an image generation federate executing at Fort Belvoir on an SGI Onyx II infinite reality.  The video was transferred from Fort Belvoir to Fort Knox sharing the same ATM as the RTI executed on.  The scene generator sent the framebuffer data, in 32 Kbytes chunks, through a socket connection, using AAL5 frames, to an SGI O2 at Fort Knox which read the framebuffer data from the socket and displayed it using OpenGL.  Thirty five frames/sec were transferred and displayed.Simultaneously, the composite analog video was hardware compressed with a codec, decompressed at Fort Knox and displayed on a digital monitor.  In order to continue the two streams, the rate on the first  had to be reduced to 18 frames/sec to avoid dropping data.ConclusionThe DMSO RTI works well as a simulation backplane for the passing of entity control data between remote locations.  The latency incurred with the RTI was within the limits of simulator control requirements.Video transmission to remote sites is feasible.  The latency of uncompressed data was manageable through transfer protocol selection.  For long distances, the cost of having to wait for packet acknowledgement prior to the sending of subsequent data is too costly, necessitating the need for a protocol without frequent acknowledgements.Compressed video introduced additional latency, even though the compression was accomplished in hardware; however, the latency, although noticeable is bearable for many simulator applications.The bottom line is that high quality imagery can be served to multiple users with lowered cost per user.This simulation is currently being implemented for two simulators:  the Multi-Function Starting Sensor Suite (MFS3) and the Long Range Advance Scout Sensor Suite (LRAS3).We extend an invitation to other R&D sensors as well as industry and academia to collaborate on extending this approach.  We are working toward a DoD infrastructure that facilitates this collaborative environment.  We want to apply these integrated high fidelity simulations to Simulation Based Acquisition (SBA) and Simulation and Modeling for Acquisition, Requirements, and Training (SMART).AcknowledgementsWithout the support and guidance of Mr. Larry Fillian, deputy director of NVESD (CECOM), this effort would have never gotten off the ground.  We are forever indebted to his belief and support of our vision.References[1]	Stephan G. Purdy, et. al.:  “A Comparison of HLA and DIS Real-Time Performance” Proceedings of the 1998 SIW Workshop, 98S-SIW-042, March 1998.[2] 	John R. Mellby, et. al.:  “Transitioning to an Object-Oriented Reconfigurable Simulation Architecture” Proceedings of the 1998 SIW Workshop, 98S-SIW-[3] 	Under Secretary of Defense for Acquisition and Technology, “Department of Defense Modeling and Simulation Master Plan, DoD 5000.59-P,” October 1995.[4] D. Florek, et. al.:  “Using HLA/RTI in an Engineering-Level Federation to Support Reconfigurable Simulation.”  Submission to Fall 1999 SIW.Author BiographiesEUGENE STOUDENMIRE is a software engineer at SAIC at Arlington VA supporting various simulation projects.TRANG ESCOBAR is a senior software engineer at SAIC, supporting the LRAS3 and MFS3 simulators at the Night Vision Laboratory at Fort Belvoir VA.MIKE CARUSO is a software developer for SAIC at the Night Vision Laboratory, Fort Belvoir VA.  He currently is working on the redesign of the Paint the Night simulation.MIKE BALDWIN is a graphics engineer at SAIC at Fort Belvoir VA.  He is currently working on the redesign of the Paint the Night simulation.MAX LORENZO is team lead of the Virtual Simulation/Prototype Branch at CECOM’s Night Vision Electronic Sensor Directorate at Fort Belvoir VA.MIKE MUUSS is a Senior Scientist at the U.S. Army Research Laboratory at Aberdeen Proving Grounds, MD.KENNY SHAFFER is a Software Engineer at Fort Belvoir VA.  He currently supports simulation projects at Night Vision Electronic Sensors Directorate at Fort Belvoir VA. EMBED Word.Picture.8   EMBED Word.Picture.8   EMBED Word.Picture.8  