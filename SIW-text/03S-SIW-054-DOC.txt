A Formal Basis for a Theory of Semantic ComposabilityMikel D. Petty and Eric W. WeiselVirginia Modeling, Analysis and Simulation CenterOld Dominion UniversityNorfolk VA 23529mpetty@vmasc.odu.edu  757-686-6210, eweisel@odu.edu  757-686-6227Keywords:Composability, SemanticsABSTRACT:  Composability is the capability to select and assemble simulation components in various combinations into simulation systems.  The defining characteristic of composability is the ability to combine and recombine simulation components into different simulation systems.  Two types of composability are considered:  syntactic and semantic.  Syntactic composability is the actual implementation of composability, and requires that the composable components be constructed so that their implementation details are compatible for the different configurations that might be composed.  In contrast, modeling (semantic) composability is a question of whether the models that make up the composed simulation system can be meaningfully composed, i.e., if their combined computation is semantically valid.  Although there has been some work on syntactic composability there has been almost none on semantic composability, though the need has been recognized.  A formal theory of semantic composability would enable the determination in a formal way of characteristics of interest of a composition of models, such as whether their combined computation is valid.  Two sets of ideas that form the basis for a formal theory of semantic composability are discussed.  First, definitions of model, simulation, composability, and validity suitable for formal reasoning are proposed.  Arguments that the definitions are appropriate for the purpose are provided.  Second, the apparent disconnect between formal reasoning and semantics is resolved using validity.  Some approaches to representing model semantics formally are identified.IntroductionComposability is briefly introduced in this section, and syntactic and semantic types of composability are distinguished.  The elements of a formal theory of composability are identified.ComposabilityComposability is the capability to select and assemble simulation components in various combinations into valid simulation systems to satisfy specific user requirements [1].  The defining characteristic of composability is that different simulation systems can be composed at configuration time in a variety of ways, each suited to some distinct purpose, and the different possible compositions will be usefully valid simulation systems.  Composability is more than just the ability to put simulations together from components; it is the ability to combine and recombine, to configure and reconfigure, sets of components from those available into different simulation systems to meet different needs.Two types of composability can be defined:  syntactic and semantic.  Syntactic and semantic composability have also been called engineering and modeling composability [1].  Syntactic (engineering) composability is the actual implementation of composability; it requires that the composable components be constructed so that their implementation details, such as parameter passing mechanisms, external data accesses, and timing assumptions are compatible for all of the different configurations that might be composed.  The question in syntactic composability is whether the components can be connected.  In contrast, semantic (modeling) composability is a question of whether the models that make up the composed simulation system can be meaningfully composed, i.e., if their combined computation is semantically valid.  It is possible that two components may be syntactically linked, so that one can pass data to the other, but semantically invalid, if the data produced by the first component is not within the bounds of what the other can validly accept.Basis for a Formal TheoryWe seek to develop a formal theory of semantic composability.  In general a formal theory has four elements:  objects, the things or ideas which are the subject of the theory; axioms, statements about the objects that are accepted as true without proof; rules of inference, which may be used applied to the axioms and previously proven theorems to produce new theorems about the objects; and a goal or purpose for the theory, typically to produce a set of interesting or useful theorems [2].  As the basis for a formal theory of semantic composability, formal definitions are provided for model and simulation, which are the objects of the theory, as well as for composability and validity, which are possible attributes of those objects.  By their nature the definitions invoke computability theory and mathematical logic, bringing with them the axioms, theorems, and rules of inference of those theories.  The ultimate goal is to produce a theory that enables the determination in a formal way of characteristics of interest of a composition of models, such as whether their combined computation is valid.DefinitionsFormal definitions of model, simulation, composability, and validity suitable for formal reasoning are proposed.  The definitions are compared to the informal definitions in general use.  Arguments that the definitions are appropriate for the purpose of developing a theory of semantic composability are provided.ModelThe official (and informal) definition of model is as follows:Model.  A physical, mathematical, or otherwise logical representation of a system, entity, phenomenon, or process [3] [4].This definition uses the purpose of a model to define it.  A model is intentionally a representation of something else, the identity of that something else is known, and the form of the model is open.  While intuitively useful, this definition is insufficiently formal to use as the basis of a formal theory.  A definition of model that effectively reverses the official definition’s specificity with respect to purpose and ambiguity with respect to form is proposed.  In other words, the proposed definition of model precisely specifies form and is ambiguous as to purpose.  (The notion of a model’s purpose will be recaptured later in the definition of valid.)  The definition is as follows:Model.  A computable function.The definition of model is crucial to the theory, so it should be examined closely.  With respect to “function”, the mathematical definition of function is meant:Function.  By a function from set X into a set Y, we mean a rule f which assigns to every member x of the set X a unique member f(x) of the set Y.  The set X is said to be the domain of the function f and the set Y will be referred to as the range, or codomain, of the function f [5].As for “computable” in the definition of model, the definition from computability theory is meant.  Informally, computable function is one that can be computed in a finite number of steps by an algorithm (equivalently, by a Turing machine or computer).  Note that computable functions are a subset of all functions (i.e., some functions are not computable, as proven by Turing [6] [7]).  Formal definitions of a computable function are available [7] [8] [9]; those definitions will not be repeated here, but will be used for the theory.Defining models as computable functions can be justified in three ways.  First, the definition of a model as a function allows the use of the existing body of mathematical knowledge on functions; composition of functions is well defined.  Similarly, the definition of a model as a computable function allows the use of the existing body of computability theory; a composition of computable functions is computable [8].  A definition of model that is unambiguous and is based on existing theory will support the goal to prove results about models.  Second, the models of interest in the theory are intended to implemented and executed as simulations on computers.  Computers (at least current non-quantum computers) have computational power equivalent to Turing machines, i.e., they can only compute computable functions.  Hence the definition of models as computable functions is a practical matter; models that aren’t computable functions are of little interest to the M&S community.  Finally, a philosophical argument has been made that the “unreasonable effectiveness of mathematics” in describing the physical world, which is the subject of models, is due to the fact that nature is computable, i.e., the laws of nature are computable [10].  Hence the definition of models as computable functions is consistent with the subject of the models.Note that the notion of a function is generally implicit or assumed in both the official definition of the term model and the way in which it is used by simulationists.  Models take input (typically, the state of the simulated system at time t) and produce output (the state of the simulated system at time t + dt, where dt is a simulation time interval).  Complex simulation systems (e.g., JSAF) may seem to do more than this, but because they are computer programs they are ultimately computing functions.  Of course, there are less-formal types of “models” that are not functions (e.g., conceptual models), but those are not of concern in the theory.With models defined as computable functions, it becomes necessary to specify the form and type of their inputs and outputs.  Recall that functions are defined to be “from set X into a set Y”, so specifying the inputs and outputs of models means defining the sets X and Y.  The following definition is proposed.Model inputs and outputs.  The inputs and outputs of models (i.e., computable functions) are vectors of integers.  In other words, both the domain X and the range Y for the models are sets whose elements are vectors of integers.This definition restricts the inputs and outputs of models to integers, and specifies vectors of integers, instead of single variables, or matrices.  Both of these points can be justified; we first consider the restriction to integers.  Practically speaking, for a theory semantic composability we are ultimately interested in models that can be implemented and run on digital computers.  All of the values that can be represented on such computers are constructed from bits (0s and 1s) and so are integers.  The so-called “real numbers” available in most programming languages are in fact integer approximations to real numbers.  Also, the restriction to integers is not quite as limiting as it might seem; the rational numbers (e.g., 1/3, 7/9) are available because by definition they can be expressed as the ratio of two integers, and so can be expressed as two elements of a vector of integers.  Theoretically speaking, the restriction to integers is consistent with the assumptions of computability theory [8] [9].  Informally, the reason for this is that irrational numbers such as ( can not be represented as a ratio of integers (by definition), nor can be they be represented as a decimal number by a finite sequence of integers.  Therefore they cannot be computed in a finite amount of time and are therefore not computable.  Computability theory is concerned with computable functions, i.e., functions that generate computable numbers.Specifying the inputs and outputs of models to be vectors, instead of single integers or matrices, is easier to justify.  Single integers, vectors of integers, and matrices of any dimension of integers are all equivalent.  Vectors of integers can be mapped one-to-one to single integers using a suitable variant of Cantor’s method for mapping rational numbers (which can be vectors with 2 elements) to single integers [10].  A similar argument works for matrices of integers to vectors.  Indeed, different presentations of computability theory use different choices; computable functions have been defined using both multi-variable functions, essentially functions on vectors [8], and functions on single integers [9].  Computability theory is ultimately concerned with formal equivalents of the computations of a Turing machine, which has a data tape that can be seen as containing a single integer or a vector of integers, depending on how the tape is logically partitioned.  We select vectors, rather than single integers or matrices, because it will be simple to distinguish between different input and output variables as elements of vectors.SimulationHaving defined model, we now define simulation in the context of the theory.  The official definition of simulation is as follows:Simulation.  A method for implementing a model over time.  Also, a technique for testing, analysis, or training in which real world systems are used, or where a model reproduces real world and conceptual systems [3] [4].In the official definition of simulation “implementing” actually seems to mean “executing”.  That is the sense of the term as commonly used; a simulation is an execution of a model, over (simulated) time, to produce a synthetic time sequence of states for the system being modeled.  For the theory the following definition is proposed:Simulation.  A sequence of executions of a model.Of course, the model being executed may be composite.  Recall that a model is a computable function.  We believe that it can be shown that under this definition a simulation can also be treated as a computable function, formed by suitable composition of the model being executed.Like the proposed definition of model, this definition of simulation is stripped of all explicit mention of the simulation representing anything, such as a real-world system.  This has been done deliberately because defining a model or simulation as representing something is assuming validity.  Validity is a property that models and simulations might or might not have, not something that they should be defined or assumed to have.  Of course, it is generally intended that a simulation is an execution of a valid model, but that is not where a formal reasoning process should start.Figure 1.  Simulation.Figure 1 illustrates the definition of simulation; it is adapted from a description of an execution of a “synchronous system” that conveys the sense of simulation [11] [12].  The model f is executed iteratively.  At each iteration (“step”) the model accepts input from the previous step (denoted m for “memory”) and from outside the system (denoted i for “input”) and produces output that is passed to the next step (m) and outside the system (denoted o for “output”).  The input i, memory m, and output o are all vectors of integers, as previously defined.  The computation at step n can be specified as:(mn, on) = f(mn-1, i n-1), e.g., (m1, o1) = f(m0, i 0)Input from and output to outside the simulation are included to allow for interactive simulation.  This is consistent with both the official definition of a simulation and with intuition.Composition and ComposabilityBecause models have been defined as computable functions, composition of models is just composition of functions, which is a well-defined mathematical concept.  Models (computable functions) may be composite, meaning that they have been defined by composing other models, or they may be non-composite, meaning that they have been specified directly without composition.  Specifying a non-composite model is done in some formalism that has been shown to be computable, such as Turing machines, (-calculus, or programming languages.The informal definition of composability given earlier conveys the intent of composability from a practical point of view, but is insufficiently formal to support a theory of semantic composability.  Because models are computable functions, any set of models can be composed, but there is no guarantee that the resulting composite function is a useful model.  This is an important point; our formal definition of model as computable functions exhibits the same distinction between syntactic and semantic composability.  Interestingly, however, it eliminates the matter of syntactic composability by defining models as computable functions, which are always composable.  The focus of composability in the theory then becomes semantic composability, the question of whether the composite model is valid.  The theory will be interested in whether properties of computable functions, i.e., validity, are preserved in composition, which leads to this definition:Composability.  A set of valid models (i.e., computable functions) is composable if and only if their composition is valid.There are two issues with this definition.  First, it depends on undefined term, “valid”.  We have not formally defined valid yet, so for now it is used without definition in order to define composability; a formal definition will be given later.  Second, under this definition the composability of a given model f is not determinable in isolation; it is only determinable with respect to a specific set of models f may be composed with.  Is it a problem that under this definition it is impossible to say “Model f is composable” in general?  Or is it an insight that composability is meaningless without reference to the set of models to be composed, and this definition reveals that fact?ValidityDefining composability required the notion of a valid composition.  We approach a formal definition of valid by way of a quantitative definition of validity.  The following notation will be used in this section:x, y	vectors of integersx1, yk	vector elementsX, Y	sets of vectors of integers f, g	models (i.e., computable functions)Let x and y be vectors of integers, with k elements.  Then they can be written:x = [ x1, x2, …, xk ]y = [ y1, y2, …, yk ]Vector subtraction is defined as follows:x – y = [ x1 – y1, x2 – y2, …, xk – yk ]Vector subtraction with absolute value is defined as follows:| x – y | = [ | x1 – y1 |, | x2 – y2 |, …, | xk – yk | ]Recall that a model has been defined as a computable function with domain and range as sets of vectors of integers.  Let f be a model, X and Y the domain and range of f, x ( X the input vector, and f(x) = y ( Y the output vector.  (Either f(x) or y will be used for the output depending on notational convenience.)A model is typically intended to represent some real-world or notional system; call it the modeled system.  Models take an input vector describing a state of the modeled system and compute an output vector describing the “next” state of the modeled system.  What “next” means is certainly model-dependent; for example, a discrete event model may compute the state of the system that results from an event described in the input.  The meaning of “next” may also be data-dependent; for example, in a flight dynamics model the input vector may include an element that is the time step, which obviously affects what the next state will be.Intuitively, the validity of a model is based on the difference between its outputs and the modeled system’s behavior.  Using the notion of a perfect model, we can define validity in a more formal way.  Define a perfect model as a notional model that computes, given as input a vector in its domain describing a state of the modeled system, an output vector that perfectly matches the modeled system’s next state.  Perfect models exactly compute the next state of the modeled system.  Let f* be a perfect model analogous to f, X and Y* the domain and range of f*, x ( X the input vector, and f*(x) = y* ( Y* the output vector that perfectly describes the next state of the modeled system after the state described by x.  (Either f*(x) or y* will be used for the output depending on notational convenience.)Define the perfect validity of model f for input x, denoted v*(f, x), as follows:v*(f, x) = | f*(x) – f(x) |The asterisk is used in the notation v* as a reminder that we are considering the validity with respect to the perfect model.  Note that validity is so far defined for a specific state, and its value is a vector (computed by vector difference).  In addition to a model’s perfect validity for a specific input vector, we may be interested in a value for a model’s overall perfect validity.  A definition of overall perfect validity for model f is as follows:v*(f) =	(	v*(f, x)	x ( XWhile perfect validity is a useful idea, a perfect model is not ordinarily available to simulationists, so calculating perfect validity may not be possible.  More likely to be available is information describing the modeled system’s behavior, i.e., a set of observations.  Those observations are represented formally as a set of ordered pairs as follows:O = { (x, y() }where x is an observed state of the modeled system and y( is the observed next state of the modeled system.  Because the elements of O are based on observation of the modeled system, they are assumed to be accurate within (, the limit of observational accuracy.  In other words, by definition:( x ( (x, y() ( O, | f*(x) – y( | ( (Note that in this definition ( must be a vector, with observational accuracy values given for each element of the input and output state vectors.Define an observational model as a model based on a set of observations.  Its domain is restricted to those input vectors that are present in the set of observations; for those inputs, the observational model returns the observed output vector.  Formally:f((x) = y( iff (x, y() ( OLet f( be an observational model analogous to f, X and Y( the domain and range of f(, x ( X the input vector, and f((x) = y( ( Y( the output vector.  (Either f((x) or y( will be used for the output depending on notational convenience.)  Now we can define the observational validity of model f for input x, denoted v((f, x), as follows:v((f, x) =	| y( – f(x) | iff (x, y() ( O	undefined otherwiseThe prime is used in the notation v( as a reminder that we are considering the validity with respect to the observation model.  Observational validity can only be defined with respect to states of the modeled system for which observations are available.  A model’s overall observational validity is initially defined as follows:v((f) =	(	v((f, x)	x ( (x, y() ( OThe previously given definitions of validity (perfect and observational) are application-neutral, i.e., they quantify a model’s validity over its entire domain and for all of the elements of the output vector.  Different applications of a model may be interested in the validity of a model for different subsets of its input domain, and may have differing requirements for the accuracy of the different elements of the output state vector.  Conceptually, making the definition of validity application-specific is straightforward; application-specific weights are applied to the differences.  However, the notation is a bit messy.  First, the definition for perfect validity is repeated and given in equivalent form using y* instead of f*(x):v*(f, x) =	| f*(x) – f(x) | = | y* – y | = [ | y*1 – y1 |,	y*2 – y2 |, …, | y*k – yk | ]Using the latter form, the application-specific perfect validity for input x for application a is defined as follows:v*(f, x, a) =	[ | y*1 – y1 | · a(x, 1), y*2 – y2 |	· a(x, 2), …, | y*k – yk | · a(x, k) ]where a(x, k) is an application-specific weight for state (input vector) x, element k.  States and elements that are more (less) important with respect to the application will have larger (smaller) values for a(x, k).The application-specific overall perfect validity is:v*(f, a) =	(	v*(f, x, a)	x ( XApplication-specific observational validity is defined similarly.So far, validity has been defined as vector quantities that may be calculated for a model, either for specific states or for the model overall, with respect to perfect or observational models, and for specific applications.  Using validity, valid can be defined as follows:Valid.  A model is valid if its validity is within a limit.This definition conveys the sense of what valid means in the context of the theory, but is not itself formal.  To determine if a model is valid, several considerations must be specified.  What is the model’s validity being compared to, a perfect model or an observational model?  Is validity being considered for a subset of the model’s inputs, or for all?  Is validity to be evaluated relative to a specific application?  Finally, what is the limit within which the validity must fall for the model to be considered valid?  Most simulationists would agree that any description of a model as “valid” has always had informal versions of these considerations implicit.  The formal definitions of validity add the ability to make the considerations explicit and quantify them.Formal Reasoning and SemanticsWith the definitions in hand, some other pertinent aspects of a theory of semantic composability are discussed.  The connection between formal reasoning and semantics is explained.  The idea of model meta-data is introduced.  Eventual goals for the theory are listed.Linking Formal Reasoning to SemanticsFormal reasoning is by definition devoid of meaning [2].  The objects of the theory are reasoned about using the axioms, theorems, and rules of inference in an entirely formal, i.e., syntactic, way.  Of course, in formal reasoning we often interpret the objects of the theory in meaningful ways (e.g., the points, lines, and planes of Euclidean geometry seem to abstract things we know from the physical world), but the reasoning within the theory does not depend on those interpretations.  It may seem odd, therefore, to seek to reason formally about semantic composability, because the semantics of the models are their meanings.  What is the connection between meaningless formal reasoning and the semantics of composability?  From the previous section it may be apparent that in the theory the connection is validity.  A model has semantic meaning precisely to the extent that it models something, and that extent is quantified and formalized via the definitions of validity.  This essential connection between formal reasoning and semantics is the basis for a formal theory of semantic composability, and is the reason composability is defined using validity.Semantic Meta-dataA number of aspects of a model have meaning, or semantics.  Both the semantics of a model’s statespace (its sets of input and output vectors, its domain and range) and of a component that implements that model may be encoded.  The encoding of the semantics of a component and the model it implements is called semantic meta-data.  The semantic meta-data should be encoded in a form that allows automatic algorithmic analysis of the semantics of compositions of components by a simulation development environment.  Ideally, the development environment will use the semantic meta-data to ensure that compositions of components are both syntactically and semantically composable.Though the need for the meta-data has been recognized, its form and processing algorithms in the context of the theory have not been determined yet.  We are examining several possibilities, including a branch of mathematical logic known as model theory [13], formal specification languages such as Z [14], and logical formalisms such as first-order predicate calculus [15].  Other forms of encoding semantics within the context of simulation have also been proposed; for example, the ModISE architecture for composable interoperability uses XML to encode the meta-data of each component [16].GoalsThe overall goal of a theory of semantic composability is to enable formal reasoning about characteristics of interest of a composition of models, such as whether their combined computation is valid.  Specific goals include:Composition validity results.  Necessary and sufficient requirements for preserving validity in a composition of valid models; an algorithm for determining if a given composition is valid; a determination of the computational complexity of that process.Model selection results.  Necessary and sufficient conditions for meeting a set of requirements with a composition of models; an algorithm for selecting a set of models that will meet a set of given requirements; a determination of the computational complexity of that process.  (The computation complexity of the component selection process has already been investigated [17]; we hope to generalize the result in the context of the theory.)Model complexity results.  Determination of the computational complexity of composite models, given the complexity of the component models.SummaryComposability is an important concept in modeling and simulation.  A formal theory of semantic composability would enable the determination of characteristics of interest of a composition of models, such as whether their combined computation is valid.  Definitions of model, simulation, composability, and validity suitable for formal reasoning are proposed as the basis for such a theory.  Validity is the link between formal reasoning and semantics.AcknowledgementsThis work was sponsored by the Office of Naval Research and the Defense Modeling and Simulation Organization under contract N00014-03-1-0089.  That support is gratefully acknowledged.  We also acknowledge the paper that initially suggested to us the idea of considering composability in a formal way [17].References[1]	M. D. Petty and E. W. Weisel, “A Composability Lexicon”, Proceedings of the Spring 2003 Simulation Interoperability Workshop, Orlando FL, March 30-April 4 2003, 03S-SIW-023.[2]	R. J. Trudeau, Introduction to Graph Theory, Mineola NY, 1993.[3]	DOD Instruction 5000.61, M&S VV&A, April 1996.[4]	DOD Directive 5000.59, M&S Management, January 1998.[5]	S. Hu, Elementary Functions and Coordinate Geometry, Markham, Chicago, 1969.[6]	A. M. Turing, “On Computable Numbers, with an Application to the Entscheidungsproblem”, Proceedings of the London Mathematical Society, Ser. 2, Vol. 42, 1936-1937, pp. 230-265; Correction, ibid., Vol. 43, pp. 544-546, 1937.[7]	M. D. Davis, Computability and Unsolvability, Dover, New York, 1982.[8]	M. D. Davis, R. Sigal, and E. J. Weyuker, Computability, Complexity, and Languages:  Fundamentals of Theoretical Computer Science,  Second Edition, Morgan Kaufmann, San Diego, 1994.[9]	R. Sommerhalder and S. C. van Westrhenen, The Theory of Computability; Programs, Machines, Effectiveness and Feasibility, Addison-Wesley, Wokingham UK, 1988.[10]	J. D. Barrow, Pi in the Sky:  Counting, Thinking, and Being, Little, Brown and Company, Boston MA, 1992.[11]	A. Borälv and G. Stålmarck, “Formal Verification in Railways”, in M. G. Hinchey and J. P. Bowen (Eds.), Industrial-Strength Formal Methods in Practice, Springer-Verlag, London, 1999, pp. 329-350.[12]	N. Halbwachs, P. Caspi, P. Raymond, and D. Pilaud, “The synchronous data flow programming language LUSTRE”, Proceedings of the IEEE, Vol. 79, Iss. 9, September 1991, pp. 1305-1320.[13]	W. Hodges, Model Theory, Cambridge University Press, Cambridge UK, 1993.[14]	B. Potter, J. Sinclair, and D. Till, An Introduction to Formal Specification and Z, Prentice-Hall, London UK, 1991.[15]	J. L. Hein,  Discrete Structures, Logic, and Computability, Second Edition, Jones and Barlett, Sudbury MA, 2002.[16]	M. Biddle and C. Perry, “An Architecture for Composable Interoperability”, Proceedings of the Fall 2000 Simulation Interoperability Workshop, Orlando FL, September 17-22 2000, 03S-SIW-073.[17]	E. H. Page and J. M. Opper, “Observations on the Complexity of Composable Simulation”, Proceedings of the 1999 Winter Simulation Conference, Phoenix AZ, December 5-8 1999, pp. 553-560.Authors’ BiographiesMIKEL D. PETTY is Chief Scientist of the Virginia Modeling, Analysis & Simulation Center at Old Dominion University.  He received a Ph.D. in Computer Science from the University of Central Florida in 1997.  Dr. Petty has worked in modeling and simulation research and development since 1990 in the areas of simulation interoperability, computer generated forces, multi-resolution simulation, and applications of theory to simulation.  He previously worked for the Institute for Simulation and Training at the University of Central Florida.  He has served as a member of a National Research Council committee on modeling and simulation and is currently an Associate Editor of the journal SIMULATION:  Transactions of the Society for Modeling and Simulation International.ERIC W. WEISEL is a Project Scientist at the Virginia Modeling, Analysis & Simulation Center at Old Dominion University.  He received a M.S. in Operations Research from the Florida Institute of Technology in 1995 and a B.S. in Mathematics from the United States Naval Academy in 1988.  He is currently a Ph.D. student in modeling and simulation at Old Dominion University.  His research interests include simulation formalism and composability.  He is also President of WernerAnderson, Inc., a private veteran-owned small business that performs research and development in the areas of defense-related operations research and simulation.  He previously served as a U.S. Navy submarine officer with experience in nuclear engineering, operations management, navigation, and joint operations. If the compositions aren’t valid, then by definition they aren’t composable. The complete theory is not covered in this paper; here we propose definitions that form the basis of one. Technically, real computers are less powerful than Turing machines, because abstract Turing machines have infinite memory (the “tape” is assumed to be infinitely long) and real computers have finite memory [7].  This means that defining models as computable functions in fact includes models that can’t be run on a real computer, which strengthens the claim that computable functions include all models of interest. This notation should not be considered final.  Using i, which is conventionally a subscript, and m, which is conventionally the size of a set, as vector names is potentially confusing.  They were used here as mnemonics for “input” and “memory”, but will likely be changed as the theory is developed. By “component” we mean a piece of software that implements a model.  Though the theory is focused on models, the semantic meta-data relates to both models and components.PAGE  