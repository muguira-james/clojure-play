An Automated Interoperability Testing SystemPaul PerkinsonWilliam BabineauRobert HeadMichael KirkChris SchwindtJohn TufaroloVirtual Technology Corporation5510 Cherokee Ave., Suite 350Alexandria, VA 22312 HYPERLINK "mailto:pperk@virtc.com" pperk@virtc.com,  HYPERLINK "mailto:babineau@virtc.com" babineau@virtc.com,  HYPERLINK "mailto:rhead@virtc.com" rhead@virtc.com,  HYPERLINK "mailto:mkirk@virtc.com" mkirk@virtc.com,  HYPERLINK "mailto:cschwindt@virtc.com" cschwindt@virtc.com,  HYPERLINK "mailto:jtufarolo@virtc.com" jtufarolo@virtc.com Erik S. Hougland, PhDDaniel DykeSusan HaselbyNAVAIR Orlando12350 Research Pkwy.Orlando, FL  32826 HYPERLINK "mailto:erik.hougland@navy.mil" erik.hougland@navy.mil,  HYPERLINK "mailto:daniel.dyke@navy.mil" daniel.dyke@navy.mil,  HYPERLINK "mailto:susan.haselby@navy.mil" susan.haselby@navy.milKeywords:HLA, Interoperability, Federation Agreements, Testing, NASMP, SBIR, Navy TrainingABSTRACT:  This paper describes the results of the first phase of the Navy’s Automated Interoperability Testing (AIT) Program, including experiences and lessons learned from applying the AIT tools and techniques to the Navy Aviation Simulation Master Plan (NASMP). Despite the criticality of federation interoperability to achieving simulation objectives, interoperability testing remains a largely manual and even ad hoc process and there is no systematic method for capturing and ensuring compliance with federation agreements. AIT is an on-going research and development effort into automated mechanisms to describe and measure federation interoperability and to specify and capture complex federation agreements. The AIT Phase I research produced important results, including identification of key components of federation interoperability necessary for producing a valid and reliable outcome to a distributed federation and a highly functional prototype system for testing federation agreement compliance. 1. IntroductionSimulation has become an important tool for improving military readiness by supporting simulation based acquisition, training, and mission rehearsal. Distributed, networked simulation is a significant advancement enabling multiple simulation components to be linked to achieve a common objective. Critical to maximizing the benefit of simulation is the need to achieve a high level of interoperability among simulation components. In this sense interoperability means not just basic connectivity, but advanced coordinated functionality where simulation components are fully coordinated and produce a valid and reliable result. Interoperability enables reuse and composability of simulation elements, thereby increasing simulation utility and reducing program costs. The DoD’s Small Business Innovation Research (SBIR) program is a multi-phased program funding small business research and development (R&D) projects, which serve a DoD need and have the potential for commercialization. Phase I focuses on demonstration of the technical and commercial merit of a concept.  The Automated Interoperability Testing (AIT) SBIR is a Navy program identifying innovative approaches to automated interoperability testing of High Level Architecture (HLA) based simulation. During the Phase I AIT effort, Virtual Technology Corporation (VTC) investigated and identified key components of federation interoperability, designed and prototyped an AIT system, and demonstrated the value of the research by applying AIT technology to Navy Aviation Simulation Master Plan (NASMP)interoperability verification. 2.  HLA and InteroperabilityProtocol standards, such as Distributed Interactive Simulation (DIS), and more sophisticated Modeling & Simulation (M&S) standards, such as the HLA, have enabled connectivity of simulation components, however, these standards do not ensure effective simulation component interoperability in and of themselves. The HLA standards were developed based on the notion that a single, monolithic simulation cannot satisfy the needs of all simulation users [1]. The standards provide the flexibility to develop, reuse, and connect simulation components (federates) into groups (federations) to satisfy a diverse set of program requirements. The HLA provides a standard set of distributed M&S services and data interchange formats that, with federation development and execution expertise, can be used to achieve interoperability among simulations.The HLA began as a DoD standard, and has evolved into an open standard – the Institute of Electrical and Electronic Engineers (IEEE) Standard 1516. The HLA standards consist of three main components:  a set of architectural rules, an object model template (OMT) specification, and an interface specification (IfSpec). Run-time infrastructure (RTI) software exists to implement the interface specification and provide simulation components with a set of distributed services. A set of software Application Programmer Interfaces (APIs) exists to provide a well-defined interface by which a federate interacts with an RTI.Fundamentally, federations evolve around the definition and application of a Federation Object Model (FOM). The FOM describes a hierarchical format for exchanging data between federates in a federation. These format descriptions are standardized (via the OMT specification) and include many of the details for federates to produce and receive data. The software APIs dictate, from a mechanical perspective, how federates interact with an RTI. Unfortunately, neither the OMT specification nor the software APIs include sufficient semantic information about the purpose or representation of data to be passed among federation participations to unambiguously define and achieve interoperability.Federation AgreementsThe Federation Development and Execution Process (FEDEP) is an iterative process for development, execution, and maintenance of HLA federations. [2] An important component of the FEDEP process is the development of a FOM and Federation Agreements. The flexibility of the HLA standards, intended to enable the tailoring of data models, data encoding, and time management schemes, can also serve as an impediment to interoperability. Key factors for interoperability, such as data model semantics and model behavior are, by design, unaddressed by the HLA standards. For example, federations are required to develop a FOM using the OMT format and containing many details about how federation data is represented. Missing in the FOM are detailed semantics: What circumstances cause a federate to create and update objects? What will cause production of interactions? What dead reckoning or other update filtering mechanisms should be employed by a federate? What actions should a federate take after receiving a particular interaction or object attribute update? These details are a necessary companion to the FOM description in order for federation participants to interact meaningfully and achieve simulation objectives. The complex relationships that often exist between federation participants means that introducing one incompatible federate can cripple an entire federation and consume valuable integration and test time. As a result, federations are forced to develop federation agreements to address important aspects of simulation interoperability and it is necessary to ensure each federation participant complies with these agreements. Since these federation agreements are not part of any M&S standard, each federation develops independent mechanisms for capturing this critical federation interoperability information. Therefore, while federation agreements serve the useful and necessary purpose of documenting a federation’s interoperability requirements, the applicability of current federation agreements to automated interoperability testing is severely limited. HLA Interoperability TestingDespite the criticality of federation interoperability to achieving simulation objectives, interoperability testing remains a largely manual and even ad hoc process, and there is no systematic method for capturing and ensuring compliance with federation agreements. There are no general-purpose off-the-shelf tools or processes for evaluating interoperability and compliance with federation agreements. The federate compliance testing available from DMSO only evaluates compliance with a set of HLA standards that themselves do not address several critical aspects of interoperability. While the purpose of simulation is to produce results, the current state of the art in interoperability testing often requires such a large amount of effort to be applied toward integration and testing that the time and resources available for achieving simulation objectives can be adversely affected. This, combined with the challenges of achieving complete semantic interoperability, has occasionally compromised the results of simulation and limited its ability to fully support program objectives. Current federation integration and testing routinely consume significant amounts of a simulation program schedule verifying basic interoperability requirements, such as data model (Federation Object Model (FOM)) compatibility, before addressing the most significant interoperability challenges required for fair fight, such as data model semantics, model behavior, and database consistency. Also, many federations consume significant resources and time periodically bringing together federates to integrate and regression test basic functionality and interoperability. Often, these efforts apply a majority of the testing resources to achieving basic interoperability, limiting the ability to diagnose and resolve more advanced interoperability issues. As the use of simulation continues to grow, so will the cost of the current, manual interoperability testing processes. AIT System Architectural ConsiderationsPreliminary investigations and past experiences with federate and federation testing efforts guided the formulation of several interoperability testing system architectural considerations. Transparency of Testing to FederatesAn important interoperability testing system characteristic is that the federate under test must operate identically to the way it was designed to work, without requiring special setup, configuration, or software changes. A system requiring special federate tailoring is very undesirable, because the process of tailoring a federate can induce new errors, or prevent existing errors from being observed. In particular, requiring specific values for RTI parameter settings (e.g., advisories, service logging) or limiting the hardware platform, development language, RTI version, or the RTI services that can be employed will severely limit the effectiveness of the testing system.Monitoring Federate ActivityDifferent approaches to monitoring the run-time behavior of a federate can be characterized as “black-box” or “white-box” approaches. Black-box approaches are only concerned with the consequences of a federate interacting with the RTI, as visible to the rest of the federation via object updates and interactions. White-box approaches consider more specific behavior between the federate and the RTI; for example, RTI ambassador and federate ambassador service calls between the federate and its local RTI component. Many federation agreements, such as those related to ticking or time-advancement strategies, require knowledge of more detailed federate-to-RTI interface behavior not directly visible to the rest of the federation. Therefore, a black box approach to testing ignores several important interoperability considerations and does not prove to be an adequate general-purpose means of testing. An interoperability testing system must support more rigorous testing by using both approaches to run-time federate monitoring.RTI IndependenceA critical characteristic of a general-purpose HLA interoperability testing system is independence from a specific RTI implementation. To achieve this, strict adherence to the HLA standards regarding RTI implementation is necessary. The HLA specification describes a specific set of HLA services and APIs. Many commercial RTI implementations augment these standards with custom APIs (e.g., for monitoring internal behavior). It is important that an interoperability testing system implementation avoid utilizing these proprietary APIs or the system will only be useful when using that specific RTI implementation.FOM and Federation IndependenceUnlike DIS, a fixed data representation is not part of the HLA standard. Instead, the HLA provides a standard framework for capturing the representation of data exchanged among federates to achieve federation objectives. The OMT prescribes the format and syntax for documenting the FOM data exchanged including objects, attributes, interactions, and parameters. Many of the concepts captured in the OMT map directly to the characteristics of an object-oriented data model definition. The OMT is designed to be flexible enough to support the representation of any simulation problem space. Federation developers are therefore free to develop object models that capture a representation of the real world domain that applies to the particular federation problem space.To accurately test the compliance of federates with the federation agreements, it is important for the interoperability testing system to understand the federation’s data model. However, it is equally important, for the interoperability testing system to be generally applicable to any federation, and not be tied to a specific FOM or federation. Instead the testing system must be able to read and understand all FOMs, including widely employed FOMs such as the various Realtime Platform Reference (RPR) FOMs and Navy Training Meta FOM (NTMF).3.  Results of the Phase I ResearchThe technical objective of the AIT Phase I effort is improved interoperability assessment and testing by establishing an Interoperability Maturity Model as a framework for interoperability assessment and improvement, developing a means to capture and document federate interoperability requirements (i.e., Federation Agreements), and prototyping an Automated Interoperability Testing system. Interoperability Maturity ModelVarious process and capability improvement models are increasingly applied to product and service development both as a means of assessment and as part of a systematic approach to improvement. Maturity models have been proposed for a range of activities from management to software and systems engineering. The principle behind a maturity model is that it captures what is regarded as good practice, along with some intermediate or transitional stages, by describing the typical behavior exhibited at each stage of maturity. This type of model serves as a valuable yardstick for programs interested in assembling a federation from off-the-shelf components and for measuring the suitability of components for use with one another, or within a particular federation. The maturity model serves as the basis for a systematic approach to testing interoperability by identifying key factors that determine federate interoperability, how these factors are defined, how they can be measured, and how testing federate interoperability can be automated. The maturity model clearly defines an evolutionary approach to an organization or program’s improvement. These, in turn, provide the intellectual foundation for an automated testing system. Interoperability is a term having wide variations in interpretation depending on the context of usage, which makes federate interoperability testing and measurement difficult. Therefore, the Phase I efforts captured in this section defined a framework for describing simulation interoperability within an HLA Interoperability Maturity Model (HIMM) to facilitate testing and measurement of federate interoperability. An elemental aspect of the HIMM is a general set of Simulation Interoperability Characteristics.  These characteristics are used to assess the capabilities of a simulation system’s interfaces, and to help assign a maturity level to those capabilities.  The characteristics are comprised of a list of factors.  Each factor has an individual set of descriptive information associated with increasing levels of maturity.  REF _Ref78044083 Figure 1 presents an illustration of the simulation interoperability characteristics. The Simulation Interoperability Factors (grouped into “general” and “specific HLA” factors in this figure) are a list of attributes germane to describing simulation interoperability maturity. Each factor includes unique descriptions for a set of increasing levels of maturity from Level-0 through Level-5. Figure  SEQ Figure \* ARABIC 1 Simulation Interoperability Characteristics (Summary)Existing research at NAVAIR TSD Orlando [3] and other locations established a multi-level interoperability maturity characterization that has helped the community to better appreciate this issue. During this SBIR research, VTC built on these and other published concepts, including [4], by considering the range of HLA interoperability specifications, both within and external to the HLA standards, and established the set of simulation interoperability characteristics presented above.  From this complex array of characteristics and maturity levels, it was constructive to define a set of HLA Interoperability Maturity Levels, and then associate the “achievement” of an HLA maturity level to a particular cross-section of the general simulation interoperability maturity factors.  REF _Ref78044208 Figure 2 shows the resulting levels of maturity within the HIMM. Figure  SEQ Figure \* ARABIC 2 Levels of HLA Interoperability Maturity It is important to note that two simulation systems with a similar or high degree of interoperability maturity are not necessarily interoperable themselves: they may be two systems modeling completely orthogonal entities that have no relationships with each other.  The value in the maturity assessment and a high degree of maturity is that there is likely sufficient information captured in order to quickly determine whether the systems have any business trying to interoperate. A large amount of what determines federate interoperability falls outside the HLA standard and isn’t otherwise documented. Thus, it was necessary to drill down into the HIMM and identify specific, key factors that determine federate interoperability, refine their definition, and document their measurement, prior to designing a means to automate testing federate interoperability. Therefore, our Phase I research focused on what causes or prohibits federates from interoperating. Our research identified 76 key factors in 12 categories for assessing federate interoperability. VTC’s AIT Phase I Final Report [5] describes a scale for measuring each of these factors and calculating an interoperability maturity value for each. The discovery of these key factors is a significant accomplishment, because it directly corresponds to the required set of federate characteristics collected and measured by an AIT system and sets the stage for successful completion of the remaining Phase I technical objectives, the AIT design and prototype. AIT PrototypeThe AIT prototype design and development effort focused on demonstrating the value added by VTC’s AIT solution. To achieve this goal, the Phase I prototype effort started with an analysis of existing federate/federation testing alternatives, such as the Defense Modeling and Simulation Office (DMSO) federate compliance testing, the Federation Verification Tool (FVT), and the Simulation Interoperability Test Harness (SITH) to identify requirements for this effort and the short comings of these tools as interoperability testing solutions. Architecture and DesignThe Test System Application is at the heart of the AIT architecture. Although a single application from the perspective of the user, it comprises several logically distinct subsystems. The agreement’s elicitation subsystem collects information about federation interoperability agreements from the user. This subsystem coalesces information from the OMT and other standard federation documents with federation-specific interoperability agreement information entered via an interview-based series of screens. The documentation generation subsystem produces developer documentation based on the federation agreement information entered into the tool, obviating the need to maintain separate representations of the information for documentation and testing purposes.The run-time portion of the Test System Application consists of the federation proxy, runtime monitoring layer, and test adjudication and results viewer subsystems. The federation proxy subsystem is capable of stimulating the interface of the federate under test. The federation proxy capability will allow federates to be tested in isolation or with other federation participants. The runtime monitoring layer is a small, optional dynamically linked library that can be seamlessly interposed between the federate under test and the RTI. This layer monitors the interaction between the federate and its local RTI component and communicates this information to the test adjudication subsystem. The test adjudication and results viewer subsystem interprets the raw data produced by the system to determine whether the federate is complying with the federation’s agreements. Results information is displayed in real time as the federate is executing and can also be stored for later viewing.Figure  SEQ Figure \* ARABIC 3 VTC's AIT Prototype ArchitectureUser InterfaceThe user interface of VTC’s AIT prototype system enables the user to construct interoperability test cases, in the form of flowcharts, from generic test step templates (referred to as “blocks”) and specialize these blocks to perform federation-specific tests by specifying a set of parameters. For example, the system has a generic “CreateAndJoinFederation” block, which is specialized by providing a federate name, federation name, and FED file. These parameters can be provided during test case development or to maintain maximum flexibility the parameters can be supplied just prior to test execution through a series of questions posed to the user. Federate/federation interoperability test cases are constructed by visually chaining blocks together and specializing the blocks by supplying the necessary parameters. When the test case is executed, it begins at the “Start” block and follows the green connector if the block passes and the red connector if the block fails.   The test case ends by reaching the “Finish” block. During test execution the application shows the pass/fail status of each step in the test case, which step is currently executing, and detailed test case output. The main purpose behind the red “fail” connector is that it gives the capability to allow for testing cleanup to occur before leaving the test.  For instance, this may be useful if you want the federation to be destroyed after every test case run regardless of whether it passes or fails.  REF _Ref78095588 Figure 4 shows the AIT prototype user interface. Figure  SEQ Figure \* ARABIC 4 AIT System User InterfaceFederate/Federation ProxyBecause of the significant interdependency and tight interaction of some federates during full federation execution, a key capability of the VTC’s AIT prototype is the ability to act as a proxy for federates that are not present during testing. By enabling testing against absent federates (through this proxy capability) the AIT prototype enables federate interaction testing to occur earlier, before all federates are fully implemented. The AIT prototype can create and control numerous federate proxies at the same time. The prototype accomplishes the federate proxy capability by enabling scripting of federate proxy behavior and playback of collected data from an absent federate. The proxy federate scripting works similarly to the test case visual scripting flowchart capability described in section  REF _Ref78099067 \r 3.2.2. The AIT prototype makes the same RTIAmbassador calls that the absent federate would otherwise be making.  VTC has also worked on incorporating playback of collected HLA federation data into the AIT prototype. This serves to directly stimulate the federate under test or provide the background virtual environment for testing. This was accomplished by incorporating a Java RMI interface between the prototype application and VTC’s hlaResults® data collection and playback tool, which can be controlled directly from the AIT graphical user interface.Test Adjudication and Results ViewTest results reporting and problem tracking are key elements of developing and delivering a quality software product that complies with federation interoperability requirements and supports achievement of program objectives. Software problem reporting provides a means of making and controlling changes to federate software, avoiding repeated mistakes, and is the start of a systematic approach to focused federate regression testing. Software problem reports are used to identify and document the functional and physical characteristics of software problems, including failed federate compliance tests, and to track those problems to resolution. Detailed and accurate test results are necessary to ensure each problem is identified, tracked, and resolved. Therefore, it is important that an AIT system track requirements to test cases, test description, test prerequisites and procedures, dates and times of test execution, and test results. The test system should also track the software version of the federate under test to ensure that over time software problems are solved and new problems are not introduced.  After a software problem is resolved, the corrected software should be re-tested, and a determination made regarding requirements for regression testing to check that the problem resolution didn't introduce additional problems.  As tests are performed by the AIT prototype system, details of the tests are logged that allow reports to be generated after testing.  These include Test Case Run History Report (details each test execution), Questionnaire Report (displays the user-supplied answers to federate-level questions that are gathered before execution of tests), Federate-Under-Test Software Version Report (presents test case run history for a specific software version), and Test Case Report (provides traceability from requirements through test cases to test results).  REF _Ref78094709 Figure 5 is an example Test Case Report generated by the AIT prototype system. Figure  SEQ Figure \* ARABIC 5 Sample AIT Prototype Test Case ReportRTI Intercept LayerFederate and federation interoperability testing requires the ability to collect detailed information about each federate’s communications within the federation and to analyze the results. The RTI-independent federate testing intercept layer provides a non-obtrusive layer capable of monitoring all service invocations between the federate and RTI (FederateAmbassador and RTIambassador) without being reliant on MOM or a particular RTI implementation to provide this information.  The layer has the ability to log all RTI ambassador and federate ambassador service calls, including parameters, return values, and exceptions. The RTI Intercept Layer enables the AIT prototype systems to perform the white box testing described in section  REF _Ref78095969 \r  \* MERGEFORMAT 2.3.2 and provides the prototype application with important federation interoperability details that are not available through standard federation interfaces (e.g., tick, time management, and data distribution management parameters). VTC has developed a technique, based on the dynamic linking facilities available in modern operating systems, for seamlessly introducing such a layer into any federate – even a federate only available in a binary executable form. The layer works by mimicking, on one side, the FederateAmbassador and, on the other, the RTIAmbassador, so both the federate and RTI believe they are communicating directly with each other rather than through the layer. The intercept layer interposes itself between the federate under test and the RTI without requiring rebuilding or relinking of the federate and is capable of operating with all HLA standards compliant RTIs.  The intercept layer sends the service details over a network connection to the AIT prototype application, which can then use the data to determine test execution results. While the performance of the layer was a major concern, several performance optimizations significantly reduced the impact on federate and federation performance. The RTI Intercept Layer is an optional component of the AIT prototype system. Object Model AnalysisObject model analysis and comparison plays an important role in the Concept of Operations of a full lifecycle interoperability testing system. The compatibility of FOMs is critical to federate and federation runtime interoperability. However, object model (SOM, FOM, and BOM) evolution is inevitable and results in numerous potentially incompatible variations of the original object model. Even reference FOMs, such as the Realtime Performance Reference (RPR) FOM, which were developed as off the shelf FOMs to facilitate interoperability, have numerous versions and several variations of those versions. Quickly identifying the differences between object models and their significance is a valuable tool for identifying and eliminating incompatibilities between object models. VTC’s AIT prototype solution contains a comprehensive object model analysis capability that performs a context sensitive comparison. Unlike simple text based diff tools, the AIT object model analysis capability understands the significance of all OMT constructs and internally builds a tree structure for comparison purposes. Therefore, VTC’s AIT solution won’t be fooled by a simple reordering of the contents of two FOMs. For any object model element (object and interaction classes, attributes, parameters, complex datatypes, enumerations, routing spaces, and notes) the tool will identify different, identical, renamed, duplicate, or unreferenced elements.  REF _Ref78094633 Figure 6 shows the Object Model Analysis WebDiff Interface. The major components of the web based interface include the navigation bar, side by side comparison, and differences details. Although object models are often very large, the navigation bar provides a complete view of object model differences hyperlinked to the object model contents. The side by side comparison view shows the object model contents aligned visually. The details table at the bottom on the interface provides the details on any differences between object models.  SHAPE  \* MERGEFORMAT Figure  SEQ Figure \* ARABIC 6 AIT Prototype FOM Analysis User Interface4.  Navy Aviation Simulation Master Plan (NASMP)The Navy Aviation Simulation Master Plan (NASMP) is the acquisition philosophy being applied to implement the CNO directed Fleet Aircrew Simulator Training (FAST) requirements, and applies to all Navy Aviation type/model/series aircraft platforms except utility and the designated Sundown Platforms.  This effort coordinates the modernization and integration of current Navy aviation simulation assets and the procurement and integration of new simulation assets into a tactically relevant environment.  Designed to complete requirements listed in the current Platform Training and Readiness (T+R) Matrices, NASMP will support aircrew training during the Inter-Deployment Readiness Cycle. NASMP is designed to lead the revolution in Navy aviation training simulation and ensure the warfighter’s ability to win in today’s network centric battlefield while minimizing total ownership cost.  The NASMP vision is to provide a seamless integration and modernization of all Navy aviation simulation assets in a tactically relevant environment that will meet the fleet’s training requirements throughout the entire training continuum (undergraduate to post-Fleet Replacement Squadron) in a cost effective manner.NASMP goals include establishing a common architecture, creating inherent operability, enabling networked service, joint, and coalition training, and reducing total ownership costs.  In order to achieve these goals, NASMP will embrace a networking architecture and a set of simulation standards and tools which accommodate the sharing of software resources, databases, simulation toolkits, and the a suite of simulation resources.  NASMP considers the entire operational cycle, including mission planning, brief, execution, and After Action Review (AAR) feedback phases.The NASMP acquisition philosophy marks a significant shift in the direction of Naval aviation training simulator acquisition.  Previously, trainers were procured that functioned independently; each with its own synthetic battlespace and natural environment.  Most supported a single virtual simulator cockpit, but some trainers had dual virtual simulator cockpits allowing pilots to practice one-on-one tactics against each other or two ship tactics against embedded or internally generated opposing forces.  NASMP will take advantage of new technologies that support interoperable distributed simulations.  In order to promote fair fight and reduce total ownership costs, NASMP will also assemble a common component software toolkit which is available for incorporation into the virtual simulators.  .  NASMP Federation AgreementsThe interoperability of NASMP platform specific simulators and support components is essential to realizing the full benefit from the distributed training system and achieving the Navy’s objective of integrated team training. Achieving this kind of interoperability requires that each NASMP compliant simulator, component, or training program to comply with the same set of federation agreements. The NASMP Federation Agreements Document (FAD) [6] specifies the agreements under which the NASMP federation exists and operates; including specific guidance for developers of NASMP federates in the following areas:HLA and RTI agreements;Data Representation agreementsFederation Operations agreementsFederation Testing agreementsStatus Monitoring agreementsEnvironment agreementsFederate agreementsPlatform-specific agreements (e.g., F/A-18)The FAD is the primary mechanism used to capture federate interoperability requirements that are not otherwise covered by the HLA standards or other application specific functional requirements documents. In the NASMP FAD these requirements are grouped into related areas to enable the easy construction of test cases. The size and complexity of the planned NASMP system, including the large number of new and retrofitted legacy platform training simulators and other components linked across the distributed training environment, underscores the importance of documenting and ensuring compliance with interoperability agreements. An example of that complexity is Asset Allocation. The process of initiating a NASMP federation, including determining which federates are available and allocating those assets to execution federation(s), is a complex, multi-phased, multi-federate, and multi-federation process called Asset Allocation. NASMP training exercises will not initialize and run without each federation participant complying completely with the Asset Allocation agreement. Therefore, it is critically important to ensure compliance of each federation participant with Asset Allocation, and all of the other federation agreements documented in the FAD. Interoperability TestingNASMP, like other system of system simulation environments, integrates component systems into a federation to achieve battlespace modeling sufficient to provide individual and team training.  These components include new systems being developed in parallel with NASMP and legacy systems developed prior to, and independently of, NASMP. As these component systems evolve to achieve individual requirements and goals, their ability to work cooperatively within the federation must be tested periodically – one such process is presented in [7]. Since NASMP consists of multiple components distributed at various locations (both operational and developmental), it is linked together using Local Area Network (LAN) and Wide Area Network (WAN) transport. Currently fielded Fleet Replacement Squadron (FRS) simulators experience high utilization rates - 90-95 percent simulator utilization for some communities [8]. Therefore, not only are the challenges of integrating and testing these distributed components significantly increased, but any downtime will result in a significant loss of training. The complexities of testing distributed systems are touched upon in [9]. To reduce the impact of interoperability compliance testing for NASMP simulators and components, a clear description of the federation’s interoperability requirements (i.e., FAD) and a means of enabling the component systems to test their federate interfaces for compliance at their local development sites are needed. This is not a new idea – see [10] – but a means to do this based on federation artifacts has not been successfully implemented yet. Given the potential return, in terms of reduced time interpreting interoperability requirements and constructing interoperability test harnesses and increased time implementing and verifying training system capabilities, providing individual federate development teams with an automated interoperability testing system is easily justified. This automated interface testing solution should be used by each component system prior to federation integration and testing to ensure that basic interfacing and federation agreement compliance works correctly – this, in turn, allows testing events to concentrate on federation-level testing and validation, rather than federate-level testing as often happens currently. With interoperability compliance verification being performed prior to system wide integration events, more time can be spent during these events verifying training system functionality. With complex federation interoperability requirements and several distributed development teams, NASMP provided an ideal environment for proving the value of AIT. Therefore, the NASMP Network Operations Integrated Product Team (IPT) Designed and developed the Federation Agreement Compliance Test Tool (FACTT) using innovative compliance verification processes and tools like those identified and developed during the AIT Phase I SBIR effort. Federation Agreements Compliance Test Tool (FACTT)The NASMP Tactical Environment (TE) IPT is currently charged with developing a virtual warfare environment generator that includes a Computer Generated Forces (CGF) model and a Weapon Server, .  The CGF model and Weapon Serverwill be integrated as federates using the HLA. In order to deliver a system capable of accomplishing coordinated team training each of these components and the F-18C DMT, which is currently under development, must comply fully with the NASMP FAD. To verify FAD compliance, the NASMP Network Operations IPT developed the FACTT. The FACTT automates a portion of the NASMP Federate Compliance Testing Process,  REF _Ref78087628 Figure 7, by supporting the development of test cases, specification of test procedures, test execution, and results reporting. Figure  SEQ Figure \* ARABIC 7 NASMP Federate Compliance Testing ProcessThe FACTT provides each NASMP IPT with a system which enables federate developers to test their federate interfaces for compliance at their local development sites prior to federation wide integration events, but also supplies federate developers with a common perception of the federation agreements thereby reducing confusion about the intent and interpretation of federation agreements. To date the FACTT has implemented and been used to test several aspects of the NASMP FAD, including federation initialization (Asset Allocation), Application Level Monitoring (ALM), XDR FOM data encoding, and CGF load testing. 5.  ConclusionsNumerous HLA federations spend significant integration and test time verifying only basic connectivity rather than substantive interoperability [4], where the greatest return on simulation investment is realized. A clear description of the federation’s interoperability requirements (federation agreements) and a common interoperability (federation agreements compliance) test harness are two valuable tools for achieving greater simulation component interoperability earlier in the federate development lifecycle. M&S industry standards for federation agreements content and representation should make development and reuse of interoperability specifications across federations easier and facilitate automation of validation and testing. The identification of the key factors that make up federation interoperability has clearly identified the federation characteristics an Automated Interoperability Testing system must collect and evaluate to determine interoperability compliance.The application of AIT technology to NASMP interoperability testing has clearly demonstrated the value of a common federation wide interoperability testing solution. Distribution of the FACTT to NASMP IPTs also revealed differences of opinion among federate developers interpreting federation data encoding scheme in the FAD. Identification of these differences prior to federation wide integration testing allowed us to avoid more significant problems at an integration event. As the NASMP federation agreements have evolved updates to the FACTT have been necessary. The AIT prototype proof of concept demonstration at the Interservice/Industry Training, Simulation & Education Conference (I/ITSEC) and the results of this Phase I R&D effort demonstrated a valuable capability. Additional development is required to commercialize the AIT technology, including:- The AIT currently works by specifying interoperability test cases that are created from federation agreements rather than capturing accurate federate behavior, which is directly documented in the federation agreements. Specifying how to test each requirement rather than the requirement itself is a useless abstraction; because it is often more difficult for the user to generate and means the tool doesn’t have federation agreements data from which to generate documentation. The user interface metaphor should be changed to support easier data entry and greater federation agreements documentation generation. - Federation infrastructure testing should be associated with federate interoperability testing (e.g., test network hardware performance during federate load testing). - Continue to investigate and improve on the RTI intercept layer, including a service call interest expression mechanism and reversing client/server roles. 6. References[1] Turrell, Chris (1999). “High Level Architecture”, Simulation Technology, Vol. 1 Issue 4. 21 June. [2] High Level Architecture, Federation Development and Execution Process (FEDEP) Model, Version 1.5, Dec 8, 1999.[3] Naval Aviation Training Systems Interoperability Maturity Model, Paterson, D., Fisher, CAPT R., 2000. [4] HLA and Beyond: Interoperability Challenges, Simulation Interoperability Workshop, Dahmann, J.D., Salisbury, M., Barry, P., Turrell, C., Blemberg, P., 99F-SIW-073, September 1999.[5] Automated Interoperability Testing (AIT) Final Report, 13 January 2004. [6] Navy Aviation Simulation Master Plan (NASMP) Federation Agreements Document (FAD) Version 1.0, NASMP Systems Engineering Federation Working Group, 28 January 2004.[7] The ALSP Joint Training Confederation: A Case Study of Federation Testing, Simulation Interoperability Workshop, 97F-SIW-061, September 1997.[8] Network-Based Training: HLA Implementation and Cost Analysis.[9] Role of Distributed System of System Testing in the Systems Development and Acquisition Process: The Joint Distributed Engineering Plant (JDEP) in the Context of Simulation-Based Acquisition (SBA), Simulation Interoperability Workshop, 01F-SIW-049, September 2001.[10] Automated Testing within the Joint Training Confederation (JTC)”, Simulation Interoperability Workshop, September 1998.Author BiographiesPAUL PERKINSON received his BS in Computer Science from George Mason University. He has twelve years of experience in software and systems engineering, specializing in large-scale distributed Modeling and Simulation (M&S). The past nine and a half years he has been employed at Virtual Technology Corporation (VTC), most recently as the Vice President of Product Engineering responsible for the research and development of several components of VTC’s commercial product line and involved in the development of numerous large-scale distributed simulation programs. WILLIAM BABINEAU received his BS in Electrical Engineering from Rensselaer Polytechnic Institute (RPI) and a MS in Computer Science from Steven’s Institute of Technology. He has over twenty-three years of experience in software and systems development in various defense and aerospace applications, including over eight years of experience in large-scale distributed Modeling and Simulation (M&S). The past four years he has been employed at Virtual Technology Corporation (VTC), most recently as a system engineer on the NASMP program.MICHAEL KIRK is a software developer for Virtual Technology Corporation (VTC).  He has been heavily involved with developing and maintaining HLA federates, including VTC’s hlaControl (commercial monitoring and control federate).  Mr. Kirk received his B.S. in Civil Engineering from Lehigh University and his M.E.S. in Computer Science from Loyola College of Maryland.CHRIS SCHWINDT is a Senior Software Engineer for Virtual Technology Corporation. He is currently supporting NASMP System Engineering at NAVAIR Orlando. He holds a Bachelor’s Degree in Physics, a Master’s degree in Optical Physics and another Master’s in Engineering (Aeroacoustics). He has worked in modeling and simulation for four years. Chris is the author of  FOMTool.JOHN TUFAROLO is a Senior Systems Engineer for Virtual Technology Corporation in Alexandria, VA, and currently manages the integration and testing efforts for the ARMY Modeling Architecture for Technology Research and Experimentation (MATREX) infrastructure team. He has over 18 years experience in distributed simulation and systems engineering on a variety of modeling and simulation projects.  Mr. Tufarolo is the current chair for the Association of Computing Machinery (ACM) SpecialInterest Group on Simulation (SIGSIM). He holds a Bachelor of Science in Electrical Engineering from Drexel University, and a Master of Science in Systems Engineering from George Mason University.ERIK S. HOUGLAND, PhD is a Computer Engineer with the Naval Air Systems Command Training Systems Division Orlando, Florida.  His responsibilities with the Navy Aviation Simulation Master Plan include fielding test tools for Federation Interoperability testing and Synthetic Natural Environment use.Susan Haselby is a Computer Engineer with Naval Air Systems Command Training Systems Division Orlando, Florida. She is an interoperability engineer on the Navy Aviation Simulation Master Plan and is responsible for HLA Interoperability Laboratory Management.Daniel Dyke is a Computer Engineer with the Naval Air Systems Command Training Systems Division, Orlando, Florida.  His the Lead Engineer for the Navy Aviation Simulation Master Plan Network Operations Integrated Product Team.  