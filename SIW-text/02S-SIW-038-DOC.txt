Monitoring, Measuring and Analyzing Federation PerformanceJoshua BersLynn CarlsonB.S.Steven B. BoswellBBN Technologies10 Moulton St.Cambridge, MA 02138617-873-3200{sboswell, lcarlson, jbers}@bbn.comKeywords:Federation Performance, Performance Measurement, Modeling Tools, Modeling Techniques, RTIABSTRACT: In order to understand the key performance issues of a federation, it is useful to be able to monitor loads placed on all the resources supporting federation execution, and to correlate resource use with simulation activity that takes place during the execution. Federation-wide monitoring is complicated by the nature of HLA federations: distributed execution requires collection and coordination of distributed performance data. Also, different kinds of resources are measurable at different processing levels and by different techniques. In this paper we report on an integrated suite of measurement tools, which, with DMSO sponsorship, we have assembled into a monitor for tracking federation performance at execution time. The monitor collects data that can be used to observe both federate host machine and network performance. Some of the features of this monitor include: the tool suite is transparent to federates and federate developers, and unobtrusive on federate machinesthe monitor is configurable to new network and machine layoutsfor each federate host machine, the tool suite collects data pertaining to several machine resources, including:memory use, including paging and swapping informationCPU usage by user processes and by the operating systemmulticast group memberships and IP stack statisticsthe tools record a detailed tabulation of multicast group utilization on the LAN segment to which the monitor is attachedall data at all sites is collected with attention to synchronization, merging, and analysisthe monitor provides the ability to integrate and correlate with output of DMSO Data Collection Tool (DCT).The paper will describe the design and implementation of the federation monitor, application considerations, and lessons learned in analyzing data collected during a trial experiment with the Environment federation (EnviroFed).1. IntroductionFor many reasons, federations and federate developers require information on the loads that they place on computing and communication infrastructure, and on the capacity of infrastructure to handle simulation activity. Even federations with mature and familiar components need performance information to estimate equipment needs and allocate resources. Moreover, most federations are occupied with evolving capabilities and new application scenarios, and they spend much of their life-cycle under development. Effective monitoring of system resources can help to uncover problem areas that otherwise remain elusive. For example, a single federate may show delayed response time, yet the source of the problem may be unanticipated congestion or contention with another seemingly independent federate. Measuring and reporting on the health of the system federation-wide is crucial to understanding existing limitations in performance.Capturing the loads placed on all resources supporting a federation is a complex problem. By their  very nature, distributed asynchronous simulations have distributed performance profiles and potentially complex causality relationships. There are many possible approaches ways to instrumenting distributed software. Different approaches will of course vary in ease of use, interpretability of output, and impact on performance that the monitors generate. In this paper we discuss one approach, in the form of a monitor, FedPerfMon, designed to capture and record CPU, memory and network usage metrics across a federation. The monitor organizes the collected data so that developers can interpret the run time consequences of federation-level activity. We begin with an outline of the requirements and design of the tool set incorporated in FedPerfMon, including tradeoffs made. Then we describe our implementation of these tools, in which we combine and modify widely-available load monitoring software. The result is applicable for use with many distributed software applications, not only HLA federations. Drawing on our experience using the tools with the EnviroFed III federation, we examine how one might use the output of the tools to analyze a federation. Finally, we will discuss some limitations of our current approach and suggest improvements and enhancements to the toolset.2. Designing an Effective Federation Performance MonitorIn this section we will highlight the design of FedPerfMon. We begin with a review of critical requirements and then discuss the tradeoff between centralized and distributed architectures. Then we detail both the load and network metrics captured by the monitor. Finally we present the design of a tool to allow the correlation of output from FedPerfMon with that of monitors of federation-level activity, such as DCT.2.1 Low impact on FederatesIn discussion with us, developers indicated two primary concerns: (1) a desire to avoid changes to federate code, and (2) a desire to keep the load generated by monitoring tools to a minimum. Also, it became clear that ease of deployment and administration of monitoring tools is key to their acceptance. Throughout the design we looked for ways to impose a minimal load on the federate CPU’s and disks as well as the network. We also tried to limit the configuration changes required by our implementation.2.2 Centralized Polling StructureWhen designing a system for gathering data from remote machines in a coordinated manner, the most obvious design choice is whether to use a centralized or decentralized collection architecture. By a decentralized design we mean one where each machine runs its own data collector, and stores collected data locally. Potentially, each collector can act independently from all others when recording its performance statistics. The advantage of a decentralized approach is that it imposes little additional load on the network. A disadvantage of a fully decentralized approach is that synchronization of data from multiple machines must be done after collection. Also, the reliance on local disk storage can impinge upon the federate host machine. Moreover, collection algorithms may be complicated by the need to guard against disk access failures and capacity limits.  By contrast, the centralized approach uses an independent machine to poll remote hosts for their data at regular intervals. Advantages of this method are that the data is kept in a single location and synchronization of data from different sources is more timely. Also issues of disk storage and reliability must only be dealt with once. A disadvantage is that run time bandwidth is required to transfer data from distributed collectors to the central collector. Another disadvantage is that the central polling process can introduce sampling offsets when gathering data one host at a time. This problem is amplified when scaling to large federations with many federate machines to be polled.Either approach can mitigate its limitations through careful implementation; the preferred choice depends on the target application. For target federations of modest size, sited on a single LAN segment, the centralized approach may be simpler to deploy and administer. In such federations, network loads induced by centralized collection are small (this point will be discussed in more detail in section 3.4), and sampling offsets are modest. The use of a single LAN segment ensures that a single central machine can quickly and easily access the remote federate machines. FedPerfMon uses a centralized data collection scheme.2.3 Performance Statistics MeasuredAn important part in the design of any load monitor is deciding which metrics to capture. In general, load metrics should:convey the state of the resource(s) being monitoredbe sensitive to changes in the amount and type of activities of interestbe readily available and in compact format be meaningful and comparable across different machines, operating systems, and network configurations.2.3.1  Host machine loadsBecause the design and mission of individual federates cause variation in which load metrics are useful, we chose to capture a broad swath of metrics and let the user decide which ones are relevant to his/her application/federation. Some of the metrics captured for each machine include:CPU time distribution between user, nice, system and idle processes Load averages (ready-to-run) for 1, 5 and 15 minute periodsDisk transfers, paging and swap activity rates (measured in events per second)Context switch and interrupt ratesLAN driver activity including input packets received and collisions per second.Any of these metrics may be uninformative in one circumstance, but pertinent in another. For example, consider the CPU load average. This statistic reports the average number of processes that are in the “ready to run” state over a period of time, usually a minute. The statistic is returned by the UNIX “uptime” command. For federates that have a machine dedicated to them, and that execute as a single process, the load average normallyis conveys little independent information. However, for federates that spawn multiple processes, or federates that share a machine with other running applications, the load average may be a valuable indicator of performance variability and context. 2.3.2 Monitoring the NetworkNetwork statistics are important for two reasons. First, communication infrastructure is one of the resources required to operate a federation, and the ability to identify capacity limitations or perturbations in that infrastructure has obvious importance. Second, messaging traffic has value as an observable, giving visibility as to what patterns of activity are occurring in the federation at any point in time.  FedPerfMon obtains network statistics both at the monitor machine, and by queries of individual federate machines. From standard utilities (e.g., ntop & tcpdump), FedPerfMon records composite information about traffic on the LAN segment to which the monitor machine is attached, including:Packet and byte counts  transmitted on the LAN segment, tracked over timeBreakdown of traffic volumes by IP protocol and by use of broadcast and multicast.The IP multicast protocol is integral to the operation of many RTI implementations, and in particular to the application of  Data Distribution Management (DDM) [1]. Therefore, a capability for fine-grained monitoring of IP multicast use is built into the FedPerfMon monitoring machine. The monitoring machine examines all multicast packets on its network segment, and it maintains a tabulation of multicast traffic, detailed by destination group, sender, and successive blocks of time. This information makes it possible to investigate flows and connectivity patterns across the federation network.FedPerfMon also polls individual federate machines for statistics kept by their IP protocol stacks. These include:Packets sent and received per protocol: TCP and UDP in packets per secondDropped packets, IP fragments created, reassembled, and other stats from the TCP, IP and UDP SNMP-MIB groupsIGMP group memberships.The goal of the network monitoring in FedPerfMon is to give a coherent picture of IP traffic and connectivity, and to provide sufficient detail for operational studies (eg, DDM strategies), without impacting the federation or its infrastructure. All of the packet capture induced by FedPerfMon occurs passively, on the monitoring machine, and imposes no load on the network or federate machines.  One limitation of our approach is that the monitor only captures activity on a single LAN segment. While well suited to most federation executions, the monitor design would need to be adapted to address simulations distributed across several routed segments (WAN). Alternatively, attaching a FedPerfMon monitor to each segment may suffice. 2.4 Correlation with Federation Activity (DCT)In order to correlate performance statistics with federation activity, we also developed an analysis tool that uses the output from the DMSO Data Collection Tool (DCT) [2]. This tool takes a DCT-created Microsoft Access database file and generates a table of counts of all object updates and interactions per sample period. We considered several ways to go about createing a tool that would gives insight into federation activities. We identified three potential ways of approaching the problem. The first way is to have the federates themselves report on the information that needs to be collected. The second approach is to utilize a “wrapper” recording layer between the RTI and the federate application [3]. The third approach is to use already existing logging services. The first two approaches permit the development of accurate, customized sensors of federate processing. Unfortunately, they are federate-specific and tend to be more intrusive than federate developers desire; they require changes to federate source code or relinking with a modified RTI library. A limitation of loggers is that they are unable to identify the generator of message traffic. Depending on the federation, this may pose a problem in deciphering which federate is associated with activity that is observed.   Nevertheless, in keeping with the goals of generality and low impact, we pursued the third approach of accessing pertinent data from an existing, general purpose logging tool. We elected to work first with log files of the DMSO DCT, since this product is freely available to the M&S community. The DCT can store logged data in a Microsoft Access database, where it is time-stamped and organized in tables on a class basis, with separate tables for each object and interaction class. Our tool processes the Access database and generates a table of counts of all classes of object updates and interactions, per sample period. The post-processed data is independent of Access and the DCT, chosen for analysis rather than playback, and organized for fusion with the other data collected by FedPerfMon. Our processing tool also works with the commercial product, HLAResults [4], which creates databases in the same format as the DCT. We note that comparable tools could be developed for use with other loggers.3. Implementation DetailsIn this section we describe the implementation of the FedPerfMon tools. The basic design implemented is that of a master-slave. The master runs on a non-federate machine that is connected to the same network segment as all of the federate CPUs. The main task of the master CPU is to monitor network traffic and to gather and record data from remote federate machines. Figure 1 shows the overall architecture of the monitor tools deployed within the EnviroFed network. In the following subsections we willll cover time synchronization via NTP, the global executive FedPerfMon script, and its helper tools: rperf, netstat, ntop and tcpdump.  Later we describe the extraction of DCT output for correlation with the output from FedPerfMon, and finally we quantify the impact of the load from the tools on the federation.Figure  SEQ Figure \* ARABIC 1.  Federation Monitor Design3.1 Time SynchronizationTo facilitate post-processing and analysis of the performance data, we time-stamp all recorded data with the Unix epoch time. Because some of the data we collect is time-stamped separately on remote federate machines, some on the master FedPerfMon machine, and some on a logger machine, time synchronization is required to assemble collected data. We recommend installing an NTP server on the master machine and NTP clients on each federate machine, as well as on the machine running the DCT, to synchronize machine clocks [5]. As a backup measure and for circumstances where NTP is unavailable, FedPerfMon samples the clocks of all participating machines at the start and end of each monitoring session. If necessary, a user can calibrate clock offsets and adjust unsynchronized time stamps to map to a common reference. However, this sort of post-calibration should seldom be necessary (or desirable).Even when clocks are well synchronized, there are limits as to how precisely the action of probes using different operating system capabilities at different sites can be synchronized. One constraint of our current design is its round-robin polling nature. The polling script polls and waits for a response from each machine in turn. Though the polling and reporting process is inexpensive for the federate machines, the protocol that conducts polling over the network takes some time. Also, machine reaction to the polling process is subject to operating system schedulers and to contention with essential federate processing, which (properly) takes precedence. Our experience is that sampling from more than 10 machines can introduce skew into the sample times, fewer if the machines are very busy. For example, while one data item may be reported precisely after a 10 second interval, successive reports from another source may arrive 12 seconds apart. That is to say, even though the reporting times are accurate, reporting intervals cannot be guaranteed to be perfectly aligned. One way to improve alignment would be to use broadcast or multicast for all polling. Another solution is to install daemons that run in a distributed manner, sampling at pre-arranged intervals. Either of these techniques may be pursued in future work. EMBED Visio.Drawing.6  Figure  SEQ Figure \* ARABIC 2. Details of the federation monitor tools, showing the layering of networked components3.2 Distributed Reusable Tools (FedPerfMon, Rperf,  Ntop, tcpdump)Figure 2 shows the detailed breakdown of the tools used. Wherever possible we re-used existing COTS or publicly licensed tools for measuring performance on UNIX machines. This helped to speed development time and ensure quick installation in new federation environments. One of the major influences in our choice of tools was the fact that initial demonstrations were planned in partnership with the EnviroFed federation, and most of the EnviroFed federate machines were running Linux, specifically RedHat version 6.1 or 6.2. The remaining federates were either SGI’s or PC’s running Windows NT. The tools gathered CPU performance data from all UNIX boxes and network stats from only the Linux hosts. Only the PC’s were left out of the active monitoring. Traffic generated by all federate machines, independent of OS, was passively monitored by our network sniffer running on the master monitor machine (see Figure 1).The approach taken for data capture is to have the master machine poll the federates at periodic intervals for performance information. The master script, FedPerfMon, is written in Perl and spawns tools to help it gather load statistics from remote hosts. The rperf utility is used to gather kernel statistics and ntop is used for network monitoring. These are configured to run for a predetermined duration and to report data with a specified sampling interval. Once started they dump their statistics to log files. The master machine also runs tcpdump to monitor multicast traffic. In addition it connects to tcp ports on the remote machines to gather TCP/IP statistics and IGMP memberships via the netstat  -s and netstat –g commands. A detailed table of collectcaptured statistics appears in the appendix.3.3 Federation Activity Correlation ToolsThe DCT post-processing tool takes input parameters: ODBC source, sample window, duration, and offset time. Since the DCT and the FedPerfMon tool suite will run on separate machines, again, it is important that the two machines have their clocks synchronized. The input parameters allow synchronization of DCT start time and sample window with the monitoring tools so that DCT tabulationsthey may be aligned according to with the variations of start and stop timessampling intervals of other monitoring tools. Since DCT acts as a federate, the timestamp it records for the updates and interactions are the times at which it is able to process the callbacks received from the RTI. These times may deviate from the actual time at which updates and interactions were generated, and from the times at which they were received at other federates. Such deviations should be taken into account when performing data analysis, but, as stated previously, reliance on a logger allows a non-intrusive insight into the federation activity that can then be correlated with monitoring tool outputs.The DCT processing tool uses the Perl database interface (DBI) to connect to the Access database using the ODBC source name that it takes as an input parameter. Once the connection has been successfully established, the start time of the data collection is obtained from a table called FederationExecutionName. This marks the beginning of the DCT collection. It is converted into a Zulu timestamp and is output for the user to see. Our tool then builds a new table in the database, called AllUpdatesAandInteractions. This table contains the type, serial number, and wall clock time for all updates and interactions that have been recorded by the DCT. The tool then begins to bin the information into sample chunks by counting the number of occurrences of each update or interaction in each time sample, taking into account the offsets and bin duration specified by the input parameters at invocation. This tool extracts and organizes information from all of tables within the Access database, and combines the information into a format resembling an Excel pivot table. Using Excel the resultant files can be easily imported and used as a pivot table, allowing multiple views of the data for analysis. The DCT post-processing tool can also be used independently of any other companion software. The tool runs on a Windows-based system having an SQL database or on Linux, using a DBI proxy server [6].3.4 Impact of Monitoring Tools on Federation PlatformTo quantify their impact on federation infrastructure, we measured both the network bandwidth and the CPU consumption of the FedPerfMon tools. The base network bandwidth usage averages about 1,400 bits-per-second per machine monitored when sampling at 10 second intervals. Since the tool suite asks federate machines to report multicast group memberships, bandwidth increases approximately 30 bits-per-second for each multicast group subscription (again assuming 10 second reporting intervals). Given this load, one can reasonably monitor hundreds of machines without significantly loading the bandwidth of an 10Mbit Ethernet segment. The monitoring processes rstatd and netstat use less than 2% of the CPU (Pentium II 500MHz) on the monitored Federate machines. These tools, when running, occupy about 1.3 Megabytes of memory. 4. Use-Case: EnviroFedOur monitoring tools were deployed at an integration exercise of the EnviroFedIII federation, and were given the opportunity to observe a small experiment with DDM by the HydroSim federate.  As indicated in Figure 3, our monitor supports conclusions similar to those reported in [7] for a separate DDM test.  Pictured are total packet volumes sent by JSAF in 10-second intervals.  Clearly, without DDM, JSAF is quiescent for periods of about one minute following HydroSim updates at 510 seconds from start of monitoring, and every 3 minutes thereafter.  Use of DDM reduces JSAF “freezes” to less than the 10-second discretization interval we used in monitoring (observers reported 6-second freezes).  The network monitoring also uncovered performance issues that were not apparent to on-site observers; namely, when using a partitioned routing space, JSAF network traffic increased by more than 50-fold, though the (still beneficial) observed impacts of DDM were unchanged.Figure  SEQ Figure \* ARABIC 3. JSAF simulation traffic without DDM, with DDM, and with DDM and a Partitioned Routing Space4.1 Analysis techniquesBy importing the captured data into Microsoft ExcelWe have ported our data files into , we were able use the pivot tableMicrosoft Excel and into the statistical package R [8] capability to plot manyfor quick investigation of relationships in the data. We also performed regression with the data captured by the DCT logger and extracted with our tool. This gave us insight into the correlations between simulation events and platform load. For example at the EnviroFed IE-3 exercise we conducted an experiment using DDM. There was a packet explosion on the order of 50 times the traffic with DDM than without. We used single variable regression analysis to find out which was the prime contributor. Figure 3 below shows the linear fit plot for ContamSensor.Free event with DDM. The slope of the regression line is 108 and the R value is 0.9.For example, Figure 4 depicts a linear regression of multicast packets on the LAN vs. DCT-logged updates of the ContamSensor class, a bio-chem sensor attached to vehicles, which JSAF models.  The data in Figure 4 were taken using DDM with a partitioned routing space.  In this case, the slope is 108, and the R value is 0.9; without partitioning (and/or without DDM), the slope value is 3.  A possible explanation would be that ContamSensor attributes are being updated with a Region, which could cause RTI-NGv4 to replicate packets to a large number of multicast groups.  However, developers indicate that ContamSensor updates are carefully associated with regions in the JSAF baseline [9].  JSAF transmissions (updates and interactions) concerning various classes tend to be very highly correlated in time, and other classes contribute to the multicast packet count and show similar patterns to ContamSensor.Figure  SEQ Figure \* ARABIC 34. Regression between ContamSensor.Free object updates and multicast network traffic when using DDM with a partitioned Routing Spaceng DDM. The slope of the regression line is 108; the R value is 0.9..Without DDM, the slope is 3 and R is 0.75. indicating that the DDM had an impact on the rate of packets sent per ContamSensor.Free event. Since there are other events that contribute to the multicast packet count, the absolute values of the regression are not important, however, the relative change from the no DDM condition indicates a possible duplication of data. One explanation may be that the publishing of updates for ContamSensor.Free were made without specifying a particular geographic region. Further discussion of our experience with EnviroFed can be found elsewhere [7].Using our detailed multicast tabulations and expanded correlation techniques, we are continuing to investigate the increase in network traffic that resulted from using a partitioned routing space.  A better understanding of how RTI-NGv4 translates DDM messages into network traffic could have particular value for federations scaling to a larger size.  5. Limitations and Future WorkAs we have pointed out throughoutdiscussed elsewhere in this paper, our implementation of the monitoring tools was conducted with priority on rapid development and ease of use in the field.  suffers from some design limitations. In this section we re-highlight shortcomings as well as point outdescribe some fruitful directions for further development of the tool suite.Scaling and monitoring across routed LAN segments are two particular areas where the current design is lacking. In many cases we chose to tradeoff limitations for ease-of-implementation. This allowed us to develop the tools in an expedited manner. For current usage, however, we recommend focusing the application of the tools to just a few federates under study. By limiting the number of machines involved it is easier to setup scenarios that can be understood and analyzed offline with the captured load data.5.1 Realtime GUI VisualizationReal-time monitoring of the platform with graphical feedback would be a greatly enhancement to the usability of the tools. A useful design would allow the operator to select a subset of machines to monitor as well as a subset of metrics. The data could then be viewed on moving trace-charts with each machine drawn with a unique colored line. This would allows for quick comparison among machine loads to determine any outliers. Figure 4 shows our first version of a graphical visualization tool, FedChart. Unlike other existing tools such as Xmeter, FedChart draws all machines on the same chart and it displays exact values when the mouse dwells over a particular sample. In addition FedChart works in conjunction with FedPerfMon allowing both realtime display and playback modes for post analysis.Figure  SEQ Figure \* ARABIC . FedChart, dynamically plots data captured by Rperf, displaying input packets per second in the top chart and Ethernet collisions per second in the bottom for the same time period.A separate display could track the overall network usage on the network in bytes and packets per second. To get insight into DDM/RTI connectivity another display might show the multicast group subscriptions of each machine as well as which machines are sending to each of the multicast groups. Careful layout is needed to get a comprehensible display.The source of the data to drive the visualizations is already captured on the centralized monitor machine and thus it should not be hard to build the GUI to read directly from these files as they are updated by the monitor.5.12 Federate specific monitoringTo fully understand tThe implications of system-based load statistics captured by FedPerfMon , it may not always reflect the actual burden of all federatessometimes be necessary to obtain detailed information about the implementation and conduct of individual federates. Gathering application/federate dependent load data, for example, the duty cycle time ofNative statistics maintained by the federate (JSAF duty cycle, for example) may lead to added insight. The aim should be to define a common framework so that data from arbitrary federates can be analyzed with common methods.5.23 Distributed monitoringMulticast messages might help to eliminate the issues of scalability of the current tools. For example, in a distributed architecture, each remote monitor would subscribe to a multicast group to receive synchronous messages from FedPerfMon. Another approach to distributed monitoring gives a schedule to each machine and depends on clock synchronization and remote timers to manage the data collection. Data cwould be stored locally and only sent to the collector at the end of the collection period to avoid network congestion.   The distributed approach also may also improve the synchronization of reporting times for different metrics on different machines.5.3 Wide area supportMany components of the monitoring suite restrict their purview to the local LAN segment.  This restriction simplifies both the implementation and use of the tools, and it seems to us a reasonable limitation for low impact, portable tools.  Nevertheless, one possible extension of existing capabilities is to support monitoring of switches and routers, and monitoring of federate machines sited on separate local networks.  For current usage, where necessary, one may simply site a monitoring machine on each separate LAN segment.  5.4 Realtime GUI VisualizationReal-time visual display of the data collected by monitoring tools will greatly enhance the relevance and interpretability of the tools.  Certainly, after-action analysis with stored data files can clarify issues that are hard to pin down at run time.  In many respects, though, developers have the greatest amount of information available to them at run time, the best ability to interpret performance data, and the strongest interest in exploring performance issues.  Our monitoring suite already captures performance data on the centralized monitor machine.  Thus we have the opportunity to develop coordinated visualization software that runs with no incremental load on any part of the federation execution.  We have begun development of such a graphical visualization tool, FedChart.  The intention is that FedChart can be run either real-time, or after-action on the data files collected by FedPerfMon.A useful design allows the operator to select a subset of machines to monitor as well as a subset of metrics. The data could then be viewed in a variety of ways.  One useful display technique is moving trace-charts with each machine drawn with a unique colored line. Such a display allows for quick comparison among machine loads to determine outliers, to identify correlations among machines, trends over time, and other patterns.Figure 5 shows a trace-chart produced by a prototype first component of FedChart. Unlike other existing tools such as xmeter, FedChart currently draws all machines on the same chart; we expect to add additional display styles as user needs suggest in the future.   FedChart includes an interactive query capability; it displays exact numeric values and annotation information when the mouse dwells over a particular sample point.  This reporting capability is useful for investigating patterns and points of interest in the performance data.Figure 5. FedChart can dynamically plot data captured by rperf, here showing input packets per second in the top chart and, in the bottom chart, Ethernet collisions per for the same time period.A separate display choice tracks overall network usage in bytes and packets per second. Another depicts object updates and interactions as recorded by the DCT.  To get insight into DDM/RTI connectivity another display shows which machines are sending to each of the multicast groups in use, and how traffic is spread among the groups. Note that in Figure 5, two distinct metrics are juxtaposed and displayed with aligned time intervals, so that relationships between the metrics may be made evident (though no compelling relationship is evident in the particular example in Figure 5).  The coordinated time-stamping performed by FedPerfMon permits similar juxtaposition of metrics from the various data sources.  FedChart is being designed to support visualization of multiple data elements under user control.  Careful layout will be needed to get comprehensible displays.6. ConclusionsMany of the insights gained from the development of the FedPerfMon tool suite should apply to others intent on understanding the performance of a federation. Here are some lessons learned:Synchronization of different measurements on remote machines is a critical yet non-trivial problem, and one that cannot be solved with complete precision.  It is necessary for analysts to determine time resolution requirements and design sampling approaches that accommodate them.Scalability of monitoring tools can become an issue in large federations. We recommend identifying subsets to monitor, e.g., a specific LAN segment in a large routed environment.  Alternately, it may be necessary to re-architecting the tools to run in a more distributed, autonomous fashion.   By limiting the number of machines involved it is easier to setup scenarios that can be understood and analyzed offline with the captured load data.  To passive monitors, various activities of a federate may be highly correlated.  Suppose, for example, that a federate maintains an event loop and on each traversal, it updates many objects of various classes.  Because they always happen in the same time frame, it may be difficult to distinguish from the outside which updates generate which network traffic.  Often monitoring tools can only suggest critical questions, which developers may then investigate in controlled tests.Our monitors record data in ASCII text, in flat-files or file formats that can be converted easily to flat-file format.  We have found the universality and immediate accessibility of this data format to outweigh potential storage or processing advantages of alternate formats.  The files load directly into spreadsheet programs and into common data analysis packages.Tough to provide general monitoring tools for all issues:Identify critical questions up-front and perform limited test around them.Synchronization of clocks on remote machines is a critical yet hard problemDetermine time resolution requirements and design sampling approach that accommodates them.Scalability of monitoring tools can become an issue in large federations. We recommend identifying subsets to monitor, i.e., a specific LAN segment in a large routed environment or re-architecting the tools to run in a more distributed, autonomous fashion.We’ve also found that there is no substitute for familiarity with the data and have described methods of analysis using Microsoft Excel. The universality of the data format: ASCII flat-files makes for easy importation into ones favorite statistical data analysis package. 7. Appendix: Detail of Data CapturedrRperf data: (most values are in events per second)unix time             %cpu            loadavg (nrun) disk xfers  pg pg sw sw  cx  in   i  i  o  o co(seconds)    hostname  us  ni  sy  id   1m   5m  15m d0 d1 d2 d3 in  o in  o  sw  tr  pk er pk er ll1000930520  localhost   1   0   0  99  0.0  0.0  0.0  0  0  0  0  0  0  0  0 414   2   2  0  0  0  0nNtop data: (values are absolute counts per time period)date     totalPkts broadcastPkts multicastPkts ethernetBytes ipBytes nonIpBytes peakThroughput(Kbps)1002220108 1660       0              30            467077     467077      0         17157.2nNetstat -g data:Subscribed multicast groups (netstat –g): IPv6/IPv4 Group MembershipsInterface       RefCnt Group--------------- ------ ---------------------lo              1      ALL-SYSTEMS.MCAST.NETeth0            1      224.0.1.60netstat -s data:IP SNMP-MIB (netstat –s): All values are in events per secondtotal packets received,forwarded,incoming packets discarded,incoming packets delivered,requests sent out,reassemblies required,packets reassembled ok,fragments created,# TCP statsactive connections openings,passive connection openings,failed connection attempts,connection resets received,connections established,segments received,segments send out,segments retransmited,bad segments received.,resets sent,# UDP statspackets received,packets to unknown port received.,packet receive errors,packets sent,# TcpExtpackets pruned from receive queue because of socket buffer overrun,packets dropped from out-of-order queue because of socket buffer overrun                            TCPDUMP MULTICAST OUTPUTtcpdump multicast use data (example):Dest IP          Src IP         BPS   PPS Ave-PPS Min-PPS Max-PPS Total(byt/pkt)-------------------------------------------------------------------------------********************************************************************************255.255.255.255 128.89.76.56  0.00e+00  0   0.0    0        1       553/     7255.255.255.255 128.89.76.36  6.91e+00  1   0.1    0        1      1216/    16224.0.0.2       128.89.76.3   3.64e+00  1   0.3    1        1      1040/    52224.0.0.2       128.89.76.4   3.64e+00  1   0.3    1        1      1040/    52224.0.1.60      128.89.76.154 1.30e+04 26   1.9   26       26    143000/   286********************************************************************************1008885327: 291 Packets in the last 11 seconds, rate of 29.10 pps        143156 Bytes or 13014.18 Bps8. References[1]	High Level Architecture Interface Specification Version 1.3, Section 9.   HYPERLINK "http://www.dmso.mil" http://www.dmso.mil.[2]	DMSO, Data Collection Tool user’s guide. See  HYPERLINK "https://sdc.dmso.mil/contents/dct13v4/index.php" https://sdc.dmso.mil/contents/dct13v4/index.php[3]	Kolek, Stephen R., Steven B. Boswell, and Harry M. Wolfson. "Toward Predictive Models of Federation Performance:  Essential Instrumenta-tion" 00F-SIW-085, Fall 2000 Simulation Interoperability Workshop (Orlando, FL, 17-22 Sept  2000). http://www.sisostds.org/siw.[4]	http://www.virtc.com [5]	http://www.eecis.udel.edu/~ntp/[6]	Descartes, A., & Bunce, T. “Programming the Perl DBI” O’Reilly & Assoc. (2000), pp. 177-181. [7]	Adelson,S.J. “A Scaleable Solution for the Hydrogeologic Simulator Environment Federation” 02S-SIW-017, Spring 2002 SIW (Orlando, FL, 10-15 Mar 2002).  HYPERLINK "http://www.sisostds.org/siw" http://www.sisostds.org/siw.[8]	http://www.r-project.org/[9]	Peter Wickis, private communication.8. 6. References[1]	 HYPERLINK "" REF FOR DDM[2]	DMSO, Data Collection Tool user’s guide. See  HYPERLINK "https://sdc.dmso.mil/contents/dct13v4/index.php" https://sdc.dmso.mil/contents/dct13v4/index.php[3]	Kolek, Stephen R., Steven B. Boswell, and Harry M. Wolfson: "Toward Predictive Models of Federation Performance:  Essential Instrumenta-tion" 00F-SIW-085, Fall 2000 Simulation Interoperability Workshop (Orlando, FL, Sept. 17-22, 2000). http://www.sisostds.org/siw.[4]	http://www.virtc.com [5]	http://www.eecis.udel.edu/~ntp/[6]	Descartes, A., & Bunce, T. “Programming the Perl DBI” O’Reilly & Assoc. (2000), pp. 177-181. [7]	NEED REF FOR OTHER SIW PAPER (ANALYSIS)Author BiographiesJOSHUA BERS has been involved with distributed performance monitoring for a year. He has also worked on spoken language technologies for five years, and prior to that with networking systems for shared data and voice delivery. He has a Masters of Media Arts and Science from MIT and an AB from Dartmouth College.LYNN CARLSON has been involved with distributed performance monitoring for a year.  As a member of the Distributed System and Logistics department at BBN Technologies she has worked on the development of distributed agent software for the 6 years.  She has a Bachelor of Science Degree from Roger Williams University.  STEVEN B. BOSWELL has been involved with distributed simulation for six years, and prior to that with statistical modeling and software development for research on air transportation and biomedical problems. He has a Ph.D. in Mathematical Sciences from Rice University. As a Member of the Distributed Systems Group at MIT Lincoln Laboratory, he participated in the development of the STOW RTI prototype and the DMSO RTI v1.3. Simple Network Management Protocol - Management Information Base  The number of seconds since midnight Jan 1, 1970 GMT Open DataBase Connectivity