Accreditation Authorities, Who they are, where to find them, and what do they do?Lisa CainBobby HartwayDanny ThomasAEgis Technologies Group631 Discovery DriveHuntsville, AL 35806256-922-0802 HYPERLINK "mailto:lcain@aegistg.com" lcain@aegistg.com HYPERLINK "mailto:bhartwar@aegistg.com" bhartwar@aegistg.com HYPERLINK "mailto:d.thomas@aegistg.com" d.thomas@aegistg.comRandal WallaceNASAMarshal Space Flight CenterHuntsville, AL 256-544-2940 HYPERLINK "mailto:Randal.L.Wallace@msfc.nasa.gov" Randal.L.Wallace@msfc.nasa.govKeywords:‚ÄúVerification, Validation and Accreditation‚Äù, ‚ÄúNASA‚Äù, ‚ÄúAccreditation Authority‚ÄùABSTRACT: The Accreditation Authority is critical to the Verification, Validation and Accreditation process ‚Äì yet arranging their participation is problematic.  This paper discussed their qualifications, duties, and the selection process.  It also provides real-world examples taken from representative classes of models and simulations.WHAT IS ACCREDITATION? -- NASA‚Äôs Modeling and Simulation (M&S) Verification, Validation and Accreditation (VV&A) Recommended Practices Guide (RPG) defines accreditation as the official endorsement that a model or simulation or federation of models and simulations and its associated data is acceptable for use for a specific purpose. Accreditation is the formal approval for use for a specific purpose of an M&S, given that the M&S and its associated data have been objectively and quantitatively evaluated by a cognizant SME and evidence exists supporting the ability of the M&S to produce real-world-representative data within the operational ranges of, and within the accuracy required by, the specific purpose.  In short accreditation is the end user‚Äôs determination that an M&S is acceptable for the intended use.  To understand what this means and how to implement it, consider the simple relationship of provider and consumer of information.  The provider is the simulation user or analyst who generates data.  The consumer is the manager or decision maker who uses data to support decisions.  Each has responsibilities in assuring good design decisions.  The provider should be responsible for certifying, assessing, verifying, validating, and/or quantifying the uncertainty of the data given. The decision maker should be responsible for approving or accrediting the simulation that is used.  Figure 1 shows the relationship between providers of M&S capability and the decision makers using the information generated by the M&S.  Figure 1:  Provider and Decision Maker RelationshipsCREDIBILITY ASSESSMENT VERSUS ACCREDITATION ‚Äì The recently approved NASA Standard for Models and Simulations, NASA STD 7009, has as its purpose to ‚Äúensure that the credibility of the results from models and simulations (M&S) is properly conveyed to those making critical decisions.‚ÄùThe requirements and recommendations of 7009 are intended to assure thata. Decision makers can assess the credibility of results from M&S used in decision-making.b. Responsible Parties are assigned to all M&S employed for key decision-making.c. Credibility of the M&S results can be traceable to the program or project requirements.Accordingly 7009 contains a methodology to assess the credibility of simulation results.  The authors have dealt with this topic in a number of papers[1][2][3][4]; Figure 2 summarizes this relationship. This figure introduces the ‚Äúcredibility chain-of-custody‚Äù diagram, which is a workflow diagram of the key M&S development and use activities. The workflow progresses left to right, from initial requirements development, through production of an M&S Results Report, with assessed credibility, ready for use by analysts to conduct their analysis studies for the specified problem the for which the M&S was developed or chosen. The diagram is divided into three major partitions, 1) activities before M&S use, 2) activities of M&S use, and 3) activities after M&S use. These activities are all color-coded consistently throughout this presentation material, according to the color-code legend shown at the bottom.The key features of this workflow are explained in following slides, but here we simply want to emphasize the roles and context for ‚ÄúTraditional VV&A‚Äù compared to the new ‚ÄúM&S Results Credibility Assessment‚Äù activity. A primary distinction between these two is that VV&A should be conducted before M&S use, and credibility assessment can only be conducted after M&S use.Figure 2: Context of M&S VV&A and Credibility in Chain-of-Custody for M&S Results CredibilityACCREDITATION AUTHORITY QUALIFICATIONS ‚Äì   The Accreditation Authority should be the person responsible for the use of the results from the simulation.  Ideally that person should control the resources necessary to correct any deficiencies noted during the verification and validation process.  Figure 3 shows some of the other qualifications for an ideal Accreditation Authority.  Figure 3: Authority, Qualifications and Responsibilities of Accreditation AuthoritiesEXAMPLES OF SUCCESSFUL ACCREDITATION AUTHORITIES ‚Äì NASA has successfully employed several approaches to Accreditation Authority selection.  This diversity of approach is consistent with the diversity inherit in NASA M&S applications.  The following four examples illustrate the range of approaches.  Example (1) The Environments and Constraints SIG chairs, have accredited the Meteoroid Engineering Model (MEM) to characterize the meteoroid environment for the Constellation program.  This model is described and referenced in the Natural Environments Definition for Design (NEDD) and the Design Specification for Natural Environments (DSNE) documents.  MEM was developed by the Government and contractors in the Natural Environments Branch, EV44, at MSFC in cooperation with scientists from the University of Western Ontario.  MEM applies to a wide variety of uses, but the users are not experts in meteorite flux prediction.  NASA choose a recognized technical authority to accredit this M&S.  Example (2) SpaceNet is a logistics network analysis tool used to plan lunar and Martian missions.  It was jointly developed by the Jet Propulsion Laboratory and the Massachusetts Institute of Technology, but used by the Constellation analysis group at the Johnson Space Center. The analysis group manager is the accreditation authority.  He assembled a panel of Subject Matter Experts to review the verification and validation evidence presented by the developers.  After careful review of the panel‚Äôs findings, the Analysis manager accredited SpaceNet.    Example (3) Campaign Manifest Analysis Tool (CMAT) was developed by NASA at the Langley research Center.  This M&S is used by the developing organization to address high-level trades.  The Accrediting Authority is the Headquarters sponsor for the tool.  Example (4) The Fully Unstructured Navier-Stokes 3-Dimensional code (FUN3D) is an example of common confusion in selecting accreditation authorities.  FUN3D is a COTS computational fluid dynamics ‚Äúcode‚Äù.  It has a somewhat open development community that employs rapid-development methods.  It has a diverse user-industry community that overlaps the development community.  When it was used by the NASA Ares Aero group as a predictor of aerodynamic loading of the Ares launch vehicle, some suggested the Ares Loads Panel, as users of the results, should accredit it.  The Loads Panel maintained that the Aero Panel, as suppliers of the results, should accredit them.  This push/pull accreditation conflict remains to be resolved.  In this instance, a ‚Äúworkflow‚Äù diagram, showing the flow of information from development, through execution, through final data use, will be helpful. But the real issue to be resolved is one of ‚Äúresponsible party ownership‚Äù. This is a too common problem, and there is no single solution.REFERENCES[1] A Common M&S Credibility Criteria-set Supports Multiple Problem Domains, JANNAF 2007;  [2] An Automated Process for Credibility Assessment, Huntsville SCS 2007[3]The Role of Credibility Assessment In Verification, Validation and Accreditation, Huntsville SCS 2007[4] Notional Scoring for Technical Review Weighting As Applied to Simulation Credibility Assessment, SISO 2008.  Author BiographiesLISA CAINE is a Systems Engineer with AEgis Technologies Group in Huntsville, Alabama. She supports the Exploration Mission Space Directorate (ESMD) VV&A Program. Ms. Caine earned a BS degree in Engineering and Information Systems from Michigan State University and a Masters in Project Management from Keller Graduate School of Management in Chicago.  She was also recently awarded the Project Management Professional certification through the Project Management Institute (PMI).BOBBY HARTWAY is a Senior Research Scientist with AEgis Technologies Group in Huntsville, Alabama.  He has developed a new paradigm for simulation characterization and requirements development for space and defense systems. He is using this paradigm to support NASA‚Äôs activities for integrated management of modeling and simulation.  Mr. Hartway is a Certified Modeling and Simulation Professional (CMSP).DANNY THOMAS is a Senior Research Scientist with AEgis Technologies Group in Huntsville, Alabama.  He is supporting NASA‚Äôs effort to institute consistent management practices for simulation development and use.  He has developed simulations for space and defense. Mr. Thomas is a Certified Modeling and Simulation Professional (CMSP).RANDY WALACE is a senior engineer on loan to NASA through cooperative agreements between the Marshall Space Flight Center, the University of Alabama in Huntsville, and the US Army Space and Missile Defense Command.  He has extensive experience in the use and development of large, complex M&S in support of tactical and strategic missile defense programs.Figure 7.0-1: Expanded View of Draft M&S Characterization Taxonomy for NASA Constellation Program