Automated Interoperability Testing: Process and ToolkitWilliam BabineauVirtual Technology Corporation5510 Cherokee Avenue, Suite 350Alexandria, VA 22312babineau@virtc.com  Stephen BerglieVirtual Technology Corporation5510 Cherokee Avenue, Suite 350Alexandria, VA 22312sberglie@virtc.com  Erik Hougland, PhDNaval Air Warfare Center Training Systems Division12350 Research ParkwayOrlando, FL 32826Erik.Hougland@navy.mil  Susan HaselbyNaval Air Warfare Center Training Systems Division12350 Research ParkwayOrlando, FL 32826Susan.Haselby@navy.mil  Keywords: NASMP, Automated Testing, Simulation, Training, HLA, RTIABSTRACT: Simulation has become an invaluable tool to enhance military readiness through force structure analysis, simulation-based acquisition, mission rehearsal, and training. Distributed simulation – networking multiple, separate simulation components together – has become a key technology in realizing the benefits of simulation, allowing a wide range of simulation components to be brought together for a common purpose. In order to fully realize the potential of distributed simulation, however, all of the distributed components must be highly interoperable. Testing the interoperability of distributed components at and beyond the level of basic Federation Object Model (FOM) and Runtime Infrastructure (RTI) compliance is a challenging but necessary task that often consumes resources disproportionate to a simulation-based program's primary training objectives.Virtual Technology Corporation and NAVAIR are cooperating in a phase II SBIR program that is developing an Automated Interoperability Testing (AIT) tool that has the potential to dramatically reduce the resources required to perform interoperability testing. Leveraging principles from the Object Management Group's (OMG) Model Driven Architecture (MDA) and Unified Modeling Language (UML), the AIT allows federation interoperability tests to be designed, specified, and executed using mechanisms defined in the open UML specification, primarily activity and class diagrams. Adopting an AIT-based test suite and procedure can reduce the time and money federations spend debugging integration issues. Lessons learned from using the AIT to test interoperability in several federations are presented, along with plans for further enhancements to support interoperability testing at higher levels of abstraction.1	Introduction“Train as You Fight!” – These are the traditional words of the Military Training Community.  They inspire us to provide tools that maximize the military readiness of those on the battle field.1.1	Project ContextSimulation has become an invaluable tool to enhance military readiness through force structure analysis, simulation-based acquisition, mission rehearsal, and training.  Distributed simulation – networking multiple, separate simulation components together – has become a key technology in realizing the benefits of simulation, allowing a wide range of simulation components to be brought together for a common purpose.  To achieve this common purpose, we need to certify interoperability between many participants in training exercises.1.1.1	Simulation DefinedThere are three major types of simulation used to support training.  The first of these are the actual players in “Live” simulators, actually an “On Board Training” component of a tactical weapons system.  Next are the actual players in “Virtual” simulators, otherwise known as “man in the loop” simulations of weapons systems, designed to provide training in the entire weapons system or specific tasks performed by the system. To round out the battle space environment provided by “Live” and “Virtual” training systems, “Constructive” simulations are added.  These are computer based representations of various weapons systems, and they provide synthetic forces that act as opposing forces and cooperating friendly forces.  These simulations are the threat generators for both individual training systems and large scale systems such as ONESAF, MODSAF, and JSAF. 1.1.2	Interoperability ChallengeThe forces being trained come from a variety of sources, since aviation, surface, and subsurface force components must operate together effectively.  This paper is written from a Navy Aviation viewpoint.  Navy Aviation training starts with the Navy Aviation Simulation Master Plan (NASMP) which is part of the Fleet Aircrew Simulation Training (FAST) program.  From this starting point, NASMP integrates with the Navy Continuous Training Environment (NCTE) in support of Fleet Synthetic Training exercises.  Beyond Navy training are Joint and Coalition training, as implemented by the Joint National Training Capability (JNTC).  The facilities that utilize this training are located throughout the United States, and indeed around the world.  The training can be provided on a schedule or to meet the real-world needs of the warfighter. In addition to modeling the battle space to present a realistic tactical and operational challenge, Navy training emphasizes communications between the forces being trained.  Verification of interoperability – both of the simulation components, and the communications components – is essential to enable exercises to be performed with the expected interoperability and minimal integration.Interoperability means not just basic physical connectivity, but advanced coordinated functionality, where separate simulation components work together in a fully coordinated manner (sometimes referred to as “fair fight”) producing a valid and reliable outcome.  Efforts to verify this kind of interoperability via automated testing means have been the topic of many papers, including [1]. The goal of testing toward this type of substantive interoperability is to maximize the benefits of all types of simulation, whether constructive, live or virtual, and to significantly improve on the value of simulation, creating the opportunity for greater Return on Investment (ROI) from simulation.Thus, the challenge of interoperability requires a two-fold solution.  The first part of the solution is specifying the standards that the components must comply with.  These standards start with a foundation of computer and network standards and add the High Level Architecture (HLA), including the Run-Time Infrastructure (RTI) Application Programming Interface (API), the Federation Object Model (FOM), and federation agreements.  The second part of the solution is verifying compliance with these standards via a testing program.  The numbers and complexity of the tests to verify compliance mandates that the testing process be automated in its development, performance, and reporting. 1.1.3	Addressing the NeedThe seminal inspiration for this work was NASMP.  The standards for NASMP are still evolving at this time and include all of the standards elements previously discussed.  Ultimately, however, the requirements on NASMP components are published in a Federation Agreements Document (FAD) [2] and Guidance, Rationale, and Implementation Manuals.  This FAD has changed as the architecture of NASMP has evolved to meet training requirements.  The FAD is a controlled and configuration managed product, and each version has an accompanying Federation Agreement Compliance Test Tool (FACTT) to support testing the implementation of the requirements specified in the corresponding FAD.   Specifically, the FACTT certifies compliance with the procedural, interface, and data requirements of its parent FAD.  As more weapons systems enter the NASMP training federation, the FAD requirements are becoming more numerous and complex.  In early recognition that this would occur, NAVAIR Training Systems Division (TSD) published a Small Business Innovation Research (SBIR) topic, Automated Interoperability Testing, N03-022.  Three (3) Phase One contracts were issued in 2003.  Based on the results of these, two (2) Phase Two contracts were issued in September, 2004.  The Phase One results and Phase Two plans were presented at the Fall 2004 SISO Workshop.  This paper presents the results of one of these Phase Two efforts.1.2	SBIR Phase 1 Results SummaryVirtual Technology Corporation’s phase I SBIR project focused on three elements of the problem – identifying and specifying the components of interoperability, developing a prototype AIT system, and demonstrating that prototype.1.2.1	Components of InteroperabilityThe HLA began as a Department of Defense (DOD) standard, and has progressed to an open standard via the Institute of Electrical and Electronic Engineers (IEEE) – IEEE Standard 1516. The HLA standards were developed based on the notion that a single, monolithic simulation cannot satisfy the needs of all simulation users [3]. The architecture consists of three main components:  a set of architectural rules, an object model template (OMT) specification, and an interface specification. The standards provide the flexibility to develop, reuse, and combine simulation components (federates) into groups (federations) to satisfy a diverse set of user requirements. The HLA provides a standard distributed modeling and simulation (M&S) operating environment that, with federation development and execution expertise, can be used to achieve interoperability among simulations. RTI software exists to implement the interface specification and provide a set of distributed services. A set of software language APIs exists to provide a well-defined mechanism by which a federate interacts with an RTI. Fundamentally, federations evolve around the definition and application of a FOM. The FOM prescribes a hierarchical format for exchanging data between federates in a federation. These format descriptions are standardized (via the OMT specification) and include many of the details for federates to produce and receive data. The software APIs dictate, from a mechanical perspective, how federates interact with an RTI. Unfortunately, neither the FOM OMT specification nor the software APIs includes sufficient semantic information to be unambiguous about the purpose or representation of data passed among federation participations to be sufficient for defining or achieving interoperability. Therefore, additional “extra-standard” mechanisms have evolved for capturing federation interoperability rules.The Federation Development and Execution Process (FEDEP) is an iterative process for development, execution, and maintenance of HLA federations [4]. An important component of the FEDEP process is the development of a FOM and Federation Agreements. The flexibility of the HLA standards, intended to enable the tailoring of data models, data encoding, and time management schemes, also can serve as impediments to interoperability. Key factors for interoperability, such as data model semantics and model behavior are, by design, entirely unaddressed by the HLA standards. The complex relationships that often exist between federation participants means that introducing one incompatible federate can cripple an entire federation and consume valuable integration and test time. As a result, federations are forced to develop federation agreements to address important aspects of simulation interoperability and it is necessary to ensure each federation participant complies with these agreements.Since these federation agreements are not part of the HLA standards each federation develops independent, ad hoc mechanisms for capturing this critical federation interoperability information. A significant part of an automated interoperability testing solution will include devising methods for capturing interoperability information in a way that facilitates automated testing.1.2.2	Prototype AIT SystemDespite the criticality of federation interoperability to achieving simulation objectives, interoperability testing remains a largely manual process and there is no systematic method for capturing and ensuring compliance with federation agreements. There are no general-purpose off-the-shelf tools or processes for evaluating interoperability and compliance with federation agreements. The federate compliance testing available from the Defense Modeling & Simulation Office (DMSO) only evaluates compliance with a set of HLA standards that do not themselves address several critical aspects of interoperability. Therefore, while the purpose of a simulation is to produce results, the current state of the art in interoperability testing often requires so much effort to be applied toward integration and testing, that the time and resources available for actually running the simulation is minimal. This, combined with challenges of achieving complete logical interoperability, has often compromised the results of a simulation program and limited its ability to fully support the required program objectives. Current federation integration and testing routinely consume significant amounts of a simulation program schedule verifying basic interoperability requirements before addressing the most significant interoperability challenges required for fair fight, such as data model semantics, model behavior, and common databases. As the potential of simulation is increasingly realized, the size, scope, and frequency of simulations will grow, and as they do, so will the challenges and costs of the current, manual interoperability testing processes.During the AIT Phase I SBIR effort, VTC developed a software design, process, and prototype tool capable of providing automated testing and verification of candidate federates for compliance with HLA federation agreements. This effort reinforced the following benefits of automated interoperability testing: •	Increasing the effectiveness of testing and level of interoperability achieved by distributed simulation components and thereby maximize the utility of simulation to support program requirements •	Dramatically reducing the time and effort needed to integrate and test federations•	Allowing more resources to be available for execution of simulations•	Facilitating Validation, Verification, and Accreditation (VV&A) of federations•	Increasing the stability of federations•	Providing a clear understanding of the reusability/composability of components in a federation •	Making it significantly easier to produce high quality results from complex distributed simulations1.2.3	Demonstrating the PrototypeTo assess the utility and usability of the phase I prototype AIT technology, VTC applied it to NASMP in the development of that program’s FACTT. The NASMP Mission is to lead the revolution in Navy aviation training simulation by defining standards and technologies to be applied to future simulator systems that will link training across platforms through a common synthetic battle space using network technologies and HLA standards.  Because of the diverse set of NASMP simulators, NASMP federations are particularly complex, distributed systems. To manage that complexity NASMP federations have intricate startup and shutdown sequences, data encoding schemes, and other rules for federate interoperability documented in a federation agreements document.  The VTC AIT team installed and demonstrated the FACTT and supported testing of the Next Generation Threat System (NGTS). NGTS is a joint Air Force/Navy effort to build a new Computer Generated Forces (CGF) simulation system. The experience applying the AIT technology to a NASMP complaint federate yielded enthusiastic support, wonderful results, and valuable feedback. With the help of the FACTT the NGTS team was able to debug an elusive data encoding scheme problem that had manifested itself as inconsistent data flow to recipient federates, demonstrating the usefulness of a commercial AIT system. The NGTS team also provided valuable feedback about improving the user interface capabilities to make the tool more user friendly.The phase I prototype was demonstrated at Interservice/Industry Training, Simulation & Execution Conference (I/ITSEC) - This consisted of a demonstration of the test editor and execution interfaces, five example test cases, and a demonstration of comprehensive results reporting and object model analysis capabilities. The prototype application is shown in  REF _Ref136913518 \h  \* MERGEFORMAT Figure 1. The phase I prototype’s automated interoperability testing system maintains flexibility to test the unique characteristics of any HLA federate or federation. The tool maintains this flexibility by enabling the user to construct test cases from generic test step templates (referred to as “blocks”) and specialize these blocks by specifying parameters. For example, the system has a generic “CreateAndJoinFederation” block, which is specialized by providing a federate name, federation name, and Federation Execution Data (FED) file. These parameters can be provided during test case development or to maintain maximum flexibility the parameters can be supplied during test execution through a series of questions posed to the user. Federate/federation interoperability test cases are constructed by visually chaining blocks together and specializing the blocks by supplying the necessary parameters. During test execution the application shows the pass/fail status of each step in the test case, which step is currently executing, and detailed test case output.Figure  SEQ Figure \* ARABIC 1 - Phase I Prototype ApplicationThe five test cases demonstrated at I/ITSEC were in increasing order of complexity and conceived to demonstrate the phase I prototype’s significant value added by demonstrating the capabilities of far less capable tools, such as DMSO federate compliance testing, Federation Verification Tool (FVT), and the Simulation Interoperability Test Harness (SITH), and also demonstrating capabilities well beyond those tools.  A full description of the phase I efforts and results can be found in [5].1.3	SBIR Phase II Efforts SummaryThe phase II efforts have been focused in five (5) areas, as described in the following sections of this paper.1.3.1	Broader Test SupportDrawing on lessons learned from the phase I prototype, an understanding developed that the AIT needed to provide a rich set of fundamental testing primitives, which could be combined to define increasingly complex testing.  These testing primitives are called blocks, where a block represents a unit of behavior or a function.  Blocks are used to represent each of the distinct capabilities of the test tool and each block will have different requirements for user-specified parameters.  In some cases a very fine-grained block is called for (e.g. RTI API level), while at other times very coarse-grained blocks may be more useful (e.g. to perform a throughput test between two hosts).In order to determine what blocks would be needed to provide a robust testing environment, a sample federation was developed.  This federation was accompanied by a FAD that defined several federate roles needed to provide federation management as well as provide training.  Among the federate roles are three of note for testing – the FCF, which manages the members of an asset pool of federates that can be used to support training operations, the MTS/IOS, which manages the state of federates supporting a specific training operation, and the ALM which monitors federates for technical problems (invalid RTI operations, crashes, detected errors, etc.) during training operations.  A top-down decomposition of test cases used to test the sample federation with accompanying FAD was performed and combined with a bottom-up synthesis of test behaviors and functions that would implement test sequences needed to implement test cases.  Thus, a brief excerpt from the spreadsheet detailing the top-down analysis contains the following breakdowns:Test Action – Emulate FCF (e-FCF), MTS/IOS (e-MI), and ALM (e-ALM) through asset pool lifecycleFunctions Needed:Subscribe to Interaction(s)Publish Interaction(s)Create FederationJoin FederationResign FederationDestroy FederationSend InteractionCollect InteractionsWait For InteractionWait For n SecondsWait For Federate(s) to JoinWait For Federate(s) to ResignTest Action – Monitor Federate Under Test (FUT) publications and subscriptionsFunctions Needed:Check Federate Publications – composed of:Subscribe to Interaction(s)Publish Interaction(s)Collect InteractionsSend InteractionWait For InteractionVerify Publication InterestCheck Federate Subscriptions – composed of:Subscribe to Interaction(s)Publish Interaction(s)Collect InteractionsSend InteractionWait For InteractionVerify Publication Interest REF _Ref136944706 \h  \* MERGEFORMAT Table 1 presents a brief excerpt from the bottom-up functional analysis that was performed.  These analyses provided data to support mapping from abstract testing behaviors or capabilities to implementation-level behaviors – and substantiated the selection of testing blocks implemented in the AIT.Table  SEQ Table \* ARABIC 1 - Bottom-up Functional Requirement AnalysisSupported HLA FunctionParameters and FunctionscancelIgnoredSyncPointvoidcollectDiscoveriesvoidcollectInteractionsvoidcollectRemovalsvoidcollectUpdatesvoidcreateFederationExecutionvoiddestroyFederationExecutionvoiddisableAsynchronousDeliveryvoiddisableTimeConstrainedvoiddisableTimeRegulationvoidenableAsynchronousDeliveryvoidenableTimeConstrainedvoidenableTimeRegulationvoidgetDiscoveredFederatesjava.util.CollectiongetFederateTimedoublegetJoinedFederationDetailsjavax.management.ObjectNamegetOMTcom.virtc.rad.hla13.omt.OMTgetPortintgetRateorg.apache.commons.lang.math.FractionignoreSyncPointvoid1.3.2	Virtual Control IntegrationIntegrating AIT as a plug in component to Virtual Control (VC) provides access to a number of services and capabilities provided in that architecture that support test and verification of large-scale simulation system infrastructure. VC is a multi-tiered architecture for monitoring and controlling large-scale distributed simulation systems.  REF _Ref137291489 \h  \* MERGEFORMAT Figure 2 shows the components of the VC architecture. The architecture has been designed and developed for scalability to monitor arbitrarily large simulation systems.Figure  SEQ Figure \* ARABIC 2 – VC Multi-tiered ArchitectureThe characteristics of the system architecture include:Thick/thin clients (VC GUI) for supporting numerous simultaneous interfaces to system monitoring.An application server (VC Server) which provides a central point of control for common managed object (e.g., components of hosts (processors, drives, etc.), network devices, etc.) update, monitoring, and management services. The server propagates data updates to the thick/thin clients. Databases (VC DB) which support managed object persistence. A hierarchical agent topology which supports localized data collection, support for various network configurations, and send-side message filtering. Agent (VC/SNMP Agent) services provide system monitoring and control data to the VC application server. Commands and a command management structure which encapsulate system monitoring and management functionality (e.g., LaunchProcess(), etc.)VC’s commands and command structure have many similar attributes to the AIT testing blocks. VTC has begun developing an initial design for integrating AIT’s test scripting capabilities with VC’s command structure.1.3.3	Modeling Testing as UML2 Activity DiagramsThe problem the AIT is meant to address has much in common with issues that are being addressed by the Model Driven Architecture (MDA) efforts coordinated by the Object Management Group (OMG).  One of the goals of MDA is to use software models to actually produce end products rather than as simply early-lifecycle artifacts.  The effort has lofty goals and is relatively immature, but it leverages some component technologies that already exist in a relatively robust state and which are directly relevant to our efforts on AIT.  Specifically, the recently revised UML specification UML 2.0, and some transformation and representation technologies based on eXtensible Markup Language (XML), including XML Metadata Interchange (XMI).The UML 2.0 specification defines a graphical language for visualizing, specifying, constructing, and documenting the artifacts of distributed object systems [6].   It provides detailed semantics for constructing activity diagrams.  The issues that were identified early in the AIT project surrounding control flow and aggregation are specifically addressed by the UML 2.0 specification.  The UML defines all of the components necessary to build activity diagrams complete with swimlanes or partitions.  Further, the UML specification clearly defines the mechanisms by which activity diagrams can pass control and data to nested activities.  Finally, UML defines clear semantics for control flow within activity diagrams, and provides the constructs necessary to model parameterization and handling of return values.Thus, the analysis performed during the project led us to the following conclusions:•	Incorporating a UML 2.0 metamodel into AIT to capture test specifications would provide a more comprehensive capability than we would likely get from a homegrown model.•	Incorporating OMG specifications-based technologies will likely extend the usability and usable-lifetime of the AIT solution developed.•	More robust test models that could be usable beyond AIT’s immediate testing scope can be developed.1.3.4	Incorporating Model ReuseExperience maintaining tests has shown the need for an ability to re-use sections of testing procedures.  Designing tests for cases where a common process is utilized repeatedly or where a tedious process is initiated from separate tests has required a great deal of duplication of testing procedures.  This, in turn, also means that when the testing procedure changes the AIT tests have to be changed in multiple places.  In order to alleviate this maintenance problem, the AIT design introduces a “Group” concept, which aggregates test blocks and allows them to be addressed or called similar to the way that subroutines work in software programs.Aside from reducing maintenance overhead of developed tests, the group concept is central to the way that users will build-up tests from primitive blocks.  The user interface allows users to create blocks that aggregate other blocks indefinitely.  This allows complex tests to be organized using a top-down approach, hiding the details of the primitive blocks until the user really needs to see it.  This also allows testing procedures to be segmented in a manner similar to the way federation agreements segment requirements, that is, organizing them by sub-function and linking sub-functions together to implement higher order federation functionality.  For example, the process of starting up a federation may require a number of discrete actions before actual evolution of the simulated battlespace can be computed.  Testing this federation startup can be implemented as a group, which is used as a significant part of one test (verifying the implementation of federation startup agreements) as well as being re-used in other tests to get the federate-under-test into a state to test the implementation of other agreements.  If, at some later point in time, the federation startup agreements were to change, the group concept will limit the changes to that group – all of the tests that use that group will be unaffected by those federation agreement changes.1.3.5	Examining Sample FederationsAs the project evolved, the AIT team sought out sample federations, learned about their federation environment and agreements, and applied the lessons learned back into the design of the AIT. Three of these sample federations provided significant feedback to the AIT project and are summarized in the following sections, with a more complete treatment in section 3 of this paper.1.3.5.1	Joint National Training CapabilityThe federation agreements that the AIT development team are most familiar with – both by team member experiences, and by experience through the phase I project – involve using the RTI in standard mode without Data Distribution Management (DDM) utilization.  To broaden the working knowledge base of the AIT development team, and to ensure that the AIT would properly support connectionless-mode federations, an understanding of the JNTC was sought and achieved.1.3.5.2	Joint Single Integrated Air Picture (SIAP) System Engineering Organization (JSSEO)JSSEO has the lead within the Department of Defense (DoD) for improving SIAP capabilities.  To achieve the SIAP, JSSEO is developing the Integrated Architecture Behavioral Model (IABM) – a platform independent model (PIM) defining the common “business logic” for creating and maintaining the SIAP among all participants.The IABM is being developed in increments called Timeboxes.  Before release, each timebox undergoes a rigorous set of tests using both standalone and distributed system based test environments.  As a “reality check” on the early design of the AIT, the JSSEO team evaluated the AIT and provided feedback to the AIT development team.1.3.5.3	Bridge Federate TestingNAVAIR defined and specified the need to test a specialized bridge federate connecting federations requiring data translation between dissimilar federations.   The AIT team was awarded a phase III contract to apply an early prototype of the AIT to provide Independent Verification and Validation (IV&V) of the HLA interface and NASMP FAD compliance capabilities of the bridge federate.2.	Model Driven TestingThe OMG has been leading a movement toward MDA, a framework based on detailed specifications that is intended to improve portability, interoperability, and reusability of software through careful separation of concerns.  The challenges that MDA aims to solve are a superset of the challenges that federation integrators face as federations develop, and it specifically shares interoperability goals with the AIT SBIR effort.MDA is a model-focused approach to systems development, and it is a framework built on top of other OMG standards including UML, XMI, and MetaObject Facility (MOF) [7].  The architecture allows models to be built that specify the characteristics of a system and environment for a specific purpose.  These models are generally composed of diagrams and descriptions that may by expressed in a variety of modeling languages.  The most powerful characteristic of these models is that they are independent of the platform that supports them.  The MDA leverages its supporting technologies including XMI and MOF to render models in machine-readable formats, which opens the door for a wide range of transformation technologies to be applied including processes to automate code or documentation generation, and even to automate validation against requirements or testing of models.  Facilitating testing based on models is where the AIT SBIR effort has focused its efforts.Since its adoption by the OMG in the mid 1990s, the UML has become the de-facto standard modeling language for specifying and visualizing software systems.  The UML specifies a wide variety of diagram types to specify the structure and behavior of software systems.  A common structural diagram is a class diagram which specifies the characteristics of a class.  Examples of behavioral diagrams include interaction diagrams, state, and activity diagrams.2.1	Selection of UML2 Activity DiagramsActivity diagrams support the specification of sequences and conditions of actions, as well as specification of data flows between actions [8].  They are similar to interaction or sequence diagrams (also specified by the UML), however they are different in a subtle but important way.  Interaction or sequence diagrams model sequences of actions or method invocations made between specific objects in a system.  Activity diagrams model only the sequence of actions, and do not explicitly associate the actions with a particular owner.  Further, activity diagrams lend themselves more readily to modeling features commonly needed in designing test suites including iteration, looping, and nesting of behaviors.  For these reasons, the AIT has been based on the activity diagram metaphor.  Its user interface provides, among other things, a mechanism to build activity diagrams that specify tests to be executed within the AIT environment.Activity diagrams are composed of actions (defined in the UML) linked together in sequence by control flows.Figure  SEQ Figure \* ARABIC 3 - Basic Elements of an Activity DiagramControl flow in activity diagrams follows a flowchart-like pattern from one node to the next, subject to constraints called guard conditions, which are Boolean expressions optionally specified on each control flow.   REF _Ref138391909 \h Figure 3 above depicts some of the basic elements of an activity diagram.  Note that the diagram captures both control and data flow.  Additionally, a set of control nodes exists that specify particular behaviors.  These control nodes include initial nodes, fork nodes, and final nodes.  Actions execute synchronously, or block, until they return, at which point control flows out of the action node.  A fork node is a special control node that has one incoming control flow and one or more outgoing control flows, which gives modelers a way to create parallel activities.  Finally, a pair of control nodes exists to support the creation and deletion of objects: CreateObjectAction and DeleteObejctAction.  AIT users use the CreateObjectAction to create instances of the classes contained in the AIT’s underlying UML model. Data flow in activity diagrams follows a flowchart-like pattern as well; data can only flow between object nodes, however.  A set of special object nodes called pins is supported, and these pins can be attached to actions in an activity diagram to represent input or output parameters.  Pins are named and “typed” to prevent users from connecting mismatched input or output types.   REF _Ref138391909 \h Figure 3, above, includes both input and output pins on its CallOperationAction, as well as data flows that connect these pins to input and output ActivityParameters.  Note also that the OutputPin on CreateObjectAction is mapped to the “target” InputPin on CallOperationAction.The UML defines two actions that, together with the CreateObjectAction, provide the bulk of the AIT functionality.  First, CallOperationAction is an action that takes an object as input (via a “target” pin), and specifies which operation on that target object should be called.  Once an AIT user has included a CreateObjectAction in his activity, its output can become the input (via a dataflow directed to a “target” InputPin) to subsequent CallOperationActions.The CallBehaviorAction is similar to the CallOperationAction except that instead of specifying the invocation of an operation on a target object, it specifies the invocation of another behavior (In the UML an Activity is a subclass of Behavior).  For AIT’s purposes, gives users the ability to create and use something roughly equivalent to subroutines in their test suites.  The AIT allows users to specify ActivityParameterNodes, a construct from the UML that allows activities to be parameterized, greatly improving the potential for reusing activities.  Users simply create data flows that link to InputPins on the CallBehaviorAction, and the AIT’s activity interpreter maps those InputPins to the called activity’s ActivityParameterNodes at runtime.Activity diagrams are only a part of a UML model, and require other things to be present in the model in order to be useful for more than just visualizing an abstract process.  As mentioned, the AIT relies heavily on the CallOperationAction defined in the UML standard.  This action essentially points to operation for execution, and that operation must be present in the same UML model as the activity.  The AIT, built on top of Virtual Technology Corporation’s Virtual Control™ tool, populates each UML model it creates with the Virtual Control distributed service APIs, which makes these far-reaching APIs accessible to test authors via CallOperationActions.  AIT users, when developing activities to carry out testing, have a rich “operation palette” from which to choose operations for inclusion in activities.Simple activity diagrams that make use of a series of CallOperationActions and the required data flows are useful for simple testing, but even simple tests can be tedious to construct given the specificity required by the activities and data flows.  Careful organization of tests (partitioning them into separate activities) and judicious use of the CallBehaviorAction can reduce some of the difficulty associated with creating complicated tests.   With a little care, test designers can build activities in a way that facilitates re-use and the creation of more abstract behaviors by aggregating lower-level activities via CallBehaviorActions.2.2	Broaden HLA Test SupportSupport for an HLA Federate APIs is provided through one of Virtual Control’s service APIs.  Virtual Control provides a family of services that support creating and controlling HLA Federates.  These service APIs make up the full set of functionality from which Table 2 was excerpted.  These services are all managed by Virtual Control and allow VC client applications to create and control ControllableFederates on remote machines.  The API for the ControllableFederate is not a direct implementation of the RTIAmbassador and FederateAmbassador interfaces, but a value-added API that allows users to interact more easily with an HLA Federate.In addition to including VC APIs in the UML model along with the user’s tests, the AIT is capable of importing a user’s FOM so that models can capture all of a federation’s specific data representation details.  The AIT supports this import capability by making use of a Profile, or extension to the UML specification, which allows the model to contain various OMT-specific characteristics including elements such as Accuracy, AccuracyCondition, etc.  Virtual Control’s ControllableFederate API works with the classes imported from a FOM, so users can specify activities that create and manipulate instances of objects defined in their FOM, then send those objects directly to the ControllableFederate for distribution in the federation.2.3	Capture Test ActivitiesPerhaps one of the most interesting side effects of capturing detailed test specifications in a machine-readable format is the potential for transformation technologies to be brought to bear.  This is one of the key aspects of the MDA approach.  The AIT employs a relatively simple technique to interpret activity diagrams contained in a UML model instance, but this is only one of the possible transformations of the model that could be performed.  Given the AIT’s ability to import OMT FOM specifications and incorporate them into activity diagrams, tremendous potential exists for applying other transformations to do things such as documenting “expected behavior” for federates in a federation, or for expanding on the pervasive FOM-based code generation practice to include generation of code that implements not only the FOM structures, but also behaviors defined in the model.The additional expressiveness and extensibility of the UML and other MDA supporting technologies go a long way toward capturing both the low-level and high-level technical details necessary to bring automation to the interoperability testing challenge that federate developers and federation integrators face.3.	Environmental Analysis and FeedbackDuring the execution of the AIT project, multiple projects were sought to obtain additional insights into testing approaches and needs, and these insights led to a set of lessons learned that can assist the team in its future endeavors with this technology.3.1	Three Project EnvironmentsAs has previously been introduced, the AIT team worked with three sample federations to obtain real-world understanding and feedback to apply to the AIT development process.  The details of this effort are described in the following sections.3.1.1	Joint National Training CapabilityThe JNTC federation provides a unique environment for evaluating federate and federation automated interoperability testing, because it is configured to use the RTI in connectionless mode [9]. In connectionless mode the RTI provides significantly increased reliability across even the largest scale federations at the expense of reliable communications and several of the RTI services.The RTI configured for connectionless mode leverages unique constraints on federates and federation, as well as on automated interoperability testing. The JNTC federation is restricted to only calling the following services to initiate federation events.Federation Management Join Federation Execution Resign Federation Execution The use of synchronization points is not allowed. The use of the RTI save and restore services is not allowed.Declaration Management Publish Object Class Unpublish Object Class Publish Interaction Class Unpublish Interaction Class Subscribe Object Class Attributes will not be used. Instead Subscribe Object Class Attributes with Region must be used.Unsubscribe Object Class Subscribe Interaction Class will not be used. Instead Subscribe Interaction Class with Region must be used. Unsubscribe Interaction Class Object Management Register Object Instance should not be used, instead use Register Object Instance with Region Delete Object Instance Local Delete Object Instance Update Attribute Values Send Interaction should not be used, instead use Send Interaction with Region. Request Attribute Value Update – The use of this service is discouraged. Provide Attribute Value Update – This request can be ignored. Ownership Management will not be used Time Management will not be used Data Distribution Management will be used by all federates. Refer to  HYPERLINK "file:///U:\\PROJECTS\\AIT\\JNTC\\MC02_DDM.html" Scalability and DDM for MC02 for the details on how DDM will be used for M202. Create Region Modify Region Delete Region Register Object Instance With Region Subscribe Object Class Attributes with Region Unsubscribe Object Class with Region Subscribe Interaction Class with Region Unsubscribe Interaction Class with Region Send Interaction with Region Support ServicesAll advisories will be unavailable. MOM Services MOM services will be unavailable. 3.1.2	JSSEOThe SIAP is the product of fused, near-real-time and real-time data from multiple sensors to allow development of common, continuous, and unambiguous tracks of all airborne objects in the surveillance area. All airborne objects must be detected, tracked, and reported.  Further, each object must have one and only one track identifier and associated characteristics to be incorporated into SIAP.  The SIAP is modeled by the IABM, which is tested by both standalone and distributed system test environments.The distributed system test environment is known as the IBuild and is an instantiation of the Joint Distributed Engineering Plant (JDEP) technical framework.  The IBuild is a High-Level Architecture (HLA) based environment consisting an HLA runtime infrastructure (RTI), a scenario driver providing ground truth, a data collection tool, and several simulation components (HLA federates) used to provide the inputs required to fully stimulate and test the IABM functionality.Once mature, the IABM will also be integrated into a variety of weapon systems to provide the common SIAP capability.  To help mitigate integration risk, the IABM is being used in a series of Hardware-in-the-loop (HWIL) events with these weapon systems to test how the IABM operates in a realistic setting.  These events are a mixed federation of HLA, DIS, and tactical simulations developed and maintained by several geographically dispersed organizations.  Due to the nature of these events, much of the integration is performed during the initial stages of the actual event.  Though the federation agreements are well-documented, federates invariably show up for integration that do not fully abide by the Federation Agreements.  It is for this reason, that an Automated Interoperability Test Tool is needed.The AIT, as designed/planned, should have sufficient functionality to serve the needs of JSSEO.  However, the assessment of the tool was conducted before the final version was complete and not all functionality was implemented.  As such, the following recommendations may have already been implemented in a later release of the tool.In general, JSSEO requires the following basic capabilities:Validating federates are using the XDR encoding/decoding scheme.Validating federates subscribe to the StartTestExecution and StopTestExecution interactions and respond accordingly.Validating federates publish the correct objects and interactions and the attributes and parameters are within range for the proper units of measurement.Validating federates subscribe to the correct objects and interactions and the attributes and parameters are within the expected range.Verifying federates properly resign from the federation.The specific recommendations are as follows:Incorporate the ability to save test plans using XML and include a well-defined schema.  JSSEO has a tool that configures and automatically executes specific, requirements-based test cases.  Making these files compatible would improve interoperability between the tools.Implement the following blocks:IP ValidationEncoding ValidationWaitWait For UserCreate Object InstanceSend Interaction (with user defined parameters)Wait for InteractionWait for FederateCheck for FederateLaunch Federate (both Java and non-Java federates)Update AttributesCreate Object InstanceSubscribe All AttributesCollect Attribute Updates3.1.3	Bridge FederateThe bridge federate provides the ability for simulation experts to create, modify and transfer modeling and simulation data across dissimilar federations. To support the IV&V of both the HLA interface and NASMP FAD compliance, the AIT team needed to implement mechanisms for sending data and test boundary indicators across the bridge – the test boundary indicators were needed since some portions of the data would not be able to cross the bridge and the receiving copy of the AIT would need to correlate received data to the test case producing that data.The AIT team has provided some useful early feedback on the installer, which was evaluated and incorporated.  Among the issues reported were:Had to copy TestToolTemplate.dtd from \testTool-AIT_DEV_PH2\docs\ to C:\Program Files\AIT\Docs\ Had to copy jython.jar from \stm-core-commercial\dist\stmConsole\modules\ext to C:\Program Files\AIT\stmConsole\modules\ext\ Had to copy NG-Pro-v2.0.2.jar (attached) to C:\Program Files\AIT\stmConsole\modules\ext\ (ExecuteJythonBlock.java has a hard coded reference to it)On the "Choose Java Virtual Machine" screen, choose a v1.4.x Java VMInstall the VM automatically rather than references one on the systemOn the "Choose Install Set" screen, choose "AIT Console"Make AIT Console the only choiceIn addition, the need to support two or more interfaces, corresponding to different federations (potentially with different RTIs and FOMs), was identified as a useful enhancement to the AIT.  This, in turn, is the subject of a SBIR enhancement contract that has just been awarded as of the writing of this paper.3.2	Lessons LearnedEmbracing a general-purpose modeling language like the UML to capture test specification in the simulation domain requires that a few extensions be made to the UML itself in order to capture all of the detail represented in some of the simulation-domain standards (notably OMT).  As discussed briefly above, this was achieved by leveraging UML 2.0’s Profile mechanism.  Early in the AIT development process, a UML Profile for OMT was developed, which allowed the AIT to import all of the salient details from an OMT file for later use in AIT test models.  The bulk of the lessons learned came from the application of the AIT to the three programs outlined above.Among the lessons learned supporting integration testing in the JNTC context was that integration test designers must be aware of the limits imposed by the federation environment.  In connectionless mode, for example, the HLA MOM data was not available for monitoring.  The integration test tool relied, initially, on MOM data to determine, among other things, when federates had joined and left the federation.   Testing the federation agreements about how individual federates interacted with their RTI and Federate Ambassadors also proved challenging and significant effort had to be invested in devising a scheme to monitor these processes through a non-RTI backchannel.Both JNTC and JSSEO indicated a need to support FOM-level data validation.  The AIT currently is capable of importing an OMT representation of a FOM so that FOM-specified classes can be incorporated into tests.  This capability falls short, based on JNTC and JSSEO needs in two areas:  First, the AIT user interface does not provide a useful way for test designers to create and modify instances of FOM-specified classes.  Second, the AIT’s mechanism to support performing data validity checks will rely on the specification and evaluation of guard conditions on data and control flows in the test activity diagrams.  Guard conditions are specified in the UML in a very general way, but ultimately models support guard conditions as text strings.  Providing a user-friendly mechanism for the specification of guard conditions that support meaningful validation of FOM-level data is an area where AIT has significant ground to cover.All of the programs supported to date had a number of test requirements that were, at first glance, more passive in nature than the AIT’s activity diagram approach would support.  Tests that perform continuous validation, such as encoding validation, or tests for behaviors that federates are required to support asynchronously, fall into this category.  Designing tests by using activity diagrams tends to lead to active tests, and thinking in terms of the RTI Ambassador API reinforces that.  In order to specify tests that react to information received from the RTI, the test federate must be able to handle FederateAmbassador callbacks in a way that can be incorporated into the activity diagrams.  To date this has been achieved in the AIT by making use of VC’s ControllableFederate API which is capable of queuing these callbacks for later processing, and these methods can be included in activity diagrams in order to trigger that processing at specific times.In supporting the Bridge Federate testing, it became apparent that the nature of the testing that was necessary was markedly different than what was needed for the JNTC and JSSEO programs.  As a federation bridge, the testing strategy the AIT team had devised centered on providing the bridge a set of input data, then verifying its output against the set of rules it uses to bridge the federations.  This type of testing is almost entirely FOM data validation testing as opposed to the lower-level direct federate API or aggregated API-behavior testing that was necessary for JNTC and JSSEO.Finally, testing environments are complex, and embracing activity diagrams to capture test specifications and execute tests does relatively little to reduce this complexity.  Although mechanisms exist to aggregate activities, providing the ability to build upon lower-level activities to create abstract behaviors, the level of detail required to specify data and control flows can be overwhelming.  Since machine-executable (not just machine-readable) models are what the AIT is working toward, this level of detail is unavoidable.  Careful design of user interface components that allow rapid searching of the set of activities included in a model will become critical as test suites grow, and providing visibility into the dependencies and relationships between model components is a challenge that test authoring tools like the AIT will need to address.4	Future DirectionsAs the AIT continues to be used to support efforts like those of the JNTC, JSSEO, and Bridge Federate, many of the lessons learned will be addressed.  Additionally, development plans include work along three lines:  Expanding the set of actions available to users for inclusion in test activities, improving the robustness of the activity execution engine, and leveraging transformation technologies to provide federation documentation. Future work includes plans to provide support in AIT for other widely used simulation protocols including DIS and TENA.  This effort also includes expanding the existing HLA federate support to allow AIT to simultaneously control multiple federates that may use different RTI versions.Improving the AIT’s support for concurrency features in the activity diagram context is a focus of future work as well.  Providing precise control over forking and joining control flows in tests is functionality that will contribute significantly to large-scale testing or testing that requires the AIT to act as a proxy in order to trigger behaviors in a federate under test.Finally, leveraging transformation technologies such as Extensible Stylesheet Language Transformations (XSLT) to extract documentation from the AIT’s underlying UML model has the potential to contribute significantly to helping reduce federation integration costs. 5.	References[1]	Babineau, W., Barry, P., & Furness, C. (1998). “Automated Testing within the Joint Training Confederation (JTC)”, Simulation Interoperability Workshop, 98F-SIW-064, September 1998.[2] Navy Aviation Simulation Master Plan (NASMP) Federation Agreements Document (FAD) Version 1.4.2, NASMP Systems Engineering Federation Working Group, 7 April 2006.[3]	Turrell, Chris (1999). “High Level Architecture”, Simulation Technology, Vol. 1 Issue 4. 21 June.[4]	High Level Architecture, Federation Development and Execution Process (FEDEP) Model, Version 1.5, Dec. 8, 1999.[5]	Perkinson, P., Babineau, W., Head, R., Kirk, M., Schwindt, C., Tufarolo, J., Hougland, E., Dyke, D., & Haselby, S. (2004). “An Automated Interoperability Testing System”, Simulation Interoperability Workshop, 04F-SIW-074, September 2004.[6]	Unified Modeling Language: Superstructure, version 2.0, Object Management Group, 2005. [7]	MDA Guide Version 1.0.1, 12 June 2003, Joaquin Miller and Jishnu Mukerji Eds., Object Management Group, 2003. [8]	Conrad Bock: “UML 2 Activity and Action Models”, in Journal of Object Technology, vol. 2, no. 4, July-August 2003, pp. 43-53. [9] Hyett, M., Wuerfel, R., “Connectionless Mode and User DDM in RTI-NG V6”, Simulation Interoperability Workshop, 03S-SIW-102, April 2003.Author BiographiesWILLIAM BABINEAU received his BS in Electrical Engineering from Rensselaer Polytechnic Institute and a MS in Computer Science from Steven’s Institute of Technology. He has over twenty-five years of experience in software and systems development in various defense and aerospace applications, including over ten years of experience in large-scale distributed Modeling and Simulation (M&S). The past six years he has been employed at Virtual Technology Corporation (VTC), most recently as the Project Manager responsible for several M&S projects including the Automated Interoperability Testing phase II SBIR. He has worked on such large-scale distributed simulations as Navy Aviation Simulation Master Plan (NASMP), Warfighter 2000 Simulation (WARSIM), Joint Simulation System (JSIMS), Joint Training Confederation (JTC), and Joint Precision Strike Demonstration (JPSD).STEPHEN BERGLIE is technical lead on Virtual Technology Corporation’s AIT SBIR program.  He is involved in research and development for network and simulation management systems, and works on VTC’s Virtual Control™ product.ERIK S. HOUGLAND, PhD, is the NASMP Interoperability Engineer.  He has over twenty years of experience in training simulations and interoperability issues therein.  He received his BS in Mathematics from Rensselaer Polytechnic Institute.  He received the MS and PHD from Virginia Polytechnic Institute in Environmental Sciences and Engineering.  His research was in the application of computer and management sciences to problems in environmental management.Susan Haselby is a Computer Engineer with Naval Air Systems Command Training Systems Division Orlando, Florida. She is an interoperability engineer on the Navy Aviation Simulation Master Plan and is responsible for HLA Interoperability Laboratory Management.