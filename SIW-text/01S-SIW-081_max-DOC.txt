RTI Latency Testing over the Defense Research and Engineering NetworkMr. Maximo LorenzoCECOM Night Vision and Electronic Sensors Directorate10221 Burbeck Road, Building 309Fort Belvoir, VA 22060703–704-3185 HYPERLINK mailto:lorenzo@nvl.army.mil lorenzo@nvl.army.milMike MuussArmy Research LaboratoryAberdeen Proving Ground, MDMike Caruso, Bill RiggsScience Applications International Corporation4001 N. Fairfax Drive, Arlington VA 22203 HYPERLINK mailto:mcaruso@nvl.army.mil mcaruso@nvl.army.mil,  HYPERLINK mailto:briggs@nvl.army.mil briggs@nvl.army.milKeywords: latency, performance, RTI, objectsAbstract:In 2000, NVESD supported two major distributed simulation exercises using the Defense Research and Engineering Network (DREN) to transfer real time video as well as DIS and HLA traffic. In order to meet these expanded exercise support requirements, it became necessary to explore the impact of the network configuration against a worst case scenario representing the most demanding performance requirements.   An ATM network implementation was used during evaluation. This effort in testing and measuring RTI latency in varying network configurations is being performed in support of DMSO.1. IntroductionIn 2000, NVESD supported two major distributed simulation exercises using the Defense Research and Engineering Network (DREN) to transfer real time video as well as DIS and HLA traffic. These were the Battle Command Reengineering IV experiment, conducted in April 2000, [1] and the Smart Sensor Web experiment, conducted in August 2000. In addition to these large scale exercise experiments, NVESD conducted a series of experiments involving the simulation of second generation FLIR automated target recognition (ATR) systems and Unmanned Aerial Vehicle (UAV) systems using uncooled IR sensors, in order to clarify user requirements and explore human factors issues in a synthetic environment. Planned experiments involving NVESD and the Fort Benning and Fort Knox battle labs include the Future Combat Command and Control System (FCC2) experiment scheduled for the spring of 2001 and three Smart Sensor Web (SSW) experiments at the Fort Benning MOUT facility.In order to meet these expanded exercise support requirements, and to respond to DMSO taskings NVESD has been exploring the impact of latency across the DREN, using currently available DMSO runtime infrastructure (RTI) software, which provides the “HLA Backplane” for the Paint the Night (PTN) real-time sensor simulation, and other distributed simulation applications with which it must interact, either directly through the AMC RDEC Federation or, through a gateway, such as the DIS/HLA gateway used at NVESD by its local “Paint the Night and SWISS Together” (PST) Federation, which serves as a prototype of the emerging AMC RDEC Federation. Two main issues are relevant here: 1) What is the latency of the system?  If the system takes too long to respond, that is if latency exceeds a threshold of 250 milliseconds, the user will not be able to effectively interact with and use the system. 2) While some latency – within this overall tolerance - is unavoidable, it is also important that the residual latency be deterministic (e.g. not vary excessively from its mean).  If the system has an excessively variable response, this will create problems for the user interface, leading to overcompensating motion, negative training and inconsistent experimental results.Figure 1: DREN Utilization for Modeling and SimulationDuring previous testing in conjunction with BCR IV, NVESD measured overall latency as falling in the general range of 170-200 milliseconds.[1]  However, these tests did not measure RTI latency, and so the question as to how much latency is attributable to the RTI has been unanswered.  The RTI offers many different high level capabilities to the user above standard socket programming. This makes it easier to develop interoperable distributed simulation applications.  However, this comes at a cost and we must be aware of how much we pay for the ease of development and time saving versus developing an in-house solution. Tools are available from the DMSO software distribution site that facilitate testing the RTI (Run-Time Infrastructure) and its assorted capabilities. A number of different components may affect performance; due to time and resource constraints the RTI testing effort described in this paper was limited to latency between a large number of object updates and reflectance, and the latency in usage of interactions. The primary purpose of theses tests were to run latency tests that represented actual simulation exercise conditions.  By so doing, we were able to measure the latency in a manner that enables us to design the PST Federation software so as to intelligently select which RTI features should be used in certain situations.As the NVESD Team conducted the RTI performance testing described in this paper, the limitations of available testing and diagnostic tools became apparent. The primary DMSO testing tool, BMLatency, tests the latency of object updates. The NVESD team also planned to test the latency of interactions through a program called “HLA Ping” written by the late Michael Muuss, who unfortunately died in an automobile accident while this paper was being written. Mr. Muuss was also the author of the ubiquitous “ping” utility found on all UNIX systems. In order to capture RTI scalability, the NVESD developed its own test program to perform a “mass object test.”This paper, therefore describes the procedures and results from three specific tests:Scalability, using the Mass Object TestLatency of object updates, using the BM Latency TestInteraction latency, using HLA Ping 2. Computer Architecture and Network EnvironmentThe tests were done on SGI computers running IRIX version 6.5 using one configuration of 16 Mips 250 MHz CPUs with 5 gigabytes of memory and another configuration with 8 195 MHz Mips CPUs with 1 gigabyte of memory.  The test was run using RTI 1.3NG (Next Generation) version 3.1. The Night Vision and Electronic Sensors Directorate's LAN (Local Area Network) network is built upon ATM technology using both OC-3 (155 mbits) and OC-12 (622 mbits) as the "back bone". The remote site, Fort Knox used a similar LAN based on ATM technology with OC-3. Both sides were connected on the DREN at OC-3 allowing for a point to point connection between these sites.3. Tests Conducted: Approach and Procedures a. Mass Object Test DescriptionEvaluation of scalability represents a significant conceptual and technical challenge. The approach we adopted in attacking this problem was to minimize the number of variables in order to ensure that results were not impacted inadvertently. The core of this test was a scenario that included two federates: one federate produced a number of objects and the other federate received these objects and responded with a reply.Our benchmarking object had two attributes, the first was an array of 128 bytes of dummy data, and the second was a standard Unix timeval structure.  The producing federate joined the federation, created a specified amount of instances of the benchmark class just described and then went into a synchronization mode. The synchronization mode was necessary so that the latency numbers were not affected by the mechanics of starting up a federation and waiting for all federates to create and discover each other's objects.  The start up phase was a one-time occurrence and was not calculated as part of the performance test.  The producing federate was ready to synchronize after all object instances required of it were created.  The consuming federate was synchronized after it discovered all the objects that were published by the producing federate.  The consuming federate only instantiated a benchmark object when it discovered one from the producing federate. Thus on the consuming side there was a one to one correspondence between each object created by the producing federate and each object received by the consuming federate.  The consuming federate used the benchmark instances that it created to send the time stamp of objects being updated by the producing federate back to the producing federate.  This was necessary because the only way to edit an object was for it to be owned by the federate; and to keep the test simple, we only wanted to involve “vanilla” object routines and not bring ownership transfer into the equation. To avoid ownership, copies of the producing federate’s objects were used to reflect its timestamp.  This technique of reflecting the object back to the producer also ensured that only round trip time, not one way time, was computed. Since one of the test cases required federates to be on separate computers there was no way to absolutely ensure that the clocks within each computer were synchronized sufficiently for accurate performance measurements; this resolved the dilemma sufficiently for our purposes.Once both federates completed the start up phase of instancing and discovery they entered a mode where the only events occurring, at least at the user level of the RTI, were attribute value updates and object reflects.  This represents the simplest state the RTI can operate in for objects.Our test assumed only two federates with a varying amount of benchmark instances; the producing federate updated all instances as fast as possible. No delay was introduced between updates, enabling the RTI to be stressed.   Constant updates are more representative of how most real time applications function.b. BmLatency Test DescriptionBmLatency, one of the tools in the RTI benchmark suite, tests the latency of object updates. This test only provides one way timings and tests only one object instance per federate. Three hosts were used in this test shown in Figure 2; Eyeball, an SGI O2 was at Ft. Knox, Oohlala, an SGI O2 was at Ft. Belvoir, and Kraken, an SGI Reality Monster was also at Ft. Belvoir. These tests were run using RTI 1.3NG v3.1.  EMBED Word.Picture.8   Figure 2: Benchmark Latency Test ConfigurationBmLatency measures RTI performance in terms of the elapsed time it takes to send and receive an attribute update. For BmLatency to work properly, two instances of the program must be run. One instance is responsible for registering an object and updating its timestamp attribute with the value returned by gettimeofday(). The second instance reflects this update, and updates its own object with the received timestamp. When the first instance receives this second update, it compares the timestamp attribute value with a gettimeofday() call to determine the total latency. The average one-way latency is computed by dividing the total  latency by two. This test was limited in that the data produced only reflected tests performed on the RTI's object attribute latency speed and not interaction parameters.  c. Interaction Test Description (HLA Ping)The interaction test was similar to both object tests in that it used the remote federate to respond back with the original timestamp to compute both round trip and one way timings. The interaction tests were executed in the same fashion as the BmLatency tests where only one interaction was used per federate.  One difference between this test and BmLatency is that Data Distribution Management DDM was always on. Due to the unfortunate demise of the author of HLA Ping, Michael Muss, it was not possible to add a DDM On/off switch in time to support this test.  The HLA Ping test did not flood the RTI with interactions since the tool was incapable of this; however, this would be another interesting test to perform in the future.4. Test Resultsa. Mass Object Test ResultsTable 1 shows the statistics of the tests performed during this test:Number of Objects Per federateTotal Number of ObjectsMean Round Tr Latency (msecs)Min Round Trip (msecs)Max round Trip (msecs)100200187.01643.19492.6275001000560.252153.121465.6110002000925.661292.0972103.28120024001300.28500.4252392.81500300012210.01534.43622836Table 1: Mass Object Test ResultsThe chart clearly shows that increasing the number of object instances past 3000 objects hinders the RTI performance.  The number of samples taken represents how many reflects the producing federate received. In order to isolate a variable, the number of objects in this case, a constant number of reflects (1000) was used for each run. This test is designed to measure how well the RTI manages objects even when all of the objects are not in use.In the best case the number of objects would not affect the performance while maintaining the same number of samples but this is not the case with the RTI - the mere fact that objects exist affects its performance.During the tests the RID file was modified by changing the number of callbacks from the amount determined at run time by the RTI to one per tick.  One callback per tick degraded performance by about thirty percent – this can be attributed to the increased amount of subroutine calling of tick since it only provides one callback with each call.Rtiexec/fedex HostRegistering HostReflecting HostMean Latency (msecs) 128 Byte Attribute size, No DDM,2 Federates,1000 samplesMean Latency (msecs) 128 Byte Attribute size, DDM,2 Federates,1000 samplesOohlalaEyeballOohlala30.596917.5904EyeballEyeballOohlala30.889816.9279EyeballOohlalaEyeball30.1917.0395OohlalaOohlalaEyeball30.60617.9756OohlalaKrakenOohlala20.16516.88199OohlalaOohlalaKraken20.2516.22345Table 2: Latency Tests using DMSO RTI benchmark toolb. BmLatency Test ResultsTable 2 shows the BM Latency test results from a data collection conducted on September 19, 2000 across the DREN between Fort Belvoir and Fort Knox. The tables show columns representing which host is responsible for running the management part of the RTI, namely the rtiexec and fedex processes.  The registering host creates and sends the initial object instance holding the timestamp to be later compared against the actual time.  The reflecting host merely copies this timestamp and returns it to the registering host. An initial look at the results indicates that use of DDM (Data Distribution Management) provides a significant performance boost. The evaluations did not demonstrate any sensitivity to which host runs the rtiexec/fedex processes or which host is creating or reflecting the object attributes since the latency times are the same. The major difference comes into play when federates are run on the same LAN versus across the DREN.  Looking at the last two rows it can be said that the LAN scenario has much lower latency than running across the DREN and with DDM on, the latency is halfed.Rtiexec/fedex hostPing serverPing clientMean Latency (msecs) One wayMean Latency (msecs) Round tripOohlalaOohlalaOohlala37.57084.989OohlalaOohlalaEyeball48.94883.415EyeballEyeballOohlala34.85275.143EyeballOohlalaKraken37.16965.198Table 3: Interaction Latency tests using ARL HLA Ping Toolc. Interaction Test Results (HLA Ping)Table 3 shows the interaction test scenario results from the September 19th test runs, using HLA Ping. The first column represents which host was responsible for running both the rtiexec and the fedex processes.  The second column shows which host was actually sending out the ping request.  The third column is the receiving host of the ping request and also sends a reply to the sending host.  The last two columns show the measured latency in milliseconds of the one way and round trip time. The results are very close for all scenarios.  This test shows that that whether one sends interactions along the DREN or within a LAN the latency can be expected to be roughly the same, unlike object updates. Interestingly enough it looks like interactions don’t perform as well as object updates do.  This test was not a mass test like our first trial run using BM Latency to measure object updates, however, it was meant to be the RTI version of the standard network utility called ping.  Again it appears that the rtiexec process and the fedex process do not have much impact on the latency numbers of interactions.5. Comparison between BmLatency and Mass Object TechniquesThe BmLatency tool authors claim that the overhead of the consuming federates reflect event would inflate the latency measurements if multiple objects were to be employed.  This is true and must be addressed.  Each federate that deals with objects must implement the reflect callback if it wishes to hear about things from other federates and this implementation is left to the user. In the mass object test we performed the reflect callback of the consuming federate using a hash table to lookup the reflecting object, then parsed the binary data into the object’s local variables.  Once that was done the object's attributes were copied into the responding object for the reflecting object and sent back to the producing federate where the performance statistics were computed.  The overhead within the consuming federate’s reflect callback was as small as possible, in any real HLA application this would only take more time. In other words, the overhead in the consuming federate had to be accepted when measuring tests. No other way was available to use round trip time calculations without using the method described, and round trip timing was the most accurate method for distributed network latency measurements under these circumstances. 6. ConclusionThese tests highlight some of the key considerations for using the RTI.  The significant performance boost provided by the usage of DDM is helpful but it would be beneficial to understand why this is.  In this case one would think the RTI is doing more work through region and extents matching involved with DDM and therefore have a higher latency but this is not true – why?There is also a significant difference between the latency of interactions and object updates even with DDM enabled. The one way latency for the interactions looks doubled in comparison to the one way latency of the object updates.Our experience highlighted a need for a robust tool set for measuring RTI performance in various network configurations. The desired functionality would include tests involving time regulation and time constraining federates.  The best effort transport type is understood to be more efficient than the reliable transport mechanism but what exactly is the difference between the two?  Are they drastic enough that best effort should only be used in real time or are they close enough that reliable would suffice with just a little more latency than best effort? Other future test tool functionality that would prove interesting would be utilities to measure the effects of DDM (Data Distribution Management) on mass amounts of objects, delayed updates of objects similar to the way a gateway would operate under dead reckoning conditions, and the number of federates producing and consuming a varying number of objects all at once within one federation.  DDM could be tested in two ways. First one region could be devoted to the entire set of object instances involved in the test and second assigning a region to each instance.  Improved techniques for raising the number of object instances would also be useful.  This is important since, from the beginnings of HLA, there has been significant interest in simulations consisting of tens of thousands of entities. In our experience, setting up a federate that caught a user defined signal and created a certain amount of objects at the time the signal was caught, enabled us to reach about ten thousand objects but with the lack of object updates. A tool that supported the latter as well would be optimal.As hardware becomes cheaper it is inevitable that operating systems such as Linux will become very popular in distributed simulation while the super computers of yesterday are still in use. This combination of different architectures requires the two genres of computers to communicate in order to work together.  The RTI provides no means for this so it is worth while to investigate the cost of XDR (External Data Representation) or externalization of data on top of the latency delivered by the RTI.A complex and very large test matrix could be developed encompassing all the test cases described above that would be most beneficial to the HLA development and simulation application community. The standardization of testing criteria would facilitate sharing and comparison of latency test results, enabling a fuller common comprehension of these problems across the modeling and simulation community.	References:[1] Lorenzo, Maximo; Riggs  B., Caruso M. and Schell D. “Integration of Engineering Level Sensor Federation into a Brigade Level C4ISR Experiment Using RTI V1.7” SIW Paper Number 00f-SIW-050, September 2000MIKE CARUSO is part of the Night Vision Paint the Night sensor simulation development team.  Mike has 3 years of C/C++ experience and has been primarily working on HLA distributed simulations.  He has BS in computer science from CUNY (City University of New York).MAX LORENZO is the Chief of the CECOM Night Vision and Electronic Sensors Directorate Virtual Prototyping Systems Branch. He has a B.S. in Geology from The College of William and Mary.MIKE MUUSS worked with advanced computer systems for more than two decades. He was a Senior Scientist at the U. S. Army Research Laboratory, leading efforts in real-time ray tracing, and high-resolution physics-based multispectral synthetic image generation. From 1981 to 1997 he led the U. S. Army Ballistic Research Laboratory (BRL) Advanced Computer Systems Team in research projects concerning CAD/CAE, Graphics, Networking, Operating Systems, Parallel Architectures, and Command and Control. Mr. Muuss was the principal architect of BRL-CAD®, a third-generation constructive solid geometry CAD/CAE system, which is now in use at over 3000 sites, supporting very fast ray tracing of extremely large geometric models (up to 1 trillion polygon equivalents). He contributed significantly to the design and development of ARL's network infrastructure, one of the best within DoD. His early contributions to the development of TCP/IP and the Internet include the PING program and default route for UNIX, and editing the TCP-IP digest and UNIX-Wizards digest. Mr. Muuss was born in 1958, received a BES in Electrical Engineering from the Johns Hopkins University in 1979, and subsequently received numerous awards and citations for his work.  He was a two-time winner of the U.S. Army Research and Development Achievement Award.Tragically, Michael J. Muuss was killed in an automobile accident on the 20th of November 2000.  He has been described as a “National Treasure” as well as in Internet circles, the “wizards’ wizard”.  He will be missed by all who knew him and worked with him.BILL RIGGS is the SAIC Team Lead for the development team supporting the NVESD Paint the Night Program. Mr. Riggs has eleven years experience in developing computer generated forces and virtual terrain databases. He has a M.S.F.S. degree from the Walsh School of Foreign Service, Georgetown University.