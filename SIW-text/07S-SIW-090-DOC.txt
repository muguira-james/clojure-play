Transitioning from Training Simulations to Instructional GamesLee A. Belfore IIGary MorrisonAmy B. AdcockElectrical and Computer EngineeringEducational Curriculum and InstructionOld Dominion UniversityNorfolk, VirginiaLBelfore@odu.edu, GMorriso@odu.edu, AAdcock@odu.eduABSTRACT:  Training simulations have long been the mainstay of technology-supported instruction.  As the technology evolves and is accepted, new applications for the technology emerge as well.  Driven by the entertainment industry, gaming has become a pervasive recreational endeavor.  This paper explores aspects of gaming as applied to instructional applications, based on current instructional design research.  In addition, an approach is outlined that incorporates both training simulations and instructional games as part of an overall instructional approach.  An overview for evaluation and assessment is described to for this approach.Keywords:Instructional Games, Instructional Design, Training, Evaluation and AssessmentIntroductionInstructional simulations have shown their efficacy in many venues, allowing the creation of educational and training content that provides the learner with an effective learning experience.  Many sciences and technologies are involved in the creation of effective training simulations.  The training simulation requires the proper design of the educational content, an effective user-computer interface, and finally the technological platform to host the instructional application.  The technologies used to create the simulations include special purpose user interfaces, immersive virtual environments, and network connectivity to support distributed activities.  From a sufficiently broad perspective, these are the same issues of concern to game designers.Games have traditionally been a source of entertainment that provides users with engaging visual, audio, and in some cases haptic responses.  Furthermore, with massively multiplayer on-line games (MMOGs), gamers have social interactions and have formed on-line communities in these MMOGs.  Furthermore, game environments provide the player with the opportunity to make decisions and experience the outcomes.  Because of the highly entertaining and appealing content, people who play games are also quite motivated to continue playing the games.  Capturing this motivation in an educational and training application could result in a more effective training result.Games share some commonality with training simulations.  Instructional simulations and games provide a user interface that allows the input and interaction with a simulation model that conforms to the real-life scenario and requirements.  Games provide the opportunity to incorporate simulated content as well as external connections, that are also a feature of distributed instructional simulations.  In this paper, we plan to describe how current instructional design methodologies can be used to create instructional simulations to support training activities.  Furthermore, we will explore how these methodologies can be used to balance instructional requirements between simulation and instructional games.This paper is organized into six sections including an introduction, a motivation behind and evaluation of instructional games, a review of instructional design as related to training simulations, instructional simulations and games, assessment, and a summary. DiscussionA recent report from the Federation of American Scientists calls for a greater use of serious games for educational purposes [2]. Immersive environments created by games and simulations have a face validity suggesting the engaging environment is beneficial to learning. The proposed approach combines the features of simulations and serious games to develop skills and knowledge that are then further developed to allow for generalizations and transfer of knowledge.Simulations and simulation-based games provide the learner with an opportunity to learn and test ideas in a safe environment that accurately reflects and conveys real-world experiences. Instructional designers can carefully structure simulations for various levels of expertise ranging from novice to expert. For example, a training scenario for a novice might be highly focused with limited data or stimuli in contrast to the amount of data provided to more experienced learner to manage cognitive load. Simulations and games have been used extensively in education and training for over 30 years. The research, however, has not found simulations or games to be advantageous over more traditional forms of classroom instruction. These shortcomings can be addressed by managing the timing of the instruction so that the learner is not overwhelmed by information prior to engaging in the simulation. Similarly, the system will adapt the information and scenarios based on the learner’s level.Simulations for Training and Learning SimulationsInstructional design guidelines can be applied to the design and development of simulations and models [1,9]. While the research on simulations is mixed, there is sparse research on the effectiveness of gaming for learning.  The objectives of instructional simulations are varied depending on the training requirements and goals.  The simulations are interactive and recreate salient aspects of real life scenarios in order achieve learning objectives. Related LiteratureSimulations are conceptual or operational models of natural or artificial systems that respond to user input [1]. When the learner is immersed in a rich, technological environment that models a real life situation with a challenge of winning (e.g., solving a problem), the environment is often referred to as a serious or instructional game. A number of recent publications by Prensky [11] and national summits [2] have called for increased utilization of and research on serious games. Games have been used for educational purposes for over 30 years [6,13-15]. While there are articles and book chapters describing simulation and game design, there are relatively few studies describing research on the effectiveness of games, especially the newer, rich multimedia simulations and games for instruction. For example, of search of ERIC (Current Index of Journals in Education and Resources in Education Index) for computer-based gaming studies in the last 10 years did not produce any empirical studies. Well-designed instruction can facilitate a learner’s progression from novice to expert. Research examining characteristics of experts indicates that as learners gain content knowledge, they also gain awareness of how to approach problems with increasing sophistication [18-20]. Learners who have progressed to this level of knowledge commonly take just as much time as a novice to solve a problem, but present several different approaches to problem resolution. This sophistication leads to an adaptive expertise, which allows learners to easily solve an array of problems [18]. As they reach this level of familiarity, integrating new and more complex domain knowledge becomes easier. A continuum of simulations from simple to simulation-games as proposed can provide an instructional environment to achieve this type of learning.Current research suggests that differences in levels of expertise call for differences in the design of instructional messages. As Mayer [22] points out, tests of multimedia environments have shown marked differences in the way high and low domain knowledge users interact with instruction. Failure to account for a learner’s knowledge base will result in an instructional environment that can actually hinder knowledge progression. Similarly, cognitive load theory [17] provides a theoretical construct for designing instruction that support the learner throughout their progression from novice to expert by accounting for the load on working memory. Cognitive load theory proposes that instructional messages are most effective when they are designed to account for the learner’s cognitive system. Too much load on the learner’s cognitive system (through redundant or overly detailed messages) hinders learning. Empirical studies have indicated that as learners gain expertise, they are more likely to be negatively affected by redundant messages [16]. Effective message design should accommodate learners by modeling, scaffolding and fading as content knowledge increases. Given our current state of knowledge about how individuals learn, can we design an effective, immersive simulation environment? One factor to consider is when the learner is presented with information needed in the simulation. If learners are engaged by the simulation without adequate prior knowledge, they may lack the ability to generate hypotheses, collect appropriate data, and make good interpretations of the data [1,12]. However, when learners are provided the information as needed during the simulation, they performed better than learners receiving the information prior to the simulation [4]. Instructional Simulations and Games in Learning Applications In the context of current cognitive research [18], simulation and simulation games provide a technology platform for learning activities. Consider for example, a media rich simulation that starts with the learner navigating a vehicle to a house where a crime has occurred. A novice might navigate the vehicle into the driveway and immediately enter the house. In contrast, an expert would observe the street, the driveway and yard for tire marks, footprints, and other evidence related to the crime before entering the house. Such a media rich simulation can overwhelm the novice while providing a challenge to the experienced and the expert investigator. The problem, then, is how to create an environment that provides a challenge for all users, but provides an efficient and effective environment for the novice as he or she works to become an expert.Figure 1 provides a graphic representation of the instructional manager system. This system is based on multiple data sets. For initial instruction, learners will access simulations that develop a basic conceptual model [23-25] or schemas that are essential for understanding crime scene investigation. Through this “test and observe” approach afforded by a simple simulation, the learner can develop appropriate schema and learn to think like an experienced investigator [10, 21]. In addition, the designer can more easily control the amount of information available to the learner in the simulation to better control cognitive load. Once the learners have developed the appropriate schema using individual simulations, they can access a simulation-game that provides a rich, multimedia environment. During simulation-game play, the learner will refine the schema, develop generalizations, transfer knowledge, and develop new schema. Both instructional approaches utilize the same scenarios that make the system cost effective and efficient for learning.Evaluation and AssessmentThe evaluation of this simulation and simulation game applications can be organized into two distinct parts, formative and summative. Evaluation in the first part is formative in nature and the second part is summative.Formative Evaluation and Assessment.The traditional formative evaluation design approach iteratively evaluates the system resulting in increasingly more robust prototypes. Data from the evaluation of each prototype will be used to modify both the instruction and the interface.  Furthermore, evidence-based instructional intervention design isolates various attributes of the instructional environment to determine the most effective approach.  While the training/learning environment is being developed, formative evaluations are conducted.  Critical to the development of the simulations is the use of cognitive task analysis [7] requiring the involvement of subject-matter experts. During this phase, the evaluation will focus on interface design and the development of simulations and a representative scenario designed to teach critical skills and knowledge that will work with the open source engine. Assessment will also be made of the context used in the simulations to determine if they are appealing and motivational, and that they help develop the appropriate knowledge and skills. The user interface will be evaluated to determine the most effective design. Last, developmental and small group testing can be used to improve the effectiveness and design of the simulation. Data from the evaluation would be used to modify the prototype.Summative Evaluation and Assessment.Once the instructional environment and content are completed, learner achievement is measured using a summative assessment.  The goal of the summative is to either determine the improvement in achievement, as measured in test scores, or to compare among alternate content delivery approaches [5]. EMBED Word.Document.8 \s Figure  SEQ Figure \* ARABIC 1  Instructional Manager Research DesignThe use of computer or web-based delivery of a simulation affords us the opportunity to use randomized assignment of individual learners to provide a rigorous study of the effects of the simulation and gaming environment. There is also an opportunity to randomize learners if used in team environments in a CAVE. For example, Levin has been critical of current research design practices and suggests that researchers use an evidence-based approach for instructional intervention research [5]. Similarly, Hsieh et al. reported [3] a significant drop in intervention research in leading educational psychology journals during the 1995-2004 time period while Morrison and Anglin [8], found a similar drop in intervention research in a leading instructional technology journal during the same time period. To address the issue of evidence-based intervention research we will use a research design similar to Figure 2 as part of the evaluation. The following questions will guide the research. First, when should the learner receive information salient to the problem? Second, how do a simulation only, simulation game only, and a simulation and game affect the efficiency and effectiveness of the instruction? Third, do learners make use of the various options in the instruction? Fourth, do experts and novices differ in their use of the simulation and simulation game? Fifth, is there a difference between individual and dyad learning and use of the instruction? EMBED Word.Document.8 \s Figure  SEQ Figure \* ARABIC 2 Research DesignSummaryIn this paper, an approach for employing training simulations and instructional games has been presented.  Based on current instructional design methodologies, instructional content can be created that makes the best use of the available technologies to enhance the overall learning experience.Referencesde Jong, T., & Van Joolingen, W. R. (1998). Scientific discovery learning with computer simulations of conceptual domains. Review of Educational Research, 68(2), 179-201.Federation of American Scientists (2005). Harnessing the Power of Educational Games.   Retrieved November 10, 2006, from http://fas.org/gamesummit/Resources/Summit%20on%20Educational%20Games.pdfHsieh, P., Levin, J. R., Acee, T., Chung, W., Hsieh, Y., Kim, H., et al. (2005). Is educational intervention research on the decline? Journal of Educational Psychology, 97(4), 523-529.Leutner, D. (1993). Guided discovery learning with computer-based simulation games: Effects of adaptive and non-adaptive instructional support. Learning and Instruction, 3, 113-132.Levin, J. R. (2004). Random thoughts on the (in)credibility of educational-psychological intervention. Educational Psychologist, 39(3), 173-184.Liggett, W. H., & Stephenson, L. K. (1979). ORAM: an Adaptable Frame Game. Journal of Geography, 78(2), 57-63.Militello, L. G., & Hutton, R. J. B. (1998). Applied cognitive task analysis (ACTA): a practitioner’s toolkit for understanding cognitive task demands. Ergonomics, 41(11), 1618-1641.Morrison, G. R., & Anglin, G. J. (2006). Educational Technology Research in ETR&D: Trends in Methodologies Used, 1953-2006. Paper presented at the Annual Meeting of the Association of Educational Communication and Technology.Morrison, G. R., Ross, S. M., & Kemp, J. E. (2007). Designing effective instruction (5th ed.). Hoboken, NJ: John Wiley & Sons, Inc.Papert, S. (1980). Mindstorms: Children, Computers, and Powerful Ideas. New York: Basic Books.Prensky, M. (2001). Digital game-based learning. New York: McGraw Hill.Schauble, L., Glaser, R., Raghavan, K., & Reiner, M. (1991). Causal models and experimentation strategies in scientific reasoning. The Journal of Learning Sciences, 1, 201-239.Stolovitch, H. D. (1982). Frame Games and Game Chains: A Technology for Interactive Teaching and Learning. Viewpoints in Teaching and Learning, 58(3), 66-83.Thiagarajan, S. (1971). Design, Development and Validation of Instructional Games. Working Paper 9.1. Indiana University.Thiagarajan, S. (1973). TAG: An Example of Operational Gaming Viewpoints in Teaching and Learning, 6, 97-107. Kalyuga, S., Chandler, P., & Sweller, J. (1998). Levels of expertise and instructional design. Human Factors, 40(1), 1-17. Sweller, J., Instructional Design in Technical Areas, (Camberwell, Victoria, Australia: Australian Council for Educational Research (1999). Bransford, J., Brown, A., & Cocking, R. (Eds.) (1999). How people learn: Brain, mind, experience, and school. Washington, DC: National Academy Press.  Chi, M. T. H., Glaser, R., & Rees, E. (1982). Expertise in problem solving. In R. J. Sternberg (Ed.), Advances in the Psychology of Human Intelligence: Vol. 1. (pp. 7-76). Hillsdale, NJ: Erlbaum.Chi M. T. H., & Glaser, R. (1985). Problem solving ability. In R. Sternberg (Ed.), Human abilities: An information processing approach (pp. 227-250). San Francisco: Freeman.Bruner, J. S. (1963). The process of education. New York: Vintage Books.Mayer, R. E. (2001). Multimedia learning. Cambridge, UK: Cambridge University Press.Mayer, R. E. (1989). Models for understanding. Review of Educational Research, 58,(1), 43-64.Mayer, R. E. & Anderson, R. B. (1991). Animations need narrations: An experimental test of dual-coding hypothesis. Journal of Educational Psychology, 83, 484-490.Mayer, R. E., Moreno, R., Boire, M, & Vagge, S. (1999). Maximizing constructivist learning form multimedia communications by learning from multimedia communications. Journal of Educational Psychologist, 91, 638-643.Author BiographiesLEE A. BELFORE II, Ph.D. is an Associate Professor of Electrical and Computer Engineering at Old Dominion University, where he has worked since 1997.  He received a B.S. in Electrical Engineering from the Virginia Tech, a M.S.E. in Electrical Engineering and Computer Science from Princeton University, and a Ph.D. from the University of Virginia in 1990 in Electrical Engineering Dr. Belfore is the author or co-author of 50 scholarly publications in various areas.  His research interests include simulation, web-based visualization, virtual reality, data compression, and fault tolerant computing.  He is a Senior Member of the Institute for Electrical and Electronic Engineers (IEEE), the American Society for Engineering Education (ASEE), and Sigma Xi.GARY R. MORRISON, Ed.D. is currently a Professor of Education, Curriculum and Instruction at Old Dominion University since 2004.  He received a doctorate in Instructional Systems Technology from Indiana University in 1977. During the past twenty years, he has taught courses in instructional design, message design, distance education, instructional technology research, design of computer-based instruction, and individualized instructional methods.  In recent years, his research has focused on instructional strategies, cognitive load theory, distance education, and the integration of technology into the classroom. Dr. Morrison is senior author of Designing Effective Instruction with Steven M. Ross and Jerrold E. Kemp; and Integrating Computer Technology into the Classroom with Deborah Lowther. Dr. Morrison is author of over 20 book chapters, 50 journal articles, and 100 presentations on instructional technology. He is also the associate editor of the research section of ETR&D, and serves on the editorial boards of the Quarterly Review of Distance Education and Computers in Human Behavior. He is also the past president of the Design and Development and Research & Theory Divisions of AECT. In 2006, Gary was elected president elect of AECT's Distance Learning Division.AMY B, ADCOCK, Ed.D. is an Assistant Professor in the Darden College of Education at Old Dominion University since 2004.  She received her Master of Science and Doctor of Education degree from the University of Memphis in Instructional Design & Technology. In addition, she worked for three years as a research assistant in the Institute for Intelligent Systems.  Her research interests include links between cognitive processes and instructional design and effective uses of innovative learning technologies. More specifically, her research has focused on meaningful design of animated pedagogical agents, serious gaming, and simulation environments.PAGE  		