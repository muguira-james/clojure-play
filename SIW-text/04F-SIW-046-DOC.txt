Lessons Learned: Applying the Multispectral Terrain Database Development Process to Legacy Mission Simulation EnvironmentsJim Zeha , Dan CaudillaBret Givensc, Rob Subrc , Eric WhitfieldcBrian MillerdEric Lestereand Karl Spuhlf				aAFRL/VAC, 2180 8th Street, Suite 1 Wright-Patterson AFB, OH 45433;bAmtec Corporation, 500 Wynn Drive, Ste 314, Huntsville, AL 35816;cGeneral Dynamics, 5200 Springfield Pike, Dayton OH 45431;dUSA Night Vision Electronic Sensors Directorate, 10221 Burbeck Rd. B309, Ft. Belvoir, VA 22060;eCG2 Incorporated, 6000 Technology Drive Building 1, Suite A, Huntsville, AL. 35805;fThe Boeing Company, PO Box 516, MC 064-1481, St. Louis, MO 63166ABSTRACT:  Last year, the Air Force Research Laboratory, Air Vehicles Directorate, Control Sciences Division, AFRL/VAC, developed a process for constructing large-area 3-D multispectral terrain databases to support Simulation Based R&D (SBR&D) concept development simulation, research and T&E activities. The Multispectral database (MsDB) development effort was intended to investigate a database development approach built from multispectral/hyperspectral imagery and elevation data that also incorporates material classification data capable of supporting realistic out-the-window visual and multispectral terrain displays and weapons/sensor models.  Integrating the MsDB into these legacy models enabled us to develop cockpit representations that immerse the warfighter, scientist, analyst, and testers into a dynamic environment suitable for engineering level test and evaluation of a multitude of systems.  This paper covers the lessons learned during integration of the MsDB with Paint the Night IR Scene Generation; integration with Mutli-Mode Radar Simulation (MMRS), which provides Synthetic Aperture Radar Simulation; and integration with SubrScene, which provides Out-The-Window Scene generation.IntroductionAFRL/VACD has developed a process for constructing large-area 3-D multispectral terrain databases to support Simulation Based Research and Development (SBR&D) concept development simulation, research and T&E activities. The Multispectral database (MsDB) development effort was intended to investigate a database development approach built from multispectral/hyperspectral imagery and elevation data that also incorporates material classification data capable of supporting realistic out-the-window visual and multispectral terrain displays and weapons/sensor models.  Integrating the MsDB into these legacy models enabled us to develop cockpit representations that immerse the warfighter, scientist, analyst, and testers into a dynamic environment suitable for engineering level test and evaluation of a multitude of systems.  In developing a fully functional and cost-efficient multispectral scene generation capability, several challenges were addressed:Authoritative representations of the synthetic natural environment (including geo-specific SEDRIS transmittals) must be richly populated, sufficiently documented, efficiently organized, and possess sufficient fidelity to meet the requirements of scene generation systems which also are linked to real-time environmental databases and HLA-compliant servers.State-of-the-art phenomenological models must be accessible to both database generation processes and the real-time/runtime simulation environment. This implies closer linkage between environmental data collection processes and runtime environmental models.Multispectral Database (MsDB) Development ProcessOur large area 3D multi-spectral terrain database is fundamentally composed of remotely sensed multi-spectral imagery, ground elevation data, and material classification feature data.  SimWright, Inc. is responsible for defining requirements for and initially processing this data for further integration into a run-time digital terrain database.  There are many crucial aspects to the development of digital terrain databases that need to be considered.  Our main focus, discussed in this section, is acquiring and utilizing a process that is not only repeatable, but reusable in the future.  Many terrain databases are generated in such a way that they may not be readily updated with newly acquired data.  The most basic case is the type of imagery and vector feature data used as input to the database generation system.  Generally, orthorectified imagery is used in conjunction with feature data that has been extracted from a separate source.  This caused spatial correlation problems between the Out the Window (OTW) database and the sensor simulation database derived from the feature data.  There may also be accuracy issues with the orthorectified imagery.  The imagery may have been orthorectified using low resolution underlying Digital Elevation Models (DEMs) and ground control.  The accuracy and quality of the orthorectified product is a function of those two input sources.  As more accurate and higher resolution DEMs and ground control are made available, it is impossible to regenerate an orthorectified imagery product using the existing orthorectified imagery as input.  Therefore, the database developer must incur the cost of having a new orthorectified product generated using the source imagery product and ground control.  This occurs over and over again as new DEMs and ground control data are made available.  One of our goals is to establish a process that attenuates these recurring costs and associated problems by acquiring and utilizing uncorrected source imagery, with a suite of accompanying metadata as input to the process.  In this particular effort we have acquired Level 1A SPOT 5 2.5m panchromatic and 10m multi-spectral imagery in the Digital Image Map (DIMAP) format.  Using this data we perform our orthorectification using existing control data.  In future revisions of this database, we can increase the accuracy of our ortho products using updated control data without incurring the cost of purchasing orthorectified imagery again and again.  The only costs incurred are that of regenerating the orthorectified imagery and subsequent derived features.  We have worked with NGA to obtain DoD Title 50 licenses for all the SPOT 5 imagery we are using.  Therefore, other DoD entities are able to acquire the source imagery and generate derived imagery products that meet their specifications.  This is a summary of the steps we have taken to achieve the aforementioned goal of creating a development process that is repeatable and reusable.  In the following sections, we discuss the requirements definition, image processing workflow, and the imagery derived products.  2.1 Requirements DefinitionWe begin defining our database development requirements by establishing the needs of the final consumer of the database.  The first requirement is that this database needs to cover a large area.  In this case the area may include up to 500 x 500 nautical miles.  The second requirement is that the spatial resolution of the imagery should be higher than 5m.  The third requirement is that the underlying elevation data must have a minimum post spacing of 30m.  The fourth requirement is that we must be able to extract basic material properties of the scene.  Based upon these basic requirements we decided to use SPOT 5 imagery as the source imagery for the database development.  SPOT 5 imagery provides 10m spatial resolution multi-spectral imagery containing 4 bands:  Near IR, Red, Green, and SWIR.  SPOT 5 also provides 2.5m spatial resolution panchromatic imagery.  SPOT 5 imagery gives us the ability to produce a 2.5m, pan sharpened, and multi-spectral imagery product covering a large area at a relatively affordable cost.  This data also provides us the spectral content to extract the basic material properties in the scene.  We decided to use DTED level 2 as the source elevation data as an affordable alternative to generating high resolution elevation data from stereo imagery.  We also decided to use existing Landsat imagery to fill in any areas where we did not acquire SPOT 5 imagery.  We have been able to locate the majority of the needed Landsat imagery residing in the NGA CSIL.  We purchased additional Landsat imagery to fill in any remaining gaps in our region of interest.  Using Landsat imagery we can generate a 15m pan-sharpened multi-spectral product.  Given this data, we are ready to begin the necessary image processing to generate the components of the digital terrain database.  Image Processing WorkflowWe begin by orthorectifying the input SPOT 5 imagery.  We utilize existing ground control information and DTED data to perform the orthorectification.  Currently we are utilizing in-house software to generate the Rational Polynomial Coefficients for the imagery and to perform the orthorectification.  After we orthorectify each 10m MSI and 2.5m panchromatic image frame, we pan-sharpen the 10m imagery using the 2.5m panchromatic imagery.  This process produces a 2.5m multi-spectral image for each corresponding frame.  An example of the multi-spectral imagery is given below.  Figure  SEQ "Figure" \*ARABIC 1:  Example of False Color SPOT 5 Multi-spectral Imagery over San Francisco, CAThe imagery is still shown in false color.  SPOT 5 imagery does not contain a blue band to produce a true color image.  Therefore, we must implement a color transformation to convert the data to a true color image.  However, we first begin by generating a radio metrically balanced multi-spectral image mosaic of the entire scene.  The radiometric balancing removes any abrupt transitions between the image frames thus eliminating any visible seams.  We utilize the multi-spectral mosaic as a base from which to extract material features including various types of vegetation and man-made structures.  We transform the false color multi-spectral mosaic to a true color image mosaic.  This image mosaic will be used for the OTW simulation.  An example of a true color image transformation is shown below.  Figure  SEQ "Figure" \*ARABIC 2:  Example of True Color SPOT 5 Imagery over San Francisco, CAFigure  SEQ "Figure" \*ARABIC 3:  Example of features extracted from a SPOT 5 sceneOnce we have generated the orthorectified image mosaics we begin extracting features from the imagery.  We utilize the multi-spectral imagery mosaic as a base from which to extract features such as grass, trees, urban areas, dirt, lakes, and oceans.  We use supervised classification algorithms to extract these features and manually refine them to remove spurious data.  We use the true color image mosaic to manually extract roads, rivers, railroads, and airport runways.  It should be noted that for manual extraction we can use either the true color or false color image mosaic as a basis for extraction.  However, the true color imagery provides better visual queues to the extractor for these features.Once we have extracted the features, we apply the appropriate feature attributes to the resulting shapefiles.  When this is complete we have a complete set of input data for use in generating the digital terrain database.  These products are:2.5m true color image mosaicFeatures with appropriate attributes30m digital elevation data2.3 Higher Resolution Imagery IntegrationOnce we have generated a control base image mosaic in the form of the 2.5m true color SPOT 5 mosaic, we can then integrate higher resolution imagery into the scene as insets.  These insets may be Ikonos, Quickbird, aerial, or a number of other imagery data types.  Currently we are not integrating high resolution insets into the data.  However, we have generated the data in such a way that this can be done easily as new data arrives.  We orthorectify the high resolution imagery and ensure the boundaries between it and the SPOT 5 imagery match geospatially.  We then adjust the radiometric properties of the high resolution imagery to be consistent with the underlying SPOT 5 imagery.  Next we extract new high resolution features from the inset imagery.  Finally we merge the high resolution features with the features derived from the SPOT 5 imagery.  This data is then supplied to the digital terrain database generation system.  This process may be repeated as more high resolution inset imagery arrives.  Lessons LearnedIntegration with Paint the Night (PTN) IR Scene Generation3.1.1 3.1.1. echnologies and concepts.                                                                                                ObjectivesAs part of the process NV ESD was tasked to provide three correlated MSDB database products. 1) A run-time IR database for NV ESD’s PTN IR simulation.2) A Compact Terrain Database (CTDB) for use with the mission modeling software3) Database products suitable for conversion to MMRS format.Lessons LearnedOne of the biggest issues that had to be addressed was buildings.  PTN represents all terrain objects as discrete 3-D geometry and does not utilize overhead imagery directly as textures. Therefore information on building volumes must be provided in the Geographic Information System (GIS) source in order to accurately generate the building models.  Because of the size of the area of interest and the density of buildings the feature extraction process was only able to extract the buildings in areas of particularly high interest. This turned out to primarily be airports.  But, in order to obtain a level of realism it was necessary to populate the area with structures.  In the rest of the database, urban areas were identified in the GIS data.  Those urban areas were outlines of regions on the terrain that were built up with man-made structures. A 1 km x 1km “typical” urban model was constructed with 3-D buildings, roads and trees.  The area was based on an urban area within that database.   That model was then repeated to fill each of the urban areas.  Parts of the 1 km x 1 km square that fell outside the irregular boundaries of the urban areas were trimmed from the final database.Figure 4:  (Left) Typical Urban Area. (Right) PTN rendered IR sceneA second issue was redundant point data.  Both automated extraction and feature deconfliction algorithms result in many collinear or nearly collinear points in the GIS feature datasets.  The number of points to process affects both the speed of database generation and the efficiency of the final runtime database.  Two things were done to minimize those issues.  First, our database creation tools were modified to remove collinear points.  Second, we requested reduced point datasets from the originator of the data for the worst cases.The CTDB dataset provided the most difficulties.  The size and complexity of this area of interest pushes the CTDB format to its limits.  In addition, because the CTDB reader for the mission model to be used in this exercise is not complete, our best test platform for the CTDB generated is the Army’s OneSAF Testbed Baseline Semi-Automated Forces (OTBSAF).  When trying to test the CTDB we were consistently reaching limitations of OTBSAFs ability to render the data.   In most cases, we were unable to determine if these were CTDB or OTBSAF limitations.  This resulted in us producing a less feature rich CTDB that we are able to test in OTBSAF and an untested full feature CTDB.  Hopefully, once integration of CTDB into the Air Force’s mission model is complete, the final testing can be done.Integration with Multi-Mode Radar Simulation (MMRS) Synthetic Aperture Radar (SAR) GenerationObjectivesThe primary objective in developing the MsDB radar database generation process was to provide:A database generation process supporting a broad spectrum of radar types and radar effects. The highest radar image fidelity supporting real-time operator-in-the-loop simulations.The most expeditious, economical and efficient database generation process in terms of time and man-hours invested. Maintain absolute correlation between the visual-out-the-window imagery and radar imagery.To implement the above objectives it was essential to consistently keep in the forefront the essential physical phenomenon that most significantly contributes to the most important features in the final radar image.  The top two contributors in order of importance are:Geometry – shape, size, location, etc of entities (buildings, vehicles, cultural features).Material Composition – steel, wood, stone, soil, etc.3.2.2 Database Process ApproachTo maintain maximum correlation between the simulated visual and radar imagery it was decided to use the visual database as the ultimate source information from which the radar runtime radar database was to be formatted.  This actually elevates the visual database above the basic visual requirements since it requires some information not used by the visual IG system.  For example the database would require material classification information. By using this approach the physical location of entities and elevation rendering are assured to correlate.  One objective was to define a source visual database architecture that could be automatically formatted into a radar runtime database without any human interaction other than loading the visual database onto the MMRS and initiating the formatter software.  Generally the formatting process for a significantly large database takes from 30 minutes to one hour.  This formatting process only needs to be done once.  The advantage of using a formatted database rather than using the visual database directly in the MMRS is that the formatted database streamlines the runtime database and performs preprocessing algorithms that allows for higher fidelity artifacts to be processed in the timeframe allocated to generate and display a radar image.  As implied above, using the visual database places an additional burden on the visual database development process. It requires that the visual database process include data required by the radar simulation, a requirement often overlooked or misunderstood by visual database developers.  It requires that all materials, stone, steel, soil, etc, must be identified accurately using consistent references from database to database.  It may also require multiple levels-of-resolution models of structural entities such as buildings, vehicles, etc.  The reason for this requirement is that visual models can often be adequately enhanced using photo textures.  For example the roof of a building may have air conditioners, vents, antenna, etc that are adequately represented by including a photo texture for the roof.  For the radar the geometry and material making up the air conditioners, vents, antenna, etc are the major contributors to what the radar image will look like, and this information is not easily extracted form a photo texture during real-time operator-in-the-loop simulations.  Thus for high resolution target areas the visual database will require high fidelity models to support high fidelity radar image generation.Another requirement placed on the visual database is the need for a consistent naming convention that identifies entities unsuitable for generating high fidelity radar imagery.  The naming convention provides a means for the MMRS formatter software to recognize these entities and replace them with suitable radar models.  One such typical entity is trees.  Generally visual databases use intersecting polygons with tree textures attached to the polygons.  The geometry of these intersecting polygons produces unacceptable radar returns.  The MMRS supports a comprehensive library of radar tree models that can be automatically substituted in place of the visual tree models.3.2.3 Lessons Learned Typically OpenFlight visual database are formatted into an MMRS runtime database.  This approach was automatically assumed to be part of the MsDB process.  The first sample database submitted, however, took an incredible 24 hours to format.  The reason was later determined that the visual elevation database consisted of gridded polygons derived from an elevation grid based on a grid spacing of 32 meters.  This grid spacing is not unusual for modeling higher fidelity areas of interest, but not the entire database.  It was determined that flat surfaces covering kilometers in area were broken into 32 meter triangles.  Further investigation revealed that the visual system used only a gridded elevation rendering rather than a polygonal rendering.  The visual database was converted into a polygonal rendering only to satisfy the typical MMRS approach. Once this was understood it was obvious that the visual elevation representation was virtually identical to the MMRS runtime format.  Entity models were still provided in OpenFlight format.  The result was that the formatting process was reduced from 24 hours to 30 minutes.Integration with SubrScene Out-The-Window Generation 3.3.1 ObjectivesThe pertinent objectives for creation of the visual database are:Produce an optimized run-time database based upon simulation requirements.Properly classify and model elements within the visual database to allow maximum correlation between the visual runtime and derived IR and radar databases.Create a repeatable process for multi-spectral database creation.Production of a simulation-optimized database was the driving factor behind the choice of tools and associated processes.  In general the visual database encompasses a large field of view along with high update rates.  Knowing this, one can use the profile of the simulation to fine-tune the parameters and fidelity of the database runtime along with the processes and tools used in the database generation.  Thus, a visual database tailored to the performance characteristics of the simulation can be created.  For example, increased visual fidelity should be given to simulations centered on low-flying aerial vehicles vs. simulations primarily comprised of high-flying vehicles.  Knowing this, the database developer can create a run-time with fidelity representative of the predicted altitude of operations.The ultimate goal for creation of the multi-spectral environment is a highly correlated product consisting of both IR and radar sensor database representation derived from the source visual database. Basing the sensor products on the visual database allows for a high-degree of correlation, but places a distinct demand upon the visual database developer to maintain proper material classification throughout the source database as well as geometrically correct models that may not easily be extruded from available imagery.  Previous iterations of the IR and radar databases have not been fully correlated with the visual database and work is underway to both properly material classify and geometrically model culture within the visual run-time in order to allow future builds to be based upon visual source data and achieve the goal of a highly correlated multi-spectral environment package.Current correlation factors being addressed by the project include:Geometry matching between the visual run-time and the spectral databases.Maintaining correct material representation throughout the multi-spectral environment.  For example, wood or steel must be identically classified throughout the environment to allow for easy identification and correct sensor representation.  Level and quality of the imagery should match for all visual surface representation within the environment.Creating a single geometric representation of the visual environment, applying a common material classification, and correlating imagery based surface textures are all an integral part of the repeatable environment creation process we are moving towards.  Speaking specifically of the visual run-time, an effort has been made to document all aspects of the creation process, from data acquisition and storage to final build out with material classification schemes fully implemented.  Although this process will most likely always be far from cookie cutter, it is hoped that through documentation of our pitfalls along the way, the process can be simplified for multi-spectral environment builds.3.3.2 Database Process ApproachTo meet the needs of this project we needed a tool that would let us change database fidelity while maintaining a degree of correlation for a specific simulation tool.  From the beginning we contemplated using in house developed tools, but in the end felt COTS tools such as TerraVista and Creator Terrain Studio would meet our requirements with quicker results.By using COTS products, we achieved a majority of our goals. Our primary database generation tool of choice is TerraVista.  It supports several of the output formats that we desire.  Data for the database build is received in GeoTIFF, Shape Files, DTED, and some supplements from DAFIF.  TerraVista allows continual addition and subtraction from the source database.  Since the system is source driven settings for fidelity for imagery and geometry correlation are easily changed between builds.First step in the process is naturally to upload data to the generation tool.  The process of loading data can be a large task.  We have found that most of the work for this process is done in loading the data and setting up the build options.  Options for level of detail options and imagery resolution are important factors, which were dependent on the simulation requirement.  For the Radar and IR database we select options that give the highest fidelity at all costs.  Output formats are selected and the database is built.  Time of build depends directly on area of coverage and the complexity of the scene desired.  The generation tool can build various levels of fidelity which can require anywhere from a day to several weeks to accomplish.Once the output format is built, the data is converted to a fast runtime format.  The fast runtime format optimizes geometry structures (not changing geometry) into optimized clusters.  It also reduces scene complexity for faster runtime culling.  Finally the data is stored in a native loadable format.  Standard output formats such, as OpenFlight and TerraPage can be used, but if used directly, have to be translated at runtime with performance cost.3.3.3 Lessons LearnedSome pitfalls for the visual database with regards to the radar and infrared database are still present.  Due to initial direction and the fidelity of each of the run times, some of the correlation goals cannot be met at this time.  Some of our current run-time models only support geo-typical databases with polygon-encoded materials.  Current generation tools are using geo-specific imagery based databases with image-encoded materials for a greater level of image correlation.  An upgrade for the run time tools in question would satisfy the imagery correlation but add the requirement for a new data set for material classified imagery verses material classified shape files.  Other correlation problems are with the elevation data.  Due to different database generation tools, the elevation output or models are different.  They are close, but not exact and require terrain clamping for entities on the ground.  To fix this would require a generation of the visual database as the starting point for all the tools rather than the raw data for each run time to start with.  Even with these deficiencies, the requirements for this database build and the areas of miscorrelation are minimal.4. SummaryThe Air Force Research Laboratory continues to improve the process for constructing large-area 3-D multispectral terrain databases to support Simulation Based R&D (SBR&D) concept development simulation, research and T&E activities. The MsDB is built from multispectral/hyperspectral imagery and elevation data that also incorporates material classification data capable of supporting realistic out-the-window visual and multispectral terrain displays and weapons/sensor models.  The resulting MsDB is being successfully integrated into legacy models used to immerse the warfighter, scientist, analyst, and testers into a dynamic environment suitable for engineering level test and evaluation of a multitude of systems.  Author BiographiesJim Zeh holds a 1980 BS in Electrical Engineering from the University of Cincinnati and a 1987 MS in Computer Science from the University of Dayton. He has over 20 years of experience in Modeling and Simulation to support Air Force Research Laboratory 6.2 and 6.3 research and has authored several conference papers and technical reports.  Mr. Zeh is currently the project lead for the Simulation Based Research and Development effort within the Air Force Research Laboratory.  This involves coordinating the Simulation Based Research and Development effort throughout the Department of Defense, NASA, Industry, and Academia.  Dan Caudill received a BS Degree in Aerospace Engineering from the University of Cincinnati in 1989 and has worked for the Air Force Research Laboratory’s Aerospace Vehicles Technology Assessment and Simulation (AVTAS) Branch (AFRL/VACD) since graduation. Mr. Caudill has more than 14 years of technical experience in developing and conducting various flight simulation experiments and has been involved with many types of simulation studies including flying qualities simulations, avionics suite evaluations, networked simulation studies, and mission level evaluations of future air vehicle concepts. Mr. Caudill has served as a simulation software developer, simulation operator, test director and project lead. Currently, Mr. Caudill is the project lead for future aerospace vehicle concept evaluation studies being conducted in the AVTAS facility.Bret Givens has more than 20 years of experience in software engineering for aeronautical weapon systems.  He is currently Chief Engineer and Program Manager for the Simulation-Based Technology Assessment (STA) Project.  Mr. Givens has integrated new hardware systems and sub-systems, reconfigured existing hardware and software, and ported drivers and application software.  Under STA, he is providing constructive and virtual simulation data to support the development, integration, maturation and assessment of air and space vehicle technologies.  Mr. Givens has extensive experience with Distributed Interactive Simulation (DIS) and the High Level Architecture (HLA).  Mr. Givens is very active with the Dayton Area HLA User’s Group and Dayton IEEE Computer Society and served as the chair for five years.  He also led the adoption of CMM compliant practices within the local office of General Dynamics.  Mr. Givens served as the secretary of the Research, Development and Engineering (RDE) Forum for the Simulation Interoperability Workshop for five years ending in 2002.Rob Subr received his BS Degree in Computer Engineering from University of Idaho in 1995.  He was commissioned into the Air Force in 1995 and worked at the Air Force Research Lab providing simulation support for numerous Air Force programs.  As a Veridian engineer, he supplies technical experience in visual simulation for the Joint Strike Fighter Project at AFRL/VACD.Brian Miller is a Mechanical Engineer with the CECOM Night Vision and Electronic Sensors Directorate. He has his B.S.M.E from Virginia Tech and his M.M.E from Catholic University. Eric Lester received his BS in Electrical Engineering in 1995 and his MS in Electrical Engineering in 1998, specializing in digital image processing and pattern recognition, from the University of Tennessee, Knoxville.  Mr. Lester has authored several publications in the area of imagery and LADAR processing.  Currently Mr. Lester is the Director of Geospatial Image Processing at SimWright, Inc. in Huntsville, AL.  In this capacity he leads a number of geospatial image processing projects and business development efforts in this area.Karl Spuhl received his MS Degree in Electrical Engineering in 1968 from St. Louis University and his BS Degree from Washington University in St. Louis in 1959.  Mr. Spuhl has worked for the Boeing Company for the past 41 years and has been working in Flight Simulation for over 33 years.  Mr. Spuhl is an Engineer Scientist supporting the Center for Integrated Defense Simulation at Boeing.  He is an Adjunct Professor at Washington University and has been teaching engineering for the past 19 years.Eric Whitfield received his BA Degree in Computer Science from Ohio University in 2001.  He has been working as a Software Engineer for General Dynamics (formerly Veridian Engineering) since 2001.PAGE  