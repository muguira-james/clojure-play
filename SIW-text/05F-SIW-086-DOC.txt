05F-SIW-086Latency Testing In the USAF Distributed Mission Operations Environment Jerry SzulinskiLockheed Martin CorporationDistributed Mission Operations Center4500 Aberdeen Dr. SE, Building 942Kirtland Air Force Base, New Mexico 87117505-853-0252, DSN 263-0252 HYPERLINK "mailto:jerry.szulinski@kirtland.af.mil" jerry.szulinski@kirtland.af.milChad SimpkinsLockheed Martin CorporationDistributed Mission Operations Center4500 Aberdeen Dr. SE, Building 942Kirtland Air Force Base, New Mexico 87117505-853-1536, DSN 263-1536 HYPERLINK "mailto:chad.g.simpkins@lmco.com" chad.g.simpkins@lmco.comKeywords:DMOC, Virtual Flag, DIS, DIS Filter, TSR, Latency. ABSTRACT:  This paper describes recent latency tests conducted by the Distributed Mission Operations Center (DMOC), formerly known as Theatre Aerospace Command and Control Simulation Facility (TACCSF).   The tests were conducted to quantify the amount of latency introduced by the DIS Filter and the TACCSF Software Router (TSR).  The DIS Filter and the TSR have been thoroughly described in previous SISO/SIW papers; the Filter and the TSR are DMOC’s workhorses for managing the ever-increasing bandwidth requirements of large-scale exercises such as the Virtual Flag.  They employ similar features to those present in High Level Architecture: sender-based filtering and multicast protocol.  The M&S community always seems to want the answer to: “How much latency do they add?”  This paper answers that question, as well as the methodology for answering it.IntroductionThe US Air Force Distributed Mission Operations (DMO) concept is one of the most successful applications of Modeling and Simulation (M&S) for warfighter training.  The DMO spans virtually the full spectrum of aircrew training beyond individual pilot flight training.  It concentrates on team, inter-team, large force, and theater-level sensor-to-shooter kill chain training.  Individual flight simulators have been available for several decades, but it was not until the widespread application of distributed technologies that theater-level distributed exercises in synthetic battlespaces, such as the Virtual Flag, could take place.Virtual Flag exercises go far beyond simply training pilots to fly aircraft.  These quarterly exercises, hosted by the DMO Center (DMOC) at Kirtland Air Force Base in Albuquerque NM (USA), link hundreds of warfighters in realistic and robust scenarios.  Virtual simulators, in which warfighters can practice force employment the same way they would with real weapon systems, represent most US Air Force airborne sensor, command and control, and shooter platforms.  Many live, ground-based systems, as well as Joint and Coalition participants, add to the training realism and depth.In order to provide robust and realistic scenarios to the warfighter trainees, it is also necessary to augment the live assets and virtual simulators with constructive simulations.  The live-virtual-constructive (LVC) mix includes about 75 simulation nodes, and up to about 10,000 individual simulated entities [1].The simulation nodes span the entire continental United States, including Alaska, and will soon include East Asia, and Europe; in all, there are up 40 to remote sites connected during DMOC exercises.  DMOC is the central hub connecting these nodes through a number of networks with varying bandwidth availability and protocol support [2].  Such a complex heterogeneous network of networks requires creative ways of managing the available bandwidth and the data needed for the simulations to interoperate.  The problem is that the Virtual Flags generate about 6.5 Megabits per second (Mbps) worth of data, whereas many of the sites are connected through T-1 lines with just 1.51 Mbps of maximum theoretical available bandwidth.  This means that even under ideal conditions there is about 4 times as much data being generated as there is available bandwidth to certain remote sites.  In practice, to ensure a reasonable amount of reliability, the remote sites can receive only about 500 Kbps; about one third of the maximum theoretical T-1 throughput.Another challenge encountered when conducting large scale real-time training exercises are legacy simulations that were not designed to participate in such robust environments.  Many of these simulations have hard limits of external entities they can interoperate with, some as few as 64.  The environment presented to these simulations cannot exceed these limits or undesired effects can occur.  The effects include degraded performance, simulations ignoring some of the entities, or in the worst cases the simulations crash.The DMOC has created two distinct software applications to help manage these challenges.  The DIS Filter performs simulation data filtering and reduction duties on the Distributed Interactive Simulation (DIS) data, which is the main interoperability protocol, used in the Virtual Flag exercises.  The DIS Filter is the most prolific application in the USAF’s DMO environment with about 40 instances scattered throughout the DMOC and other simulation sites.  The DMOC was formerly known as the Theater Aerospace Command and Control Simulation Facility (TACCSF).  The TACCSF Software Router (TSR) is an application, which acts as a network gateway for a simulation site.  It allows for data metering and conversion to different network protocols depending on each network’s characteristics [3,4].  The criticality of these applications in executing the Virtual Flag exercises has been proven time and time again.  However, DMOC has been frequently asked about the performance implications of using these applications, and the time has come to find answer to the recurring question: “How much latency do they add?”  This paper answers that and other related questions.Latency TestbedThree simple software applications were created in support of the latency test: Sender, Repeater, and Receiver.  These applications were created to provide a controlled and repeatable test environment, and to specifically test the latency introduced exclusively by the DIS Filter and the TSR.  To date, the tests have been run in isolated environments with dedicated local networks, and no other simulation data present.  In the future it is desired to have a persistent latency-monitoring environment that provides users with continuous feedback during exercise execution.  It needs to co-exist with the ongoing exercise, and more importantly, needs to operate across wide area networks (WANs).Four test configurations were designed to test the latencies introduced by the DIS Filter; each configuration was tested with five different loads of increasingly more PDUs per second.  One additional test configuration was created to test the TSR, and again, five PDU loads were run.  The PDU loads were varied from 500 PDUs per second up to 2500 PDUs per second in 500 PDUs per second increments.These values were carefully chosen as they represent the fairly heavy network loads DMOC experiences during their Virtual Flag exercises.  Five hundred PDUs/sec represents roughly the upper end of the typical load seen on DMOC’s numerous T-1 connections.  One thousand PDUs/sec is about the maximum practical limit that can be expected for a T-1 to handle.  Two thousand PDUs/sec represents roughly the upper end of the typical load seen on DMOC’s internal local area networks (LANs), whereas 2500 PDUs/sec is about the most that is ever seen in this environment.  Note that 2500 PDUs actually creates 5000 PDUs on the network because all of the PDUs sent by the Sender were then also sent from the Repeater thus effectively doubling the PDU count.The basic methodology for the tests was to run a DIS PDU Sender application on one host computer.  The platform of choice was a Personal Computer with Red Hat Enterprise Linux 4 operating system and a network interface card (NIC) configured in 100 Mbps mode.  Another similarly configured computer was running the Repeater application.  The Repeater received the PDUs, and it immediately sent them back on the same network.  The Receiver application was running on the same host as the Sender, and it received the PDUs which originated from the Sender and bounced back through the Repeater back to the Receiver.  By placing the DIS Filter or the TSR between the two hosts, and by measuring the round-trip time and dividing it by two, the latency introduced by that equipment could be calculated.  The computer hosts were connected via a Cisco 3550-48 Fast Ethernet switch.It is worth noting that no true real-time operating system or other similar facilities were used for this test.  This may cause the results to be slightly skewed, but the intent of this experiment was to get reasonably accurate results without going to extremes.  It was determined these minute potential variations were negligible when compared to the carefully controlled environment the test team was able to establish.  Additionally, this environment represents the real-life situation of most of the M&S applications.  Indeed, both DIS Filter and the TSR run on Red Hat Linux without any real-time kernel modifications.Furthermore, absolute transport time of the packets through the individual networking gear or the Repeater was not calculated directly.  Instead, a baseline test was performed without the systems under test (Figure 1) giving the cumulative time needed for packet transport from one host to the other.  This baseline test was then subtracted from the actual results for each configuration, thus effectively canceling out the transport time due to these other factors. This configuration also permitted operating without sophisticated GPS or other time synchronizations schemes, as these might not be available in the future instantiations of the latency testbed.  They were not necessary because the Sender and the Receiver were both located on the same host computer, thereby ensuring perfect time synchronization.Each test was run until 65,000 PDUs were sent and received using User Datagram Protocol (UDP) Broadcast protocol.  Round-trip times were recorded, and average latencies were computed at the conclusion of each test.  The round-trip time represents the time it takes to send the packet through an application socket, receive it in another application on another host, resend it back to the originator, and receive it back from the application socket.  The software applications created in support of the tests are described in further detail below.SenderThe Sender is a simple application that creates a socket through which it connects to the appropriate port and network.  It then generates bursts of DIS Entity State PDUs [5] to model the way that most simulation applications behave.  These bursts are at 10 millisecond intervals (100 Hz), and they produce 1/100 of the desired PDUs/sec load.  This means that when running at the maximum load, 2500 PDUs/sec, the Sender is sending 25 PDUs as quickly as it can 100 times per second.The DIS PDU Timestamp field uses (231 – 1) values to represent one hour, thereby providing resolution of 1.676 µs (Ref. [5].  An alternative was considered to use a more accurate method of timestamp storage, perhaps overlaying it on top of the data portion of the PDU, but it was determined that resolution of 1.676 µs was sufficient when dealing with the values that can be expected from these tests.  The Timestamp field is populated immediately prior to the PDU being sent to the socket.The remainder of the PDU header and data portion is also populated with valid Entity State DIS PDU data.  Since neither the DIS Filter nor the TSR maintain the concept of persistent data, the Entity State PDUs generated by the Sender represented two pre-determined entities with unique enumerations, and positional data.RepeaterThe Repeater application is responsible for accepting the PDUs sent by the Sender, and simply sending them back onto the network on another Ethernet port.  It creates socket connections for two ports on the same network, incoming and outgoing and it simply goes into an infinite loop of “receive-send PDU cycle.”  It receives PDUs on one port, verifies that the Exercise ID is correct, and immediately sends them back to the other port.  No other processing, timing functions or data manipulation occurs in the Repeater.ReceiverThe Receiver application receives the returned PDUs and calculates their round-trip time.  As other applications, it also initially creates a socket, and then it goes into an infinite loop of listening for incoming PDUs.  As the PDUs arrive, the application makes a call to the Linux gettimeofday system function; it compares that time to the PDU’s Timestamp value and determines the round-trip time.  It then stores these values in an array.  At the end of each test, the Receiver writes the stored round-trip raw time values to a file for further analysis.Latency TestsTest 1:  Baseline latency between DIS systemsThe objective of Test 1 was to establish the baseline Local Area Network (LAN) latency between two DIS systems.  The specific protocol is actually irrelevant because as far as the network is concerned the test demonstrated the amount of time it takes to broadcast a UDP packet from one computer to another.  Figure 1 shows the network configuration for the test.  Five tests with increasing loads were run in this configuration; the results can be found in Table 1.The table shows the average round-trip times and the one-way system-to-system UDP transport time.  Slight performance variations might be experienced by choosing different NICs, or networking equipment, but these variances are unlikely to be significant in M&S environments.PDUs/secondMean Round-trip Time (µsec)Mean One Way Time (µsec)500891445.510001255627.51500154877420001927963.5250021371068.5Table 1. Test 1 Results: Baseline Latency Between DIS SystemsAs Table 1 indicates, the one-way transport time is limited to well under 1 millisecond under most loads.  Even though these PDU loads represent upper limits of most DIS exercises, they should be fairly insignificant from the network loading perspective.  Given the DIS Entity State PDU is 144 bytes in size [5], it makes 2500 PDUs/sec equivalent to 2.88 Mbps, or less than 6% of 100 Mbps network when round-trip traffic is present.  It was, therefore, somewhat puzzling to find that there was a correlation between the numbers of PDUs sent per second and their latency.  The team agreed that they expected the latency to be fairly constant at these, relatively low, network loads.  It was speculated that perhaps the non-real-time nature of the operating system, NIC buffering of the packet bursts, the Receiver’s system calls, or file IO caused the creeping of round-trip times.  Nevertheless, all of these reasons should be factored out when further tests are run since these conditions should remain constant during subsequent tests.Test 2:  DIS Filter Pass-throughThe DIS Filter has become ubiquitous in DMO, and quite frequently it is the first tool users turn to when attempting to integrate into the massive Virtual Flag and DMO environment.  It has even been noted that at times users put their simulators behind the DIS Filter even before their real need has been determined.  These Filters then may or may not provide any filtering at all.  It was important to determine what is the latency penalty for using the Filters in such cases even though they might not be doing any filtering at all.In this test (Figure 2) the Sender, Repeater and the Receiver were configured similarly to Test 1.  The DIS Filter was also added, and the applications were configured in such a way as to have all PDUs pass through the Filter on the way to the Repeater, and back to the Receiver.  The following is the PDU sequence in Test 2:The Sender sends the PDU stream on Ethernet Port 3000.The DIS Filter receives the PDUs on Port 3000, performs no filtering, and sends all of the PDUs on Port 4000.Repeater receives the PDUs on Port 4000, and immediately returns them on Port 5000.The DIS Filter receives the PDUs on Port 5000, performs no filtering, and sends all of the PDUs on Port 6000.The Receiver receives the PDUs on Port 6000, and calculates the round-trip time.Even though the Filter was configured to not perform any filtering, nonetheless there is some processing associated with it.  The application receives the packet from the NIC and performs byte swapping from the network byte order to the host byte order.  Next the packet contents are examined and tested against the following criteria: Exercise IDPDU TypeEntity IDEntity TypeGeographic locationFinally, byte swapping occurs once again to put the PDU into the network byte order before the PDU is sent on its way.  The DIS Filter and the Entity State PDUs were configured in such a way as to ensure all of the PDUs passed through.The results of this experiment can be seen below in Table 2.PDUs/secondMean Round-trip Time (µsec)Mean One Way Time (µsec)Latency Due to DIS Filter (µsec)5001342671225.510001731865.5238150021251062.5288.5200026651332.5369250029851492.5424Table 2. Test 2 Results: Latency due to the DIS FilterBy subtracting the results of Test 1 from the results of Test 2, the latency due to the presence of the DIS Filter can be calculated.  As the right-most column in Table 2 demonstrates, the packet delay incurred by the presence of the Filter is minimal.  Even at the heaviest load, it was less than half of a millisecond.  Under more typical loads, the latency remained around about a quarter of a millisecond.  Note that this is additional latency due to the presence of the Filter, not the total latency between the DIS systems.  Since the Filter did not perform any filtering in this test, this time delay can be considered a “price of admission” for simply having the Filter in the configuration.Test 3:  Filtering 50% of the PDUsTest 3 was designed to learn how much latency is incurred when the Filter performs the useful function of actually filtering PDUs.  The network configuration remained the same as in the previous test (Figure 2).  The Filter is highly configurable, and it would be impossible to test all potential paths through the code since they greatly depend on the type of PDU being examined.  It was decided that the worst-case scenario for an Entity State PDU should be tested.  The geographic filtering is the last and most computationally intensive check against which an Entity State PDU is examined – the test was designed to employ this criterion for pass/fail decision.The Sender application was configured to continue sending streams of two different Entity State PDUs that represented entities in two different locations.  One location would pass, and the other would fail.  The PDU path followed the same pattern as in the previous section, but with one major difference.  Half of the PDUs were filtered out when they reached the DIS Filter for the first time (Step 2), and only the remaining half was sent along to the Repeater.  On the return trip, all of the remaining PDUs were allowed to pass through the Filter back to the Receiver.The round-trip time results of Test 3 can be found below in Table 3.  These results are compared with the round-trip time results from the previous test where no filtering took place.  PDUs perSecondMean Round-trip Time (50% filtering) (µsec)Mean Round-trip Time (No Filtering) (µsec)50011251342100013331731150015432125200017452665250019482985Table 3.  Test 3 Results: Round-trip times 50% Filtering vs. No FilteringThe measurement of absolute one-way latency can be quite unreliable in this test because the packet load going out was not the same as coming back due to the 50% filtering on the way out.  However, if the total round-trip times are examined and compared to the times where no filtering occurred at all, some very interesting conclusions can be drawn.  The obvious conclusion is that filtering a significant number of PDUs actually leads to lower latency for the remaining PDUs.  It can also be concluded that the time savings from sending fewer PDUs more than sufficiently offset the penalty of performing one of the most computationally complex operations in the whole DIS Filter application – the geographic location filtering.  This conclusion can be reached because the overall round-trip times were lower in Test 3 where more computations were performed, but where fewer overall PDUs were being sent than in Test 2.  Comparing the results of Test 2 to Test 3 could provide some further analysis.  For instance Test 2 load of 500 PDUs/sec is similar to Test 3 load of 1000 PDUs/sec since 50% of Test three’s PDUs are filtered.  This exercise is left up to the readers.Test 4:  Virtual LANsOne of the techniques DMOC uses to manage the large amounts of data generated during the Virtual Flag exercises is Virtual LANs (VLANs).  VLANs allow for physical data separation of DIS PDUs generated by different groups of simulations.  Separating the data serves two primary goals.  First, simulations internal to the DMOC, where bandwidth is not an issue, are not overrun by an extremely high numbers of PDUs for processing.  This is especially useful when the simulations interact with only a small portion of the entire synthetic environment, or when dealing with legacy systems which were not designed to handle such high loads.  Second, high levels of data loss have to be prevented when bandwidth limitations for simulations external to the DMOC prohibit sending the entire environment.Where the prior tests were more theoretical in nature, Test 4 is getting closer to the true operational picture of the DMOC architecture.  There are many latency sources in distributed simulation, and now that the latency due to the presence of the DIS Filter has been established (Test 2 and 3), it is possible to investigate how other factors contribute to the overall latency.The network layout for this test can be seen in Figure 3.  It is similar to the previous test, except that now the Cisco switch is serving two VLANs instead of one, as it did previously.  The DIS Filter now acts more like it does in its typical operation.  Having each of its two NICs service each LAN, it is essentially a gateway between the two VLANs.  As it passes PDUs from one to the other, it also performs filtering duties.  In this test, however, once again no filtering will be performed.  This test measures only the DIS Filter and VLAN latencies.Table 4 contains the summary of the results of Test 4.  These results are once again compared to the results from Test 2 where the DIS Filter was passing data on a single LAN.  Note that round-trip times are listed, and once again the latency is roughly linear with the number of PDUs sent.  A slight increase in round-trip times can be found when two VLANs are used instead of one.  When considering solely a one-way trip, the additional latency is limited to an additional 118 µsec in the worst case.  PDUs per secondMean Round-trip Time (2 VLANs) (µsec)Mean Round-trip Time (same LAN) (µsec)50016341342100019001731150022222125200026022665250032212985Table 4.  Round-trip times two VLANs vs. single LAN It is not exactly known why there is additional latency when dealing with two VLANs instead of only one.  One could speculate that with two VLANs there is less network traffic per LAN, and therefore the latency should be less.  In the case of two VLANs, the DIS Filter’s networking should also be split between two NICs instead of just one, having to handle twice the load as in the case of a single LAN (i.e., Test 2).  Although, it is possible that maintaining network traffic on multiple VLANs causes the Cisco switch to incur additional latency.  No definitive conclusions could be reached without further study of the issue.  Engineers conducting this study concluded that the additional latencies involved with using multiple VLANs were small enough to not be concerned with their precise origin.Test 5:  TACCSF Software Router (TSR)The TSR has seen increased use in recent years as the DMOC is participating in more exercises conducted on other organizations’ networks.  Using Broadcast protocol is undesirable in these situations, and hence the DMOC created the TSR to convert broadcast data from multiple LANs and Ethernet ports to a single multicast stream.  The source network information is packed into the header, and the packets are sent to the remote TSRs via a “bridging” multicast address.  This is similar to what some HLA applications do.  The packets are sent one a time as they arrive; in normal operation there is no buffering of packets.The objective of Test 5 is to measure the latency introduced by the TSR as a system.  The TSR application typically works in conjunction with other TSRs on the WAN, since one TSR encodes packets, and other TSRs decode them prior to forwarding the packets onto their LANs.  Test 5 was configured to replicate this situation (Figure 4).The TSR does not perform any specific processing on DIS PDUs; consequently the data contained within them was irrelevant.  Nonetheless, streams of Entity State DIS PDUs were once again used as in the previous tests.  The data flow was as follows:Sender generates a stream of Entity State DIS PDUs (500, 1000, 1500, 2000, 2500) and broadcasts them to a LANTSR-1 receives the PDUs and converts them to TSR packets [4]TSR-1 sends the TSR packets via Multicast to TSR-2 (this would normally happen on a WAN)TSR-2 receives and unpacks the packets, and broadcasts the DIS PDUs to its LANRepeater receives the PDUs, and immediately re-broadcast them backTSR-2 receives the PDUs and converts them to TSR packetsTSR-2 sends the TSR packets via Multicast to TSR-1 (this would normally happen on a WAN)TSR-1 receives and unpacks the packets, and broadcasts the DIS PDUs to its LANReceiver receives the PDUs, and calculates the time delta for the entire tripThe results of Test 5 can be seen below in Table 5.  PDUs per secondMean Round-trip Time (µsec)Mean One Way Time (µsec)Added latency due the TSR system (µsec)50026961348902.5100030401520892.51500393219661192200046552327.51364250063733186.52118Table 5.  TACCSF Software Router (TSR) System LatencyThe absolute one-way transport time using the TSR system can be seen in the third column.  This time varied from 1.3 milliseconds to 3.2 milliseconds.  By subtracting the results of Test 1 from these times, the mean latency due to the presence of the TSR system (two TSR applications) can be calculated.  The results are presented in the right-most column of Table 5, and they vary from 0.9 milliseconds to 2.1 milliseconds depending on the load.At the onset of this experiment, it was believed that the TSR would have lower latency than the DIS Filter because it is considerably simpler, smaller, and more efficient than the Filter.  Although as the results of Test 3 indicated, the time spent in the application is not the main contributor to latency.  Instead, it is the amount of “network” handling that the packet requires before reaching its destination.  Even though the TSR is very simple and efficient, its use contributes more to latency than the DIS Filter because each packet has to be read from the network, and sent back to the network twice with the TSR system (two TSR applications) whereas just once with the Filter.  The reason for this is that the TSR acts as a transport utility encoding the data when sending it, and decoding it when receiving at the remote location.ConclusionLatency is an often talked about topic in M&S, but not always understood and seldom measured.  Most agree that more latency is bad, and that additional devices placed in the path of network packets add to it.  This experiment was conducted to fairly accurately quantify the latency introduced by two of the most prolific software applications in the USAF’s DMO simulation environment: the DIS Filter, and the TACCSF Software Router (TSR).  When reading this paper and analyzing the data, it is important to remember that the latencies are the ones added by the use of the particular devices.  These numbers have to be added to whatever other latency sources are present in the environments, such as a long haul connection to a remote site, any data conversion prior to reaching the host simulation, etc.  The results published here quantify the latencies introduced by the DIS Filter and the TSR, and also provide some absolute times for transport times of PDUs between DIS applications.  The absolute times are calculated from just before the packets are sent by applications to their interfaces to just after they are presented to the applications.The results must always be taken in the right context; while they provide raw numbers, they do not provide any interpretation of how much latency is too much in the DMO.  The answer, of course, depends on which nodes and applications are being discussed; it is a topic for future works.  Also, it must be remembered that an application running at 60 Hz has a 16.67 millisecond cycle.  The numbers seen here are much smaller than that – latencies varied from less than a millisecond, to just over 2. One test reveals particularly interesting results that could be far reaching.  The results of Test 3 could be interpreted that the DIS Filter actually reduces the packet latency.  It is not being suggested that the DMOC has come up with a way to defy the laws of physics.  It does seem however, that when filtering a large number of unnecessary PDUs, the remaining PDUs will actually arrive at the remote application quicker than they would if no filtering was performed.  This claim cannot be confirmed at this point, as more testing would be required to ensure there is no other side effect occurring due to the latency testbed design.Over the next few months DMOC will continue with additional latency testing of other devices, both software and hardware.  These will include simulation devices such as some DIS/HLA Gateways, Trusted Bridge Federate, which is DMOC’s multi-level security (MLS) solution, as well as some networking equipment such as specific routers, switches, and physical networks.  In the future DMOC will conduct similar tests using HLA protocol as the technology takes greater hold in the DMO environment.  Given the complex DMO synthetic battlespace environment, it will also be necessary to examine how Data Distribution Management (DDM) will affect the experiments.The desired end state is to have a DMO-wide persistent latency-monitoring infrastructure that can be used during exercise execution time.  This is another step in becoming pro-active instead of reactive when conducting large scale theater-level exercises such as the Virtual Flags.References[1]	Joe Sorroche: 05F-SIW-106 “Joint Red Flag: Merging Live, Virtual, And Constructive Assets For A Joint Services Training Exercise” September 2005[2]	Joe Sorroche, Jason Atkinson: 05S-SIW-121 “Building a DMO network” April 2005. [3]	“DIS Filter Software User’s Manual” 2005[4]	Joe Sorroche, Jerry Szulinski: 04S-SIW-77 “Bandwidth Reduction Techniques Used in DIS Exercises” April 2004. [5]	IEEE 1278.1, 1995 Author BiographiesJERRY SZULINSKI has 16 years of experience in the modeling and simulation field, and currently works for Lockheed Martin as a Principal System Engineer at the DMOC.  During his career he has been involved with constructive, live and virtual simulation including rotary- and fixed-wing flight simulation, physics-based weapons effects modeling, live air combat maneuver ranges, mission rehearsal and exercise support for Navy’s Fleet Battle Experiments and Air Force’s Virtual Flag exercises.  His primary area of expertise is within distributed simulation (DIS/HLA), visual simulation and computer networking.  He holds a Bachelor of Science in Aeronautical Engineering from Embry-Riddle Aeronautical University.CHAD SIMPKINS works for Lockheed Martin as a Software Engineer at the DMOC. He has been involved in the DIS Filter and TSR enhancements, and software development supporting Virtual Flag exercises.  He holds a Bachelor of Science in Computer Science from the New Mexico Institute of Mining and Technology. 