Verification, Validation, and Accreditation for Modeling and Simulation: Present and FutureYang MingZhang BingWang ZicaiSimulation CenterHarbin Inst. of Technology, P.O.Box 126Xi Dazhi Street No. 92, 150001Harbin, Hei Longjiang Province, P.R.China86-451-6416571,6416570sim@hope.hit.edu.cnKeywords:Modeling and Simulation (M&S); Verification, Validation, and Accreditation (VV&A); M&S Credibility; Software EngineeringABSTRACT: In this paper, the authors present a general overview on the Models and Simulations (M&S) Verification, Validation and Accreditation (VV&A). This paper firstly looked back the history of Models and Simulation VV&A. Secondly, the paper discussed the concepts and principles that formed the foundation of M&S VV&A science. Then, the authors talked about the VV&A process in the life cycle of modeling and simulation, as well as the tailoring and optimizing problem of VV&A progress. This paper also gave an overview of the Verification and Validation techniques and the automation tools for VV&A manager and practicer. Finally, some conclusions of the current status of M&S VV&A of simulation system and the future trend of M&S VV&A was present.1. HISTORYModels and Simulations (M&S) credibility has concerned the modeler and simulation experts since the early age of simulation applications. In 1962, Biggs and Cawthorner studied the problem of Bloodhound missile simulation evaluation. In the 1970's and 1980's, the M&S credibility became the subject of many simulation conferences and academic meetings. In 1974, The Society of Computer Simulation set up the Technical Committee on Model Credibility (TCMS) with the objective to establish M&S credibility related concepts, terms and guidelines. Since 1981, the annual Summer Computer Simulation Conference (SCSC) and Winter Simulation Conference (WSC) had panels and tutorials on M&S credibility. The Military Operation Research Society (MORS) of U.S.A held several technical meetings on the M&S VV&A. During this time, the concepts and techniques of M&S VV&A were researched extensively and profoundly. In the 1990s, the increasing demand for high performance simulation systems raised the importance of the study of M&S VV&A. M&S VV&A attracted more attention from the military departments and government. Quite a few policies and guidelines were built up to satisfy the need for the effective M&S VV&A. In 1993, the Department of the Army (U.S.A) published Pamphlet 5-11, Verification, Validation and Accreditation of Army Models and Simulations[1], with the objective to "assist the M&S developer, proponent, and application sponsor in conforming to the VV&A policies in AR 5-11. This pamphlet also provides guidance for the development, execution, and reporting of all VV&A activities. In early 1995, the Defense Modeling and Simulation Office (DMSO) of Department of Defense (DOD) of U.S.A. form a VV&A Technical Support Team to develop and write guide of recommended VV&A practices for the DOD, and the team finished the task which resulted in the most useful VV&A reference book titled DOD: Verification, Validation and Accreditation (VV&A) Recommended Practices Guide (DOD VVARPG)[2]. In 1996, DOD of U.S.A. issued the DOD INSTRUCTION 5000.61, DOD Modeling and Simulation (M&S) VV&A[3], which implements policy, assigns responsibility, and prescribes procedures under DOD Directive 5000.59, DOD Modeling and Simulation Management (1994, January). In 1997, IEEE Computer Society balloted and passed Std. 1278.4, Recommended Practice for Distributed Interactive Simulation (DIS) Verification, Validation and Accreditation (VV&A)[4], which provides guidelines for practicing and planning VV&A activities through the DIS exercises life cycle.2. CONCEPTS AND PRINCIPLES2.1 ConceptsAccording to DOD Directive 5000.59 and DOD VVARPG, the VV&A concepts can be stated as following:Verification, Validation and Accreditation (VV&A): M&S credibility is measured by verification and validation (V&V) and formally approved as adequate for use in a particular application. The entire process is known as VV&A. (DOD VVARPG)Verification: is "the process of determining that a model implementation accurately represents the developer's conceptual description and specifications." (DOD Directive 5000.59)Validation: is "the process of determining the degree to which a model is an accurate representation of the real world from the perspective of the intended use of the model." (DOD Directive 5000.59)Accreditation: is "the official certification that a model or simulation is acceptable for use for a specific application."The relationship among VV&A and M&S credibility is shown by Figure 2.1.  EMBED Visio.Drawing.5  Figure 2.1  VV&A and M&S CredibilityA distinction should be noted when applying the terms of Verification and Validation. Verification focuses on M&S capability and traceability, and can answer the question "Did I build the thing right?" whereas validation focused on M&S validity and credibility, and can answer the question "Did I build the right thing?"2.2 PrinciplesThe VV&A principles form the cornerstone the VV&A management and practice. Understanding and applying these principles is essential not only to the success of an M&S development effort, but also to the efficient application of VV&A resources and the credible integration of M&S into your specific application. DOD VVARPG provides 12 guiding principles of VV&A that are proposed for VV&A practicer's consideration and use. These principles and brief discussions follow up here:(1) Principle 1: There is no such thing as an absolutely valid model.Complete V&V would require the model or simulation to be tested and examined under all possible combinations of input conditions. Although some automated tools are available to assist in this task, time and budget constraints usually preclude exhaustive testing. As a result, the scope of V&V is usually tailored to the credibility requirements of the application.(2) Principle 2: VV&A should be an integral part of the entire M&S life cycle. In reality, VV&A is a continuous activity performed throughout the entire life cycle of an M&S application. Tying VV&A to the M&S life cycle offers the opportunity to detect errors as early as possible and correct them at less cost and risk to the overall program. (3) Principle 3: A well-formulated problem is essential to the acceptability and accreditation of M&S results.It has been said that a problem correctly formulated is half-solved. The accuracy of the formulated problem greatly affects the accreditation and acceptability of M&S results. It must be recognized that, if problem formulation is poorly conducted, resulting in an incorrect problem definition, no matter how fantastically the problem is solved, the M&S results will be irrelevant.(4) Principle 4: Credibility can be claimed only for the intended use of the model or simulation and for the prescribed conditions under which it has been tested.Application objectives and requirements dictate how faithful the representation of a process, phenomenon, or system must be when compared with the real world for simulation results to be considered useful. Because the requirement for accuracy varies with the intended application, it is clear that a model's or simulation's credibility must be judged with respect to application-specific requirements and objectives. (5) Principle 5: M&S validation does not guarantee the credibility and acceptability of analytical results derived from the use of simulation. Model or simulation validity is a necessary, but not a sufficient, condition for the credibility and acceptability of the analytical results derived from use. If the M&S objectives and requirements are incorrectly identified or the problem is improperly formulated, analytical results derived from the use of a model or simulation can be invalid or irrelevant; however, the model or simulation can still be found to be sufficiently valid by comparing it with the improperly defined system and requirements and with respect to the incorrectly identified objectives.(6) Principle 6: V&V of each submodel or federate does not imply overall simulation or federation credibility and vice versa.Each submodel may be found to be sufficiently credible, but this does not imply that the whole M&S application is sufficiently credible. The allowable errors for the submodels may accumulate and become unacceptable for the whole model or simulation. Therefore, the whole model or simulation must be tested even if each submodel has been tested individually and found to be sufficiently credible. Similarly, the determination that a federation has sufficient credibility cannot and does not imply that the individual federates are credible. (7) Principle 7: Accreditation is not a binary choice.Because a model is an abstraction of a system, with inherent assumptions, limitations, and approximations, it is unreasonable to expect perfect representation of all aspects of the modeled system when compared with test or other data. Consequently, it is more useful to consider the outcome of V&V activities in terms of a degree of confidence in M&S results that is expressed on a scale from 0 to 100, where 0 represents absolutely incorrect and 100 represents absolutely correct.(8) Principle 8: VV&A is both an art and a science, requiring creativity and insight.Cost-effective VV&A requires creativity and insight. Knowledge of the problem domain, expertise in the M&S methodology, and prior modeling and V&V experience are required for successful VV&A. (9) Principle 9: The success of any VV&A effort is directly affected by the analyst.Analysts are key players in the use of M&S, from assisting in defining the problem to selecting the model or simulation to be used to running the simulation to interpreting the results. Analysts can be found in virtually all roles within the M&S life cycle, including those of V&V agent, program manager, M&S proponent, and even accreditation authority.(10) Principle 10: VV&A must be planned and documented.Careful planning is required for successful VV&A efforts. Tests should be identified, test data or cases should be prepared, tests should be scheduled, and the whole VV&A process should be documented. (11) Principle 11: V&V requires some level of independence to minimize the effects of developer bias. Model developers may fear the repercussions that negative results may have on their performance appraisal. Model developers, however, have the most knowledge about the models and simulation. Trade-off must be made between the budget and the level of confidence and trust in the model or simulation developer, as well as the requirements of the management chain to demonstrate independence.(12) Principle 12: Successful VV&A requires data that have been verified, validated, and certified.The credibility of M&S results is related directly to the credibility of data used as input to or resulting from model use. Data need to be reviewed for accuracy and consistency. Guidelines are being developed to provide insight into the tools, techniques, processes, and procedures that assist the model user in determining data credibility.3. M&S VV&A ProcessOsman Balci[5] presented considered that the life cycle of simulation study composed of 10 phases, 10 progresses and 13 VV&T stages. The DOD VV&ARPG presented a generic VV&A process including determine VV&A requirement, initiate VV&A planning, V&V the conceptual model, V&V the design, V&V the implementation, V&V the application, perform acceptability assessment. Simone M. Youngblood[4] et al studied on DIS VV&A process and design the DIS nine-step VV&A process model (Figure 3.2) which is a useful tool for VV&A planner in designing VV&A process for DIS Exercise. Peggy D. Gravitz and William Jordan[7] presented an IDEF0 process model of the DIS VV&A process model. A VV&A effort should be cost-effective, responsive, and sufficient too succeed. To realize this objective, the VV&A process should be tailored to fit the purpose of the application and the types of simulations. Tailoring, the selection of verification and validation activities, based on requirements and resource availability, is don as part of the VV&A planning process to determine the most appropriate and cost-effective ways too address the application requirements and acceptability criteria. Paul R. Muessig[8] researched on the issue "How much V&V is enough for an accreditation" in the SMART project. William Jordan[9] et al studied planning, optimizing, and costing VV&A for Distributed Interactive Simulation. EMBED Visio.Drawing.5  Figure 3.2  DIS 9 Step VV&V Process Model4. Techniques and Automation4.1 TechniquesMany V&V techniques, most of which are derived from software engineering, exist today. DOD VVARPG introduces 76 V&V techniques and 18 statistical techniques as illuminated in Figure 4.1. This is a great reference for M&S practicer. Osman Balci[6] categorizes the M&S VV&T techniques into six distinct credibility assessment perspectives: informal, static, dynamic, symbolic, constraint and formal. Robert G. Sargent[10] defined some M&S techniques and related to different aspect of model credibility. S.Y. Harmon[11] describes some mathematical relationships between project risk and simulation result and utilize these relationships to define a collection of simple rules to guide the decisions for validating simulation tools and their product.Object oriented analysis and design, artificial intelligence, fuzzy techniques and neural networks, as well computer and network technology are used extensively in every stage of modeling and simulation development. These information techniques, on one hand, enhance the simulation functionality and performance. On the other hand, these increase the difficulty in applying to the VV&A M&S life cycle. More powerful V&V methodology and techniques should be studied to keep up with the development of simulation technology. Levent Yilmaz[12] compared the V&V for object-oriented simulation models and V&V for the procedural simulation models. Gary Q. Coe and Lester Foster[13] discussed V&V of object oriented simulations, based upon the DOD High Level Architecture (HLA), Federation Execution Development Process. Dale K. Pace[14] studied the VV&A issues for computer simulations which employ artificial intelligence techniques. EMBED Word.Picture.6  Figure 4.1   V&V Techniques From DODVVARPG4.2 AutomationVV&A process in the life cycle of M&S involves plenty of work of management, analysis, documentation, and data handling. A great deal of manpower and time may be consumed in the VV&A process. The larger and the more complex the simulation system is, the greater efforts the VV&A needs, and the less effective and accurate the VV&A may be. Automation tools should be developed in order to increase the efficiency and effectiveness of VV&A activities in life cycle of M&S development and reduce the cost (money and time) and risk in the VV&A process. Jean M. Graffagnini and Simone Youngblood[15] develop the VV&A report templates to assist and facilitate the DMSO initiative to standardize VV&A data/information within common repositories in order to form a basis for M&S use, reuse and accreditation decision. Lewis, Robert O[16] developed a VV&A quick planner tool that assists in the planning, adaptation, and design of VV&A programs for DIS applications. A full VV&A automation tool should at least have but not limit to the following functions:VV&A Database: VV&A database should have the abilities to collect, record, query and modify data and information the VV&A process needs.VV&A Process Management Tool: The VV&A Process Management Tool can assist the VV&A managers in managing, coordinating and overlooking the VV&A progress.VV&A Documentation Tool: The VV&A Documentation Tool helps the VV&A designer and reporter finish the VV&A plan, interim report, V&V reports and accreditation report.VV&A Techniques Selection Tool: VV&A Techniques Selection Tool assists the VV&A practicer selecting the most effective techniques applying in the VV&A activities.5. ConclusionVerification, Validation and Accreditation in the life cycle of Modeling and simulation is the main trend in evaluation and improving of models and simulations credibility. This paper gives the reader a very comprehensive introduction to present VV&A studies. Great efforts are still necessary to make more simulation sponsor, proponent, developer and user to understand VV&A concepts, principles, process and techniques.The increasing of scale and degree of complexity of simulation system add the difficulty and cost of VV&A process in life cycle of M&S. Research on more powerful VV&A techniques is important to improve VV&A effectiveness and accuracy. VV&A automation tools are of great need in VV&A practices and deserved to study more intensively in future work.6. References and Bibliography[1] Department of the Army. 1993. Pamphlet 5-11: Verification, Validation, and Accreditation of Army Models and Simulation.[2] DMSO. 1996. Department of Defense Verification, Validation and Accreditation (VV&A) Recommended Practices Guide[3] Department of Defense. 1996. Department of Defense INSTRUCTION 5000.61: DoD Modeling and Simulation (M&S) Verification, Validation, and Accreditation (VV&A).[4] IEEE Computer Society. 1997. IEEE Std 1278.4: Recommended Practice for Distributed Interactive Simulation - Verification, Validation, and Accreditation (VV&A)[5] Osman Balci. 1997. Principles of Simulation Model Validation, Verification, and Testing. Transactions of the Society for Computer Simulation International, Vol.14(1),1997 March, pp.3-12[6] Osman Balci. 1995. Principles and Techniques of Simulation Validation, Verification, and Testing. Proceedings of the 1995 Winter Simulation Conference., pp.147-154[7] Peggy D. Gravitz, William Jordan. 1995. An IDEF0 Process Model of the DIS VV&A Process Model. Proceedings of the SCSC'95, 627-632[8] P.R. Muessig and D.R. Laack. 1996. Cost Effective VV&A: Five Prerequisites. Proceedings of the SCSC'96, pp. 409-414[9] William Jordan, Dr. Dusty et al. 1995. Planning, Optimizing, and Costing Verification, Validation, and Accreditation (VV&A) for Distributed Interactive Simulation (DIS). Proceedings of the SCSC'95, pp.597-602[10] Robert G. Sargent. 1994. Verification and Validation of Simulation Models. Proceedings of the 1994 Winter Simulation Conference, pp.77-87[11] S.Y. Harmon. 1999.  Why Validation? 1999 Spring Simulation Interoperability Workshop, No. 047[12] Levent Yilmaz. 1998. Verification and Validation Guidelines for Object-Oriented Simulation Models. Proceedings of SCSC'98, pp. 645-650[13] COL Gary Q. Coe and Lester Foster. 1996 Verification & Validation of Object Oriented Simulations. 15th DIS Workshop, No. 017[14] Dr. Dale K. Pace. 1996. Verification, Validation, and Accreditation Issues for Computer Simulations Which Employ Artificial Intelligence Techniques. Proceeding of the SCSC'96, pp.3-7[15] Jean M. Graffagnini and Simone Youngblood. 1998. The Verification, Validation, and Accredition Report Templates. 1998 Fall Simulation Interoperability Workshop, No. 062[16] Lewis, Robert O. A Rapid Planning Tool for Verification, Validation, and Accreditation (VV&A) of Distributed Interactive Simulations. Proceedings of Summer Computer Simulation Conference, San Diego, CA. July 1994, pp. 696-701[17] Yang Ming, Zhang Bing, Wang Zicai. The VV&A and Credibility Evaluation of Distributed Interactive Simulation System (DISS). Journal of Harbin Institute of Technology. 1999, No.10