Analysis Process Overlay for the FEDEPShirley PrattIndependent Consultant, Orlando, FL 32817pratts@gdi.netJames TottenAir Force Research Laboratory, Sensors Directorate2241 Avionics Circle, Wright-Patterson AFB, OH 45433-7303James.Totten@sensors.wpafb.af.milLeroy JacksonTRAC-Monterey, P.O. Box 8692, Monterey, CA 93943jacksonl@trac.nps.navy.mil Andrew MeltonComputer Systems Center Inc., 6225 Brandon Ave., Suite 520, Springfield, VA 22120amelton@csci-va.comKeywords:Analysis, FEDEP Overlay ABSTRACT: During the Fall 1997 Simulation Interoperability Workshop, the Analysis Forum formed the Analysis Working Group (AWG) to draft guidelines for analysis in Advanced Distributed Simulation (ADS) environments such as the High Level Architecture (HLA). With its ability to link together models and simulations of different types (live, virtual, constructive, etc.), applications (analysis, training, engineering, etc.), domains (air, land, surface, etc.), fidelity and resolution, ADS provides new analysis opportunities and challenges. Analysts need to be actively involved throughout the HLA Federation Development and Execution Process (FEDEP), not just during the last “Execution and Analysis of Results” phase. Over the past year and a half, the AWG has solicited papers and collected input on analysts’ roles within each of the 5 major phases in the FEDEP. This paper summarizes the results gathered to date in the form of a draft analysis process overlay for the FEDEP. The relationships with other process overlays, such as fidelity management, testing and verification, validation, and accreditation (VV&A) are also discussed.1. IntroductionWith its ability to link together models and simulations of different types (live, virtual, constructive, etc.), applications (analysis, training, engineering, etc.), domains (air, land, surface, etc.), fidelity and resolution, Advanced Distributed Simulation (ADS) provides new analysis opportunities and challenges. To use ADS effectively for analysis, analysts need to be actively involved throughout the development and execution of such simulations. For example, they need to define the study questions and identify the measures of merit used to answer the questions. They need to help develop the scenario and select the appropriate simulation components and interactions. They must decide how many runs to make and how to collect the data. Analysts must note any artifacts or limitations that might affect the simulation results. They are responsible for reducing the collected data and interpreting the results.To facilitate the successful use of ADS for analysis, the Analysis Forum formed the Analysis Working Group (AWG) during the Fall 1997 Simulation Interoperability Workshop (SIW). The AWG was chartered to draft guidelines for analysis in Advanced Distributed Simulation (ADS) environments such as the High Level Architecture (HLA). Over the past year and a half, the AWG has solicited papers and collected input on analysts’ roles within each of the five major phases in the Federation Development and Execution Process (FEDEP). This paper summarizes the results gathered to date in the form of a draft analysis process overlay for the FEDEP. The relationships with other process overlays, such as Fidelity Management, Testing and Verification, Validation, and Accreditation (VV&A) are discussed. 2. Analyst Role in the FEDEPThe five major steps of the FEDEP are summarized as follows [1]:Step 1: The federation sponsor and federation development team define and agree on a set of objectives, and document what must be accomplished to achieve those objectives.Step 2: A representation of the real world domain of interest is developed, and described in terms of a set of required objects and interactions.Step 3: Federation participants are determined (if not previously identified), and a FOM is developed to explicitly document information exchange requirements and responsibilities.Step 4: All necessary federation implementation activities are performed, and testing is conducted to ensure interoperability requirements are being met.Step 5: The federation is executed, outputs analyzed, and feedback provided to the federation sponsor.This five-step process is very general and is meant to be applicable to building any HLA federation. It can be implemented in many different ways depending on the nature of the application. As a user of HLA federation, the analysts’ role appears mainly toward the end of the FEDEP in the “Federation and Execution and Analysis” step. However, due to the complex nature of simulation linkages, analysts need to be actively involved throughout the process.  They can not be simply handed the data after a simulation execution and expected to “turn the crank” to generate the results. Analysts must be considered an integral part of the federation development team.3. Analysis Process OverlayTo reinforce the point that analysts must be involved throughout the FEDEP, the AWG has drafted an analysis process overlay for the FEDEP (see Table 1).  This overlay outlines the recommended role of analysts during each step of the FEDEP. Since analysts work with many types of simulations, the analysis process is meant to be applicable to a wide range of applications (e.g., analysis, training, engineering, testing and evaluation, etc). It may be implemented in different ways, however, like the FEDEP. Details of the overlay will be discussed in the following sections.FEDEPRequirements DefinitionConceptual Model DevelopmentFederation Design and DevelopmentFederation Integration & TestFederation Execution & AnalysisIdentify sponsor needsDefine federation objectivesDevelop scenarioPerform conceptual analysisDevelop federation requirementsDesign federationDevelop federationPlan executionIntegrate federationTest federationExecute federation Analyze results Prepare feedbackAnalysis Process  Identify study question Develop high level analysis plan Identify measures of merit (MOMs)Develop detailed data collection and analysis (DC&A) planGenerate run matrixTest DC&A planMake observations and provide real-time feedbackAssess limitations / credibility of resultsOverlap with other ProcessesDerive fidelity requirements from functional scenario description Verify and Validate (V&V) federatesCheck data to be exchanged and source data V&V federation  design and developmentTest federation V&V federationTable 1. Analysis Process Overlay for the FEDEP3.1 Requirements DefinitionAs part of the federation development team, the analyst helps identify the sponsor needs and define the federation objectives. The analyst also identifies the key study question(s) to be addressed. He/she then begins developing a high level analysis or study plan with testable hypotheses, e.g., a null hypothesis and alternatives. The analyst ensures that the federation objectives are framed as testable hypotheses and that the objectives are associated with questions that can be answered.3.2 Conceptual Model DevelopmentIn the Conceptual Model Development phase, the analyst assists in the development of the scenario, the Federation Conceptual Model (FCM) and the federation requirements. The analyst needs to be involved in identifying where the assumptions lie in the conceptual model and how sensitive the results will be to those assumptions. This ties in with the fidelity requirements of the federation which the analyst helps derive from the functional scenario description of the federation execution, a task from the Fidelity Management Overlay [2].  He/she must ensure that all the products from this phase (e.g., the scenario, the FCM, the federation requirements and the fidelity requirements) support answering the key questions identified in the earlier phase.The analyst also decomposes the study questions into its constituent parts. He/she helps define the independent variables and the measures of merit (MOMs). The latter includes measures of effectiveness (MOEs) and measures of performance (MOPs). MOEs measure how well a model, system or individual performs a task or meets an operational objective or requirement under specific conditions and are generally used to distinguish among alternatives. MOPs measure how well a model, system, or individual performs its functions in a given environment and is closely related to inherent parameters (physical and structural) but measures attributes of system behavior. MOPs are often used in computing the MOEs but may be used to rank the alternatives when the MOEs do not give sufficient insights into system differences. [3][4]3.3 Federation DesignIn order to be able to understand and interpret federation interactions, limitations and results, the analyst needs to be involved in the design and development of the federation. He/she participates in the selection of the appropriate simulation components to be federated taking into consideration the types of objects, algorithms and source data each uses and their overall compatibility when federated. He/she must ensure that the federation is able to generate the data elements needed to compute the MOMs needed to answer the key study questions.The analyst must also assess the federates individually and the federation as a whole to identify potential artifacts and limitations that might affect the simulation results. To do this, the analyst’s tasks overlap with those from the VV&A process overlay. The analyst assists with verifying (determining whether a model implementation accurately represents the developer’s conceptual description and specifications) and validating (determining the degree to which a model is an accurate representation of the real world from the perspective of the intended uses of the model) the individual federates. He/she checks the data to be exchanged by the federates and the source data to be used by the federates for consistency and correctness. He/she helps verify and validate the overall federation design and development. [5]After the federation design and development has been completed, the analyst must develop a detailed data collection and analysis (DC&A) plan to capture the critical data elements during the federation execution and obtain the desired analytical products from them. He/she identifies the composition and mapping of the data elements as well as any data reduction tools needed to produce the analytical products that answer the question. He/she determines the best approach to capturing the data. The most common ones include local data logging by the individual federates, distributed data logging by one or more data logger federates, or some combination of the two approaches. Data collection methods, issues and lessons learned are discussed further in [6].3.4 Federation Integration and TestHere the analyst may assist with the federation execution planning and integration, but his/her main role in this step begins during federation testing. During federation testing, the analyst implements and tests the DC&A plan. He/she checks that the required data can be collected at the necessary time and resolution, in the correct format, under the right conditions, etc. He/she ensures that the data can be used to produce the appropriate analytical products and discern causal effects. The analyst also determines an appropriate run matrix to support sensitivity analysis and also to achieve the goals of the simulation.To ensure that the federation is generating the credible data, the analyst needs to participate in tasks from the Testing and VV&A process overlays. In particular, the analyst helps test federation integration, functionality and scenario capabilities. These testing activities ensure that the federates can exchange data to interact appropriately, that the federates can perform the required role specified in the scenario, and that the scenario events can be executed or achieved by the simulation  [7]. The analyst also assists with verifying and validating the federation. This V&V activity ensures that the federation implementation accurately reflects the intended design and that the credibility of the implementation against the real world is appropriate for the federation’s intended use [5].Throughout this phase of the FEDEP, analyst must again note any other potential artifacts or limitations that might affect the end results of the simulation. These include but are not limited to inadequate RTI performance, slow network throughput rates, poor individual or overall system performance, etc.3.5 Federation Execution and AnalysisDuring the federation execution, the analyst will collect data according to the DC&A plan and monitor the federation execution to ensure that the simulation components and network are performing as required. He/she may produce real-time or near real-time feedback for immediate use by exercise management, federation participants, and/or other analysts. He/she may also note specific events, odd occurrences or glitches that might be useful to reference later.  After the federation execution, the analyst ensures that the data can be correlated to answer the questions. Raw data is transformed into the desired analytical products according to the DC&A plan. He/she estimates the uncertainty in the results and assesses the impact of all the artifacts, limitations, glitches, etc, previously noted. The analyst prepares a report containing the final results along with a measure of confidence in those results.4. SummaryTo effectively use ADS environments such as the HLA, analysts need to be actively involved in their design, development and execution. The AWG has compiled information on analysts’ role during the five major phases of the FEDEP. This information has been presented as a draft Analysis Process overlay for the FEDEP. The overlay complements the existing FEDEP but highlights the need for analysts to be involved throughout the process, not just during the last “Federation Execution and Analysis” phase. The overlay is meant to serve as an initial guideline for conducting analyses using the HLA. The authors welcome comments for its improvement. AcknowledgmentsThe authors thank all the Analysis Forum working group members for their input to this overlay and numerous other individuals who attended the Analysis Forum meetings, provided feedback on the reflector and authored papers about their experiences.References[1]	Defense Modeling and Simulation Office:  “Federation Development and Execution Process (FEDEP) Model”, Version 1.3 Draft, December 9, 1998.[2]	Roza, M., van Gool, P. & H. Jense: “A Fidelity Management Process Overlay onto the FEDEP Model”, Paper 083, Proceedings of the Fall 1998 Simulation Interoperability Workshop, Orlando, FL, September 1998.[3]	Defense Modeling and Simulation Office: “Glossary of Modeling and Simulation Terms”,  HYPERLINK http://www.dmso.mil/docslib/mspolicy/glossary/ http://www.dmso.mil/docslib/mspolicy/glossary/, January 15, 1998[4]	Training and Doctrine Command (TRADOC): Pamphlet 11-8.[5]	Youngblood, S. : “A VV&A Overlay to the HLA Federation Development Process”, Presentation  at the Fall 1998 Simulation Interoperability Workshop, Orlando, FL, September 1998.[6] 	Neuberger, T.: “A Progress Report: Recommended Practices for Data Collection in HLA and Other ADS Environments”, Paper 129, Proceedings of the Fall 1998 Simulation Interoperability Workshop, Orlando, FL, September 1998.[7]	Horst, M., Roberts, D., & Old, J.: “Testing Overlay for the FEDEP”, Paper 104, Proceedings of the Fall 1998 Simulation Interoperability Workshop, Orlando, FL, September 1998.Author BiographiesSHIRLEY PRATT has developed computer graphics applications and networked simulations for the past seven years. Ms. Pratt has a BA in Applied Mathematics from U.C. Berkeley and an MS in Oceanography-Physics from the U.C. San Diego. She has taught Computer Science courses at Naval Postgraduate School and has worked on several projects for U.S. Army Training and Doctrine Command (TRADOC) Analysis Center (TRAC). She has served as Secretary for the SIW Analysis Forum since 1997.JAMES TOTTEN has worked in Electronic Warfare and Modeling and Simulation for more than 40 years.  He has an Electrical Engineering degree from the University of Cincinnati with graduate work at The Ohio State University. Mr. Totten is currently a Principal Electronics Engineer in the Sensors Directorate of the AFRL.  His primary focus is Information Operations and Warfare and facilitating the use of technology programs in future operational capability.MAJOR LEROY JACKSON is an Army officer with over twenty years of enlisted and commissioned service. He graduated with a BA in Mathematics from Cameron University in 1990 and with an MS in Operations Research from the Naval Postgraduate School in 1995. He is currently an operations research analyst at the U.S. Army Training and Doctrine Command (TRADOC) Analysis Center (TRAC) in Monterey, California and continues graduate studies in operations research at the Naval Postgraduate School.ANDREW MELTON is the director of Simulation Analysis at Computer System Center Inc. in Springfield VA. He has a MS in Operations Research from the Naval Postgraduate School focusing on Modeling and Simulation analysis. He works extensively in the Joint Theater Air and Missile Defense (JTAMD) arena developing analysis methodologies to study operational concepts. He responsible for coordinating the use of Advanced Distributed Simulation for operational concept and technical feasibility studies.