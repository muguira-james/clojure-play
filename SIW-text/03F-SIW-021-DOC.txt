Ground Truth Lethality/Vulnerability Considerations in a Distributed Wide Area Environment (RDE Command 1stApp Case Study)Geoff SauerbornU.S. Army Research LaboratoryATTN: AMSRL-WM-BF (Sauerborn)Aberdeen Proving Ground, Maryland 21005-5066 HYPERLINK "mailto:Geoffrey.Sauerborn@us.army.mil" Geoffrey.Sauerborn@us.army.milKeywords: Vulnerability, Lethality, Assessment, Wide Area Network-.ABSTRACT: This paper lists requirements needed for accurate vulnerability assessments in a distributed environment.  Implications specific to wide area networks (WANs) are discussed.  Lessons are drawn from a recent distributed simulation experiment within the U.S. Army Research Development and Engineering (RDE) Command (1stApp).  Conclusions drawn from 1stApp apply to lethality assessment for any WAN interoperable simulation exercise.The conclusions drawn from specific examples are that data required for lethality assessment (and resulting damage) must be transmitted unambiguously and with effectively insignificant latency.  The paper also shows how the RDE Command Modeling Architecture for Technology and Research EXperimentation  (MATREX) architecture can support these requirements in the future.1. IntroductionThis paper lists requirements needed for accurate vulnerability assessments in a distributed environment.  The two basic conclusions are not surprising:  1. Assuming that vulnerability data are correct, they must be transmitted correctly (with well-defined interfaces; error checking is an added bonus).  2.  Effective data latency must be very low (or zero) to allow a common picture of the environment from any one node.  This also ensures the timely delivery of damage results for proper simulation realism.These lessons were relearned during a recent distributed simulation experiment within the U.S. Army Research Development and Engineering (RDE) Command (called 1stApp).  1stApp was the “first” integration effort in a new Army science and technology objective (STO).  The STO’s purpose is to provide a persistent, distributed simulation environment.  Among other things, this environment would be applied to the evaluation of U.S. Army Future Combat Systems (FCS) and Objective Force concepts.  It will link together Army laboratories, research engineering centers, and U.S. Army Test & Evaluation Command (ATEC) capabilities and resources into a modeling and simulation (M&S) environment.  As stated, 1stApp was just the first integration effort within this STO.  Specific objectives for the 1stApp exercise were to:1)	Provide insights into the networked fires process and performance for FCS. 2)	Define the baseline capability of this virtual distributed laboratory for modeling and simulation as it transitions toward a combined architecture with joint virtual battlespace (JVB) to form the Modeling Architecture for Technology & Research Experimentation (MATREX).1Although portions of 1stApp and the draft MATREX architecture are touched upon, this paper is focused on how they apply to lethality assessment.  There are other sources that address their broader programmatic or architectural details.1,2,32. Representing Damage-Causing Events in a Distributed EnvironmentThe distributed interactive simulation (DIS) protocol is sufficiently well defined to describe the initial conditions for most vulnerability calculations (of ballistic and high explosive conventional weaponry).4 Shockwaves, fragmentation, shot lines, and so forth can be derived by knowing the type of threat, its terminal velocity vector, and impact point relative to the target.  DIS provides these data via the detonation protocol data unit (PDU).  This information in conjunction with the target’s position and orientation at the same instant completes the necessary parameters for a vulnerability analysis to commence.  These same data have been mapped to the Department of Defense high level architecture (HLA)5,6 via the real-time platform reference federation object model (RPR FOM).7Where can things go awry?Data errors.Data latencies.2.1  Data ErrorsModels and simulations that have been verified and validated (V&V) might tend to  operate within expected boundaries, having little or no errors in that sense when used appropriately.  However, taking a V&V model into a distributed environment (or interfacing a distributed simulation in ways not previously tested) allows the possible introduction of errors) and really requires its own V&V process within the federation development process (FEDEP).8  Normally, interfacing mistakes (errors in the way data are formatted, packed unpacked and transmitted) are spotted fairly quickly, particularly when a data element is required as input to a distributed component’s algorithm.  (The receiving algorithm will usually and abruptly fail, but in instances when the data set is used less frequently or less rigorously, component interface errors may be difficult to determine.)  One such example from 1stApp was from the WeaponFire interaction.  In this instance, FiringLocation from a long range cannon entity was translated as a 12-byte data set where 24 bytes (a vector of three double precision numbers) was expected.*  This transmission formatting error was found because the ARL lethality server (the VL server) subscribed to the RPR-FOM’s WeaponFire interaction.  The VL server did not use FiringLocation directly, yet the error was discovered because each data item that is received is tested for validity in at least an elementary way (byte length checking).  This is accomplished in an HLA convenience application programming interface (API) (or middleware) called “HLAfc.”  HLAfc was developed as a byproduct while supporting the Virtual Proving Ground and has been used to increase the ease by which numerous federates could be implemented quickly into an HLA federation.  It has been used to obtain HLA certification on various native HLA federates.  Like other HLA middleware solutions, HLAfc consumes a federation object model (FOM).  During initialization, applications using HLAfc automatically have their subscriptions tested against the data types declared in the FOM.  An exception occurs if an unknown or misused data type is found.  During run time, outgoing and incoming data are tested for consistency (but currently only in terms of the number of bytes expected).  Of course, more rigor could always be added – probably with an option to turn error checking off (for faster execution) after an HLA federation has been tested for correctness.Tools have been available help federation developers in verifying their implemented interfaces.9  Sometimes however, because of scheduling or various other reasons, these tools are not applied before federation integration.  Using some form of middleware framework would be a logical place to assist in federate data consistency checking and fits well with the MATREX architecture’s design strategy.(  That is, this important error source (interface data inconsistency) could be tested on a federation-wide scale without any additional effort on the part of the federates by having federates interface with a standard middleware framework.2.2  Latency “Errors”It is debatable whether latency should be called an error.  There really are no simulations without latencies.  Rather, latency “errors” occur when the system architecture or consumers of distributed data do not properly account and compensate for latency in the schema.Before distributed simulations existed, we (the modeling and simulation community) still developed distributed simulations.  They were merely locally distributed within the same computer, often within the same process (functions, subroutines, etc.).  Since data had to move from one distributed component to another (and within components) and since this was not instantaneous, there were therefore measurable latencies by definition.  Yet, the structure of the programming environment and simulation modules treated the concept of time consistently.  Effectively all simulation components would be time lock stepped.**  Therefore, methods properly compensated for this latency so that it was not even a consideration (other than how long the run will take).What has changed between calling a library and calling a remote method or a distributed service?  One major change is that the traditional simulation environment was created within a tight design architecture.  Whether event driven, parallel discrete event driven,time stepped, or wall clock time driven, simulation designers and software developers knew when, and when not to advance the simulation “clock”.  Time was managed in a consistent way among all modules in the environment.  Weapons systems analysis simulations (consumers of vulnerability / lethality data) were no exception.  Often, these simulations took the form of stochastic processes (Monte Carlo), a simulation method suited quite well to trade-off analysis, such as battle simulations).  Usually with these types of Monte Carlo simulations, there is a known set of outcomes (with an associated probabilistic distribution associated with those outcomes). Data latency is not a damage assessment issue for such classical time-constrained simulations since it is guaranteed that time will, in effect, stand still until the damage state is resolved. Along came distributed simulation environments (SIMnet, DIS, ALSP, and HLA) allowing a plethora of simulations and tools to be brought together; however, each may have had its ancestry based on different concepts of how time was managed.   DIS and HLA have generally been popularized in a network-distributed and real-time environment.  This is not to say that these specifications do not apply to non-real time.  This is truer with HLA than the others.  That is, HLA in particular has specific time management services that effectively allow the “time locked step” method to be applied (as well as just about any other time scheme imaginable).10Yet, the point remains that DIS and HLA have generally been thought of in terms of a network-distributed and real-time environment.  Often, DIS-based applications are wrapped behind the RPR-FOM and (correctly) declared to be HLA compliant.  This requires a DIS-to-HLA bridge.  With regards to latency, two assumptions are made (but well advertised) in DIS.  In DIS, recommended latency should not exceed 300 ms application to application and (100 ms point to point).Dead reckoning algorithms are applied and the sender shall update when a “threshold” is reached.As an example of how this applies to lethality, this means that, ideally, a targeted entity will not have moved out of its threshold update limit between the time a firer shoots an unguided ballistic munition and the time the munition hits its impact point.  The DIS specification in a real-time environment still allows small mathematical errors to be introduced.  By way of example: suppose an entity was “just about to” update its position information (because it has exceeded its dead reckoning threshold).  At the same instance, a munition is launched using an aim point based on where the firer perceives the entity to be.  (Yet we know the entity is aware that its own position has already drifted from perceived truth to the point where it must now re-issue a position update.)  Obviously, the firer is aiming at the wrong point and the round will therefore impact at some mathematical difference from where it “should have” impacted.  These errors could be quantified if one knows the weapon system specifics as well as tactical information (such as doctrinal “aim points”).3.  1stApp Latency ObservationsDuring 1stApp, there were times when these dead reckoning boundaries were far exceeded.  At one point during initial integration runs with the larger of 1stApps scenarios (about 2000 entities were in the large scenario), it was discovered that latencies were measurable in the minutes (this was before an RTI configuration file parameter was modified and latencies were vastly reduced).  The physical network had low average latencies of around 30 ms and bandwidth limits were never seriously challenged.3  Thus, the physical architecture was not the major factor.  The problem therefore lay with the software simulation environment.  Part of this software environment was the required software layers:  Bridging DIS-to-HLA passing HLA across the wide area network and then going through another HLA-to-DIS translation.  The latency problem was revealed when it was discovered that entities between the Army Missile Research Development, and Engineering Center (AMRDEC) (located at the Redstone Arsenal, AL; see figure 1) and the Night Vision Laboratory (Ft. Belvoir, VA) were not being damaged by a munition threat.  Upon further examination (via voice communication between the two remote sites), it was finally discerned that the entities in question had passed by the detonation location some minutes ago and were now far beyond harm’s way from that particular detonation.  This latency occurred in the larger exercise scenarios involving over a thousand entities and it increased the longer the scenario advanced.MaK technologies support helped to reduce that lag by a large factor by recommending an RTI configuration file parameter (toggling asynchronous data delivery).  Yet there was still noticeable latency, though it was vastly reduced.  It is still debatable whether the problem was the RTI, the bridge, or combination of these two in the context of the entire systems (to include number of entities, time of run, scenario or other factors).  The fact that the munition detonation event was able to pass through the software stacks apparently unabated seems to indicate that the RTI was not the problem.  The argument could then be made that entity updates were being queued in the bridge (the gateway), causing the latency.   However, if that were solely the case, (that is, if the gateway as opposed to the RTI was the sole source of delivery latency), then this author does not understand why changing the way the RTI behaved should not have had such a dramatic influence; yet it did.  At the very least, RTIs and gateways cannot be treated as independent, and a thorough knowledge of both is recommended. This case study from 1stApp points out two problems and a MATREX solution:The latency was not immediately discovered until after several integration runs had occurred.When the problem did occur, the component or components causing the latency were not immediately diagnosable.The point to this discussion is that focusing on the latency source (and especially discerning that there was latency) and measuring it was problematic.3.1  How the MATREX Architecture Should HelpThe DIS specification provides a “time stamp” field in all protocol data unit headers.  If 1) all systems are synchronized, and 2) each simulation properly fills the time stamp field, then 3) a network monitoring tool should be able to measure latencies through the software stack.  Unfortunately both 1 and 2 need to be followed (not always the case) and certainly 3 (the tool itself) needs to be available preferably at each node of the network.The MATREX architecture provides for at least the first two requirements.  First, the MATREX draft V0.5 Federation Object Model provides for the time stamp field.  Second, the future MATREX middleware services (alluded to earlier) could fill the time stamp automatically for federate applications, thus avoiding the case where simulations miss-fill or do not fill the important time stamp field.  Provided that the remote systems are synchronized( and a general simulation support tool monitors these latencies at each node, such latencies can be caught “instantly”.4.  SummaryOur two basic conclusions remain the same:1. Assuming that vulnerability data are correct, they must be transmitted correctly (with well-defined interfaces; error checking is an added bonus). 2.  Effective data latency must be very low (or zero) to allow a common picture of the environment from any one node and provide valid vulnerability assessments, “effective” in the sense that there will always be latency, but that latency must be compensated for in some manner.1stApp helped to remind us of these lessons and the MATREX architecture provides means for both these requirements to be fulfilled in the future.Other factors could help to improve the vulnerability analysis process.  These factors include features that any vulnerability analysis tool should have such as tracking and databasing all state changes, being able to save state (in the event of a simulation “save” or other unexpected pause) in order to support uninterrupted monitoring and retain a memory of previous vulnerability events.  Also, the process of preparing for vulnerability assessments itself is a relatively long lead time process and should begin at the same time that a simulation scenario is being formulated (with supporting tools to track configuration of damage assessment data).  Such tools and factors apply to battle simulation vulnerability assessment in a distributed wide area environment as well as a local environment.  Thus, they are not specific to a distributed wide area environment (this paper’s focus).  Therefore these factors are not detailed at this time.Author BiographyGeoff Sauerborn has 20 years experience in modeling and simulation for the U.S. Army.  Three quarters of this experience is related to weapon systems effectiveness and analysis. Most recently he has supported Army test & evaluation through the Developmental Test Command’s Virtual Proving Ground program.  He conceived of and developed a lethality server for distributed environments and ran this server during 1stApp (Spring 2003).References:[1]	“Modeling Architecture for Technology and Research EXperimentation  (MATREX) System Architecture Description”, Coordination Draft, V0.8, May 15, 2003[2]	Weber, et. al., “The Many Lessons from the RDE Command 1st Application - But did we learn anything?”, Simulation Interoperability Standards Organization (SISO), Fall 2003 Simulation Interoperability Workshop (SIW), (03F-SIW-005), September 2003.[3]	Roose, et. al., “The Architectural Design, Implementation and Observations of a Network, Built over the DREN, in Support of the Distributed HLA Simulation Federation of the RDE 1st Application”, Simulation Interoperability Standards Organization (SISO), Fall 2003 Simulation Interoperability Workshop (SIW), (03F-SIW-029), September 2003.[4]	“IEEE Standard for Distributed Interactive Simulation - Application Protocols”, Institute of Electronic and Electrical Engineers (IEEE), Inc., IEEE, Std 1278.1-1995, 21 September 1995.[5]	U.S. Department of Defense, “High-Level Architecture Rules Version 1.3”, 5 February 1998 (20 April 1998 Document Release).  Also: “High-Level Architecture Object Model Template Version 1.3”, and “High Level Architecture Interface Specification, Version 1.3”.[6]	“IEEE Standard for Modeling and Simulation (M&S) High Level Architecture (HLA) — Framework and Rules”, IEEE, Inc.,  IEEE Std 1516-2000, 21 September 2000.  Also: “IEEE Standard for Modeling and Simulation (M&S) High Level Architecture (HLA)—Federate Interface Specification”, IEEE Std 1516.1-2000 and  “IEEE Standard for Modeling and Simulation (M&S) High Level Architecture (HLA)—Object Model Template (OMT) Specification” IEEE Std 1516.2-2000.[7]	Guidance, Rationale, and Interoperability Manual for the Real-time Platform Reference Federation Object Model (RPR FOM), Version 2.0D14v2, 11 March 2002, pp 78, Table 7-5 MunitionDetonation Parameters, pp. 77 Table 7-4WeaponFire Parameters.[8]	"A VV&A Process for the HLA FEDEP", Simone Youngblood, presentation, Fall 1999 SIW, VV&A Forum, 16 September 1999.[9]	“Federation Verification Tool: FVT 1.3v4”, availble from the Defense Modeling and Simulation Office,  HYPERLINK http://sdc.dmso.mil/ http://sdc.dmso.mil/, fvt_support@gtri.gatech.edu.[10]	Fujimoto, R.M.  "Time Management in the High Level Architecture." Workshop on Parallel and Distributed Simulation (PADS), Vol. 71, No. 6, pp. 388-400, December 1998. * Most likely, three single precision floating point numbers were passed, representing the firing enity's x,y,z position in a local coordinate system.  Incidentally, this error had no known ill effects as the entity's position state via the RPR FOM BaseEntity.GroundVehicle was updated correctly.  Location from BaseEntity.GroundVehicle is the data set used by most applications who require entity position.  An error here would have been spotted almost immediately.( See MATREX reference [1]: Appendix D, D.4.2 Accrediting MATREX Federations.** "time lock stepped":  That is, events effectively are advanced through time in such a way that they would be in sequence.  (As opposed to events occurring in a way that makes their cause appear to happen after their effect.)   Ideally, under this method, time is regulated in a constrained manner such that each participant does not advance to its next time step until all events within the current time interval are processed.  Time management is a federation-wide consideration.( Using [Network Time Protocol (RFC 1305)] should be quite sufficient (RFC 1305 allows for gigabit-per-second regime bandwidths)Figure 1.  1stApp Distributed Sites:  (Redstone / MRDEC, Belvoir, Aberdeen, Orlando, Redstone / Redstone Technical Test Center (RTTC))