Practical Approach for Verification and Validation of Test Event FederationsJoseph M. OlahScience and Technology Corporation (STC)500 Edgewood Road, Suite 205Edgewood, MD 21040410-679-1612, x216olah@stcnet.comKeywords:Verification, Validation, Federation, Test and EvaluationABSTRACT:  This paper provides a practical approach for the verification and validation (V&V) of test event federations that have neither a well defined development/integration process nor a formal V&V program integrated into the development program.  Since test event federations are built to generate data for analysis to aid decision-making, the objective of federation development should be to assure that the federation simulations are being built correctly to produce suitable data. V&V attribute assessment can aid in achieving this objective.The paper starts with definitions from Department of Defense modeling & simulation and V&V guidance to clarify common misconceptions.  Next, a V&V concept is promoted based on building the right thing right and on assessing V&V attributes.  The paper then suggests how to scope and prepare the extraction of the V&V attributes for assessment.  The end product is a practical approach to assess the V&V rigor of the development process of a test event federation.IntroductionThis paper provides a practical approach for the verification and validation (V&V) of test event federations that have neither a formal V&V program nor a well defined development/integration process.This paper discusses definitions and concepts of V&V, attributes that should be found within a proper V&V program, and list tasks to accomplish the assessment.Though the V&V concept presented in this paper can apply to the development of simulations in general, the activities are specific to the development of simulations for test event federations.Point of ViewThe author believes that Verification and Validation (V&V) should be viewed from its essential idea – that of building the right thing right [1] -- rather than by its activities.  Tasks should lend themselves to meeting this idea; the activities should follow from the tasks.A proper V&V program will assure a suitable federation for a test event.  One can feel confident that the development process will produce a federation of simulations that meets the needs desired by the customer.Purpose of Test EventThe purpose of a test event is to gather data for analysis that leads to a decision.  Requirements for the test event then depend on the data it is to produce.  The test event may require a federation of simulations to produce this data.  Therefore, the capabilities of simulations to be integrated into the federation should be based on their share of data production.  Correctly choosing, tweaking, and integrating these simulations into the federation (so that it properly produces this data) is what the V&V is to ensure.Theoretical vs. PracticalTheoretical V&V assumes a given federation development process exists and that the V&V planning has started before the development process, and resources allow an embedded V&V team. However, even if the development process is not well defined, code is already being written, many vendors are writing the code, and the federation V&V team is small and on the outside, there may still be V&V in the development process.Though the federation development may not follow a formal process such as the Federation Development Process (FEDEP) or may not have a formal V&V program, it may well have attributes of V&V.  The practical approach is to extract these attributes for review.Definitions and PointsThe Department of Defense modeling and simulation (M&S) guidance defines verification essentially as “M&S reflects requirements” and defines validation essentially as “M&S reflects intended use” [2].  Unless one has requirements that contradict the intended use, verification and validation are essentially the same.  Also, guidance seemingly ignores the differences when listing activities of verify requirements, validate conceptual model, verify design, verify implementation, and validate results.  Each of these activities is defined as “confirming/determining that” some state exists [1].  If one person has verified the requirements, while someone else has validated the requirements, what have they done differently?  Theoretically, there may be a difference between verification and validation; but in practical terms, it does not matter.This paper makes a distinction between simulation and software application.  By definition, a simulation is “a method for implementing a model over time” [3]; whereas software is “a set of computer programs, procedures, and associated documentations concerned with the operation of a data processing system ...”.[3]  Or to paraphrase, software is data processing, while simulation is an implementation of a model.  Software works on real data; simulation is a representation.  This difference is noted as many test event federations are made of a few simulations and dozens of software applications.Federations are simulations and software, data templates, and coordinated execution.  This paper defines two types of federations: those that are built to provide some sort of capability, and those that are built for a test event.  This paper will call these “capability” federations and “event” federations, respectively.  Capability federations are built for rather generic purposes; event federations are built for rather specific purposes.  Capability federations may come on media or are in a simulation facility; event federations usually include live entities and data collection systems.  Capability federations are reused; event federations typically are not.  Capability federations can be built in an orderly manner as described in guidance such as the writings of Robert O. Lewis.  Requirements are known and stable, design starts and proceeds into implementation, and testing is planned for continuous improvement.  However, an event federation may have many separate sponsors, developers, and providers.  Verifying the objective, validating the design, integrating the federates becomes problematic as the objective or requirements change.  Without an event federation sponsor, the qualification testing may be just a couple days before the event.  There may be problems of allocation of resources without central authority.  Since an event federation will probably not follow the idealized Federation Development and Execution Process (FEDEP) found in guidance, it is especially important to stay focused on the essential idea of V&V.V&V ConceptsThis author notes three common concepts of V&V and gives them the names of administrative, inspection, and process.  Administrative V&V is a rating of software development documents for obtaining an accreditation signature.  Inspection V&V examines the software to determine whether it was built correctly.  Process V&V examines the development process itself to see if it is mitigating potential errors.  Administrative V&V has no effect on the federation development; Inspection V&V effects the federation development only in asking that it be repeated if problems are found; only Process V&V effects the federation development by guiding it.This paper contends that only the Process V&V concept fits the essential idea of building the right thing right.  Process V&V works on the development process; it is not an inspection of the federation itself.  When one says the federation is being V&V’d, one is essentially saying “the federation is being inspected”.  But if V&V is about the process of building the right thing right, one should be saying “the federation is being built using V&V”.  This is important, for test event federations may not be completed until just before the test event.  If problems are found at this point, there is little time to fix them.V&V activities should be coupled with federation development activities so that the V&V can have effect.  If one looks into V&V rather late, say after the simulation has already been built, then it is too late to worry about the simulation’s requirements, design, and implementation; one will just have to see if the simulation works and will provide what the federation needs.  V&V should be in place before development begins; however, V&V can be added while in development, though it will have less effect.  For legacy simulations/federates, inspection is all that is left.Process V&V is continuous; it is not done just once.  The assessment of the V&V is continuously updated so that both the developer and customer will know the problems ahead of time and be able to correct the process.Attributes of V&VProcess V&V will add rigor to the federation development process.  A proper V&V program will formally state how the development process will confirm the federation’s requirements, conceptual model, design, implementation, and acceptance.  However, a development program without a formal V&V program may still have a V&V rigor.A development process with a V&V rigor will have at a minimum the following seven attributes [4]:Proper federation development planningCoordination between developer and customerUnderstanding of intended use by developerStable, understood, and agreed capability and data requirements.Developer-Customer agreement on the conceptual modelSound software engineeringAdequate documentationThese attributes can be used in assessing the V&V within the federation development. The assessment can suggest to the developer additional V&V rigor for the process.Now that the paper has discussed the V&V essential idea, definitions, concepts, and attributes, the paper will discuss activities of V&V.  The next sections talk of scoping, prepping, extracting, and assessing the V&V of a test event federation.Scoping V&V EffortBuying a car makes for an analogy for scoping activities of assessing federation V&V.  When buying car which provides the best payoff?Determine commuting and hauling needs, seek opinion of Consumer Reports, and test drive, orWrite Goodyear about tire material, write AC Delco for battery reliability curves, and find other detailed component information?Knowing how to assess what car to buy means knowing the scope of information to investigate.  This paper suggests that the V&V effort be scoped by federate level, data requirements, and federate type.For federations, this paper recommends looking from the top rather than from the bottom.  Find the federates that are being integrated into the event federation.  See if these federates have been judged as acceptable or at least previously used.  Hopefully this has already been done before they have been submitted as candidates for the federation.  This should be the work of the federate developers and not the federation integrator.  There may be problems at lower levels of the federation, but this author believes that problems at the top will point towards the location of the trouble below.Greater emphasis should be placed upon the elements of the federation that produce data needed for the evaluator.  This should be defined by the data source requirements, intended uses, and objectives.  Once this is known then determining the capabilities needed becomes much more clear.  The federation may have much generic capability that is not needed for the event.  Much less emphasis should be placed these elements.Finally, the emphasis should be placed on simulations over software applications.  Simulations are representations, and representations can be a great source of error in both the model itself and its fidelity.   The reuse of software applications (such as data collection tools) has implication that they work properly.  Usually simulations can be modified for an event. Software applications tend to be pre-packaged so that even if a flaw is found, there may be no way to have it corrected.Prepping for V&V AssessmentBefore beginning to extract the V&V rigor of the development of an event federation, identify the federation developer/integrator, sponsor, and user.  Find out who will make the decisions for how the federation will be built and tested.  It will be difficult to assess the V&V of a software development process if there is no access to it.Next find the objective of the event and find the intended uses of the federation.  Look to see if the intended uses support the objective.Understand the layers of development.  There may be development at the levels of event federation, capability federation, federates, software applications, and data.  See what V&V activities are happening at each of these layers.With layers of V&V, it will be necessary to develop relationships across the federation community.  Since there may not be a true chain of command, one needs to convince others that the V&V team is there to help (and the team needs to be able to help them).  The auditing should look to see what V&V is happening rather than demanding extra tasks, especially if those tasks do not support the essential idea.Indicators of V&VAs auditors, the V&V Assessment Team needs to know whether the V&V of the process is of quality.  The real issue is whether there is sufficient evidence that adequate V&V rigor has been implemented and pursued for the development of the event federations.This paper suggests a few indicators by attribute for determining if an organization is carrying out a proper V&V program.  Key word(s) of each attribute are emboldened.(1) Indicators of proper planning:There exists a clear path from the present state to the desired end state.Arrangements have been made to qualify the federation.(2) Indicators of coordination:The developer can identify the customer and the federation developing/integration organization.Developer and customer are resolving issues.There exist agreements between the developer and customer. (3) Indicators of understanding of objective and intended use:The development activities support the objective and intended use.The customer and developer agree on the objective of the test and the intended use of the event federation.(4) Indicators of understanding of requirements:The developer can specify the data needed and state the capabilities needed to produce that data.The requirements are traceable to actual data needs of the event.(5) Indicators of agreement of conceptual model and of adequate design:Consistent schematics of the conceptual model.Both the developer and customer know the assumptions and limitations of the federation.Fidelity, data collection, capabilities, and interfaces are addressed.(6) Indicators of good software engineering:Proper tools are used and adequate processes are implemented.Good configuration management is implemented.(7) Indicators of adequate documentation:The process is clearly explained.Documents of different levels are harmonized.At this time, the author cannot suggest an exact measure of the V&V rigor of a development process.  However, tracking V&V issues and ensuring that mitigation plans are made and implemented will reduce the developmental risk. Extracting V&V RigorWith preparation and with indicators in mind, activities to extract V&V of the federation development can now be planned.  This section lists the defining question for each attribute followed tasks that the author believes should be undertaken to answer the respective question.(1) Does the federation have proper planning?Find the simulation development plan and whether it is being followed. See what V&V activities are included are within the development plan.  Find if there is a formal V&V plan. See if the V&V plan has relation to the development process.(2) Is there coordination between the developer and the customer?Look for an agreement on the intended use between the developer and customer.  Find what the developer knows about the customer, about who will apply the simulation, and about who will use the data.  Find how the developer and customer are communicating about the development efforts to include requirements and conceptual model.  (3) Are both the objective of the test event and is the intended use of simulations within the federation understood?Compare how the customer and developer describe the objective of the test event and the intended use of the federation.  See how the intended is use is understood among the different layers of development.(4) Are requirements and capabilities complete?Find how representations and data collection are being addressed.  Find how changes and refinements of requirements are handled.  Find if there is cross reference between specifications and requirements.  Find if specifications change as requirements change.  Find whether requirements trace to actual needs and not to “nice-to-have” wishes.(5) Is the conceptual model consistent and non-contradictory?  Does the design track to requirements?See whether description of the federation is consistent among documents, briefings, and interviews.  Find if there is a formal definition of the federation (such as a Venn diagram, UML listing, or some logical view).  Find how the customer and developer understand assumptions and limitations of the federation.Find whether all representations are included and of correct fidelity.  Find whether the data collection will handle output needs.  Find if the Federation Object Model (FOM) addresses data and functions needed, and if the interfaces are adequately defined.(6) Is sound software engineering being used?Find what software engineering processes and tools are being used.  Find the methods being used to test quality of the federation.  Find what configuration management has been implemented.(7) Is there adequate documentation?Find what documentation is available.  Find how the development process is explained.  Find how intended use, requirements, and conceptual model are harmonized.Assessment of the V&VThe assessment of the V&V of a federation development process must be continuous and reported in a timely manner to those that can affect the development process.  Since issues for each attribute can be listed, the customer  will have a grasp of the development status.  This will allow the customer to make more objective decisions in resources dedicated to the federation development.Summary of V&V ApproachThe approach to V&V presented here is one of assessing whether the development process of an event federation contains a V&V rigor.Test event federations are built to collect data to aid in making decisions.  These federations may need simulations to produce part of this data.  Evaluators must be ensured that those simulations will produce correct data.Since the essential idea of V&V is building the right thing right, V&V must work on the federation development process itself.Even if there is no formal V&V program or if the development process is not well defined, one can still assess V&V.A development process has a V&V rigor if it shows the attributes of proper planning, coordination, intended use, requirements, conceptual model, design, engineering, and documentation.References “Key Concepts of VV&A”, Defense Modeling and Simulation Office (DMSO) Recommended Practices Guide (RPG), 2006.Management of Army Models and Simulations, AR 5-11, Department of the Army, Washington DC, 2005.Online M&S Glossary (DoD 5000.59-M)” Defense Modeling and Simulation Office website, (https://www.dmso.mil/public/resources/glossary/)Pierson, Jeffrey L., “An Approach of ‘V&V’ for Models and Simulations (M&S) Development Program”, Science and Technology Corporation, Edgewood, MD 2007.Author BiographyJOSEPH OLAH is a Systems Engineer/Analyst at the Science and Technology Corporation in Edgewood, MD.  He is member of the Future Combat System (FCS)’s Modeling & Simulation Office (MSO)’s Independent Verification & Validation (IV&V) Team.  He has over twenty years experience as a programmer-analyst for the Department of Defense that has included V&V support to the Army Materiel Systems Analysis Activity (AMSAA) and accreditation support to the Army Evaluation Center (AEC).