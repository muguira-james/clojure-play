Comparing Synthetic Natural Terrain Elevation Using Image Processing State Consistency and Event Integrity MonitorDavid L. FisherTerry S. McDermott, PhDJames L. BryanMonica A. JanesNorthrop Grumman Information Technology12000 Research Pkwy, Suite 236Orlando, FL 32826407-243-2010, 407-243-2026321-235-3896, 407-243-2009, 407-243-2017david.fisher@ngc.com, terrence.mcdermott@ngc.com, james.bryan@ngc.com, monica.janes@ngc.com Keywords:latencyterrain, monitordatabase, networkcomparison, performanceimage processing ABSTRACT:  Variation in terrain database elevation among participating systems in a distributed simulation can cause many unwanted side-effects.  Ground threats or targets can be buried or may float above the terrain; aircraft may fly through mountains; intervisibility differences may result in detection advantage to one participant over another; missiles can fly out of a hill to attack an unsuspecting aircraft. Most simulation practioners recognize that databases over the same geography may differ in particulars of terrain content and elevation. However, there are remarkably few available current tools to understand or quantify or compare the situation for specific databases.  This paper describes activities undertaken recently by the Air Force Distributed Mission Training Operations and Integration (DMT O&I) Contractor to analyze and compare several training system elevation databases and source data. The approach taken was to bring elevation data from various databases into a common form and resolution, and then to analyze and compare them using image processing and display techniques.  The ready availability of low cost and open source tools for image based analysis facilitated this methodology. Databases explored included both Computer Generated Forces (Threat Environment Generator) terrain and elevation post data extracted from visual database polygonal terrain skin.  Comparison with underlying DTED source was performed as well.  By conducting these studies, the DMT O&I was able to quantify differences and make database variation visually evident.  In addition, the basis for exploring variability in line of sight among databases has been established.In forming federations of distributed simulation systems whose internal protocols vary between DIS and HLA, some form of gateway may be used to resolve protocol differences.  With a system of gateways in place, they may take on additional responsibility.  For example, they might facilitate network utilization optimization or provide support services to the distributed simulations, like protocol filtering or entity state dead reckoning.  To perform these additional functions, the array of gateways must internally maintain some level of simulation state.  This implies that data packets may no longer maintain their originating identity or uniqueness as the information is transferred from one simulation system to another through the system of gateways.  This being the case, direct comparison of message content cannot be used to evaluate whether state information is maintained consistently across the simulation systems or to determine network performance issues such as latency or lost data.  This paper proposes an approach to developing a tool to overcome this problem, which we call “The State Consistency and Event Integrity Monitor”.  By continuously monitoring elements of the simulation’s entity state data at distributed sites, time-synchronized state data can be collected, compared and evaluated outside the context of the ongoing simulation.  The results can provide for the early detection of simulation, network performance, and latency issues or detecting negative training issues; and allow corrective or investigative action to be carried out while a distributed simulation exercise is occurring rather than trying to troubleshoot it after the completion of the simulation.  Software located at each simulation site will “sniff” the local network to determine critical information such as scenario entity positions and events.  This information is then sent back to a centralized “Master” state monitor program.  By comparing the collected data as a function of time among the sites to the ground truth (from the data origination site), the Master can alert simulation site engineers through a variety of alarms that anomalous conditions have been detected or that certain thresholds have been exceeded.  Analyzing the distribution and the type of simulation state error data, the troubled area can be isolated, or at a minimum, the search for the troubled area can be narrowed.  Corrective action can then be taken with reduced impact to the distributed simulation exercise.1. IntroductionThe Air Force’s Distributed Mission Training Operations and Integration (DMT O&I) program is leading the effort to establish standards for interoperability The concept for this tool is to provide distributed simulation event run-time monitoring through comparison of entity state and event interaction data among participating sites.  In forming federations of distributed simulation systems whose internal protocols vary between DIS and HLA, some form of gateway is often used to resolve protocol differences.  With a system of gateways in place, additional capabilities can be provided to support the execution of the distributed simulation.  For example, the gateways might provide the ability to optimize network utilization or provide support services to the simulations, like filtering and dead reckoning.  To perform these additional functions, the complex of gateways must internally maintain some level of simulation state.  This implies that data packets may no longer maintain their original identity or uniqueness as the information is transferred from one simulation system to another through the gateway system.  This paper assumes that all state models within the simulations are equivalent (errors are not due to the model) and that the cause of any of the issues described subsequently are not model-based.  that will enable routine, distributed, multi-platform mission package training on the Distributed Mission Operations Network (DMON) for many and varied Air Force simulator training systems. Among the many interoperability challenges in this endeavor are those associated with differences in modeling of terrain over the same areas.  The systems are being developed individually, over many years, and often are adaptations of legacy systems.Differences in terrain modeling include differences in resolution (the detail of represented objects), difference in terrain shape (elevation, morphology), differences in terrain content (objects visible in the terrain), differences in data representation (e.g., elevation “posts” versus polygonal terrain skin, photorealistic imagery versus geo-typical terrain cover, and 3D models versus only 2D imagery); and difference in overall shape and relative locations resulting from different choices of projection or round-earth representation.  Content, detail and representation, if decided in isolation, are usually dictated by the functional and training demands to be met by the data, and the budget and time available to build the terrain model.  It is not surprising, then, that differences will be evident in virtually any two independently developed terrain databases over the same geographic extent.Consequently, one can expect terrain data in simulation systems to vary in one or more of the aspects listed above.  To recognize, identify and understand these differences and then to minimize resulting training impact is one goal of the O&I contractor.  Another goal is to try to identify ways through standards to minimize the variability – and hence training impact –  going forward as new systems are built and legacy systems are upgraded and replaced. Also, we are using the term state data in a very general sense; that is, it refers to the state of any simulation data.  For example, it might be voice communication data or the positional data of an aircraft.2. ObjectiveToward the initial goals of recognizing, identifying and understanding the variability faced in existing systems, the O&I undertook comparison of several sample databases.  The specific goals were twofold.  First to identify available procedures and tools that enable a useful analysis, and secondly to get some preliminary insight into real databases pertinent to the Air Force distributed training systems.  This paper describes some of the tools and procedures identified and samples of the results obtained.  Samples from two fighter threat environment generator databases, the JCATS simulation (used to provide the tactical environment for the Joint Stars simulator),  source DTED and a sample OpenFlight database provided by AFRL in Mesa, AZ were acquired, compared and contrasted.  All samples were subsets of the general Nellis Range area.  The examples shown in this paper all come from the geocell (1 degree by 1 degree) whose southwest corner is 117 West longitude, 37 North latitude.Many readers may be aware of two tools that were not used in the effort being reported and may wonder why. The two tools are the Side-by-side Viewer, which was developed by AcuSoft, Inc., in association with the development of SEDRISDue to the likely lack of one-to-one correspondence of the state data between the originating simulation site and other participating sites, direct comparison of data messages cannot be used to evaluate whether accurate state information is continuously maintained across the simulating systems.  The inability to directly compare data may be directly related to gateway functions such as protocol transformation, dead reckoning or filtering, network latency, or software restrictions.  Elements of the simulation’s entity state data at the distributed sites can b, and an older toolset, ZCAP, edeveloped at the UCF Institute for Simulation and Training under the auspices of STRICOM. ZCAP was developed in the mid-90’s on Unix, using X-windows, and has some features that could be used to perform some of the analyses reported below.  However, at the time of this effort, it was not readily available to the author, nor was a suitable platform on which to execute it.  Side-by-side Viewer is readily available from the SEDRIS web site.  Its focus is more on consistency of feature content, and, as its name suggests, systematic traversal of 3D worlds on a visual basis. Because of the exploratory nature of the efforts reported here, we decided that more generic tools would be easier to bend to our will and adjust to specific needs as they arose.						 continuously monitored while time synchronized state data is collected at each of these sites for comparison and analysis outside the context of the ongoing simulation – but potentially concurrent with the simulation execution.  This allows for the early detection of network or simulation problems or for the detection of negative training issues, providing the ability for investigative or corrective action to be carried out while a distributed simulation exercise is occurring rather than canceling it or trying to troubleshoot it after the completion of the simulation.3. Technical ApproachThe basic approach pursued by our study was to extract/convert the elevation data from each subject database	to a common format that could be analyzed using image processing and display tools, and then to apply appropriate techniques to provide both basic quantitative and visual comparisons.  An effort was made to limit software investment and to keep an eye on generalizing the tools and procedures used to provide a toolset applicable to routine database exploration and analysis and also database troubleshooting and scenario verification. While image processing requirements for the effort were generally basic, tools were needed that would support a range of image formats and be capable of producing geo-referenced, registered, layered displays.  In addition, to make processing of large areas routine, the tools needed to be designed for this purpose or incorporate scripting or a programmers API.  We selected the Python scripting language for its development simplicity and rich complement of available tools for processing imagery and geospatial data.  The other main tool used was a low-cost raster geographic information system (GIS), Manifold ( HYPERLINK "http://www.manifold.net" http://www.manifold.net ).  The latter was initially used mainly for display, but with familiarity, we have found that nearly all (but not all) functions we wanted to perform could be handled within this single tool.  All computation was conducted on a Windows 2000 Dell Latitude laptop computer with a 2Ghz processor and 1 Gigabyte of memory.  To avoid potential delays from technical problems that might arise if we tried to manipulate data sets that were too large, we conducted most analysis over areas of from one to four geocells in size.  One exception was to prepare a color-coded relief map of most of Korea using Manifold, which consisted of twenty-five geocells.Elevation Grid DevelopmentUsing Python, we developed utilities to read and convert the elevation data from the threat generation system databases and JCATS database to ArcInfo ASCII Grid (AAGRD) format.  This format was used because of its simplicity, ease of debugging (you can easily read and verify the values), and wide-spread support in available freeware and commercial tools.  The few extra seconds to read and write files was a small price to pay for ease of use. It is also notable, as an example, that an 11.275MB AAGRD file for a geocell of elevations at 3 arc second spacing (same as Level 1 DTED) compresses using WinZip to 1.762MB.  This is significantly smaller than the corresponding DTED file of 2.835MB.  Hence, the use of text representation is not really so bad – particularly for experimental projects – in the current desktop computing environment.  .	OpenEV ( HYPERLINK "http://openev.sourceforge.net/about.html" http://openev.sourceforge.net/about.html), a freeware display, manipulation and format conversion tool developed by Atlantis Scientific, was used to read NGA DTED files and write the data to AAGRD format.  Manifold can also serve this function, but we had access to OpenEV first.  Finally, the DART option for Terra Vista (by Terrain Experts) was used to produced DTED-formatted elevation data from the OpenFlight polygonal terrain provided by AFRL This data was converted to AAGRD in the same way as National Geospatial Intelligence Agency (NGA) DTED.	A couple of details should be mentioned.  In some cases rescaling and re-projection was required to bring the data into a common reference frame and resolution.  Specifically, JCATS elevation is projected to Universal Transverse Mercator (UTM) while all the other data obtained was natively in a latitude/longitude grid.  Also, the two threat Generation system databases used different elevation post spacing.  We used Manifold to re-project the sample JCATS grid to latitude/longitude using bilinear interpolation. Also using Manifold, we down-sampled to the lower resolution and also up-sampled to the higher resolution for comparisons.   Finally, the JCATS sample data was at 35 meter spacing, which we simultaneously re-sampled to 3 arc seconds as part of the re-projection step.While it took a while to get all these steps identified and working, none of the processing took more than a few minutes for a geo-cell (1degree x 1 degree) with the exception of the extraction of DTED formatted elevation from OpenFlight.  For this operation, it took DART about 5 hours to import the OpenFlight, but only a minute or so to generate the DTED.  Subsequent discussion with Terrain Experts suggests that the import process can be speeded up significantly with some DART import settings that would take advantage of the fact that textures could be ignored for our purposes.  At this time, however, we have not attempted to verify this.The study being reported was experimental, and it focused on just a few geo-cells. However, our conclusion from this experience is that it is quite feasible to develop routine procedures for extracting and converting elevation data to a common format from a variety of  source databases using readily available COTS tools, most at minimal, if any, cost.   The main development effort was the necessarily custom code to convert the simulation system databases to a common, image-based format.Grid AnalysesAs data files, there is essentially no difference between an image and an elevation grid, other than in the interpretation of the information contained in the data.  Both are (for single band images) arrays of numeric data stored in some format.  Even though the interpretation of images and elevation data is quite different, image display techniques can be useful in comprehending the spatial organization and variation in elevation.  Using Manifold, we were able to easily display the various elevation data superimposed on each other.  Manifold actually distinguishes between elevation grids (called surfaces within Manifold) and images, but allows grids to be converted to images.  Using transparency and flickering (rapid alternating display of the two overlaid images) we can develop a strong impression of the qualitative difference between two different elevation grids over the same area.  Figure 1 shows the display as a grayscale image of a single geo-cell (Southwest corner at 117W37N) of Level 1 DTED within the Nellis Range area.  It should be regarded as a map in which whiter corresponds to higher elevation, darker corresponds to lower elevation.Figure 1: DTED as ImageUsing the fact that the image represents elevation data, lighted shading can be used to enhance the display providing a stronger sense of elevation relief.  This effect is shown in Figure 2.  Figure 2.  DTED in grayscale reliefThere is no change in the underlying data, only in the way the data is rendered into a picture.  Other renderings may  help to accentuate – or mask - features and/or provide visual appeal.  Figure 3 shows the same relief map rendered with a color table applied that provides more the look of a typical relief map.Figure 3.  Colorized DTED with shadowingOne of the advantages of digital display, or course, is the ability to zoom in and examine details.  Figures 4a,b and c  show a small area at the top center of the same geocell for the DTED, and the two threat environment generation databases.Figure 4a. DTED close-upFigure 4b. Threat Generation System A close-upFigure 4c. Threat Generation System B close-upThe differences as well as similarities are pretty evident qualitatively.  When viewed in a dynamic environment, in which these images can be overlaid and selectively “flickered,” the comparison shows that the general registration is quite good.  However, even in Figure 4 the differences in detail  are evident to the eye.  The blurry quality of Figure 4b results from the fact that Threat Generation System A used a very coarse resolution data (posts at an interval of just under 400 meters).  The lack of information between the posts results in the loss of feature content upon interpolation.  Figure 4c in contrast is fairly clear (100 meter posts), but the faceting of the underlying polygonal surface with which it correlates is quite visible.  The need for correlation between visuals and threat environment elevation apparently led to a derivation process the result of which reflects this visible polygonalization of the terrain.The point of these comparisons is not that one database  is better than another. Rather, it is that the decisions made in design that lead individual simulators to different choices in establishing elevation databases inevitably result in differences that are not only visible, but quantifiable.To get a more quantitative look at the differences observed, we wrote a utility that would compute difference grids, their statistics and histogram of the grids.  Figure 5 shows the difference of 4b minus 4c with color changes at selected thresholds.  The coloring assignsFigure 5. Difference image with thresholds applied black to any differences less than or equal to –100, blue for –100 to –2, red –2 to 2, green 2 to 100, light brown 100 to 200, and dark brown greater than 200. A few conclusion jump out:Approximate equality ((2) is relatively infrequent 4c is above 4b (blue) in flatter terrain4c is often below 4b in the mountainsThere are more large differences than you might expectBy performing a basic statistical analysis on the difference image, we can quantify the variation seen in the image itself.  Specifically,  we find that the Mean is  negative 6.391 meters and Standard Deviation is positive 22.812 meters.  Note that a difference of six meters is more than enough to cause nearly any vehicle generated in one threat system to be “buried” if not artificially re-located to the ground (ground-clamped) when represented in the other.The histogram of the Figure 5 difference image is shown in Figure 6.  While about 80% of the differences were within a fairly small interval around the mean, even these modest differences can cause visual or performance anomalies.  Perhaps more troublesome is that consistently modest differences are confined to relatively flat areas.  This suggests that special care may be required to ensure consistent simulation behaviors when the action takes place in mountainous areas or their foothills.It is worth mentioning that all the images shown here were produced using the Manifold software.  For this level of examination and comparison, it proved to be quite versatile and fairly intuitive tool.  Of course, there are many other tools available in the market today that would allow the same kind of graphics to be prepared.  The main point is that powerful insights are potentially available with minimal effort with the easy availability to such tools.Software located on the gateway (or at least in a computer at a network location logically equivalent to the gateway relative to the local area network) at each simulation site would “sniff” the local network to gather critical simulation information such as scenario entity positional and event data.  This information would be packaged and sent to a centralized “Master State Monitor” site.  Applying user defined threshold values and ground truth data (data from the site originating the simulation data),; the master site would analyze and compare the collected data.  If an anomaly iswere detected, the master site would alert simulation site engineers through a variety of alarms that an anomalous condition has been detected or a certain threshold has been exceeded.  By analyzing the type of simulation error and the routing of the data through the wide area network, an engineer can troubleshoot the problem to isolate and correct it to reduce the impact to the distributed simulation exercise.To initialize the monitoring, the master site would distribute, start and control an intrusive monitoring agent to each participating site in the simulation prior to starting the distributed simulation.  This agent would package all or part of the state data and send it back to the “Master State Monitor”.  The Master State Monitor’s purpose is to gather data from each of the distributed agents to filter, analyze and determine if there are any apparent network or simulation problems.  The master would in turn display the analysis data within a user friendly single window (for easy detection) graphical user interface (GUI) and provide a visual and audible signal if any network issues are discovered or threshold values are exceeded.  Utilizing the comparison of state data from all the participating sites against the collected ground truth data, and based on subsequent analysis, probable causes for network or simulation issues can be determined and displayed.  Comparison of state data with ground truth requires that all the collected state data be synchronized in time, to allow analysis to be made as a function of time of each sites state data when compared to the ground truth data.  In a perfect system, each site’s state data should map to the originating ground truth data as a function of time.  In addition to providing the analysis data and warnings, possible corrective actions can also be suggested.It is unlikely that there will be a desire to collect all state data and to monitor it continuously.  This would heavily tax the bandwidth of the local area and wide area networks with all the data being sent to the master in addition to the primary simulation data.  The storage capabilities of the master site would also be challenged.  A more realistic scenario for this tool would be to monitor selected state data generated by each site and only periodically transfer this data to the master for comparison and analysis.  Which data is monitored could be chosen during the event planningevent-planning phase or before the running of the event – or even interactively during the event.  An alternative would be to have the tool automatically select which data to monitor.  This selection can be done randomly or by use of artificial intelligence.  The selection of which state data to monitor can be allowed to change dynamically by the artificial intelligence software to match the needs of the system, to optimize bandwidth usage efficiencies, or in response to previous monitoring results.  This dynamic selection of acquired data can be implemented to better pinpoint arising network or simulation issues.  Since data is only collected periodically and sent to the master, bandwidth usage should be minimal, as should CPU consumption by the agents although the agents would normally be on their own computer.Certain monitoring and sampling parameters will need to be established.  The sampling period for comparison and analysis may be chosen during event planning or the software can be programmed to automatically make the selection.  As for the type of state data selected for monitoring and comparison with ground truth data, the data type can be predetermined and manually established by the master site, or the data type can be dynamically selected using artificial intelligence techniques to help match the needs of testing or to help pinpoint arising network or simulation issues.  In addition, sampling rates may vary directly with the rate of change of the state.  For example, an aircraft flying at 500 knots requires sampling at a much higher rate than a tank traveling 10 MPH.The monitor software should be able to monitor, record, and alert for simulation issues such as fair fight and negative training during runtime.  For example, shots are taken at, but miss an aircraft being simulated at one site, but according to the aircrafts positional data at the site where the shots are taken, the shots hit.  Assuming the models are correct at each of the sites, this fair fight and negative training issue might be caused by latency and/or dropped position data messages.  However, without monitoring, the trainers or participants may never know about these issues during runtime and have a false impression of actual capabilities based on the limitations of the distributed simulation environment.If during a training event these issues arise, there are several possible courses of action.  An attempt can be made to correct the problem during runtime without stopping the event or pausing the simulation to attempt to correct the problem.  If the problem cannot be corrected immediately, the decision might be made that it is not critical enough of to stop the event.  In the worst case, if the problem is of a critical nature, the event might be terminated with the loss of training opportunity and the dollar cost of the time spent on planning and executing the simulation.  In addition to the runtime analysis and comparison of state data, the agents and master can be used to help validate the distributed simulation environment and pinpoint issues by running other tests.  These tests, for example, might include ping tests from the tool agents and master to the other agents and gateways within the simulation to check for latency or could include scripted system configuration tests that can be run prior to the simulation event to ensure that every site has the same radio simulation settings.The master and agent software would be designed to:Use a combination of test methodologies to track down potential network or simulation issues.Use ping tests between gateways and test computers to separate (?) network from gateway issues.Collect workstation data to determine internal, individual computer issues.Use threshold values to alert engineers of out-of-range data.Collect network data for statistical and analytical purposes.Figure 1 presents a diagram showing tthree simulation sites sending state data to the “Master” for analysis and comparison.  The ground truth originates from the site simulating that state.  The following examples illustrated how this tool may be used to track down several potential distributed simulation problem areas.Example 1:  If the timing of identical positional state data of a simulated aircraft received at all other simulation sites is 500 ms behind the original data at the simulating site (ground truth data), the master could deduce that the latency problem is due to an issue with the gateway or the network at the site simulating the aircraft.  Additional ping tests between the sending site gateway and the other participating sites would then be used to help further delineate between network issues and problems associated with the originating gateway.Example 2:  If the positional state data of an aircraft being simulated at each site have negligible latency from all the originating simulation sites except for one, at which all the state data is 500 ms behind the ground truth data, the master would suspect the cause to be anomalies with the network or the gateway at that specific site.  Again, the use of ping testing from all the test sites to all the others could be used to further isolate the problem.Example 3:  In order to reduce bandwidth utilization, one of the expected functionalities of the gateways is to provide dead reckoning of entities.  This allows the extrapolation of some state data such as position, and reduces the need to continuously update this data across the wide area network.  Updates are only sent when changes from the dead reckoned positional data occur.  If the positional state data for a particular entity deviates from the ground truth at one site, then periodically and suddenly corrects, the master would suspect the cause to be intermittently dropped data packets causing a deviation from the dead reckoning positional data at that gateway as the cause. EMBED Excel.Chart.8 \s                                       			Figure 6.  1.1 Frequency The State Consistency and Event Integrity Monitor ArchitectureHistogram for terrain height differences3.3 Height from TerrainThe data for Threat Generation System B was developed to be consistent with the terrain elevation for the corresponding aircraft’s visual display.  The evident faceting in the data is an artifact of this, we suspect.  It makes sense that if a relatively coarse polygonal structure for the visual terrain were required to keep polygon count down to ensure sufficient Image Generator performance, then elevation data closely consistent with. The interpretations of the comparison and analysis results will depend upon the specific functions provided by each particular gateway.  Therefore, the resulting suspected cause of problems will be a function of the specifics for each gateway, which will require tailoring of the analysis for each particular application.  In order to tailor the comparisons and analyses, using the master application, the simulation engineer would define a threshold value for the position of each type of entity, fixed wing aircraft for example.  The master would determine if the aircraft was within its designated location parameters and notify the simulation engineers by alarms on the GUI if it is off course.the terrain would reflect the polygonal structure.  Part of the interest in demonstrating the extraction of height data directly from polygonal terrain models was simply to demonstrate that it is quite feasible using commercial software.  This objective was accomplished as described in Section 3.1 using Terra Vista/DART.  Another reason was to use that capability to compare the heights between databases.  Since only the one visual database was available to us, we compared the heights obtained both with the source DTED and with the Threat Generation System B data.It turned out that the height data extracted from the AFRL-provided terrain model was visually so close to the DTED that it was hard (though not impossible if zoomed in enough) to discern differences.  For that reason, no image is included here.  As shown in Figure 7, the statistics of the difference between this data and the source DTED were still significant. However, the differences were much smaller than typical differences measured for other cases.  The mean difference was 0.777 meters with a standard deviation of 3.389 meters.   EMBED Excel.Chart.8 \s Figure 7.  Histogram of heights extracted from polygonal terrain model minus the source DTEDA few observations about Figure 7 deserve note.  First, the standard deviation is quite small compared to that in Figure 6, and compared to many other differences computed during the investigation being reported. Second, the range is significantly tighter.  Third, the general shape of the distribution is remarkably similar – with 75%-80% of the data within a standard deviation of the mean.  This similarity of distribution shape persisted throughout our investigation. One importance of this last comparison is that it demonstrates the technical possibility of quite closely reflecting  the underlying source terrain heights in a polygonal terrain model using off-the-shelf tools.  Almost surely the price you pay for this, however, is dedicating a significant portion of your polygon budget to the terrain surface.  Whether achieving a closer match, but still not really close is worth the polygon cost is a trade-off faced in the database design.  At a minimum, the technique described here can be used to analyze the product of terrain construction with respect to preservation of heights and evaluate the potential impacts early in the construction process.   Recognizing the differences might aid in other design and performance decisions along the way. The differencing and statistical operations used here are available in varying forms in several of the available COTS and freeware/open source tools.  Our choice to develop utilities for these purposes was one of convenience partially determined by the order in which we gained access to existing tools.3.4  Other Techniques3.4.1 Viewshed  AnalysisIn addition to the techniques already described, we performed some additional analyses which might be useful for others to be aware of.  One of these is called viewshed analysis.  Viewshed algorithms calculate the area that is visible within the terrain from a specified eye-point, usually assuming straight-line line-of-sight.  There are many algorithms available in the literature, as well as comparison of how the results from various algorithms differ.  Some take earth curvature into account, for example, while others may not.  Viewshed analysis can be beneficial for many land-based activities such as deciding where to site radio communications towers, deciding where to place look-outs, selecting hidden locations, etc.  For airborne platforms, terrain masking, either visual or from radar, may be analyzed.  Viewshed analysis is less frequently found in free-ware, open source or low-cost tools.  One freeware tool with a seemingly good selection of viewshed options is Microdem/TerraBase II, version 6.03.  Unfortunately, the import options available in Microdem were not easy for this study.  Happily, Manifold recently added a viewshed capability that is pretty easy to use, though does not have a lot of algorithm options.  Running the viewshed analysis on our laptop over a geocell takes just a very few minutes, so it is not too costly to experiment with this capability.  The visual results can be overlayed to produce some interesting and useful graphics. An example of a viewshed overlay on an elevation image is shown in Figure 8.Figure 8. Comparative ViewshedIn Figure 8, a small patch of JCATS elevation data is displayed as an image within the context of a larger Threat Generation System Database (the pale blue and gray areas).  Layered on top of the elevation image are the results of two view-shed analyses.  The blue areas show the areas visible from the indicated viewpoint using the Threat Generation Terrain.  The superimposed green areas are visible from the same point within the JCATS world.  What this display clearly shows is that there are substantial foothill areas (those that are blue but not overlaid by green) where a JCATS ground threat entity is vulnerable to visual detection from a Threat Generation System airborne entity though it cannot “see” the airborne threat.3.4.2 FilteringFiltering is another image processing technique that is widely used for manipulating images and array data.  One class of filters that proved to be useful is used in studying landforms.  These are filters that seek out local maxima or minima within in an image or array of data.  This class of filters does not seem to be a part of most common image processing packages targeted at commercial users, where the focus is on visually interesting transformations.    One advantage of using Python for the limited programming done for our experiments is that the Python Numarray module (available freely, of course) has a nice generic filter routine that makes it very easy to write custom filters.  Most of the messy work is done by the provided module leaving essentially only the actual computation logic for the experimenter to complete.  Using Python/Numarry, we developed a filter that will generate an image of the same size as the input image with a nonzero value only at the locations that are larger than all the surrounding values.  Surrounding values are those in a rectangle of pixels centered on the location.  We used 3x3 and 5x5 filters.  We were looking for correlation between the high points within the various datasets.  Figure 9 is a sample showing high points based on a 3x3 filter.  The scene is from the area in the upper central part of that shown in Figure 4.Figure 9. Sample high points – close-up Fig. 4The red dots are high points in the original DTED.  The yellow and blue are high points from the two threat generation systems databases.  The fact, already mentioned, that one of the databases is coarser than the other explains why the yellow “dots” are larger. In general, the correlation was very weak – based only on visual assessment.  One can see some correlation along sharp ridges and peaks, but otherwise, it looks quite disorganized.4.0 Summary and ConclusionDue to the expense and effort associated with coordinating and executing large distributed simulations, it is important to identify issues that might produce negative training impacts or affect lower the training effectiveness of distributed simulation training events.  If these issues are caught and isolated during the training event run time, corrective action can be taken to prevent cancellation or negative training.  Due to the utilization of gateways and their functions such as maintaining simulation state, there is currently no ability to provide a direct comparison of data messages to determine run-time network or simulation anomalies or issues.  The alternative is to develop a tool that uses indirect comparison as described above to quickly and cost effectively provide for runtime monitoring of distributed simulation training events.One class of problematic issues relates to variation in elevation among the terrain representations of the simulations participating in a distributed event.  We have presented here a report of some activities at the DMT O&I conducted to gain insight into terrain height variation among diverse simulations.  The techniques employed were based on generally low cost or open source (e.g., Python, OpenEV) tools that enable the manipulation of gridded data sets as images.  Using these tools, we were able to easily convert source data sets into a common, geo-referenced image format supported by most commonly available tools.  Examples were given here illustrating the use of image display, colorizing, overlay and image analysis as well as some specialized analysis techniques such as viewshed analysis and filters.  As a result of this work, we are better able to positively identify the magnitude of the potential impacts of varying terrain height and potentially analyze specific concerns with concrete computation rather than speculation.  Feasibility of a low cost, easy to use set of tools for this sort of analysis was demonstrated.Author BiographiesDR.  DAVID L.  FISHER is a software developer for Northrop Grumman Information Technology in Orlando, FL.  Since receiving his doctorate in Physics from the University of Texas at Austin in 1995, he has worked in the areas of theoretical plasma physics, laser-plasma interactions, laser wakefield acceleration, the detection of explosive materials, millimeter wave sensor technologies and applications, and in the development of an Internet business.  He is presently developing several software tools.  David has 20 professional publications and 2 patents (1 pending).DR.  TERRY S. MCDERMOTT is Senior Scientist for the Constructive/Virtual Simulation Technology and Training Department of Northrop Grumman Information Technology in Orlando, FL.  He hasDuring  over 220 years of experience in the aerospace/defense industry, Terry has been involved in systems engineering and simulation including system and software definition and development, system performance simulation, software design, realtimereal-time and constructive simulation, protocol development, standards development, and project management.  Currently, Terry is Lead Engineer for the development of the Synthetic Natural Environment Standard for the Air Force Distributed Mission Training Operations and Integration (DMT O&I) effort.  contributing to development of systems for visualizing tactical situations, training exercise planning and coordination, and distributed test environments.  Terry received his doctorate in mathematics from the University of Southern California specializing in Functional Analysis.JAMES L. BRYAN is a Systems Engineer for Northrop Grumman Information Technology in Orlando, FL.  He is the lead for Federation Integration and Test, and is responsible for standards development, systems analysis, and test/user tool development for the USAF Distributed Mission Training (DMT) program.  He has 8 years of experience in distributed simulation integration and test and simulator system development and design.  While on active duty with the USAF as a combat search and rescue helicopter pilot, he was the Test Director for the OSD sponsored JCSAR JTE’s Virtual Simulation program.  Jim has four professional publications and one patent pending.  MONICA A. JANES is a software developer for Northrop Grumman Information Technology in Orlando, FL.  She has over 7 years of experience in designing and developing all aspects of software including test descriptions, product specifications, and software integration.  Since receiving her Bachelor of Science degree in Computer Science from the University of Central Florida in 1996, she has developed several simulation software tools to provide visual awareness, data analysis, and common distributed infrastructures to the training world.SnifferSite 2 Ground TruthSite 3 TruthSite 1 TruthSnifferGatewaySite 1SnifferGatewaySite 3GatewaySite 2AlarmRun-TimeStateMonitorMaster