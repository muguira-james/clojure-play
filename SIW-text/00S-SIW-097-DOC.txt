Development of RTI Evaluation Tool and Application to Redstone Technical Test Center’s IR Tactical Missile Simulation  Gina P. OesterreichJulian A. ChernickHeidi W. PensellLogicon, Inc.1200 Technology Drive, Suite AAberdeen, MD  21001410-297-6966 HYPERLINK "mailto:goesterreich@logicon.com" goesterreich@logicon.com,  HYPERLINK "mailto:jchernick@logicon.com" jchernick@logicon.com, HYPERLINK "mailto:hpensell@logicon.com" hpensell@logicon.com	Kenneth G. LeSueurRedstone Technical Test CenterRedstone Arsenal, AL 35898256-842-2017 HYPERLINK "mailto:klesueur@rttc.redstone.army.mil" klesueur@rttc.redstone.army.milKeywords:HLA, RTI performance, Benchmarks. ABSTRACT:  The U.S. Army Development Test Command (DTC) contracted with Logicon to develop a Run Time Infrastructure (RTI) Evaluation Tool to evaluate multiple implementations of the High Level Architecture (HLA) Interface Specification, 1.3. The RTI Evaluation Tool effort focused on Redstone Technical Test Center’s IR Tactical Missile simulation. Defense Modeling and Simulation Office (DMSO) Benchmark programs were evaluated for applicability to the RTI Evaluation Tool and, wherever possible, modifications were made to the Benchmark programs to tailor them to the requirements of the RTI Evaluation Tool. RTI 1.3NG was evaluated using the RTI Evaluation Tool. This paper describes the RTI Evaluation Tool effort. The performance metrics of interest and the methodologies for obtaining these measurements will be examined. A discussion on the development of the RTI Evaluation Tool and the applicability of the DMSO Benchmarks software is provided. Preliminary test results are described for RTI 1.3NG.RTI Evaluation Tool OverviewPerformance is very often a critical factor in modeling and simulation systems, particularly in the test and evaluation community where high bandwidth and short latency are often requirements. As more Run Time Infrastructure (RTI) implementations of the High Level Architecture (HLA) Interface Specification become available, it has become necessary to ascertain performance information on the RTIs in order to select the RTI that best meets the performance needs of the simulation.  The U.S. Army Development Test Command (DTC) contracted with Logicon to develop an RTI Evaluation Tool to evaluate multiple implementations of the High Level Architecture Interface Specification, version 1.3. At DTC’s request, the RTI Evaluation Tool effort focused on Redstone Technical Test Center’s (RTTC) IR Tactical Missile (IRTM) simulation by incorporating its Federation Object Model (FOM) and by testing metrics that are meaningful to that application. RTI 1.3NG was evaluated using the RTI Evaluation Tool.Metrics DefinitionThe first step in the development of the RTI Evaluation Tool was to define the RTI performance measures of interest. [1] In order to develop a robust tool for evaluating RTIs that focused on the performance concerns specific to RTTC, the metrics definition included those metrics that are applicable to the IRTM simulation and those that were determined to be an important measure of overall RTI performance. RTTC indicated that attribute update latency and network overhead are important measures of RTI performance for the IRTM simulation. Additionally, attribute update throughput, object registration throughput, ownership transfer throughput, and time advance grant throughput, were determined to be important measures of RTI overall performance by a review of the Spring 1999 SISO papers and consultation with experienced HLA/RTI users. [1]Once the metrics were defined, a test methodology and results format were defined for each metric and documented in the RTI Evaluation Tool Test Plan. DMSO Performance Benchmark ProgramsPrior to RTI Evaluation Tool development, the Defense Modeling and Simulation Office (DMSO) Benchmark programs were evaluated for applicability to the RTI Evaluation Tool.The goals of the Benchmark programs are to provide performance indicators for each of the major categories of inter-federate exchange through the RTI, to provide simple and unambiguous tools that can be applied by general users of the RTI, to provide easy to understand metrics that facilitate comparison and investigation of factors influencing federation performance, and to provide source code that can be easily distributed and compiled on all RTI supported platforms. [3]There are four Benchmark programs, Attribute Update Latency Benchmark, Attribute Update Throughput Benchmark, Ownership Transfer Benchmark, and Time Advance Grant Benchmark.Evaluation of the DMSO Benchmark programs revealed significant applicability to the RTI Evaluation Tool. In order to maximize reuse, a decision was made to customize the Benchmark programs wherever possible to meet the needs of the RTI Evaluation Tool. The modifications required to customize the Benchmarks are detailed in the following section.RTI Evaluation Tool DevelopmentSeveral modifications to the DMSO Benchmark programs were required in order to incorporate the IRTM simulation FOM and to evaluate parametric variations of the metrics, such as time management and transportation type that are not capabilities of the Benchmarks. The IRTM simulation FOM was used to generate the Federation Execution Details (FED) files that were used by the RTI during the evaluation of RTI 1.3 NG. The FOM was modified to add one object and several interactions required by the Benchmark programs. Attribute Update LatencyAttribute Update Latency is defined as the elapsed time starting immediately before the Update Attribute Values (UAV) service call on the sending federate and ending when the corresponding Reflect Attribute Values (RAV) call is executed on the receiving federate. [1]The parameters that were varied were attribute set size, transportation type, the number of objects being updated, and the time management scheme. The following attribute set sizes were used in testing attribute update latency: 4 bytes, 512 bytes, 2048 bytes, and 4096 bytes. Each transportation type, best effort and reliable, was exercised for each attribute set size. Additionally, the effect of time management was examined by enabling time regulation and time constrained, again exercised for each attribute set size. This series of tests was repeated for each of objects being updated equal to 1, 100, and 1000.Modifications were made to the DMSO Latency Benchmark program to vary the transportation type and to allow the test federates to enable time regulation and enable time constrained. The modification for transportation type was accomplished by modifications to the FED file. The modification for Time Management was accomplished by modification to the Latency Benchmark code.Attribute Update ThroughputAttribute update throughput is defined as the maximum rate at which federates can exchange attribute updates per second with a zero percent message loss. [1] Message loss percentage is defined as the percentage of messages not reflected at the reflecting federate. [1]The parameters that were varied were attribute set size, transportation type, and the time management scheme.  The following attribute set sizes were used in testing attribute update throughput: 4 bytes, 512 bytes, 2048 bytes, and 4096 bytes. Each transportation type, best effort and reliable, was exercised for each attribute set size. Additionally, the effect of time management was examined by enabling time regulation and time constrained, again exercised for each attribute set size. Modifications were made to the DMSO Throughput Benchmark program to vary the transportation type and to allow the test federates to enable time regulation and enable time constrained. The modification for transportation type was accomplished by modifications to the FED file. The modification for Time Management was accomplished by modification to the Throughput Benchmark code.The modifications required to allow for variation of the attribute update methodology were not completed. The plan was to use one of the objects from RTTC’s IRTM simulation FOM in this variation of the throughput tests. Detailed inspection of the Benchmark code revealed that the Benchmarks RTI interface was tightly coupled with the data (object) that was being used in the tests and therefore, replacing the data with the IRTM simulation object would require significant changes. Architectural modifications were made to the Benchmark code to decouple the data from the RTI interface to allow for easier substitution of objects. Those modifications were completed but it was a significant and unexpected effort. The modifications required to use an object from the IRTM simulation FOM in the attribute update methodology variation of the throughput test were not completed due to time constraints.Object Registration ThroughputObject Registration Throughput is defined as the maximum rate at which a federate can register objects per second. [1]The parameters that were varied were the number of federates, and the number of objects being registered.  The number of objects registered for this test varied as follows: 10, 100, 1000, and 10000. Each number of federates, one and two, were exercised for each object number variation. The already modified Throughput Benchmark was used for this test. No additional modifications were required. It had been anticipated that this Benchmark would have to be developed. However, during code inspection, it was discovered as an existing capability of the Throughput Benchmark. This capability of the Throughput Benchmark is not documented or advertised in the Benchmark distribution.Attribute Ownership Transfer ThroughputAttribute Ownership Transfer Throughput is defined as the maximum rate at which attribute ownership transfers can occur between two federates per second. [1]The number of objects was varied for this test as follows: 1, 10, 100, and 1000.The DMSO Ownership Benchmark measures attribute ownership transfer throughput as a combination of a push/pull attribute ownership transfer. Measuring ownership transfer throughput as a combination of a push and pull may be misleading if the push and pull transaction times are significantly different. Also, it would be more useful to have the push and pull models measured separately as having this information may influence the selection of one model over the other. Ideally, the Ownership Benchmark would have been modified to measure attribute transfer throughput for the push and pull models separately. This was not done as part of this effort because RTTC has indicated that attribute ownership transfer throughput is not a high priority performance measure of interest for their current application.The unmodified Ownership Benchmark was used in the evaluation of RTI 1.3NG.Time Advance Grant ThroughputTime Advance Grant Throughput is defined as the rate at which a federation can grant time advancements, measured in seconds. [1]This test was performed first with one federate and a second time with two federates competing for RTI resources.There were no modifications necessary to the Time Advance Benchmark program.Network OverheadNetwork Overhead, for this application, is defined as a measure of the traffic on the network other than that requested by the user through the RTI Update Attribute Values (UAV) service call. Network overhead is expressed as the ratio of total network traffic, less payload data size, to total network traffic.The parameters that were varied were transportation type and the time management scheme. Both transportation types, best effort and reliable, were exercised. Additionally, the effect of time management was examined by enabling time regulation and enabling time constrained.The modified Throughput Benchmark was used for this test. One additional modification was required to isolate the UAV() service call in order to capture only network traffic relevant to the attribute update.A network monitoring tool was needed to capture the network packets. A commercially available tool, EtherPeek 3.6.2, was selected. This selection was made based on cost, ease of use, and reputation as reported by experienced HLA/RTI users.RTI Evaluation Test BedFor the RTI 1.3 NG evaluation, the test bed consisted of three Windows NT PCs on a locally isolated 10 Mbits/sec Ethernet LAN. Two PCs were used for test federate execution and the third was used to run EtherPeek.Each test federate was run on a separate computer to eliminate the effect of one federate execution on another federate execution.The Federation Execution Planner’s Workbook was used to document the characteristics of the test federate executions deemed relevant to the federation runtime performance and is included in the RTI Evaluation Tool, RTI 1.3NG Test Report. [2]RTI 1.3 EvaluationA summary of the RTI 1.3NG test results follows.65 tests were run in the evaluation of RTI 1.3NG. The majority of these tests measured attribute update latency and throughput rates.Of the 65 tests, 49 provided meaningful results. Those results are presented in the subsections that follow.Results were not obtained for 13 tests. These tests were considered failures because they either did not complete (hung) or completed in error.The update latency tests hung repeatedly when the number of objects = 1000. This accounted for the failure of 11 tests.The update latency test hung repeatedly when the number of objects = 100, attribute set size = 2048 bytes, and best effort transportation was being used. Interestingly, the same test, with the attribute set size = 4096 bytes instead of 2048 bytes, completed successfully. RTTC has reported seeing similar anomalies with the RTI.The object registration test where objects = 10 and number of federates = 2 completed with errors. The two federates were unable to synchronize before beginning the object registrations and as a result, the first federate completed its object registrations while the other federate, not recognizing the presence of the first federate, eventually timed out. This failure was reported to DMSO. The object registration tests that registered a larger numbers of objects were successful however (100, 1000, and 10000).The network overhead results were inconclusive. The network overhead associated with an attribute update was measured by isolating the attribute update so that the capture would only contain network packets associated with the update. However, the network capture contained packets that were not associated with the attribute update. Interpretation of these results requires further research. The RTI 1.3NG test results are documented in the RTI Evaluation Tool, RTI 1.3NG Test Report.Attribute Update Latency Test Results EMBED Excel.Chart.8 \s  Figure  SEQ Figure \* ARABIC 1.  Attribute Update Latency, 1 objectThe above graph presents the attribute update latency test results for number of objects = 1. The results are relatively flat over attribute size.  A slight decrease in latency for the time regulating and time constrained federate over the federate that is not time regulating and not time constrained is seen.Between the two federates that are not using time management, a slight decrease in latency for the federate using best effort transportation over the federate that is using reliable transportation is seen. EMBED Excel.Chart.8 \s Figure  SEQ Figure \* ARABIC 2. Attribute Update Latency, 100 objectsThe above graph presents the attribute update latency test results for number of objects = 100. A break in the graph is seen at 2048 bytes for the federate using best effort transportation. This test failed as mentioned in paragraph  REF _Ref474637760 \r  \* MERGEFORMAT 6  REF _Ref474637760 \p  \* MERGEFORMAT above.A significant difference in latency is seen at 4 bytes. At 512 bytes and greater, the results appear to be a consistent increase in latency, with latency being a bit higher for the federate using best effort transportation.Attribute Update Throughput Test Results EMBED Excel.Chart.8 \s Figure  SEQ Figure \* ARABIC 3. Attribute Update ThroughputThe above graph presents the attribute update throughput test results. A significant decrease in throughput over attribute set size for all test variations is seen. There appears to be no significant difference in attributer update throughput between the variations.Object Registration Throughput Test Results EMBED Excel.Chart.8 \s Figure  SEQ Figure \* ARABIC 4.  Object Registration ThroughputThe above graph presents the Object Registration throughput test results. This series of tests was executed twice. In the first series, one federate was performing object registrations. The blue line in this graph presents those results. In the second series of tests, two federates were competing for RTI resources in performing object registrations. The pink and yellow lines present the results of each federate in this second series of tests.There are no results for the two federate test at number of objects = 10. This test failed, as mentioned in paragraph  REF _Ref474637760 \w \h  \* MERGEFORMAT 6  REF _Ref474637760 \p  \* MERGEFORMAT above. The test results show a slight decrease in throughput over the number of objects.For the series of tests with two federates, the RTIexec and Fedexec processes were running locally with federate #1. Federate #1 performed similarly to the federate in the test with one federate. Federate #2, running remotely, had consistently lower throughput.Ownership Transfer Throughput Test Results EMBED Excel.Chart.8 \s Figure  SEQ Figure \* ARABIC 5.  Ownership Transfer ThroughputThe above graph presents the Ownership Transfer throughput test results. A significant increase in the throughput over the number of transfers is seen for this sampling. Time Advance Grant Throughput Test Results EMBED Excel.Chart.8 \s Figure  SEQ Figure \* ARABIC 6.  Time Advance Grant ThroughputThe above graph presents the Time Advance Grant throughput test results. This test was run twice. In the first test, one federate was performing time advance grant requests. The blue bar in this graph presents the results for that test.  In the second test, two federates were competing for RTI resources in requesting time advance grants. The maroon and yellow bars present the results of each federate for the second test.An almost four fold decrease in throughput is seen when two federates are executing this test.Lessons LearnedSeveral lessons were learned during the RTI Evaluation Tool effort:The Benchmark programs are not well documented.The Benchmark programs were not designed with substituting simulation data in mind as was discovered in the effort to use one of the objects in RTTC’s IRTM simulation FOM.Update latency bug. When the number of objects = 1000 the update latency test failed consistently.Object Registration bug. When testing with two federates and number of objects = 10 the object registration test failed consistently.Several sporadic errors/test failures were encountered when running the Benchmark programs. These were indicated by RTI error messages. Typically, rerunning the test resulted in a successful test. Restarting the RTI processes between each test reduced the number of these sporadic errors/failures.One of the goals of the RTI Evaluation Tool, as mentioned earlier, was to provide a generic tool to evaluate multiple implementations of the HLA Interface Specification, 1.3. tick() is not part of the HLA Interface Specification. In RTI1.3NG, tick() is available as part of polling mode and is used throughout the RTI Evaluation Tool. tick() will not be a part of RTI 1516. This means that the RTI Evaluation Tool will not be portable between these RTIs and possibly not commercially available RTIs without modification.Possible RTI Evaluation Tool ImprovementsThe following are possible improvements and extensions to the RTI Evaluation Tool that are under consideration:Evaluation of the Mäk RTI using the RTI Evaluation Tool. RTTC has indicated interest in the Mäk RTI.Evaluation with the number of federates being greater than two and the number of federations being greater than one.The effect of a late arriving federate on attribute update rate.Measurement of object interaction latency and throughput rates.The effect of federation complexity on update latency.Completion of the modifications to the Throughput Benchmark for attribute update methodology.Modifications to the Ownership Benchmark to obtain a more meaningful measurement.Measure throughput on a WAN, across a router or with multicast.Follow-On analysis to explain some of the complex trends that were seen in the evaluation of RTI 1.3NG such as sporadic errors, test failures, local minima in test results, and network overhead test results.References[1]	Wuerfel, R., and Olszewski, J.: “Defining RTI Performance” 1999 Spring Simulation Interoperability Workshop. Paper No.: 99S-SIW-100.[2]	Dahmann, J., Olszewski, J., Briggs, R., Richardson, R., Weatherly, R., Calvin, J., Zimmermann, P.: “High Level Architecture (HLA) Performance Framework” 1997 Fall Simulation Interoperability Workshop. Paper No.: 97F-SIW-137.[3]	Bachinsky, S., Wuerfel, R., Harrington, J., Briggs, R.: “Federation Performance Tutorial”, slide 10. 