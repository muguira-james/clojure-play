Scalable Reliable Data Dissemination for Distributed Simulations using Hierarchical InterconnectKeith SnivelyDynamic Animation Systems, Inc.12015 Lee-Jackson Hwy, Ste. 200FairOaks, VA 22033703-333-5432 HYPERLINK "mailto:ksnively@d-a-s.com" ksnively@d-a-s.comAnnette WilsonVirtual Technology Corporation5510 Cherokee Ave., Suite 350Alexandria, VA 22312-2320703-658-7050 HYPERLINK "mailto:awilson@virtc.com" awilson@virtc.comKeywords:HLA, RTI, RTI-NG, MATREX, Distributed Simulation, PerformanceABSTRACT:  To address the needs of large scale Distributed Simulations and simulations operating across Wide Area Networks or other critical connections (low bandwidth/high latency), the MATREX program developed the Hierarchical Interconnect for RTI-1.3NGjvb.  The Hierarchical Interconnect provides scalable and robust reliable data dissemination within an HLA federation using a networked topology of distributor nodes.  This connection topology parallelizes the burden of exploding peer-to-peer reliable messages as well as eliminating unnecessary duplicate messages across critical connections.  This paper provides a detailed explanation of the Hierarchical Interconnect, its benefits, and how it is used within an HLA federation.  Some basic benchmark comparisons are presented for federations using this topology versus legacy topologies.  The paper also discusses current work and future extensions for the Hierarchical Interconnect.IntroductionThe Modeling Architecture for Technology and Research Experimentation (MATREX) Science and Technology Objective (STO) supports the Army Modeling and Simulation (M&S) vision.  This vision is to design and build a simulation architecture and reference implementation using standardized components that represent key characteristics of network-centric warfighting systems.  The objectives of the STO include supporting distributed secure execution over local, metropolitan, and wide area networks; and providing an environment to study open problems in distributed simulation including scalability [1].  The work described in this paper addresses these issues for reliable data in a High Level Architecture (HLA) environment.The HLA defines two transportation mechanisms for disseminating federate data:  best effort and reliable.  Best effort data is sent to each subscriber; however its delivery is not guaranteed.  Conversely, delivery of reliable data to all subscribers is guaranteed.  RTI-1.3NG uses UDP Multicast to implement best effort data dissemination.  Reliable data delivery is based on TCP/IP connections.  To date, RTI-1.3NG has offered two configurations for disseminating reliable data:  a centralized repeater or a fully connected peer-to-peer network.While both interconnect strategies have their strong points, neither is able to satisfy all the requirements that the MATREX STO levies upon its communications infrastructure.  The centrally connected strategy can become overloaded when federations start to scale to large number of federates where the fully connected strategy requires a large number of TCP connections in this case (on the order of N2).  Additionally, both strategies can make inefficient use of WAN connections, causing redundant messages to be sent across these links.  To address these issues, the MATREX program implemented the Hierarchical Interconnect as a third option for disseminating reliable data within RTI-1.3NG.The rest of the paper provides a description of the various interconnect strategies for RTI-1.3NG and presents some comparative benchmarks.  Section 2 of this document describes the existing configurations.  Section 3 describes the new hierarchical interconnect capability.  Section  PAGEREF _Ref63753937 \h 4 provides a more detailed explanation of the implementation.  Sections 5 and 6 describe a set of benchmark tests and the results of those tests, respectively.  Section 7 draws conclusions from the test results and recommends when and how the Hierarchical Interconnect can be used to improve performance.  Finally, section 8 looks at on-going efforts and future directions.Existing Dissemination ConfigurationsThe interconnect strategy in RTI-1.3NG determines how federation components are connected and the path that reliable messages follow between processes.  These paths are used for both reliable federate data and reliable RTI administrative messages.  Note that the type and resolution of filtering (i.e. amount of sender vs receiver side filtering) is determined separately by the Data Distribution Strategy.  See [2] for more information.  The Interconnect Strategy merely determines how reliable messages flow from publisher to subscriber.2.1 Centralized InterconnectThe Centralized Interconnect strategy has each federate node connected to every other federate node through a central distribution node.  This distribution node is responsible for exploding messages to the interested subscribers, as shown in  REF _Ref59501871 \h  \* MERGEFORMAT Figure 1:  Centralized Interconnect. SHAPE  \* MERGEFORMAT Figure  SEQ Figure \* ARABIC 1:  Centralized InterconnectWhen using this strategy, RTI NG locates the centralized distribution node in the fedex process.  This strategy has the fewest connections and the simplest topology.  The federate nodes simply relay messages to the distribution node, which then is responsible for exploding them to the interested parties.With this strategy, the federate nodes have a minimum of local processing and I/O time in sending data.  However, the single distribution node presents the federation with a central bottleneck.  As federation traffic grows, such as with larger federations, the centralized distributor can be easily overwhelmed.  Furthermore, this topology still presents a problem when dealing with federations spanning a WAN, or with other critical (low bandwidth/high latency) connections.  The distribution node can only be local to a single LAN.  All other connections will require sending data over the WAN.  If multiple federate nodes are located remotely, this can result in writing identical messages across the same WAN link.2.2 Collocated InterconnectThe Collocated Interconnect strategy has each federate node directly connected to each other federate node, as shown in  REF _Ref59502745 \h  \* MERGEFORMAT Figure 2:  Collocated Interconnect.  The term “Collocated” indicates that the dissemination task is located in the same process as the federate itself.  This means that each process locally explodes any outgoing messages to each process with a (potentially) interested subscriber.  Determination of which processes are interested is handled separately by the DDM strategy.  For a discussion of DDM, see [2], [3].Figure  SEQ Figure \* ARABIC 2:  Collocated InterconnectThe Collocated strategy allows each federate node to send data directly to each other interested federate node in the federation.  This eliminates any centralized bottlenecks and can increase throughput and reduce latency.  However, this strategy does add processing and I/O time to the federate node processes which increases as the number of interested federate nodes increase.  Also, the strategy may not make efficient use of critical network connections (low bandwidth/high latency), such as those crossing a WAN, especially in large federations.  If multiple interested nodes exist across the network connection, identical data will be sent multiple times across that connection; one message for each interested node.Hierarchical InterconnectThe Hierarchical Interconnect was developed to address the issues identified above.  This strategy connects federate nodes through a network of distribution nodes.  A sample of what a federation interconnect topology may look like using this strategy is shown in the figure below.  In a sense, Hierarchical Interconnect fills in the gap between the two endpoints:  Collocated Interconnect and Centralized Interconnect.Each federation node has a distribution node parent (perhaps the “Null” parent for the top level).  The set of federation nodes with a common parent, along with the parent distribution node, form a federation segment.  In the figure, the different color connections show the connections for each of the federation segments (5 in all).   Each federation segment is fully connected (though centrally connected segments may be supported in the future).  The federation segments are linked through the parent distribution nodes, which themselves are part of a “higher level” federation segment.  Note that distribution nodes may have federate nodes or other distribution nodes (or even a mixture) as children.  The hierarchy also need not be symmetric.  In the example, Distribution Node A and Distribution Node B sit at the highest (root) level.  The B segment has only a single sublevel whereas the A segment has 2 sublevels.The Hierarchical Interconnect topology captures many of the benefits of both the Collocated and Centralized Interconnect topologies while avoiding their pitfalls.  Each federate node is responsible only for exploding messages to other federation nodes within its federation segment.  By locating federates within the same federation segment, high bandwidth and low latency requirements may be met.  However, the federate node passes off much of the explosion and filtering tasks to the distribution nodes and thus avoids the potentially large processing and I/O burden.  Additionally, data is only sent along links where one of the federate nodes “downstream” of that link has expressed an interest.  If no such federates exist, the data never leaves the originating federation segment.  In a federation spanning a WAN, each location can have a high-level distribution node with possibly additional distribution nodes below it.  The high-level distribution node at each location handles reliable data in and out of that location.  In this manner, unnecessary and redundant messages across the WAN can be avoided.  Furthermore, the local federate nodes are insulated from any flow control issues across those links.  The drawbacks to this approach are that data latency can be increased between federates in different federation segments.  In addition, Time Management services are not currently available when running with this topology though work is currently ongoing to add this capability.  All other RTI services are supported.  Finally, some additional hardware needs to be available to host the distribution node processes, and configuration and federation run-time startup is more complex.  However, at run-time, whether or not Hierarchical Interconnect is being used is completely transparent to the federate application.Implementation DetailThe publish/subscribe and data dissemination mechanisms in RTI-NG are based upon TAO’s Real-time Event Service which extends the standard CORBA Event Service.  The standard event service provides an anonymous publish and subscribe capability.  Applications may attach suppliers and consumers to an event channel instantiation.  Suppliers produce events while consumers receive events.  All events sent by suppliers are received by each consumer on a given event channel.  There are four general models of collaboration between supplier and consumer:  Push Model, Pull Model, Hybrid Push/Pull and Hybrid Pull/Push.  See Henning [7] for more details.TAO’s Real-time Event Service extends the CORBA Event Service to satisfy real time quality of service needs through event filtering and correlation and efficient and predictable event dispatching.  Rather than sending each event to every consumer, a Real-time Event Channel (RTEC) filters messages based upon the event type, which must be specified as part of the event header, and source id and forwards the event to only those consumers who have subscribed to the event type.  Consumers and suppliers provide their event interests when attaching the RTEC using a QOS structure.  The QOS structure allows for publication or subscription to multiple event types, the use of “AND” and “OR” predicates among those event types, and event correlation.  The RTEC currently only supports the Push Model for collaboration.  See O’Ryan [4] for more details. The features of the Real-time Event Service are well suited to support the data dissemination requirement of HLA.  In RTI-NG, three event channels are created within a process for each federation for which the process joins at least one federate.  Two event channels are created for reliable data: one for inbound data and one for outbound data, and a third for best-effort multicast data (both inbound and outbound).  The only difference between the event channels is the manner in which they are federated to event channels in remote processes (An unfortunate overloading of the term federate within the context of an HLA paper, though it is used as a verb when referring to connecting event channels).  The RTEC used for best-effort communications is federated using TAO’s multicast event sender and receiver.  The sender subscribes to all events published on the local event channel and forwards each event to a multicast address determined from the event type.  The receiver attaches as an observer to a local event channel and subscribes to multicast addresses based upon all the consumer interests specified on the event channel.   Any received events via multicast are then pushed onto the event channel.When federates in other processes join the federation, the reliable event channels are federated using “gateway” objects.  A gateway attaches as a consumer to a remote event channel and supplier to a local event channel.  The gateway also attaches as an observer to the local event channel to receive notifications when consumers attach to that event channel or update subscriptions.  The gateway then compiles all subscriptions for the local event channel and subscribes to those event types from the remote event channel.  This allows the event channels to appear as a single channel and the necessary events flow between the federated event channels.  Two event channels are used within the federate node LRC for reliable data to reduce contention between outbound and inbound traffic.As mentioned earlier, the Data Distribution Strategy determines the resolution of sender side filtering in RTING for data sent between federates.  This is accomplished by specifying the number of event types available to the federation and the manner in which federate subscriptions, based on both DM and DDM subscriptions/publications, map to event types.  The Interconnect Strategy determines the manner in which each processes’ reliable event channels are federated to remote processes and thus the path by which reliable data flows between federate nodes. SHAPE  \* MERGEFORMAT Figure  SEQ Figure \* ARABIC 4:  Fully Connected Event ChannelsDiagrams for the two legacy strategies are shown in  REF _Ref63754896 \h  \* MERGEFORMAT Figure 4:  Fully Connected Event Channels and  REF _Ref63754910 \h  \* MERGEFORMAT Figure 5:  Centrally Connected Event Channels.  In the diagrams, the lines represent gateway connections between event channels.  The gateway will be local to the process with the receiving event channel.  Also, ovals represent federate nodes (not federates) and rectangles represent distribution nodes.  As can be seen in the fully connected event channel diagram, the number of gateways in each federate node equals the number of other federate nodes in the federation.  In the centrally connected case, each federate node contains a single gateway to the centralized event channel.  The centralized event channel contains one gateway for each federate node in the federation. SHAPE  \* MERGEFORMAT Figure  SEQ Figure \* ARABIC 5:  Centrally Connected Event ChannelsFor Hierarchical Interconnect, the distributors take on the task of forwarding data between federation segments based upon federate subscriptions.  The distributor node, like the LRC, contains two event channels for reliable data:  one for data outbound from the federation segment, dubbed Hi, and one for data inbound to the federation segment, dubbed Low.  The distributors themselves are merely a wrapper around the two event channels.  They therefore have all the power and configurability of the Real-time Event Service.  Users may specify whether or not the distributors use multiple threads, the dispatching strategy to use as well as listening endpoints for hosts with multiple NICs. REF _Ref63755354 \h  \* MERGEFORMAT Figure 6 shows the connection topology for Hierarchical Interconnect.  Note that data flowing between federate nodes within a segment does not pass through the distributor since all federation segments must be fully connected.  Also, the event channels within the distributor node are not connected by a gateway as they are in the federate nodes  (Data only flows across this gateway in a federate node when a process contains multiple federates joined to the same federation). SHAPE  \* MERGEFORMAT Figure  SEQ Figure \* ARABIC 6:  Hierarchically Connected Event ChannelsAn important feature of the connection topology is that there are no circular paths and only a single unique path exists between the outbound event channel of a federate node to the inbound event channel of any other federate node.  This fact greatly simplifies the logic within the distributor nodes.  The default implementation of the RTEC gateway within TAO assigns a time-to-live (TTL) value to events as well as blocking gateway subscriptions from affecting the subscription set of the RTEC provided to observers to prevent events from cycling endlessly between federated channels.  With the shown topology though, this is unnecessary and these limitations are removed.  Thus, the hierarchy can be arbitrarily deep with an indeterminate number of distributors between federate nodes without having to worry about event cycling or special configurations, such as TTL parameters (Though one would want to limit the number of hops for performance reasons).Further, since the translation of federate and RTI data to and from RTEC event structures occurs within the federate nodes, the distribution nodes also do not require any knowledge of the FOM or any special tailoring to run within a given federation.  The user merely provides the hierarchy layout within configuration files and runs a launcher process on each machine which will host one or more distributor processes.  The rest is completely transparent to the federation and federates.  The distributors will be launched as needed when federates join and cleaned up when the federation is destroyed.Benchmark TestsA series of benchmark tests were run to compare the throughput and latency using the legacy dissemination configurations and hierarchical interconnect configurations.  All tests were run in the MATREX integration lab.  The majority (13 of 16) of the federate processes were run on Dell PowerEdge 2650 rack mounted machines with two Intel Xeon 2.4 GHz processors and 6 GB RAM.  The remaining federate processes (those used only in the 15 receiver runs), the rtiexec, the fedex, and the distributor processes were run on Dell Precision 530 desktops with two Intel Xeon 2.0 GHz processors and 1 GB RAM.  All of the machines had 100 Mbps network cards and were running Linux RedHat 7.2.Throughput TestThe first series of tests that was run used the BmThruput benchmark program.  In each execution, the benchmark was run for three cycles of 100000 updates with the attribute size set to 512 bytes.  The updates were sent as reliable, receive-ordered data.  Series were run varying the number of receivers, connection strategy, and bundling.  In addition, the location of the sender within the topology was varied for tests using distributor nodes. The values used for each of the variables are given in  REF _Ref62372450  \* MERGEFORMAT Table 1 below.  Note that the centrally connected configuration was not included in any of the throughput tests due to the fact that it failed under load.In the case of a distribution node per federate, the distribution nodes resided on the same host as the federate process.  For the remaining Hierarchical Interconnect configurations, federates are divided among the distributors as evenly as possible.VariableValuesNumber of receivers1, 3, 6, 9, 12, 15Connection StrategyCollocated, Hierarchical-1 Distributor, Hierarchical-2 Distributors,Hierarchcial-3 Distributors,Hierarchical-4 Distributors,Hierarchical-Distributor Per FederateBundlingNo Bundling,5ms timeout, 20ms timeoutSender locationAt root levelBelow Distributor nodeTable  SEQ Table \* ARABIC 1:  Throughput Test VariablesSymmetric Throughput TestThe next series of tests used a modified BmThruput where each process functioned as both a sender and a receiver.  This test used the same parameters and configuration as the throughput test with each federate sending 100000 updates with a 512 byte attribute for 3 cycles.  For this test, all federates were children of a distribution node.  Table 2 lists the variables in the test.The symmetric throughput tests are a “worst case scenario” for any configuration that involves repeater nodes.  The distributor nodes become responsible for a level of traffic that is a multiplicative of the number of children nodes.  It will always be better to send data directly between processes (assuming all network links are equal) in this case, but it is included for comparison purposes.VariableValuesNumber of federates2, 4, 7, 10, 13, 16Connection StrategyCollocated, Hierarchical-1 Distributor, Hierarchical-2 Distributors,Hierarchcial-3 Distributors,Hierarchical-4 Distributors,Hierarchical-Distributor Per FederateBundlingNo Bundling,5ms timeout, 20ms timeoutSender locationBelow Distributor nodeTable  SEQ Table \* ARABIC 2:  Symmetric Throughput Test VariablesLatency TestThe latency test was a modified version of BmLatency to handle multiple receivers.  The test measured the roundtrip time for sending a message with a 512-byte attribute to a specific receiver and receiving a response.  The latency reported by this test is half of the roundtrip time.  Since all receivers are subscribed to the object class and attributes, the message is sent to and received by all receivers.  However, only the targeted receiver responds.  This is accomplished by specifying the intended receiver in the user-supplied tag.  A series of runs were made varying the number of receivers, connection strategy and bundling timeout.  The values used for each variable are listed in  REF _Ref62442908 \h  \* MERGEFORMAT Table 3.VariableValuesNumber of receivers1, 3, 6, 9, 12, 15Connection StrategyCentral, Collocated, Hierarchical-1 Distributor, Hierarchical-2 Distributors,Hierarchcial-3 Distributors,Hierarchical-4 Distributors,Hierarchical-Distributor Per FederateBundlingNo Bundling,5ms timeout, 20ms timeoutSender locationAt root levelTable  SEQ Table \* ARABIC 3:  Latency Test VariablesTest ResultsThroughput TestFor the throughput test, the results using 5ms and 20ms bundling timeouts were similar so only the results of the 5ms bundling are discussed in this paper.  This behavior can be attributed to the bundled packets reaching the size threshold, 63KB, before the timeout in the 20ms case.  Also, the 5ms case already consumed nearly all the network resources, and therefore the 20ms case could not gain anything in terms of throughput.  In the graphs below, the series labeled as Hierarchical-S refer to series in which the sender was a child of a distributor.  The series labeled Hierarchical refer to series in which the sender was connected at the root level.No BundlingWhen data is sent without bundling (e.g., each call to updateAttributeValues corresponds to a call to write), the fully connected configuration outperforms other configurations for small federations.  However, when the number of receivers grows (6 in this test), using multiple hierarchical distributors exhibits better throughput than the fully connected configuration.  It is projected that the centrally connected case would have similar performance characteristics as a single distributor connected at the root.  Without bundling, the sender and receiver throughput graphs are similar.Note that even in the case of a single sender and single receiver, the performance dropped significantly if the data passed through a distributor node.  In this case, the distributor node process was processor bound, rather than I/O bound.  Executing top during the execution showed the distributor node with 99.9% CPU utilization.  We suspect that excessive memory allocation and deallocation for the large number of small packets caused this behavior.  Currently, the distributor nodes do not use the optimized memory pooling used within the LRC (Local RTI Component) of each federate and may help explain why tests which only involved the LRC were able to outperform the Hierarchical Interconnect case.  However, even given these limitations, the Hierarchical Interconnect case does ultimately prove to scale better. EMBED Excel.Chart.8 \s Graph  SEQ Graph \* ARABIC 1:  Mean Sender Throughput No Bundling (Sender at Root Level)Graph  SEQ Graph \* ARABIC 2:  Mean Receiver Throughput No Bundling (Sender at Root Level)5ms BundlingAs can be seen from the results of this test, the amount of data a sender can send is driven by the number of times it must write the data to the network.  With bundling on, the sender throughput is driven by the number of writes performed by the sender, not by the distributors.  By just looking at the sender throughput, one would reach the conclusion that using a single distributor gave the best performance.  However, the receiver throughput tells a different story.  Here, the advantage of using multiple distributors when many receivers need data from a sender is once again demonstrated.The difference between the send and receive rate can be explained by the queuing of messages within the distributor nodes.  This allowed the sending application to quickly complete its cycle of sending, while the distributor nodes took care of fully exploding messages to receivers.  It is possible for the distributor node queues to become full, at which point flow control will begin to take effect.  The hierarchical interconnect is designed so that when this point is reached, the sender reduces the rate at which it sends rather than having periods where it is prevented from sending due to full queues.  The single distributor case in  REF _Ref63481908 \h  \* MERGEFORMAT Table 4 shows the reduction in throughput from the sender so that it does not overload the distributor.  This case does show the benefit of offloading the reliable message explosion for sending processes that will have spurts of high activity. EMBED Excel.Chart.8 \s Graph  SEQ Graph \* ARABIC 3:  Mean Sender Throughput 5ms Bundling (Sender at Root Level) EMBED Excel.Chart.8 \s Graph  SEQ Graph \* ARABIC 4:  Mean Sender Throughput 5ms Bundling (Sender under Distribution Node) EMBED Excel.Chart.8 \s Graph  SEQ Graph \* ARABIC 5:  Mean Receiver Throughput 5ms Bundling (Sender at Root Level) EMBED Excel.Chart.8 \s Graph  SEQ Graph \* ARABIC 6:  Mean Receiver Throughput 5ms Bundling (Sender under Distribution Node) REF _Ref63481908 \h  \* MERGEFORMAT Table 4 contains samples of data collected to monitor network utilization.  The data is from the series of runs with 6 receivers, the sender connected at the root level, and a 5ms bundling timeout.  This data was collected by reading /proc/net/dev once a second.  The two sets of numbers shown correspond to the bytes transmitted on eth0 of the machine hosting the sending process and one of the hierarchical distributor nodes.  While the numbers are not exact due to the collection method, they do show high network utilization.  Looking at the utilization, it is easy to see that the process performing the largest number of sends is the one consuming the largest amount of bandwidth.  For example, in the case of one distribution node, the distribution node process is consuming around 90% of the bandwidth while the sender process is consuming just above 50%.  This is because the single distributor that must transmit each message to the 6 receivers throttles the sender.CaseSenderDistributor 1bytes sentMbpsbytes sentMpbsFully Connected1238313894.4758N/AN/A1234260094.1665N/AN/A1242786294.8170N/AN/A1242518694.7966N/AN/A1 Distributor774140259.06221206864492.0764642775849.03991224237493.4019758927657.90151151164587.8268779314659.45691213933292.61572 Distributors1019487477.78071227727693.6681914736469.78881226895993.6047872228666.54571226835093.6000909907869.42041228446893.72303 Distributors1232094894.0013713000454.39761228859693.7545658610850.24801204794291.9185810272261.81881217599892.8954839349064.03724 Distributors1227006093.6131302505423.07931203852491.8466302730523.09651227208693.6285309419223.60681224592693.429302100023.0484Table  SEQ Table \* ARABIC 4:  Sample Network UtilizationSymmetric Throughput TestAs with the BmThruput test, the results of the 5ms and 20ms bundling timeout cases were similar.  As was noted previously, the symmetric throughput tests are a worst-case scenario for distribution nodes.  The situation is exacerbated since all senders are generating large amounts of data as quickly as possible and doing little processing other than calling tick to receive the updates from other federates.  The single distributor case is omitted from the graphs since it behaves as the fully connected case.No BundlingUnlike the throughput test with a single sender, adding a few distribution nodes did not improve throughput in large federations.   This is because, as with the centralized distributor, the distribution nodes become bottlenecks when all of their children are sending data at a high rate.  The graphs displaying the updates sent and received per second for the no bundling test case are shown below. EMBED Excel.Chart.8 \s Graph  SEQ Graph \* ARABIC 7:  Mean Updates No Bundling EMBED Excel.Chart.8 \s Graph  SEQ Graph \* ARABIC 8:  Mean Reflects No Bundling5ms BundlingWith bundling enabled, the only case that adding distribution nodes improved performance beyond that of the fully connected network was adding a distribution node for each federate process.  This removes the burden of exploding the messages from the federate without adding a bottleneck.  The graphs below show the results for each of the configurations. EMBED Excel.Chart.8 \s Graph  SEQ Graph \* ARABIC 9:  Mean Updates 5ms Bundling EMBED Excel.Chart.8 \s Graph  SEQ Graph \* ARABIC 10:  Mean Reflect 5ms BundlingLatency TestAs expected, introducing distribution nodes increases latency over the fully connected configuration since messages must now make an extra hop.  For the latency tests, the bundling parameter drove the latency experienced.  In most cases, the latency was within 2ms of the bundling timeout.  For example, if the latency timeout was set to 5ms, test results lay within 5-7ms.The graphs below depict the latency seen during the test sequence.  As can be seen by the graphs, latency increases as the number of receivers does.  This is because the message must be written to each receiver and the ones last in the dissemination list have the added overhead of the previous sends.  Adding distribution nodes reduces the number of writes prior to a receiver, but increases latency by adding an extra hop. EMBED Excel.Chart.8 \s Graph  SEQ Graph \* ARABIC 11:  Mean Latency No Bundling EMBED Excel.Chart.8 \s Graph  SEQ Graph \* ARABIC 12:  Mean Latency 5ms BundlingConclusionsWhile the Hierarchical Interconnect is not a silver bullet to address all problems associated with reliable data dissemination within a distributed simulation environment, it does improve performance in many situations.  As shown in the single sender throughput tests, introducing distribution nodes improves performance when a large number of federates receive the data.  In each case, the federation scales better in this configuration.  Also, the Hierarchical Interconnect can be used to make efficient use of WAN connections by limiting the amount of data that must flow across these connections, as can be seen in the network load monitoring chart.  These features make it an attractive option for large, widely dispersed federations.However, if many senders are sending to many receivers, adding a few distribution nodes introduces bottlenecks if the distributors sit between those nodes.  Thus, sets of federates which will be exchanging a large amount of data with each other should be located within the same federation segment as much as possible.  Also, one can introduce a distribution node per federate.  This removes the I/O burden from the federate process and locates it in a dedicated process.  This strategy need not be federation wide but may be used on a federate by federate basis.Also, the benchmark cases show that sending many small messages can overburden the distributors and distributors do add to latency.  While efforts will be underway to mitigate these limitations, they must be considered when designing a federation hierarchy.Future DirectionsThe current Hierarchical Interconnect implementation is a first step at providing a configurable topology for disseminating reliable data.  Additional work is required to complete the implementation as well as improve performance, reliability, and fault tolerance.  Most notably, the HLA Time Management services are not available for Hierarchical Interconnect.  The RTI-NG uses a distributed consistent cut algorithm as the basis for LBTS calculations, which in turn allow federates to advance time.  The algorithm laid out in Mattern [6] and currently implemented in RTI-NG does not support intermediate node relays which may further explode messages.  The algorithm has been updated to address this requirement using a vector count reporting mechanism similar to the one discussed in Riley [5].  The implementation of this new algorithm is scheduled for completion in late Spring.  At that point, Hierarchical Interconnect mode will support all RTI services.In addition to implementing Time Management, the performance of the distributor nodes needs to be investigated.  The no bundling throughput test cases reveal that the distributors become processor bound in the presence of a large number of small messages.  This limitation is most likely attributable to memory allocation and de-allocation in that process.  Some memory management techniques, similar to those used in the federate node LRC, will be incorporated into the distributor nodes.  Additionally, the critical path of message relaying within the distributor process will be optimized to help increase throughput and reduce latencies.Beyond these near term efforts, Hierarchical Interconnect presents a wide variety of possible extensions and enhancements in order to further improve support for large, widely dispersed federations.  Most obviously, support for relaying best-effort data utilizing UDP point-to-point communications can be added.  Often times, special tunnels need to be created for handling multicast data between locations separated by a WAN connection in order to support a federation.  Using point-to-point UDP messages relayed through the Hierarchical Interconnect distributors would alleviate this requirement and simplify federation configuration.  Furthermore, it eliminates problems with hardware devices not properly supporting the desired number of multicast groups for a given federation.In addition, automatic fault recovery mechanisms can be added which will have the RTI re-launch distributors or migrate federation nodes to other distributors should the one they are connected fail for any reason (such as the host machine going down) and possibly launch redundant distributors for immediate use in any such circumstance.  Currently it is possible to restart distributors if a fault occurs without recycling the federation, but it requires manual intervention through the rtiConsole.Finally, auto configuration of the distribution node hierarchy can be implemented.  This capability would allow the RTI to determine which federates are placed in which federation segments dependent upon network location, distributor node loads and federate publication and subscription data, without detailed configuration specification from the user.  This capability can even allow the hierarchy to adapt itself in real-time during federation execution to changes from federate joins and resigns and subscription modifications.References[(] Modeling Architecture for Technology and Research EXperimentation (MATREX) System Architecture Description, May 15, 2003.[2] M. Hyett and R. Wuerfel, Implementation of the Data Distribution Management Services in RTI-NG, Simulation Interoperability Workshop, 02S-SIW-044, March 2002.[3] M. Hyett and R. Wuerfel, Connectionless Mode and User Defined DDM in RTI-NG V6, Simulation Interoperability Workshop, 03-SIW-102, March 2003.[4] Carlos O’Ryan, Doug Schmidt, Russ Noseworthy, Patterns and Performance of a CORBA Event Service for Large Scale Distributed Interactive Simulations, International Journal of Computer Systems Science and Engineering, CRL Publishing, 2001.[5] George Riley, Richard Fujimoto, Mostafa Ammar, Network Aware Time Management and Event Distribution, Workshop on Parallel and Distributed Simulations, May 2000 [6] Friedemann Mattern, Efficient Algorithms for Distributed Snapshots and Global Virtual Time Approximation, Journal of Parallel and Distributed Computing, Vol. 18, No. 4, 1993.[7] Michi Henning, Steve Vinoski, Advanced CORBA Programming with C++,  Addison-Wesley, 1999.Author BiographiesKEITH SNIVELY is a Senior Software Engineer for Dynamic Animation Systems, Inc.  He currently works on the MATREX program, supporting the 1.3 RTI Next Generation, for which he has been a developer for over 4 years.  Mr. Snively has over 7 years experience with modeling and simulation and has previously supported the Countermine and M&S Divisions at NVESD as a software developer and integrator.  Mr. Snively received an M.S. in Mathematics from the University of Virginia.ANNETTE WILSON is a Technical Lead at the Virtual Technology Corporation, Alexandria, VA.  She is currently supporting the MATREX program.  Ms. Wilson has been involved with distributed simulation for over 12 years and has been involved with the HLA since its inception.  She participated in the definition of the HLA 1.3 Interface Specification and IEEE 1516 standard as well as the development of the DMSO RTI 0.x, RTI F.0, and RTI 1.3 prototypes.OutInFed DOut EMBED Excel.Chart.8 \s InFed NodeAFed NodeCFed NodeDFed NodeBFed NodeIFed NodeHFed NodeGFed NodeFFed NodeEFed NodeJFed NodeDFed NodeCFed NodeBFed NodeADist NodeBDist NodeA-2Dist NodeA-1Dist NodeAFigure  SEQ Figure \* ARABIC 3:  Hierarchical InterconnectFed COutInFed BOutInFed AOutInFed DOutInFed COutInFed ACentral ECDistributorOutInFed BOutInFed BOutInFed ALowHiDN 2OutInFed DOutInFed CLowHiDN 1Distribution NodeFed NodeDFed NodeCFed NodeBFed NodeA