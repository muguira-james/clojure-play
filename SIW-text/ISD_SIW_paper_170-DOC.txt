A Practitioner’s View of Verification and ValidationCraig T. Doescher Jayne T. LyonsRosemary T. SeykowskiThe MITRE Corporation1820 Dolley Madison BoulevardMcLean, VA  22102-3481703-883-6742cdoesche@mitre.org,  HYPERLINK mailto:jtlyons@mitre.org jtlyons@mitre.org, rski@mitre.org Keywords:Verification and Validation, Engineering Level Simulations,Federation Development Process, Navy Test and Evaluation ABSTRACT: The Navy's Program Executive Office for Theater Surface Combatants (PEO TSC) has developed an engineering-level HLA federation focused on Integrated Ship Defense (ISD). The ISD federation development approach provides uses a direct path into the model-test-model paradigm to evaluate the federation’s potential to support Test and Evaluation (T&E).  Specifically, , with the , with evaluating the potential to plan, augment, and extend, and even replacde  future combat system test and evaluation is the project goal (T&E).  While this project is not complete, tThis paper discusses preliminary results and lessons learned from the federation verification and validation (V&V) effort, and comments on the complex, evolving inter-relationship between simulation, simulation fidelity, V&V, and live T&E.The ISD federation closely represents the actual anti-air warfare (AAW) combat system installed on Navy ships and includes both tactical code-in-the-loop and detailed physics-based simulations.  The ISD federation simulates system-level effectiveness, targetin, specificallyg the measures of effectiveness (MOEs) and measures of performance (MOPs) associated with the series of Development Tests (DT) and Operational Tests (OT) for the Ship Self Defense System Mark I conducted in June 1997.  Using the data collected from these tests and comparing results to simulated test scenarios, we employ a model-test-mode approach to conductThe federation was designed to accommodate verification and validation (V&V) practices V&V on the federation.  Additionally, because tactical software is used as a federate, the federation is capable of capturing data by producing results in formats identical to data collected during the live tests.  Thus, the tools used to analyze the live test data were will also be used to conduct V&V of the federation. 1. IntroductionIncreasing reliance on modeling and simulation for weapon system analysis, the “newness” of the High Level Architecture (HLA), and the progressive demand for explicit simulation Verification and Validation (V&V) throughout the Department of Defense (DOD) community provides the motivation for the V&V effort in development of thethe Integrated Ship Defense (ISD) Federationfederation development V&V Program. Although V&V has been part of the federation development process since the process began in FY97, the importance of V&V has risen as the Navy looks toward supporting the Test and Evaluation (T&E) community with simulation in the future.The Program Office for PEO (TSC) is leading an effort to explore the use of dAdvanced Distributed Simulation simulation(ADS) as a tool to support their acquisition process, specifically the  Test and Evaluation (T&E) process.   As part of this exploration, the V&V process must be sufficiently understood so as to properly support the T&E community with appropriate simulation tools.  The project makes use of the High Level Architecture (HLA) as the underlying architecture for connecting the simulations, also called federates, together into a federation. This paper represents work in progress and documents our planning process as well as the lessons we have learned thus far.2. The ProblemThe focus of our effort is to determine an executable V&V process using a model-test-model paradigm that is cost-effective addressingbased on marginal investment of limited schedule and constrained resources.  This process must be able to generate, collect, organize, analyze, and report to the stakeholders those data and findings pertinent to establishing an appropriate degree of confidence in the results of the ISD federation studies.The simulation V&V strategies identified, the rationale for the V&V exercises described, and the set of V&V activities specified will contribute to an audit trail by which the representativeness of the federation and the credibility of its predictions may be demonstrated.  Most importantly, we hope to provide feedback to the simulation community by which other federations may base their V&V programs. StakeholdersThere are many agencies within the Navy’s Navy T&E Integrated Ship Defense community acting as stakeholders.  The primary stakeholder is PEO (TSC) with the thought that in that the success of this project from a functional standpoint can lead to great advances in how systems are tested and subsequently acquired leading to reduced costs..  In the future, w e look to the T&E community supporting PEO (TSC) as the potential end user of a tool such as the ISD federation. Other stakeholders include those developing systems in support of the ISD mission such as Naval Surface Weapons Center, Dahlgren Division (NSWC-DD), Naval Research Laboratory (NRL), Johns Hopkins University/ Applied Physics Laboratory (JHU/APL), Naval Air Warfare Center Weapons Division, China Lake and Point Mugu, CA (NAWC-WPNS), and Naval Warfare Assessment Station, Corona, CA (NWAS).,  As system developers, these organization would be greatly impacted by a federation such as the ISD federation for it potentially provides a new method for evaluating system performance.  This is only possible if the federation results can be validated.  Organizations, such as Litton/PRC, Trident Systems, and the MITRE Corporation,Litton/PRC, Trident Systems, and the MITRE Corporation.  These organizations  have participated in either the development of federate software or their integration into a federation and have a stake in the federation’s success and therefore the V&V process.  In some cases, they also represent the developers of the individual systems that comprise the ISD systems.The final stakeholder is the distributed simulation community at large.  It is our expectation that what we learn can help others who are attempting to build federations to support the acquisition of military systems.  If distributed simulation can prove to be successful in acquiring cheaper and better systems, it will be because the resulting simulation is valid and credible.  It will be because we have succeeded in verifying and validating the resulting federation so that it is a credible representation of the ISD system.Scope and BoundThe V&V effort is scoped and bounded by time and resources.  The ISD mission is to provides provide a coordinated self-defense systems for ships that traditionally have have not had had organic defenses to protect against low-flying cruise missiles in open ocean and littoral environments.  ThePhase I of the ISD federation represents includes the critical components of the ISD suite of systems to exercise the complete completing the cycle of detect, control, and engage in a test environment.  HLA is used to facilitate interoperability between the following component legacy simulations representing the ISD components: Ship Self Defense System (SSDS) Mk I, SPS-49 air search radar, Rolling Airframe Missile (RAM) Blk 0, Close In Weapon Support (CIWS), and SLQ-32 Electronic Support (ES) System, and .  The federation also includes both aerial test targets as well as reactive real world threats.  As such, tThe federation will is designed to be used to explore whether the ISD system can meet the combat system effectiveness requiredcalculate the measures of effectiveness (MOEs) and measures of performance (MOPs) used by the test community during acceptance testing.  This type of analysis will allow us to compare simulation results to test results which is the basis for our V&V process..The duration of the project, funded through FY 99, bounds the V&V effort.  The focused V&V effort is bounded by Due to the short durationfunding shortfalls and integration issues therefore a subset of the Phase I federation will be used as a V&V testbed to explore the V&V issues further.  We will concentrate on the detect portion of the ISD functional timeline by including, the federation has been reduced to include the SSDS, SPS-49, and aerial test target federates in the federations, concentrating on the detect portion of the combat system.  Although not optimal, we hope the federation will still provide a V&V testbed and the correspondinus withg lessons learned in formulating a practical V&V process.  Issues concerning the comparison of test data to simulated data when all simulation components are not present will arise and we will examine their impact as part of this effort.  We believe that for this federation portions of the test can be separated into the detect, control, and engage functions.3. GoalsThe V&V effort has two goals.  The first is to demonstrate a feasible and practical V&V process on a federation of simulations using a subset of the the ISD federation.  Most of the V&V efforts to date focus on the V&V process required for a simulation that stands alone.  Significant V&V issues are associated with bringing together multiple simulations to meet a single objective.  Our effort seeks to understand what additional requirements are needed to V&V a federation of simulations rather than a stand alone simulation.  In the end, our objective is to determine what the minimum requirements are for V&V of a federation.The second goal backs the stakeholder, PEO (TSC), whose objective is to make better acquisition decisions and construct a federation to support the support the T&E community responsible for acceptance testing for new combat systems.  The ISD federation is an engineering-level federation of system simulations integrated with HLA.  The federation will allow analysts to determine system effectiveness by computing measures of effectiveness (MOEs) and measures of performance (MOPs) for the Ship Self Defense System collected at the Development Tests (DT) and Operational Tests (OT) in June 1997.  The long-term vision is to provide PEO (TSC) decision makers with a level of detail on combat system effectiveness that exceeds their current capability and to provide a tool to identify potential risk areas prior to testing the system at sea.  The purpose of this effort then is to understand what level of validity a federation must have to support the T&E community.4. Plan Development Our first step was to understand the types of data we had available to us to conduct V&V. Considerable data had been collected during the Ship Self Defense System (SSDS) Mark I Operational Tests (OT) conducted in June 1997.  We structured the federation to include key test components, emulate the test scenarios, and calculate the critical MOEs and MOPs  to the greatest extent possible.  We refer to this approach as model-test-model paradigm to V&V. Essentially, we intend to use the test scenarios to aid in our federation construction, compare simulation results with test data and then refine the federation as a result of comparisons.  OurT he first next step in our the V&V process development was to survey the literature of the simulation community for documented V&V processes and techniques to gain insight into the successes and failures of others.  From this survey of V&V plans and efforts, it was determined that many plans to do conduct V&V have been written, but few plans have been executed.  Most plans appeared incomplete and unexecutable due to lack of M&S requirements and acceptance thresholds, and lack of specific techniques to validate the analysis process. When developing our process we started with the fundamentals and took the process outlined by Law and Kelton [1] and mapped it with the Secretary of the Navy directive, SECNAV Instruction 5200 [2].  This is shown in Figure 1.1 below.  Law and Kelton’s process is a non-specific overarching approach that is widely accepted.  The SECNAV Instruction adds more detail, however not to the point of a “how to” guide.  Our effort focuses on the “Federation” and “Current Results Available” blocks of Law and Kelton and the last two blocks of the SECNAV Instruction.  It is in these areas that we are trying to add insights and a “how to” methodology for a federation.Figure 1.1Our process initially supported Principle 6 of the Recommended Practices Guide which states “V&V of each submodel or federate does not imply overall simulation or federation credibility and vice versa.” [5]  However, throughout the evolution of our effort, we have determined that while having knowledge of the federate V&V status is useful in the federate selection process, it is not necessary that federates be V&V’d prior to integration into the federation.  We understand that fFederates need to be V&V’d in the context of the federation.  Our Our V&V process that evolved supports this hypothesis.5. The Process “Verification” and “validation” is traditionally defined as follows:  “verification” is assessing whether we have built the model correctly; “validation” is assessing whether we have built the correct model.  Our process separates the verification steps from the validation steps and treats them as separate processes.  This separation complimented compliments the use of different scenarios to conduct the testing.  For the verification testing, very simple scenarios were constructed that are not necessarily based on real test data.  They include parameters such as simple aerial target trajectories and stationary ship platforms.  For the validation steps, the test scenarios emulate the DT/operationalOT tests as closely as possible to facilitate comparison between the simulation results and the actual test results.  The overall process is diagrammed below in Figure 1.2..Figure 1.2The verification testing was further divided into three steps that mirror what we have done in successful federation integration testing.  In the first step, single federate operation is verified by stimulating the federation with “stub” or “sister” federates.  The goal is to ensure that message data in the form of parameters of interactions and attribute updates contain reasonable values.  We assess the “reasonableness” through Subject Matter Experts (SMEs).  SME’s also check that the timeline of events is correct.  Lastly, using pre-computed values, we verify that each federate is making the coordinate transformationss correctly. Step 2 of federation verification evaluates pair-wise interactions between federates using data collection tools.  During this step we check that the parameters of interactions between federates comply with what is stipulated in the Federation Object Model (FOM) which is based on system interface documentation.  We verify that messages “make sense” using SMEs and performance data for individual systems.   We also verify coordinate system translations and time management parameters where necessary.The third step exercises all federates repeating the tests in step 2.It is noted in the diagram that federate compliance testing, required by DMSO for HLA compliance, is done in this phase.  It can be conducted either individually by federate developers before federation integration or it can be conducted in the context of the resulting federation.  Our plan is to conduct compliance testing on individual federates after they have been integrated into the federation because the tight coupling of the federate messages does not allow federates to be run separately..Validation is the determination that the resulting federation accurately represents the real system.  It will answer the questions, “have we constructed the correct federation to support Navy T&E and can we emulate a test?”  If there is an existing system, call it the base system, an ideal way to validate the model is to compare its output to that of the base system [3].  In our case, the SSDS MK I was thoroughly tested and data was collected and reported by NWAS.   Using this test data, we selected eight scenarios that vary in complexity and components.  We will then run the simulation multiple times collecting combat system message data and federate communication data for each scenario.  This simulation data will then be compared to test data captured in a series of reports produced by the Navy.  Specifically, we will examine the message traffic between the SSDS and SPS-49 radar.  We will verify message content, ordering, and time stamps.   As mentioned earlier, one of the challenges will be to determine what effect the differences in the base system and the federation have and to validly draw conclusions from each systems data.6. ToolsThe tools used for our V&V process are based on the tools currently used by the T&E analysis facility in the at-sea DT/OT tests of the combat system.  This is a unique feature of our federation and one that allows us to readily compare simulated data to real test data.  A configurable data extraction tool, called DX, in the combat system  software was retained as a capability in the federatesoftware called DX is used to collect SSDS message traffic while the simulation is running.  A test data analysis system, called “the Tool”, enables the analyst to evaluate combat system weapons message traffic and manipulate data into the required measures of effectiveness and performance.  A passive RTI data logger, developed specifically for this project,  will also be used to collect any data exchanged between federates during run time.  This tool can also be used to verify aerial target flight paths, and event timelines.   Tools such as MATLAB will allow us to plot and do statistical analysis on resulting data.7. Lessons Learned to DateBecause we have yet to apply the process and because the federation is incomplete at present, tThe lessons that we learn are most important to the simulation community as a whole.  Firstly, the V&V community would be concerned about the validity of this paper if the first two lessons were not mentioned.We have re-discovered that V&V cultures (terminology, methods, etc.) differ across the simulation community and there is a need to account for differences with education and documentation at onset of a considerable V&V effort.  We did not do this initially until we as a team were at a standstill and could not go forward.  We recommend this be done at the beginning of the federation development process.One cannot start conducting V&V early enough in the federation development process.  V&V should be of primary concern when making federate selections.  It The V&V status of federates is not however a pre-requisite for inclusion in a federation.Operational or tactical software, used as federates, introduces an interesting V&V debate on the need and level of V&V required.  Some view the need for V&V to be negligible because the simulation is the operational software.  Based on our experience, significant V&V issues arise when the operational software is changed to enable integration ine to the federation.  These changes require the federate to be validated.n. Legacy simulations have V&V histories that are often based on their accepted usage over time rather than rigorous methods and are rarely documented sufficiently.  One could spend inordinate amounts of time piecing together documentation and determining their status.  We recommend that one not take time sorting it out, rather the time would be better spent understanding the capabilities and limitations of the federate.V&V of individual federates is is not sufficient; the federation needs to be V&V’d as a single system.  Likewise, federate V&V is is not a a prerequisite for federation V&V.  A federate may have a V&V history but unless it was used in a federation for the same purpose, it needs to be re-V&V’d in the context of the federation.  We spent too much effort requiring federate developers to V&V individual federates and provide supporting documentation. We realize that this was wasteful and time would have been better spent conducting the V&V on individual federates while running in the federation where inputs, environments and assumptions are different from running standalone.We neglected early in the federation development process to document our conceptual model sufficiently.  At the time, we had not realized how important this would be to the V&V process.  The conceptual model dictates what is being built and for what reason. For this reason, the document supports the V&V process. We recommend that federation developers carefully construct their conceptual model with the V&V process in mind.  We also recommend that the conceptual model itself be validated.When designing a V&V process, it is imperative to understand what types of data are needed to conduct V&V and to influence the data collection process as much as is possible.  In our case we had considerable amounts and types of data coming from the SSDS MK I operational testing at our disposal for V&V.  We then designed our V&V approach around exploiting that data.  Problems arise, however, when data update rates are not sufficient or important data from a simulation perspective is not part of the test collection.  As a result, we were to depend on system experts rather than quantitative data to validate the simulation’s performance.Similarly, thought should be given to data formats, coordinate system translations, and how “ground truth” is defined.  There is considerable data manipulation that is done in test data reduction.  Understanding these processes is imperative when using test data for validation purposes.8.  ConclusionsWe have employed a model-test-model approach to our V&V process for the ISD federation.   Using data collected during system operational testing and following guidance from literature we formulated a plan that allows us to compare simulation results to live test data.  We realize that our V&V process is predicated on the existence of test data and additional efforts are required to understand how to use simulation to support acquisition decisions prior to the testing phases.9. Conclusion109. References [1]	Averill Law, W. David Kelton: Simulation Modeling and Analysis, McGraw-Hill, New York 1991.[2]	Secretary of the Navy, SECNAV Instruction 5200.[3]	Jerry Banks: Handbook of Simulation, John Wiley & Sons, Inc., New York 1998.[4] DOD Instruction 5000.61 “DOD M&S VV&A”, 29-April-1996[5]  DOD VV&A Recommended Practices Guide, Nov-1996, COMOPTEVFORINST 5000.1[6]  “Use of M&S in Operational Testing”, 5-Sept-1995. Author BiographiesJAYNE LYONS, an electrical engineer at MITRE Corporation, is tasked with the technical leadership of the integration team for the ISD federation.  She is a member of the ISD Verification and Validation Team.ROSEMARY SEYKOWSKI is the Acting Department Head for the Navy Science and Technology Programs Department at the MITRE Corporation, McLean, VA.  She is a lead member of the ISD Verification and Validation Team as well as the Naval Functional Team Leader for the Joint Countermine Operational Simulation.CRAIG DOESCHER is a Senior Operations Research Analyst at the MITRE Corporation, McLean, VA.  He is a member of the ISD Verification and Validation Team as well as the Army Functional Team Leader for the Joint Countermine Operational Simulation.