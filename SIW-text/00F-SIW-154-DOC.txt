Measuring Training Effectiveness in the Simulation DomainMatching Training Metrics to Simulation Capability and Performance RequirementsMr. Carl HainesSubmarine Training Systems Program Manager Naval Sea Systems Command code 92L1Arlington, VARick Severinghaus Tom BelkeTodd ClareBoozÂ·Allen & Hamilton, Inc.816 Greenbrier Circle Suite 208Chesapeake, VA 23320severinghaus_rick@bah.com, belke_tom@bah.com, clare_todd@bah.comKeywords:Occupational Standards, Common System Interfaces, Metadata, training trend analysis, Measures of Performance, Measures of Effectiveness, portable handheld data device(s).ABSTRACT: This paper describes a methodology developed to integrate three aspects of Category I simulation/simulator design - 1) requirements implementation, 2) warfare mission area "operator" proficiency standards, and 3) measurement of training effectiveness. Using a U.S. Submarine Force example, an argument is presented for the simulation community to standardize data interface requirements to facilitate performance data mining and quantitative measures of effectiveness (MOEs) for comparisons among systems, user groups, and among simulator trainers, simulator models and real world data.IntroductionThe paper describes a methodology developed to integrate three aspects of Category I simulation/ simulator design - 1) requirements of implementation, 2) warfare mission area "operator" proficiency standards, and 3) measures of training effectiveness. In addition, this paper considers the related use of metadata standards for use in cross-system, simulation, and historical comparisons.Why are Occupational Standards Needed?Occupational standards are "operator" proficiency standards in warfare mission areas that measure the level of expertise of warfighters in warfare skill areas. Why are such standards needed, and why should they be developed? [The examples that follow in this article are in submarine warfare areas; however, the same principles may be applied to many other warfare areas.]The following examples might illustrate why such standards are of value. Example 1: A submarine Commanding Officer's (CO's) sonar control party goes to a team trainer facility, as represented in Figure 1. After the team is presented with a series of operational simulation scenarios, the staff of the team trainer facility determines that the sonar control party is "below average." The Commanding Officer requests to know what objective criteria were used to determine that his sonar team was rated "below average." Is the training center staff in a position to quantitatively show the CO how his team rated against other submarine sonar teams based upon well-defined metrics (see Figure 1)?Example 2: A submarine combat systems team undergoes a rigorous sequence of simulated approach and attack exercises in a team trainer environment. At the end of several days of intense training, the evaluation team leader tells the combat systems team that they have improved considerably since theyFigure  SEQ Figure \* ARABIC 1. Based upon what objective criteria is this sonar team to be evaluated and compared?attempted the first mission scenario. The Commanding Officer asks for specific information to show where his team has improved, where they have not, how they compare to other submarines, and where further training is needed. Is the senior evaluator in a position to provide the Commanding Officer with quantitative comparison data?2.1 Answering the need for evaluation standardsOver the course of the past 2 years, under the direction of the Submarine Trainers & Training Directorate of the US Naval Sea Systems Command (NAVSEA92L1), a set of Warfare Team Trainer Occupational Standards has been developed for application in evaluating two major aspects of the design of the Submarine Multi-Mission Team Trainer (SMMTT). SMMTT is a new generation, DIS and HLA compliant, virtual man-in-the-loop simulator. The SMMTT is a Category I simulation of the major combat system functionality of the U.S. Submarine Force's front line fast attack submarines. Commonly called a "team trainer," the SMMTT simulates and/or stimulates sonar, fire control, weapons, non-acoustic sensors, and communications capabilities. Through such "sim/stim," submarine Commanding Officers are provided the capability to exercise their entire battlestations team in a realistic environment in a variety of warfare mission areas. Watch teams of up to two dozen personnel may be exercised, depending on the exercise tactical scenario and its complexity.Recognizing the complexity of the SMMTT simulation design and the difficulty in assessing how effective the design could be in serving the training needs of the Submarine Force in a post Cold War environment, NAVSEA embarked on a project to develop warfare mission area Team Trainer Occupational Standards, with an objective to map these Standards to the capabilities of the SMMTT. In the course of developing the Occupational Standards, an additional objective was identified as achievable, that of developing watch team performance measures in terms of the specified standards. The ensuing project effort resulted in the mapping of watch team performance measures to both the SMMTT's provided capabilities and the Occupational Standards.A Need for Standardized Interfaces.Standardization of metadata for system interfaces is area where specific performance data may be leveraged for valuable comparisons. A wealth of historical performance data is stovepiped within various operational and training databases. However, such data is not available for such things as:Cross-class comparisons of collected training dataTraining trend analysisComparison of future R&D systems and weapons with legacy systems and weaponsHistorical comparisons among different weapon systemsCalibration of simulations against real world dataThe existence of  common system interface standards, as illustrated in  Figure 2, could provide such comparison of diverse data sets.Figure  SEQ Figure \* ARABIC 2. Standardization of common system interface standards would make quantitative cross-system data comparisons possible.For example, it is commonly asserted that the ASW proficiency of US submarines has, in general, decreased since the end of the Cold War. However, such assessments tend to be highly subjective. With common interface standards in place, it might be possible to provide a more quantitative assessment.Figure  SEQ Figure \* ARABIC 3. A multitude of potential uses of system metadata exist including training and future R&D.A multitude of other uses of system metadata exist as shown in  REF _Ref489028632 \h Figure 3. For example, if a program manager wanted to know if a contemplated weapon Engineering Change Proposal would correct a meaningful design deficiency, a existing weapon performance could be used to model the ECP. Quantitative results might show that the contemplated design change would simply not be worth the cost.An example of such an application of quantitative metadata is to consider a future new torpedo weapon system. What would happen if the acoustic search frequency of the weapon were decreased by 5 kilohertz?  REF _Ref489031666 \h Figure 4 shows a the results of a comparison between such a future notional weapon and hypothetical torpedo firing data from an existing weapon. Note that the availability of sufficient metadata would enable one to simulate a significant number of identical torpedo runs while only varying the search frequency. In our example, the results indicate that modifying the search frequency would improve the performance of the weapon.Figure 4. Use of historical weapon performance data for quantitative R&D weapon comparisons between existing and future weapons systems.Common system interface metadata definitions would also enable the sharing of modeling & simulation data between joint and coalition systems and their associated tactical simulations and trainer models as depicted in Figure 6.Figure 6. Common system interface metadata standards would facilitate linkage among dissimilar systems.In particular, common system interface metadata standards for performance data would enable comparison of C4ISR system performance data. The use of a common metadata interface standard would provide one the ability to compare similar mission performance data among different systems and/or different simulated environments.Considering MOEs and MOPsQuantitative Measures of Effectiveness (MOEs) and Measures of Performance (MOPs) are the cornerstone concepts for establishing warfighter operational proficiency standards. The differences between MOEs and the more well known metric, MOPs, are generally not well understood. MOPs tend to be more objective and system oriented. MOEs tend to be more subjective and operationally oriented.For example, Figure 5 (see previous page) shows a submarine sonar system that provides acoustic sensor information to the Officer of the Deck (OOD) and Fire Control Tracking Party (FCTP). Operator thought processes and skill levels are typically evaluated using Measures of Performance (MOPs) criteria. For example, did the operator use his cursor and assign tracker(s) to a new possible sonar contact? How long did this take to perform? Did the sonar supervisor make use of all the correct displays when the ship "Manned Battlestations?" How long did it take to make a new contact report to the Officer of the Deck? These and similar metrics are typically well known and documented for systems. The commonality of such data with other systems is not.Measures of Effectiveness (MOEs) are a comparatively newer concept for evaluating warfighting performance. MOEs tend to be more subjective and mission-oriented. As an example, to what extent did the OOD demonstrate proficiency in correlating multi-source contact information. Such a correlation might be attained via many different methodologies and still be correct. Thus, the criteria for MOEs tends to be less well-scripted and less button/knob/click-oriented and more results oriented.Portable Handheld Data Input DevicesThe advent of portable handheld data input devices adds a new option for man-machine system interfaces that enable tactical evaluators to not be "tied to" a computer terminal, while still being able to enter data electronically through what is, in effect, an pre-formatted electronic clipboard. Use of portable handheld data input devices can streamline inputting of quantitative MOP and MOE data by training evaluators, as shown in Figure 7 (see previous page). Figure 8. Use of portable handheld data input devices would streamline inputting of quantitative MOP and MOE data by training evaluators.A handheld Occupational Standards (OCCSTDS) measurement tool can bring online evaluation capability to the watchstation. A handheld data input device provides a portable stylus-driven database system in a "point-and-click" electronic format, with a supporting database engine which performs the bulk of the analysis work. This capability allows evaluators to generate near real-time reports that are standardized, searchable and trackable.  This can be a valuable addition to an evaluator's 'toolset': for the SMMTT OCCSTDS project, approximately 280 observable watchstation tasks or behaviors have been identified, mapped to a data set of some 20,000 performance measures, references, and proficiency standards. Use of such a device provides an OCCSTDS tool that can be used to evaluation specific watchstation performance, with reachback to relevant reference publications, performance criteria, and historical watchstander records. The handheld device also has the advantage of facilitating the synchronization of shipboard and shore-based training data for subsequent trend analysis.The platform for the handheld OCCSTDS tool is a two-pound stylus-driven tablet computer (see Figure 8 on previous page). The palmtop/ armtop ergonomic hardware design is typically shock mounted and ruggedized. Commercially available handheld devices nominally include a 4 GB harddrive, 233 MHz processor, 64 MB of RAM, and are loaded with software capable of synchronization with Windows 98 and Microsoft Access.By way of example (refer to Figure 9 on previous page), a staff evaluator is watching a Commanding Officer during battlestations for strike (TOMAHAWK land attack missile) operations. On the handheld device, the evaluator would select battlestations "STRIKE," and then "AO/CO". In response, the OCCSTDS tool will present relevant Occupational Standards/Supporting Standards  associated with strike operations  in the appropriate windows of the  display. "Supporting Standards" are displayed only to extent they have been programmed into the database. Using the displayed information, AO/CO performance in each  occupational standard can graded by the evaluator. This is accomplished by  'pen-tab'  of the desired knowledge level (list of choices, bottom right of the display; equivalent to a scale of 0 to 5). Upon completion of the evaluation, entered performance data can either be sent manually or automatically for processed, summarized, and delivered to appropriate display formats for exercise debrief and feedback . Use of the handheld device has many advantages. For example, any desired information display is available to a system training evaluator via  a maximum of 4 or 5 pen taps. Additionally, since the relational database stores information at every keystroke, the evaluator can monitor several watchstanders simultaneously  without losing evaluation data. Additional features include:Listing of all OCCSTDsA complete self-contained description of knowledge level requirements for each OCCSTD.At any level of evaluation, a button push allows the input of a context-specific text note.One-step synchronization of data between shipboard and shore-based systems.Applications of MOE/MOP dataSenior operational commanders will be better able to quantitatively measure the warfighting skills of their assigned units through the use of MOE/MOP data. For example, Figure 10 (below) shows how the use of quantitative MOP/MOE data would enable Commanding Officers to be provided with meaningful readiness trend data in specific warfighting skill areas. In this particular case, a hypothetical submarine sonar watchstanding team is rated at roughly quarterly intervals. The orange vertical bar in each of the 4 performance categories shown (Jun 00 data) shows sonar team proficiency upon returning from a lengthy ASW mission. The second set of data (red bars, Aug 00 data) shows the results of an equivalent ASW mission, but performed as a SMMTT team training/simulator exercise 2 months after the crew returned to port and turned its attention to other work.  As expected, team performance declined across-the-board. The third set of data (Dec 00) shows the results of an performance evaluation of the same ASW mission, performed at-sea in local Submarine Operating Areas 6 months following deployment, but just after a week's dedicated sonar operator refresher training in port using the SMMTT simulator. As might be expected, the sonar team's performance is shown to have measurably improved as a result of the dedicated training period.Figure 10. Use of quantitative MOP/MOE data would enable CO's to be provided meaningful readiness trend data in specific warfighting skill areas.Standardization RecommendationsUsing a Submarine Combat Systems simulator as an example, it has been shown that it is possible make use of collected warfare mission performance data in a number of ways:To evaluate specific watchstander performance in a team training environmentTo facilitate comparisons of specific performance measurements with performance benchmarks (themselves developed through collection and analysis of data in a consistent manner)To facilitate analysis addressing how well a simulator's functionality actually serves User training requirementsTo assist Acquisition Managers assess needs for simulator improvements and/or capabilities changesTo aid comparison of  'real world' performance to that achieved within a simulation/simulator. Within the Submarine training domain, effort is ongoing to develop appropriate tools for mapping requirements to specific implementations, and to develop data collection tools to facilitate the types of analysis and performance evaluation activities just listed.What is missing in this effort is the existence of a set of accepted standards for the format(s) to be used for performance measurement data. Such standards would be of significant value to activities using MOP and MOE data to serve also related program analytical work.  Referring again to Figure 4,  it should be noted that there currently are no standards to support the hypothetical analysis postulated.  Raw data does exist for real in-water torpedo firings, a paper based data  set covering decades' worth of Submarine Force at-sea exercise torpedo firings.  And, data for  hundreds of torpedo firing runs, conducted in the SMMTT simulator, can be obtained. BUT, there are no standards in place specifying either metadata or data structures  which would support building of databases to facilitate comparison, benchmarking, and/or calibration of in-water results against results obtained in Submarine Force simulators.Such capability would be useful in a number of applications, two of which will be mentioned here.a. In simulation exercises,  torpedo in-water  performance and target "hit criteria" are controlled by software algorithms.  It would be useful to be able to calibrate those algorithms against known, real world, in-water performance, so that tactics practiced in the simulator would yield results comparable to that achieved at sea on the Navy's tactical acoustics ranges.* With appropriate data standards and data collection, this should be possible.b. As in the example in Section 3 of this article, calibration of the simulator to in-water results could make it possible to use the simulator as a 'trial platform' to try out changes to torpedo design parameters, if in fact Fleet operators could be convinced that results in the simulator faithfully approximated  in-water results (at least for a demonstrated set of conditions). That being the case, it might then be quite useful to tactics development activities to be able to try out a parameter change (torpedo sonar frequency or torpedo operating speed) in the simulation environment for a quick assessment  of  those changes on end results, even if only qualitative in scope.*1. Physics based models exist and are available to the Submarine Force to serve Acquisition program and Test & Evaluation program needs for torpedo performance models. But these models are of much higher complexity and cost than what is needed to serve training needs.ConclusionsThis article has attempted to show the value to be realized by coordinated and integrated matching of  training requirements to training standards, and further to demonstrate the benefits to be realized by mapping identified training standards to the capabilities of virtual man in the loop/Tier I simulations/simulators.Enhancements to the quality of simulation based training can be achieved through a process of matching warfare mission area "operator" proficiency standards to training requirements, and to then provide automated tools for data collection to facilitate effective analysis of training performance.It has also been argued here that the performance data collected can be useful for other purposes, specifically:Calibrating simulation algorithms against "real world" results, at least within some defined subset of the simulator's capability.Assisting tactics or systems development activities, at least in a qualitative way, in evaluating contemplated changes to existing warfare systems. Assisting simulation system developers in evaluating the need for changes and/or improvements in simulator functionality to better serve training needs and requirements.Developing a robust capability in these areas has been restrained by a lack of standardized formats for the collection and storage of data documenting operator warfare mission performance and mission results in the simulation domain. Having such standards, and appropriate metadata to govern cross-program utility , could add significant  functionality to Tier I combat systems simulators now applied in practice solely to the training purposes. And, standardizing appropriate data interface requirements would  facilitate data mining  for performance assessment and quantitative measures of effectiveness (MOEs) analysis purposes.End use capabilities enabled by such simulation domain standards might include comparisons among systems and user groups, as well as among simulator trainers, simulator models and real world data. Such standards would improve an acquisition manager's ability to more accurately specify the implementation of requirements both in software code and in simulation representation.The need for the standards recommendations discussed here stems from the knowledge gained/developed in the performance of the Occupational Standards development project.  This project work will continue under the direction of the Navy's Submarine Trainers and Training Systems Directorate, as it continues efforts to provide the Submarine Fleet high fidelity, realistic simulation based training capability.9. References[1]	Richard J Severinghaus and James P Held: "Submarine Warfare Officerâ€™s Occupational Team Trainer Standards" Unpublished report to NAVSEA code 92L1, April 2000.[2]	Robert J. Howard, Doug Clark, Terry Foreman: "Architecture and Approaches Used in Acoustic ASW Trainers" Paper 00S-SIW-053, Workshop Papers, Simulation Interoperability Workshop, Spring 2000. [3]	Peggy D. Gravitz, Steve J. Swenson: "The Navyâ€™s M&S Standards Development Activities" Paper 00S-SIW-086, Workshop Papers, Simulation Interoperability Workshop, Spring 2000.[4]	Jeffery J. Bergenthal, Douglas Clark, Paul W. Maassel, Dr. Carrie Karangelen Root: "Development of a Naval Training Federation" Paper 00S-SIW-054, Workshop Papers, Simulation Interoperability Workshop, Spring 2000.Author BiographiesMR. CARL HAINES is the Submarine Training Systems Program Manager and Technical Data Branch Head in the TEAM SUBMARINE directorate at NAVSEA. His responsibilities include acquisition, configuration control, and life cycle support of U.S. Submarine Force training systems products and services, including attack and ballistic missile combat systems manned simulators, weapons, navigation, sensor, and communications simulators, part task trainers, and numerous submarine systems maintenance and operations simulators and trainers. He is also responsible for operation and maintenance of all NAVSEA cognizant SSN/SSBN CCS/HM&E Submarine training facilities and team training laboratories. Mr. Haines has been employed at NAVSEA since 1985.  Prior to that, he worked with an Engineering Services support contractor upon completion of active Navy duty and  transfer to the fleet Reserve in 1982.  While in the Navy, he served on various classes of  SSN and SSBN submarines and completed two tours of shore duty,  one at Naval Submarine School and one as the HM&E Submarine Training Program Coordinator (TPC) in the offices of the Chief of Naval Technical Training, Memphis, Tennessee.RICHARD J. SEVERINGHAUS is a consultant with Booz,Â·Allen & Hamilton, Inc. He has extensive experience in operating and managing stand alone and networked virtual simulators, primarily in applications for training of Naval forces, and is involved in ADS systems design integration work. Within the Simulation Interoperability Standards Organization (SISO), he serves on the Conference Committee of the Simulation Interoperability Workshop (SIW), and is a member of the TRAINING and EMF Forums. He also has significant experience as an Exercise Manager in large scale distributed simulation exercises.THOMAS J. BELKE is a consultant for Booz,Â·Allen & Hamilton, Inc. His focus centers on information technology and related requirements definition. Tom is the author of recent articles including: Technology Driven Upgrades Give Subs Combat Advantage (National Defense, October â€˜95), Controlling C3I System Life Cycle Costs (Sub. Qtrly Review, July â€˜96), Coping With Open System/COTS Supportability (Sub. Qtrly Review, October â€˜96), Incident at Kangnung: North Koreaâ€™s Ill-fated Submarine Incursion (Sub. Qtrly Review, April â€˜97) and The Spirit of Human Bombs (Sub. Qtrly Review, April â€˜98; East Asian Review, Summer, â€˜98). Mr. Belke is author of the new book, JUCHE: A Christian Study of North Korea's State Religion (Living Sacrifice Books, 1999).TODD CLARE is a consultant for Booz,Â·Allen & Hamilton, Inc. His focus centers on information technology, including display technologies, learning systems and learning methodologies.PAGE  Figure 5. MOPs tend to be more objective and system-oriented; MOE's, more subjective and mission -oriented.Figure 7.  Handheld device data would be electronically ported into a trainer site's RDB for near real-time analysis & feedback to fleet operators.Figure 9. OCCSTDS watchstander evaluation data is entered into a series of user-friendly displays on a portable handheld device.