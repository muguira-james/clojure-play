Live-Virtual-Constructive (L-V-C) Semi-Automated Data Collection in a Classified Distributed Test Environment using the Integrated Level Hierarchy (ILH) and the Test and Training Enabling Architecture (TENA)Jason BolinU.S. Army Redstone Technical Test CenterHuntsville, AlabamaCSTE-DTC-RT-T-IM, Bldg. 7884Redstone Arsenal, AL 35898-8052 HYPERLINK "mailto:dbrowning@rttc.army.mil" jason.bolin1@us.army.milRyan NormanU.S. Army Redstone Technical Test CenterHuntsville, AlabamaCSTE-DTC-RT-T-IM, Bldg. 7884Redstone Arsenal, AL 35898-8052 HYPERLINK "mailto:dbrowning@rttc.army.mil" ryan.t.norman@us.army.milJosh SellsU.S. Army Redstone Technical Test CenterHuntsville, AlabamaCSTE-DTC-RT-F-FL, Bldg. 7884Redstone Arsenal, AL 35898-8052 HYPERLINK "mailto:darryl@amtec-corp.com" jsells@rttc.army.milAdam TroupeERC IncorporatedHuntsville, Alabama555 Sparkman Dr., Suite 1622Huntsville, AL 35816 HYPERLINK "mailto:darryl@amtec-corp.com" atroupe@rttc.army.milKeywords: MSDE, ILH, TENA, Distributed TestingABSTRACT:  The fifth Multi-Service Distributed Test Event, or MSDE, was executed in August 2005 to universal success.  Among many other accomplishments, MSDE demonstrated the seamless collection, distribution, and analysis of distributed data on a classified network. This was possible by close coordination between Virtual Proving Ground’s (VPG) ILH standard meta-data and the MSDE TENA Logical Range Object Model (LROM). ILH standard is a universal Army Developmental Test Command (DTC) naming convention for “Systems” under test, related “Projects”, and all other meta-data necessary to uniquely identify a digital “Resource”. All Army DTC facilities have participated in the development, deployment, and population of the ILH.In order to maximize reusability on either a centralized or distributed scale, the TENA community standard Object Models, such as the Platform Object Model, were extended with the MSDE LROM where possible.  Use of standardized object models facilitated the reuse or minor modification of generic applications, such as data loggers or a 3D Viewer.  Additionally, the MSDE LROM included the VPG community standard ILH Object Model, which enabled range assets to be situationally aware of the test under execution.  Therefore, data was easily distributed amongst physical or simulated TENA-enabled range assets.  This data was then automatically referenced into ILH by using its TENA-provided ILH meta-data association.  In addition, data that is easily distributed within TENA can be compared to range truth data that is also available on the TENA Network.    The ILH meta-data standards enabled the TENA collected data to be programmatically referenced in the ILH to await data reduction at the engineer’s convenience. Once reduced, the collected data can then be published from the multiple data collection points to the Inter-Range Command Center (IRCC) where the data can be shared and searched by all participants across the command. Since the ILH was designed to eliminate the need for moving test data to a centralized location, utilizing a centralized storage location for ILH referenced files is obviously not a recommended solution. However, this was the only feasible solution due to the classified nature of MSDE and a lack of time to implement a classified data-store at each of the distributed data collection points.The MSDE test event proved to be a success at RTTC as well as across the command and showed that when you have standards in place such as the ILH meta-data and a standards-based TENA LROM, data can be distributed, cataloged, manipulated, published, stored, analyzed, and searched quite easily. IntroductionThe Multi-Service Distributed Event (MSDE) was executed in August 2005 to universal success.  Among many other accomplishments, MSDE enabled the integration of live range assets into a classified distributed test environment.  In order to incorporate live assets into such a large distributed test, Redstone Technical Test Center (RTTC) implemented a Military Operations in Urban Terrain (MOUT) Scenario for inclusion in the overall event plan.  The MSDE provided RTTC an excellent opportunity to improve both test processes and infrastructure.  This scenario had two overarching objectives.  First, the scenario provided a tactical engagement of both Live and Simulated Blue and Red Dismounts in a classified urban test environment and the opportunity to mix live, virtual, and constructive forces.  Second, the scenario allowed for the successful collection of test data during the tactical engagement to facilitate post-test data analysis.  The MOUT Scenario was designed to serve as a risk-reduction event in order to meet these overarching objectives, with the goal of determining if the processes were in place to collect valid test data and perform relevant post-test data analysis in a distributed test environment. This paper will give a high level view of how data collection was performed across all MSDE test sites, with detailed explanations of how data collection was handled at RTTC. Integrated Level Hierarchy (ILH) Data standards have been designed, developed, approved and promoted by all test centers to establish a standard database architecture known as the Integrated Level Hierarchy (ILH).  The ILH database schema (Figure 1) is the minimum required data architecture each test center’s data system must use to communicate with other test centers.  Metadata within the ILH provides a “card catalog” on what is being tested, who conducted the test, project information on tests being conducted, types of tests conducted, test event information, and unique identification of test articles. All of this metadata is used to identify, store, and retrieve digital objects known as resources. The referenced digital objects are items such as documents, test data, images, models, simulations, and software. To insure intra-range unclassified data integration, the ILH enables the development and real-time deployment of metadata standards via database table replication across the command. The ILH structure and metadata will enable distributed queries for data pertaining to distributed tests and enable standards for external connections.  SHAPE  \* MERGEFORMAT Figure 1: ILH database schemaDue to the classified nature of the MSDE event the ILH was centrally located at White Sands Missile Range (WSMR). This was not the preferred method of implementation however it was the most cost and time efficient option since WSMR already possessed a classified filestore and database. Having the ILH reside at a central location would unfortunately require all participants to send their data to WSMR and this would also require detailed post test analysis to occur at WSMR. This would prove to be a major headache as some of the participants were collecting 5-10 Gigs of data each day of the event. The various methods used to send the data to WSMR will be discussed later as well as the automated processes which enabled WSMR to correctly place each participant’s data in the correct location in the ILH. The ILH structure formed the data backbone for the MSDE data collection, storage, and analysis process (Figure 2). EMBED Visio.Drawing.6  Figure 2: Integration of HotFolder, ILH, ILH Viewer, StarShip, & TENA for MSDE Data Collection3.0 HotFolder A HotFolder is a folder on either a local or networked machine that has a Windows service watching it at all times for the creation of XML files. Upon dropping an XML file into the HotFolder, a “file created” event is raised. Once the “file created” event is fired, the HotFolder service spawns an ILH Processing object which will parse the XML file and perform the necessary steps in order to store the resource file(s) in the file store and update the database (Figure 3).  The ILH HotFolder is used to automate the storage of resource files that are associated with ILH tests, projects or systems. The XML file which triggers this process is called the ILH Header file. The ILH Resource Header contains the minimum amount of ILH metadata to uniquely identify and associate a digital resource with a test.  Below is a basic representation of the make-up of the XML header file (Figure 4). The header also associates any “child” files (such as pictures, videos, or documents) to the same test. The Windows service which is parsing the XML file then uses the ILH Header information to insert a new record for the file (resource) or group of files (resources) in the database.  After the new record has been created in the database the ILH Header file is updated with any missing information and the Header file and child files are moved to the filestore. Since the header files contain all the information about the resource files in the file store, it is possible to reconstruct the data base using just the XML files should the database become corrupted or have any other catastrophic failure. INCLUDEPICTURE "cid:image001.gif@01C65FC6.B6392700" \* MERGEFORMATINET Figure 3: HotFolder Internal Workflow <ILH_ResourceHeader>…metadata that identifies the test, project or system that is associated with the resource file(s)…<ILH_ResourceFiles>                <File>                                	metadata about the resource (file) being added, one for each file		….                </File>                <File>                </File>                …</ILH_ResourceFiles></ILH_ResourceHeader>Figure 4: Sample ILH Resource Header 4.0 TENA Logical Range Object Model OverviewDefining the use of Hotfolders and ILH as part of the Data Collection process is only part of the solution.  It is also essential to define and deliver the physical data to be collected in the distributed environment.  The Test and Training Enabling Architecture (TENA) and its Logical Range Object Model (LROM) paradigm were used in order to accomplish this mission.  First, the definition of this data will be examined through the use of the MSDE LROM.  Secondly, delivery of the data will be examined through the use of the RTTC TENA Platform Logger.  Please see the accompanying paper “Live-Virtual-Constructive Data Fusion in a Classified Distributed Test Environment using the Test and Training Enabling Architecture” for more information about TENA.The TENA LROM is used to define both the data to be transferred in a distributed environment and the interoperability rules between disparate sites.  Since TENA is object-oriented, an attribute defined in the LROM is set and accessed in the same manner by all applications.  For MSDE, a standards-based TENA LROM, called the MSDE LROM, was defined.  In order to maximize reusability on either a centralized or distributed scale, the TENA community standard Object Models were extended with the MSDE LROM where possible.  MSDE Applications simply subscribed to the TENA Standard Platform object to receive data from any L-V-C Platform within the simulated battlespace, regardless of whether the battlespace represents the actual MSDE event or any other current or future event that utilizes the TENA Standard Platform.  Additionally, the MSDE LROM included the VPG community standard ILH Object Model, which enabled range assets to be situationally aware of the test under execution through the distribution of test metadata throughout the distributed battlespace.  Therefore, data was easily distributed amongst physical or simulated TENA-enabled range assets.  It was then possible to automatically reference into ILH by using its TENA-provided ILH meta-data association.  Use of standardized object models facilitated the reuse or minor modification of generic applications, such as data loggers or a 3D Viewer.  The RTTC TENA Platform Logger provides an ideal example for the utilization of standardized objects.5.0 RTTC TENA Platform LoggerThe purpose for any experiment is to collect data.  The complexities of setting up the distributed testing process and the collection of distributed testing lessons learned limited the amount of system test data that could be examined in past Distributed Test Event’s.  RTTC’s work on the Unmanned Systems Initiative (USI) included a first attempt at developing some of the systems required for a minimally automated data collection and archival process of data collected on live assets.  However, the USI data collection process presented problems for MSDE since data collection during USI was distributed across instrumentation interface applications.  The many different data loggers each needed access to the data store location, which presented a large problem in a classified distributed environment, such as MSDE’s network configuration.  To mitigate this, a “one tool to log it all” design approach was used at the MOUT site for MSDE.  This tool was the TENA Platform logger.The TENA Platform Logger is a .NET Windows application that subscribed to every TENA community standard Platform SDO publishing on the live TENA network, including any platforms or messages that were pushed through the DIS-TENA gateway.  The TENA Platform Logger also subscribed to and logged messages that were issued via TENA allowing it to create a complete XML database of the platforms and events in the scenario as it unfolded for post test analysis or replay.  XML was chosen as the database format for several reasons.  First, XML is plain text and therefore is easily recorded with minimal development resources and without the need for extra software licenses for relational database suites, such as MS SQL Server or Oracle, or for proprietary binary data formatting libraries.  Second, the fluidity of the scenario and delay in the delivery of the most recent version of the TENA community standard Platform object model used in the exercise required a database that could easily grow and change with the changing scenario.  Finally, XML is an independent standard which allows for tools to be easily developed for post test analysis.  At the very least, XML generates a test data file that is easily read by human eyes even without additional software development.  One draw back to using XML is, of course, the resulting size of the database.  However, this was remedied by integrating Zip compression of the generated database into the TENA Platform Logger application itself thereby reducing the size of the database by roughly 100 times before archival in the ILH.  During execution of the scenario, the TENA Platform Logger displayed a list of discovered TENA standard Platform SDO’s and allowed the user to “peek” at any particular Platform’s current TSPI data by simply selecting the appropriate Platform from the list.  This real-time peering also enabled “spot checks” of TSPI data from live vehicles, such as the UAVs, against the ground truth data being collected by RTTC Instrumentation.  While it did not eliminate the need for Data Analysis, these “spot checks” allowed the examination of the accuracy of on-board GPS devices without having to undertake extensive Data Analysis upon test completion.  Please see the accompanying paper “Live-Virtual-Constructive Data Fusion in a Classified Distributed Test Environment using the Test and Training Enabling Architecture” for further benefits of this real-time view of test data.  As the entities’ states were updated in TENA, the Platform Logger recorded the information in the database.  Destruction events issued by any discovered TENA Platform SDO were also recorded.  Upon the completion of the event, the Logger would take the ILH event meta-data published in TENA for the test run, zip the database files into one archive, and would FTP the file to the HotFolder. This would automatically insert the XML database into the data store in the correct location in the hierarchy tree. This automated process along with the integration of the ILH Viewer allowed for easy retrieval of the scenario for post event analysis.6.0 ILH ViewerThe ILH focuses on the management of metadata that allows users to locate and retrieve resources that are digital objects. Since the ILH is a database schema and standardized metadata the need arose for an interface to view resources, hence the development of the ILH Viewer. The ILH Viewer is a generic web based interface developed for the ILH by RTTC (Figure 5).  A web based solution was chosen to enable the continuous evolution of the application without the need to update numerous clients as well as producing a single point for access control. The ILH Viewer is a web based interface to the ILH meta-data and file store.  It enables browsing of the entire hierarchy, the meta-data, and test data files associated to any System, Project, or Test in a format similar to the directory-file organization of modern windowed operating systems.     SHAPE  \* MERGEFORMAT Figure 5: Screen shot of the ILH Viewer (taken on an unclassified development machine)7.0 Methods Used to Send Collected Data to ILHSince the ILH was centrally located at WSMR the necessity quickly arose for a way to easily transport test data collected at the various test sites to the central filestore. As the test went on several mechanisms were implemented to enable the transport of collected data (resource) to the HotFolder. These methods included utilizing a 3rd party tool that was aware of the ILH metadata, uploading the resource through the ILH Viewer, FTP’ing the file, and overnight shipping of the resource. The easiest method for users consisted of the integration with a command & control application called StarShip. StarShip was modified so that it was aware of the ILH Metadata for the current test event which it then published out via TENA. The metadata which was published was enough information that a user could easily build the ILH Resource Header file which is required for any resource that will be inserted into the ILH automatically by the HotFolder. This method was used by several test sites and proved to have the least bugs. The preferred method for moving resources to WSMR utilized the ILH Viewer’s file upload functionality; this was used by all users at one time or another and could have been used for everything were it not for the low system memory on the server. The Viewer had an unforeseen drawback, due to an ASP.NET memory limitation the total concurrent upload size was limited to 60% of the server’s memory (600mb), with the total individual upload file size limited to 200mb. Obviously this would not work for test sites whose reduced data was more than 1 Gig, so another method for moving the resource to the HotFolder was disclosed. This method was not recommended because it was highly error prone but it was necessary due to the limitations with the ILH Viewer upload functionality. To use this method the user was required to create an ILH Header File by hand and then FTP the resource to the HotFolder. As long as the user correctly created the ILH Resource Header file then the resource would be processed by the HotFolder. Otherwise it was moved to an error folder where it would require WSMR personnel to contact the test site post test by phone to determine which test the resource was collected for. The final method was used by only one test site and was recommended due to the amount of information. This involved overnight shipping several DVD’s full of test video each day to WSMR via Fed Ex. Trying to FTP the large video files would have disrupted the entire upload process for every other test site involved so mailing the disks was the logical, although not graceful, solution. 8.0 ClosingThe MSDE MOUT Scenario enabled RTTC to gain a great level of insight into the processes, procedures, and technologies necessary to collect data to facilitate data analysis in a classified distributed test environment.  The MOUT Scenario gave RTTC the opportunity to instrument and execute a tactical engagement in a classified urban test environment.  The ILH was utilized to enable a MSDE Distributed Testing Data Management process to provide the test data necessary to determine that distributed testing is a feasible option.  TENA and the Platform Logger enabled almost effortless data collection for the MOUT Scenario and aided the Data Management and Analysis processes.  Without question, MSDE and the MSDE MOUT Scenario successfully demonstrated that RTTC has the infrastructure, capabilities, and processes in place to collect actual system test data and perform post-test data analysis from any combination of live, virtual, and constructive test articles as part of a larger, distributed test environment.Author BiographiesJASON BOLIN is currently serving as Enterprise Application Development Manager for the U.S. Army Redstone Technical Test Center (RTTC) under the Information Management Branch.  Along with his primary duties, Mr. Bolin serves as RTTC Representative for the VPG Integrated Information Systems (IIS) and Range Commanders Council (RCC) Data Reduction & Computer Group (DR&CG) and is a frequent contributor to SIW conferences.  Mr. Bolin holds a Bachelor of Science degree in Computer Science from Tennessee Technological University. RYAN NORMAN is currently serving as Software Architect for the U.S. Army Redstone Technical Test Center (RTTC) under the Information Management Branch.  Along with his primary duties, Mr. Norman contributes to the TENA Architecture Management Team (AMT) as RTTC representative, to the Developmental Test Command (DTC) Architecture Focus Group as RTTC representative and Deputy Chief Architect, and is a frequent contributor to SIW conferences.  Mr. Norman holds a Bachelor of Science degree in Computer Science from the Georgia Institute of Technology.JOSH SELLS is currently serving Redstone Technical Test Center (RTTC) as a software engineer under the Flight Test Branch. His primary duties include TENA and Web software development. Mr. Sells holds a Bachelor of Science degree in Chemical Engineering with a minor in Computer Science from Tennessee Technological University and is a frequent contributor to SIW conferencesADAM TROUPE is currently serving as a software engineer for ERC Incorporated at the U.S. Army Redstone Technical Test Center (RTTC) under the Flight Test Branch.  Mr. Troupe holds a Bachelor of Science degree in Computer Science from the University of Alabama in Huntsville.Resources, the dataProducts of testing …Actual Test Event,Test Number…Type of testconducted…Who conductedtest event…Project which testing is being conducted for…What is being tested…