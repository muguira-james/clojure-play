Synthetic Environment Integrated Test BedInfrared Scene Generation to Support Sensors in Distributed TestingKevin DennenDr. Will ClaytonERC, Incorporated256-842-2030, 256-876-8677 HYPERLINK "mailto:kdennen@rttc.army.mil" kdennen@rttc.army.mil  HYPERLINK "mailto:wclayton@rttc.army.mil" wclayton@rttc.army.milTim ClardyU.S. Army Redstone Technical Test Center256-876-5555 HYPERLINK "mailto:tclardy@rttc.army.mil" tclardy@rttc.army.milKeywords:Infrared scene generation, infrared scene projection, synthetic environmentABSTRACT:The Virtual Proving Ground (VPG) is the US Army Developmental Test Command’s program to link the capabilities of the individual test centers.  As part of the VPG, the Synthetic Environments working group created the Synthetic Environment Integration Testbed (SEIT).  This program has conducted three test events with increasing levels of complexity over the last year.  The Infrared Sensor Simulation and Stimulation (IRSAS) is used to simulate IR sensors, in real-time, with three-dimensional IR scenes dynamically changing with events on the operational battlefield.  The IR scene imagery is also projected by test equipment into a live test article or can be displayed on a monitor with simulated sensor effects overlaid on the imagery.  This allows for support of HWIL testing of a sensor and also for virtual testing.  This paper describes how the IR environment was modeled as part of a larger distributed test.1.  SEIT OverviewOver the past two years, the Army Test and Evaluation Command (ATEC)  Developmental Test Command's (DTC) Virtual Proving Ground (VPG) Synthetic Environment Focus Group has completed an initiative that integrated several distributed modeling and simulation capabilities using High Level Architecture (HLA)-compliant models and legacy Distributed Interactive Simulation (DIS) applications.  The result of this effort created by the Synthetic Environments working group was the Synthetic Environment Integration Testbed ( SEIT ).   DTC's principal goal of the SEIT initiative was to develop a high-resolution representation of the natural and man-made environment from physics-based modeling and simulation capabilities in an attempt to provide a standard and common application of this environment to support joint service life cycle simulation requirements in DoD acquisition. The particular focus of SEIT was the development of a test capability targeted to support the Future Combat System (FCS). The Redstone Technical Test Center ( RTTC ) in Huntsville, Alabama developed the IR ISR thread in order to provide a representative IR sensor view of the operational battlefield through Infrared Sensor Simulation and Stimulation technologies.  These capabilities provided the synthetic environment for exercising the capability of detecting enemy targets in a battlefield environment.  This paper will outline the development, capabilities, and interactions of the IR ISR thread with other SEIT threads.Figure 1.1 RTTC SEIT Configuration2.  ProgrammaticsThe infrared scene generation software employed by RTTC is based on Multigen-Paragdigm Incorporated’s Vega API and the accompaniment LynX GUI.  In addition to the base software, add-on modules were used for DIS/HLA communications, addition of sensor effects, addition of special effects, and operator stations for controlling the sensor field of view.  Five separate sensor views were modeled using five separate Infinite Reality graphics pipes on three SGI Onyx 2 workstations at independent locations.  The sensor output utilized high performance computers obtained through the DoD High Performance Computing Modernization Program (HPCMP).In order to display the scenes in a central location, the scene generated window and flybox joystick input were transmitted over fiber optic links.  The RS-170 output from each SGI IR3 pipe was enabled via the encoder in the IR combination file.  The video output and one TTY (serial) port were then connected to a fiber optic transceiver for transmission over multi-mode fiber optic lines to the remote control center.  The video output is unidirectional while the serial connection is bidirectional.  This enabled the remote operator to view the video on a monitor and interact with the scene generation software using a BG Systems flybox to pan the scene, select and range targets and send spot reports to the other simulations participating in the exercise.3.  ModelsAll the terrain and vehicle models are in OpenFlight format and textured with grayscale pseudo-IR textures.  TerrainThe terrain model was a 32km x 32km visible waveband OpenFlight model of Yuma Proving Ground.  This terrain was developed by a different team within the Synthetic Environment Working Group located at YPG and is covered in another paper.  The visible image textures were converted to grayscale for modeling the infrared spectrum, due to the short timeframe of the demonstration and lack of empirical data.  Approximations were made to accurately model the infrared radiometric quality of the terrain image texture based on the visible image textures.  The visible image textures were converted to grayscale and tweaked to give a “looks right” view ( i.e. asphalt roads and metal buildings were a higher grayscale, shaded areas were a lower grayscale ). Red VehiclesThe depictions of the enemy’s vehicles, red forces, were supplied from the following two sources:  VPG virtual range project ( T-72 and BMP ) and from predictive models ( ZSU ) provided by Signature Research Incorporated.  The T-72 and BMP were textured with empirically measured data collected at YPG and as such are radiometrically correct for the longwave infrared spectrum.  The ZSU image textures from Signature Research were computed using the signature prediction code, PRISM.  While this signature representation in theory is radiometrically correct, the infrared signatures still remain to be validated.  A project is currently ongoing at RTTC to address the validation of predictive signature models.  An example of the VPG Yuma Summer Day core set target is shown in figure 3.1Figure 3.1 LWIR view of a BMP Model at RTTCBlue VehiclesThe representations of the friendly vehicles, blue forces, were provided from a number of sources.  Several of the vehicles were currently present and located in-house at RTTC from past exercises ( M2 and M3, UAV, M-93, and M-998 ).  The remaining vehicles were supplied from either other test centers ( C-130 - YPG, AH-64 - ATTC ) or Signature Research Incorporated ( M-113 ).  Signature Research also aided in facet decimation of the vehicle wireframes in order to aid in realtime scene generation frametimes.  Typical facet count of the vehicle models is 1000 facets.  A typical realtime model of a M-113 from Signature Research Incorporated is shown in figure 3.2.Hardware And Sensor Effect ApplicationHardwareHardware used in the demonstration consisted of 3 SGI Onyx 2 machines with 4 IR2 and 2 graphics pipes acting as a scene renderer.  Each pipe was configured to send the rendered display to a remote monitor with a BG Flybox attached via fiber optic link to act as a sensor viewpoint controller.  Figure 3.2  Predictive Signature M-113 IR ModelOutput Sensor ImageryTwo methods were used for obtaining the final output sensor imagery.  The first method is a hardware in the loop setup.  In this method, an actual live sensor was used at RTTC with the rendered image being sent to an infrared scene projector.  In this configuration, the digital imagery being rendered by the SGI is sent out the DDO-2 port to the infrared scene projectors digital electronics which in turn sends an analog signal to the infrared resistor arrays.  Based on the intensity of the rendered image, the resistors are heated to a corresponding level.  The heat generated by the resistor array is then optically projected into the entrance aperture of a live sensor.  The live sensor stimulated at RTTC was a FLIR from an Avenger platform.  The other method in use to accurately portray  sensorized imagery is to apply sensor effects through software.  This is done in the SEIT scene generation software with the inclusion of the MPI add-on module, Sensor Works.  For the purposes of the SEIT demonstration, only temporal noise was added.  However, other sensor effects that are available include blur, fixed pattern noise, gain and offset and jitter. 5.  Software Development CapabilitiesAll scene generation software for the SEIT demonstration was written using the Vega API from MPI and developed on a SGI platform.  Several features of the scene generator are 1) capability to toggle between narrow and wide field of view, 2) capability to switch sensor effects on and off on the software sensor applications, 3) capability to evoke a simulated laser range finder for calculating the depth buffer, 4)  display of range and location of entities on the battlefield, 5)  Automatic fill and transmit spot reports by the gunner when a target is detected.  The basic flow of the scene generation program is as follows.  The command line arguments of machine name and XML input file are read and parsed.  From these inputs, the correct entity information of the vehicle/sensor platform is obtained. At startup, the scene generation software begins monitoring incoming network traffic and builds a list of all available incoming sensor platforms.  Each instance of a scene generator had a unique platform, identified by the marking field in the entity state PDU.When its unique platform is detected, the observer for the scene generator is attached to the platform and the operator workstation video and flybox become active and mapped from DIS entities being broadcast.  The text file created by the MPI Lynx GUI is read and used to setup the windowing and graphics configuration and load the terrain and target models.  Next, the flybox is initialized and its input toggles and buttons are assigned to perform various tasks.  Post draw callbacks are then created to add a compass, crosshair, and target range and location to the display.  The software then enters the real time loop and monitors the flybox for input.   As the flybox buttons are depressed, different actions such as FOV switching, sensor effect switching, range and location determination and the sending of a spot report of an enemy detection are performed.Exercise Description and Interaction With Other SEIT Threads One Semi-Automated Force (OneSAF) TestBed Baseline  (OTB)  The majority of the entities simulated in the scenario were created locally at RTTC using OTBSAF version 1.0.  OTBSAF is a distributed wargaming tool that represents a full range of military operations, systems and control processes from individual combatant to battalion level forces.  The scenarios, unit placement, and tactics in OTBSAF were developed specifically to support the Army’s Future Combat System (FCS).  All of the internal traffic at RTTC was DIS based.  The connection to the remote test centers was accomplished using HLA via the MaK Technologies gateway and the RPR FOM.  During the scenario, the scene generator picks up all available network traffic and performs the appropriate actions, updating position, and state information.  Figure 6.1 shows the OTBSAF simulation during the exercise.Figure 6.1 OTBSAF Display During the SEIT IOC3 Demonstration Showing Layout of Red and Blue Forces Exercise DescriptionDuring the SEIT IOC3 exercise, the operators were instructed to search for threat vehicles and to issue spot reports on these vehicles. The gunner would pan the battlefield in wide field of view, switch to narrow field of view upon encountering an object of interest, interrogate the object, and make a determination if the object was a threat.  If the object was determined to be non-threatening, the gunner would switch back to wide field of view and resume searching.  If the object is identified as a potential enemy threat, the threat vehicle is centered in the display using the flybox and the trigger on the flybox is depressed to synthetically range the vehicle for distance and location coordinates. An auxiliary button on the flybox is then pressed to send the spot report to the STORM Role Player Workstation (RPWS) in the C4ISR thread.   The sensor view imagery of the simulations running at RTTC are shown in figures 6.2 – 6.3.Figure 6.2  Infrared View of Battlefield from RTTC M-2 Platform. ( No Sensor Effects )Figure 6.3  RTTC M-2 Infrared Sensor View of  Battlefield Image With Software Sensor EffectsStorm Role Player Workstation ( RPWS)RPWS is a multi-function workstation that enables a user to view a tactical situation in real time in the form of MIL-STD-2525B military symbols overlaid on an electronic map.  It is interfaced to the STORM Digital Army USMTF VMF Stimulator (DAUVS) which serves as a communication gateway between the RPWS workstations playing different tactical roles with the exercise.   When the “Send Spot Report”  flybox button is pressed in the IR ISR thread, a signal PDU containing the time stamped information on the reporting vehicle (ID, position, etc.) and the range and location of the threat vehicle is transmitted.  These signal PDUs are intercepted by the DAUVS server and relayed to the RPWS workstations.  All RPWS workstations then place a threat icon at the reported threat position on the battlefield map.  The system clocks on all scene generation machines in the exercise are synchronized using NTP and our local Stratus 1 time server.Figure 6.5 View of RPWS Map Display 7.0 IR ISR Future Work Several projects are underway at RTTC to advance the state of the IR ISR thread.  Currently the number of sensors that can be mapped and displayed is limited to the aggregate number of graphics pipes available on the SGI machines.  In order to improve on this number, the SEIT IR scene generation software will be ported to a cross-platform version of MPI’s Vega software called Vega Prime.  In a parallel effort, an infrared scene generation package developed by the U.S. Army’s Night Vision Electronic Sensor Directorate ( NVESD ) called “Paint the Night” that has also been ported to a cross-platform application will be evaluated for possible use.  Both of these software packages potentially allow a dramatic improvement in the number of sensor platforms that can be simulated.  This effort only applies to the sensor platforms that use the software sensor effects as the SGI version of the scene generator still is a necessity in hardware in the loop simulations.  Author BiographiesKEVIN DENNEN is a Senior Staff Engineer at ERC, Inc. supporting RTTC in the field of infrared scene generation for hardware in the loop applications.  He received his MS and BSEE degrees from Memphis State University in 1991 and 1988 respectively.  He has been working in the field of scene generation and sensor simulation for 12 years.DR. WILL CLAYTON is a Senior Staff Engineer for ERC, Inc. supporting RTTC in the area of infrared scene projection and distributed simulations.  He received his PhD from the University of Memphis in 1999 and his MS and BSEE degrees from Memphis State University in 1992 and 1990.TIM CLARDY is a Senior Electronics Engineer at the US Army Redstone Technical Test Center, Redstone Arsenal, AL.  He is the Co-Chair of the Developmental Test Command’s (DTC) Synthetic Environments Focus Group (SEFG), Co-Lead of the Synthetic Environment Integrated Test bed (SEIT) demonstration project, and Technical Lead for the Signature’s Core Area.  Mr. Clardy has over 10 years experience in modeling and simulation, distributed testing, computing technologies and electro-optic sensors and missile systems.  Mr. Clardy is also a Cisco Certified Network Professional (CCNP) and Cisco Certified Design Professional.