RTI performance in a wider scopePeter KarlssonMagnus JohanssonPitch ABNygatan 35S-58219 LinköpingSweden+46-13-13 45 45peter.karlsson@pitch.se, mj@pitch.seKeywords:HLA, RTI, performanceABSTRACT: Until now, measurements of RTI performance have usually consisted of measuring latency and throughput when sending various amounts of data between two federates. These are interesting measurements in some cases, but they only cover a very limited part of what could be the meaning of the term RTI performance. The purpose of this paper is to try to broaden the scope of RTI performance to include more RTI services than sending and receiving data. More importantly, other concepts such as fault tolerance, predictability, reliability, usability, etc should also be included in the RTI performance concept. Measurements will be suggested for some of these concepts and test results from some of these measurements will be presented. IntroductionDifferent projects that use the HLA have different performance requirements. In some projects, a short development time and reliability while executing the federation might be most important requirements. In another project, running a federation execution might be pointless if the latency when sending an attribute update between two federates is larger than 2 milliseconds. In yet another project, the developers have little or no HLA experience. That project needs good documentation and easy-to-use debugging tools to perform well.Thus, for every project it is possible to create a performance profile describing all the different performance requirements for that project. Similarly, different RTI's have different performance profiles. One RTI might be very good at delivering attribute updates quickly, while another RTI might be intuitive and easy to use while developing your federation. So naturally, when selecting an RTI for an HLA  project, the performance profile for the RTI should be able to cope with the requirements in the performance profile for the project. So, what should be included in a performance profile for a project using HLA, and how is it possible to verify that a certain RTI is able to provide the requested performance? These are the two questions that this paper will try to answer. Performance in GeneralBefore discussing performance, it is necessary to define what it is. This is not as simple as it sounds since people often mean very different things when talking about performance. According to [1], software performance can be defined as:“Software performance is the degree to which a system or component accomplishes its designated functions within given constraints such as speed, accuracy and memory usage” In an HLA federation, there are two different software components, the RTI and the federates. Since these components are usually developed and maintained by different organizations, it makes sense to split software performance into RTI performance and federate performance for an HLA federation.  EMBED Visio.Drawing.6  Figure  SEQ Figure \* ARABIC 1 RTI performance REF _Ref32218330 \h Figure 1 illustrates a typical example of RTI performance. The time it takes to send a message from federate A to federate B is typically referred to as the latency of the RTI.  EMBED Visio.Drawing.6  Figure  SEQ Figure \* ARABIC 2 Federate performance REF _Ref32218179 \h Figure 2 illustrates an example of federate performance. The LRC (Local RTI Component) delivers a callback to the federate. The time it takes for the federate to process the callback is typically referred to as the federate callback latency.In addition to software performance, there is also hardware performance, i.e. how fast are the computers, what kind of network is used, etc.Finally, there is the user performance concept which, according to [2], deals with for example how long time it takes for a user to complete a certain task. EMBED Visio.Drawing.6  Figure  SEQ Figure \* ARABIC 3 The relationship between the performance concepts REF _Ref32201143 \h Figure 3 illustrates the relationships between the different performance concepts. The user uses one or more federates to accomplish a certain task. The federates uses the RTI services to communicate between each other, and the hardware they are running on to perform the tasks the user has requested. The RTI uses the hardware it is running on to perform the services requested by the federates.It is beyond the scope of this paper to fully investigate all the different performance concepts of an HLA federation. This paper will focus on RTI performance. The RTI Performance ProfileThe purpose of the RTI is to provide the services described in [3]. These services will be used by the federates to accomplish the tasks requested by the user. So the performance of an RTI can be determined by measuring the RTI services. The difficult question to answer is, what should be measured and how should it be measured?  REF _Ref32212627 \h Figure 4 can be used to illustrate an example of an RTI performance profile. On each axis, a measurement is plotted. A line is drawn between each plot, and the size of the area of the resulting pentagon suggests how well the RTI performs in a general-purpose environment. Unfortunately, this is extremely subjective. Which are the requirements that an RTI needs to fulfill to be labeled "excellent" regarding fault tolerance? What is the difference between decent and good when looking at deployment? How much better is RTI A that has a slightly bigger pentagon compared to RTI B? Until the HLA community has agreed on a complete RTI performance profile-testing suite, these questions are impossible to answer.   EMBED Visio.Drawing.6  Figure  SEQ Figure \* ARABIC 4 Example performance profileIn the following sections, each component in the RTI performance profile will be described. Also, for some of the components, some measurements will be suggested and some example test results will be presented. StabilityThe stability of an RTI includes a couple of different areas. The purpose of looking at stability is to make certain that the RTI behaves in a predictable way in every circumstance. An RTI that delivers 100.000 interactions but fails on the next one because it has run out of memory is a typical example of an RTI that is not very stable when considering interaction delivery. The most common way to do a stability test is to perform some kind of long run test and record any anomalies during the test, such as performance degradation over time, memory leaks or unexpected crashes. This kind of testing usually gives interesting results, but it is difficult to achieve any quantifiable results.Another aspect of stability is differences between identical executions. The same test is executed several times, and the minimum, maximum and average values are calculated. This kind of testing produces results like "95% of the interactions sent from federate A to federate B has a latency lower than 1 millisecond and the remaining 5% has a latency lower than 10 milliseconds". For a simulation with real time requirements, this is a very interesting type of measurement.A couple of tests have been done at Pitch regarding the stability of latency between identical executions. Naturally, this kind of testing can be applied to almost any test case, not only latency testing. The hardware used during the tests was two Compaq Evo W4000 computers (2.0 GHz P4 processor, 512 MB RAM and a clean Windows 2000 server installation). The two computers were connected to a switch and they were exchanging data at 100 MBit/s.  EMBED Visio.Drawing.6  Figure  SEQ Figure \* ARABIC 5 Test setupThe CRC (Central RTI Component) was running on the same machine as one of the federates. The test application consists of two federates, one sender and one receiver. The sender starts a timer and sends an interaction to the receiver, which responds with another interaction. When the sender receives the response, the timer is stopped. The test program is implemented in C++ and it uses the Windows high-resolution timer functions (QueryPerformanceCounter). Regarding RTI settings, advisories has been turned on, bundling has been turned off and all interactions has been sent using reliable mode. The tick(min, max) function has been used with the parameters 0.01 seconds and 1.0 seconds for min and max respectively. EMBED Excel.Chart.8 \s Figure  SEQ Figure \* ARABIC 6 Latency stability for pRTI( REF _Ref32653900 \h Figure 6 shows the latency stability for pRTI( 1.3. The Y axis represents time in milliseconds, and the X axis illustrates the size of the data being transmitted. For each data size, 500 interactions has been sent and the latency of each interaction has been calculated. The maximum latency represents the latency of the slowest interaction out of the 500. The 95% below value illustrates an upper latency limit of the interactions.  95% of the interactions sent have a latency which is equal to or lower than the 95% below value.  EMBED Excel.Chart.8 \s Figure  SEQ Figure \* ARABIC 7 Latency stability for RTI:NG REF _Ref32655872 \h Figure 7 shows the same kind of measurement done for RTI:NG v6. Please note that the only attempt at tuning the performance of RTI:NG has been to use different tick parameters. No other RID file modifications has been done except for those mentioned above. pRTI( has not been tuned at all. EMBED Excel.Chart.8 \s Figure  SEQ Figure \* ARABIC 8 Latency stability for RTI:NG, different tick REF _Ref32656145 \h Figure 8 shows the same test with RTI:NG v6. This time, the tick parameters being used where 0.0001 and 1.0, respectively. When running with an even lower min parameter, the result is not improved. So in this environment, a good min parameter to tick would be something around 0.0001. Tuning will be further discussed in section  REF _Ref32656526 \r \h 6 -  REF _Ref32656526 \h Speed and in section  REF _Ref32656494 \r \h 8 -  REF _Ref32656494 \h Deployment. Scaling is yet another stability aspect. What happens when the amounts of data increases? What about time management calculations involving a lot of federates? Is the latency when delivering attribute updates affected if there are 10.000 objects in the federation? What happens to time management calculations when there are both a lot of objects and a lot of federates in a federation? Is it affected at all? There are a lot of parameters that can be scaled up, and some results are affected by changes to more than one parameter. Therefore, it is very difficult to come up with a suitable scalability test suitable for any situation. Nevertheless, it is still an important concept that needs to be taken into consideration when evaluating performance of an RTI. Another aspect of scaling is analyzing the complexity of the algorithms that are used to implement the different RTI services. See [4] for a description of the complexity of various RTI services that are implemented in pRTI( 1516.Fault ToleranceThe fault tolerance of an RTI is pretty difficult to measure, since faults can occur in so many different ways. It is assumed that the RTI handles all errors according to the HLA specification [3] correctly. However, there are some errors that can occur that are not mentioned in the specification. Also, the exceptions that are thrown can contain more or less information about what went wrong.One of the most common faults is that one federate stops responding. Usually, the federate has crashed or entered an infinite loop, or the network connection to the machine where the federate is running has been terminated. The HLA specification [3] does not specify what an RTI should do when a federate stops responding. Currently, there are a lot of different approaches to this problem, ranging from RTI's that needs to be restarted whenever a federate stops responding, to RTI's that removes the federate from the federation in a graceful way, making sure that the federation execution can continue without the missing federate.  Another aspect of fault tolerance deals with the connection-less mode that some RTI's have implemented. This means that the RTI has no CRC, but instead all the information is distributed to the different LRC:s. This has the advantage that the federation is not dependent on being able to contact one centralized instance. However, the disadvantage is that there is more communication needed between the LRC:s when this mode is used.As mentioned above, fault tolerance can be accomplished at several different levels. For example, an RTI reads a FOM file whenever a new federation execution is created. The RTI expects the file to be formatted in a certain way and that the file contains the appropriate MOM data. The HLA specification [3] states that the RTI should throw an exception if the FOM file could not be located, or another exception if the content of the FOM is invalid. However, it is up to the RTI implementer to decide what information an exception should contain. Information such as which line it was in the FOM that caused an error can greatly reduce development time. If the exception also contains information about what the incorrect line should look like, it is a lot easier to correct the error. Of course, this applies to all kinds of exceptions thrown by the RTI.SpeedLatency and throughput are two typical examples of speed measurements of an RTI. How fast can a message be transmitted between two federates, and how fast can a certain amount of data be transmitted? The advantages with this kind of testing are that the test programs are easy to implement and the results are fairly easy to interpret. One major disadvantage is the tuning that is needed to obtain high speed. Which tick parameters should be used? If the RTI has a RID file, what should it contain? How is it possible to determine when optimal performance has been achieved? Will the RID file which is developed for configuration A be suitable for configuration B? When comparing RTI’s, this is especially difficult since different tuning parameters might be optimal for different RTI’s. Also, the tuning requirements differ between RTI's. Some provide fast speed using the default configuration while others require extensive tuning.A lot of different RTI services can be measured with respect to speed. Sending interactions, updating attributes, saving and restoring, registering objects, doing time advances, transferring ownership are some examples. There have been several papers presented at previous SIW's, containing different speed measurements done in various environments. DevelopmentWhile developing a federation, the RTI often appears as a kind of black box to the developer. If something does not work the way the developer intended it is pretty difficult to discover what went wrong. This has changed slightly during the last year. Currently, some RTI's include a GUI which presents the federations currently running and information about the federates participating in the federation. There are also COTS tools available that present similar information. Other features such as being able to log RTI service calls also helps to reduce the feeling that the RTI behaves as a black box. This information can greatly reduce the time spent looking for errors and thus increase the overall performance of the project. The problem with federates that stops responding was discussed in section  REF _Ref32915765 \r \h 5. This problem also applies to the development performance of an RTI. If the user constantly has to restart the RTI whenever a federate stops responding, a lot of valuable development time is spent restarting the RTI.  The startup time for a project can also vary a lot. Installation instructions, sample federate code, sample build scripts and make files are very useful for the developers to verify their installation and to get started developing their own federates. The possibilities to get support from the RTI vendor is also an important factor when considering development performance.DeploymentSometimes, the development environment for an HLA federation is not the same environment as the production environment where the federation will be executed . For example, in a development environment the machines running the federates are connected via a LAN and located in the same room. In a production environment, the federates might be located all over the world and connecting to each other via a WAN, for example the Internet. This adds several new requirements when running in the production environment, such as firewall compatibility and the ability to locate errors in a distributed environment. Also, tuning may become an issue. The federation has probably been tuned to run in the development environment. If the production environment is different, the federation might need to be re-tuned for the production environment. An RTI that needs a lot of manual tuning thus has a lower deployment performance compared to one that needs little or no manual tuning.The updating mechanism of an RTI is also an important deployment factor. How easy is it as a user to perform an RTI update? Are the changes between different RTI versions documented in a proper way, so it is possible to know the impact of the update? Is it possible to use different LRC versions together or do they all need to be updated to the same version?Future WorkFor SQL servers, there exists a standardized collection of performance tests, see [5] for more information. The performance tests are being maintained by an independent organization consisting of representatives from most of the largest computer system and database companies worldwide. Implementing something similar to measure RTI performance is probably a bit too ambitious at the moment. The SQL server market is a lot bigger than the RTI market, and the effort needed to establish such an organization is probably bigger than the benefits gained from having one. If the usage of HLA continues to increase, this opinion might need to be revised. However, it is important to note that simply because the SQL server community has a standardized collection of performance test, the performance discussions have not ceased to exist. Instead, comments like "SQL server A can only run the performance test suite quickly, any other real world application is slow" are nowadays fairly common.ConclusionsA lot of factors affect the choice when trying to select an RTI. After reading this paper, it might be even more difficult to do the selection. But by carefully analyzing the concepts in this paper that are relevant to the business requirements of the organization where the RTI will be used, the resulting decision will hopefully be a better one.References[1]	John J. Marciniak: "Encyclopedia of software engineering", John Wiley & Sons, New York 1994.[2]	Kristoffer Bohmann: "User performance metrics", http://www.bohmann.dk/articles/user_performance_metrics.html, July 2000.[3]	IEEE: "IEEE 1516, High Level Architecture (HLA) – Federate Interface Specification", March 2001.[4]	Mikael Karlsson, Lennary Olsson: "pRTI 1516( - Rationale and design", Simulation Interoperability workshop, 01F-SIW-038, September 2001.[5]	Transaction Processing Performance Council: "TPC benchmarks", http://www.tpc.org, February 2003.Author BiographiesPeter Karlsson is a software engineer at Pitch working as a consultant and product developer. He received his MS in Computer Science & Engineering from the University of Linköping.Magnus Johansson is a software engineer at Pitch working with product development and testing. He received his MS in Computer Science & Engineering from the University of Linköping.