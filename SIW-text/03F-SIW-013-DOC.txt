Development Baton HandoffsLawrence M. RootSimVentions, Inc. 3330 Bourbon Street, Suite 129Fredericksburg, VA 22408540-372-7727  HYPERLINK "mailto:lroot@simventions.com" lroot@simventions.comJerry OesterheldNaval Surface Warfare CenterDahlgren Division 17320 Dahlgren RoadDahlgren, VA 22448-5100540-653-8645 HYPERLINK "mailto:oesterheldjw@nswc.navy.mil" oesterheldjw@nswc.navy.milMark McAuliffeSimulation Technology CenterUS Army RDECOM12423 Research ParkwayOrlando, FL  32826Phone: 407-384-3929 HYPERLINK "mailto:Mark.mcauliffe@us.army.mil" mark.mcauliffe@us.army.mil  Keywords:C4ISR, Modeling & Simulation, BOMs, DoDAF, CADM, MDA, Life CycleABSTRACT: Since our involvement in tactical software development for C4ISR systems, which dates back to the early 1980’s, the handoff of the “development baton” between engineering disciplines has never been smooth. Inevitably, when (and if) the development effort is completed, the end product is never what the user(s) actually desired. Over the years, various technologies and processes emerged that assist in this baton handoff. These have included: Ada, OOA/OOD, C++, CASE, TQM, CMM, Spiral, RAD, IDEs, UML, MOO/MOE/MOP, IPTs, Collaborative Environments, Web Services, MDA, etc. However, none, whether alone or together, have yet solved the handoff problem. The problem remains one of missed communication and understanding as the project transitions through its technical life cycle.This paper examines an approach to smooth the baton handoff problem by applying modeling and simulation across the engineering disciplines. The approach starts by utilizing the rich descriptions and meta-data collected and described within the DoDAF and its associated Operational, System, and Technical Views. We will explore the effect of using the DoDAF as the main meta-data source for the system to be built, and how one can extract the needed technical information to establish use cases, scenarios, and a working model of the system. This model starts at the conceptual level, being represented as a set of use cases and simulated using Petri nets. As testing and validation measures permit, it migrates toward a constructive simulation described via a set of Base Object Models (BOMs). This in turn, migrates toward a mature simulation that can interact with both live, as well as other virtual systems using HLA, DIS, or other interoperable simulation architectures. As the model is validated within this simulation environment by all stake holders (i.e., engineers, developers, testers, users, and analysts) the task of transforming the model into the real system begins.  As pieces of the model are implemented, these real pieces will replace the simulated pieces, until all simulation pieces are removed – and the actual tactical software is complete. The ProblemIntroductionMost DoD related development projects are guided by a series of documented-oriented reviews focused on expressing a project’s progress and conveying a team’s understanding to others. Over the years, the series of documents used to facilitate these reviews have been labeled by many different names such as DoD-1679, DoD-2178A, DoD-498, IEEE 12207, J-STD-016, and most recently, the PEO (TSC) TRM.The “baton handoffs” between engineering disciplines for a project occur almost exclusively at meetings and reviews in which these series of documents are discussed. These reviews represent and reflect several types of project details such as allocated requirements, functional requirements, hardware requirements, software requirements, preliminary design, critical design, in-progress reviews, test readiness reviews, etc. as depicted in Figure 1.1 below.Figure 1.1 – International Council on Systems Engineering (INCOSE) Version of the DoD Requirements RaceAll stake-holders may be required to attend these reviews and provide concurrence in order for the effort to move forward.  If concurrence is reached by the stake-holders, the baton handoff is carried out allowing the project to transition to the next stage. This handoff consists of the delivery of the set of documents tied with any comments and resolutions shared and reached at the meetings.The Baton ProcessTo better understand this baton metaphor, let’s examine the process as illustrated in Figure 1.2.  If a project is currently at its entry stage, and the meeting is the review of Allocated System Requirements (ASR), then the operational requirements document (ORD), capstone requirements document (CRD) and any other related understandings might characterize the baton that is to be handed off from the need-setters to the system architects. Upon handoff, the system architects then take control of the baton and proceed to run toward the System Requirements Review (SRR).  At this point the baton (in the form of additional documents, comments, and resolutions) is handed off from the system architects to the system developers. This pattern continues until the final baton handoff is made from the IV&V certification team to the end-users. Figure 1.2 – Baton Development PatternThe pattern that we’ve described is a common process observed for DoD system engineering projects.  While it is well intended, it is not without flaws.  Specifically there is an overwhelming reliance on informal, verbal communication for identifying what needs to be done.  The expectation with this approach is that members and stake-holders will have “heard the same thing,” “drawn the same conclusions,” “correctly conveyed the conclusions to others” and “correctly captured and reflected everything in the documents.”  Furthermore, there is the anticipation that those farther down the track understand the issues decided earlier in the race.  This reliance on verbal communication and anticipation that what is communicated is commonly understood is a faulty mechanism for ensuring success. In reality, as the baton travels through the race, it becomes farther removed from those who started the race, and becomes more fragmented as the number of runners carrying some version of it increases.Also, over the course of the race, or project life, representatives and players may come and go.  They are forced to pick up a baton that has been dropped by a previous individual.  The likelihood that the mark will be met and that the finish line will be crossed on schedule is unlikely.   There are many other potential issues that may occur as well.  The question that must be asked is. “Does this process work as well as we would like?”The Repeat GameThis process is actually similar to the game “Repeat” that teachers occasionally play with their students. The focus of the game “Repeat” is to teach listening and speaking skills. The teacher lines up the students in a single file row (called the “repeat line”), writes a phrase on a piece of paper, and whispers that phrase into the ear of the first student in line. Each student, in turn, whispers the phrase to the next student in line until it has reached the end-of-the-line.   At that point the last student would loudly pronounce the message received. The funny part of this game is that the phrase that is shared in the end is quite likely different than the phrase shared in the beginning.  There are some interesting traits to this game however.  When the participants of this game have an understanding of the terminology being used, it seems to help in the exchange, and usually results in a closer end-of-line version that appears related to the initial phrase. Whereas, when the participants of the game do not have a qualified understanding of the terminology, the end result often reflects total confusion.  Likewise, these same issues are observed in DoD related development that uses the baton handoff process.To be successful, this process relies very heavily on subject matter experts remaining intimately involved to ensure that the race remains on course. In the terms of the Software Engineering Institute (SEI), the current process that we’ve described is really adhoc, because it relies on the super-hero mentality, that is, without the subject matter experts personally and intimately involved in the project life cycle, the chances of success is drastically reduced.Real-World ExamplesTo fully understand the issues, let us examine a list of real-world examples where baton handoff problems took place.  It should be noted that our identification of these examples is not meant to imply that the highlighted project or program had significant failures.   On the contrary, unless identified otherwise, many of these projects successfully transitioned to a fielded tactical system despite handoff issues.   JSIMS As presented at the spring 2003 Navy Modeling and Simulation Office (NAVMSO) conference, the Joint Simulation System (JSIMS) is being transitioned and all efforts/source turned over to the Joint War Fighting Center (JWFC). The primary purpose of JSIMS was to support training and education of ready forces by providing realistic joint training across all phases of military operations for all types of missions. CDR Brian Hudson stated that “JSIMS failed due to the separation over time between the real training objectives (the goals) and the actual implementation.  The JSIMS effort got side tracked in trying to manage “cost vs. fidelity” instead of “Training Objectives vs. M&S Capability”– which kept the team from focusing on the overall goals.” He also said, “JSIMS died because they did not do a good job defining requirements that meet the stated goals and then mapping them to design.” This is an example of the failure to successfully perform the baton handoff from requirements (architects) to design (developers).GWS CoBRAThe Navy’s Gun Weapon System’s (GWS) Coordinated Battleforce Replay and Analysis (CoBRA) program was established to provide the needed tools and environment for analysis and replay of GWS activities and interactions related to gun fire support within the host combat system.  The requirements for the CoBRA environment were initially laid out and established via a document and a series of meetings and reviews.  As the requirements transitioned to the developers, the baton was dropped, and a misunderstanding of the requirements arose. The developers interpreted the requirements differently than the architects.  As a result, the misunderstanding in the requirements ended up causing a delay in product delivery.SGS/ACThe Shipboard Gridlock System with Automatic Correlation (SGS/AC) sits between the host combat systems on Navy platforms and the tactical data links (TDLs).  Its main capabilities are to provide data registration and data fusion of the TDL information with local sensor information on behalf of the host combat system.  SGS maintains interfaces to the host combat system in support of these capabilities.  These interfaces, which help to make up the baton, are supported through a series of documents (Interface Requirement Specifications (IRS), Interface Design Document (IDD)) and reviews (Interface Control Working Group (ICWG)) to help ensure compliance and consistency. However even with this process, uncoordinated change to the interface implementation occurs as the baton transfers from system-of-system level to individual systems. Even after initial integration and “common understanding and compliance is achieved,” the technical agreements, tweaks, and understanding of the interface do not get documented (i.e., reflected in the baton). This causes issues for other systems implementing this interface because the baton they will use does not reflect reality.  Also, the “larger systems,” leveraging their perceived importance due to their size, often make changes to the interface or its data content without notifying all stake holders – in effect fragmenting the baton.CAV/UCAV DeploymentIn this case, the need-setters and the system architects were unaware of restrictions levied by the system developers and the actual tactical systems involved.  In preparation of activities in Afghanistan the DoD committed a number of Combat Air Vehicle (CAV) / Combat Unmanned Air Vehicle (UCAV) systems to the theater. However, upon implementation of the deployment plans, it was found that the planned number of active units could not be achieved due to limited communication pipes between the controlling ground systems and the active CAV/UCAVs. In fact, the operational requirements that multiple CAV/UCAV systems needed to operate simultaneously in the same theater space were never identified prior to development. This miss caused ripples in the overall battle plans, resulting in unplanned changes and delays.  This issue should have been known well in advance of the deployment, and should never have become an issue in theater. This is an example of a dropped baton between the end-users and the need setters.CECThe Cooperative Engagement Capability (CEC) is a major combat system upgrade that launched the Navy into the world of sensor-netting and network centric operations; however, it did not properly include all appropriate stake holders in its tactical fielding and deployment plans. Once placed onboard Navy platforms, major issues arose surrounding the integration of the CEC into the host combat systems and their existing tactical data links. Many of these issues may have been avoided with proper baton handoffs. Are Things Getting Better?Despite the issues and syndromes that often occur, it appears that things are getting better.  Slowly, the methods through which the baton is handed off are being improved and becoming more automated resulting in better baton handoffs. There has even been a focused effort on tracing requirements across baton handoffs. However, the major advances to date that have helped improve baton handoffs have been mostly contained internally within individual engineering disciplines.  What is lacking is better handoff support among the collection of engineering disciplines.The advancements for each engineering discipline are described in the list below:As a software community, we have seen advances in software languages: Ada, C++, Java, Meta-Languages (XML), Scripts; advances in operating systems: proprietary, UNIX, Posix; advances in software development techniques: Functional Design, Object Oriented Analysis (OOA) / Object Oriented Design (OOD), components, Web Services, Unified Modeling Language (UML), Model Driven Architecture (MDA); and advances in overall software development processes used by managers: Waterfall, Spiral, Rapid Application Development (RAD), eXtreme Programming, Rational Unified Process (RUP). As a simulation community, we have seen advances in interoperability technologies: DIS, HLA, reference FOMs, BOMs; and programs emphasizing greater reuse and flexibility of interoperable systems: Distributed Engineering Plant (DEP), Joint Distributed Engineering Plant (JDEP), and the Composable Mission Space Environment (CMSE) program.As an architectural community, we have been targeting better modeled and more responsive Command, Control, Communications, Computers, & Intelligence (C4I) capabilities. Examples include: Joint Technical Architecture (JTA), Global Information Grid (GIG), C4I Surveillance and Reconnaissance (C4ISR) Architecture Framework (AF), Joint C4ISR Architecture Planning/Analysis System (JCAPS), DoD Architecture Framework (DoDAF), and C4ISR Core Architecture Data Model (CADM) to name a few.As an organizational engineering community, we have seen increased focused and awareness pertaining to enhancing quality and performance. Examples include: personality profiles and team building exercises, quality assurance (QA), total quality management (TQM), ISO 9000, simulation-based acquisition (SBA), capability maturity models (CMM), and integrated product teams (IPTs).These advances have allowed us to begin to close the gaps in the communication chain, especially within the respective disciplines, but there still remain areas for dramatic improvement between the major disciplines. The handoffs that are most prone to dropping the baton are between the need-setters, the system architects, the system developers and the system users.  The problem remains one of missed communication and understanding as the project transitions through this life-cycle.Full-Cycle Automated EngineeringThe opportunity we as a community have is to build upon the advances that have already been made, and continue to automate the baton handoffs. With the goal being to completely automate the “repeat line” such that “full-cycle automated engineering” has been achieved. What do we mean by “full-cycle automated engineering?” We mean that transitions between “needs – to – architecture – to – simulation – to – code – to – fielded system” must be automated such that changes at either end are automatically reflected throughout. And, not just for the system, but for the system-of-systems it lives within.  Can this dream be achieved? Is it possible to establish a single living model of a system such that it starts out as a set of needs and ends in a fielded system that meets those needs? Can we move beyond just producing written documentation and view graphs that capture the information required to handoff a baton, and advance it to a living model that acts as the baton? Will this living-model be capable of generating any and all documentation needed, a living-model that all project stake holders work with and from? Can this model, through its maturity process, become the real system and support both simulation and tactical needs?The Opportunity	The opportunity exists to bring together the ill-connected communities of need-setters, system architects, system developers, and system users. Furthermore, the opportunity exists to not only bring these communities together, but to do it in an automated way, such that the paradigm of the “repeat line” has been completely destroyed and replaced by a “model-centric” paradigm.  The “repeat line” becomes transformed from what was shown earlier in Figure 1.2, to what is shown below in Figure 2.1. Figure 2.1 – Model-Centric ParadigmThis paradigm shift in the way the baton is managed will allow the project to move more precisely, quickly, and efficiently toward successful implementation.  It allows all communities to collaborate on the same baton, ensuring consensus and common understanding. As projects transition through their life-cycle, the maturing solution is what is being managed, not just a set of documents, view graphs, and conversation notes. The baton is no longer handed off, everyone owns it and has direct access to it, and therefore has first-hand knowledge of it.Specific recent advances that will allow for this paradigm shift from a “repeat line,” using baton handoffs, to a “model-centric” community, who together transform the baton into the fielded system, are discussed in paragraphs 2.1 through 2.4.System Architecture Domain: DoDAF & CADMThe DoD Architecture Framework allows a description of the system to be captured as a representation of its defined problem domain in terms of their components, what those components must do, how those components must interact with one another, and the rules and constraints under which the components must operate.  Within this Architecture Framework, there are three primary views that combine to describe the architecture.  These are the Operational View, the Systems View, and the Technical View.  The Operational View is a “description of the tasks and activities, operational elements, and information flows required to accomplish or support a military operation.”[1] The Systems View is a “description, including graphics, of systems and interconnections providing for, or supporting, warfighting functions.”[1] The Technical View is “the minimal set of rules governing the arrangement, interaction, and interdependence of system parts or elements, whose purpose is to ensure that a conformant system satisfies a specified set of requirements.”[1]The results of describing and capturing the system architectural views in accordance with the DoD Architecture Framework is the production of a series of products that hold the architecture-based data and information (i.e., graphical, textual, tabular, and characteristic) pertinent to their descriptions. Tables 2.1 through 2.4 provide a summary of products called out within the DoDAF [2].In addition to these architecture products, the DoDAF also calls out the C4ISR CADM.  The CADM is: geared towards providing a common meta-data set for repositories of architecture-based data and information; establishes a common approach for storing, retrieving, and sharing the architecture-based data and information contained in the DoD Architecture Framework products; and, provides the ability to perform automated queries of the architecture-based data and information while the Architecture Framework offers the ability to formulate standardized views for use in comparison and integration.The DoDAF, however, does not address the blueprint-to-implementation transition process.  Yet one of the biggest benefits of a transition process, in helping to reach implementation status, is the capability to simulate and assess the architecture prior to implementation so that duplications, non-required capabilities, and shortfall, gaps, and deficiencies can be determined.  Modeling and simulation can step in and fulfill the role of “transformation agent.” As the transformation agent, M&S is able to provide a well defined road map (i.e., the Federation Development and Execution Process (FEDEP)) from which to drive the concept from architecture to simulation.System Developer Domain: MDA “Model Driven Architecture (MDA) provides an open, vendor-neutral approach to the challenge of interoperability, building upon and leveraging the value of Object Management Group’s (OMG's) established modeling standards: Unified Modeling Language (UML); Meta-Object Facility (MOF); and Common Warehouse Meta-model (CWM). Platform-independent Application descriptions built using these modeling standards can be realized using any major open or proprietary platform, including CORBA, Java, .NET, XMI/XML, and Web-based platforms.” The above quote, which was taken from the Object Management Group’s (OMG) web site, attempts to capture the essence of the model driven architecture concept in a single paragraph.  The concepts and ideas contained within the MDA have caught the excitement and imagination of many in the tactical software development realm.  Offering the possibility of driving the development of software from models instead of documents, the MDA and any future derivative, is poised to play a large part in automating the baton handoffs. The MDA has the potential of allowing the software development domain to become almost entirely model driven.The MDA concepts of platform independent models (PIM) and platform specific models (PSM) have helped to clarify the line between systems engineering and software engineering. Over the past several years, this line has become blurred due to the advent of Rapid Application Development (RAD) environments, visual-based software development, Computer Aided Software Engineering (CASE) tools, and the rise of software development to software engineering status. Separation of the problem space between the PIM and the PSM allows the system engineers to provide a platform independent model of the system from which the software engineers can generate the specific model for their intended platform.One should not perceive MDA as a new or radical way of developing software.  It is a refinement of a number of trends that have improved the way organizations produce software.  The classical software development cycle is very labor intensive and fraught with errors in interpretation and translation from requirements phase through the final deployment phase.  A new software development process, such as proposed by the MDA, is required to produce software quickly meeting market or deployment goals and reducing the number of errors introduced in the development cycle while allowing for the management of change.   The MDA is a framework for software development defined by OMG.  Key to MDA is the importance of models in the software development process.  The MDA software development process is driven by the activity of modeling your software system. One of the major differences between it and standard software processes Table 2.1 –Supporting Framework Products – All ViewsApplicableArchitecture ViewProduct ReferenceArchitecture ProductEssential SupportingGeneral NatureAll Views (Context)AV-1Overview and Summary InformationEssentialScope, purpose, intended use, environment depicted, analytical findings, if applicableAll Views (Terms)AV-2Integrated DictionaryEssentialDefinitions of terms used in all productsTable 2.2 –Supporting Framework Products – Operational ViewsApplicableArchitecture ViewProduct ReferenceArchitecture ProductEssential SupportingGeneral NatureOperationalOV-1High-level Operational Concept GraphicEssentialHigh-level graphical description of operational concept (high-level organizations, missions, geographic configuration, connectivity, etc.)OperationalOV-2Operational Node Connectivity DescriptionEssentialOperational nodes, activities performed at each node, connectivity and information flow between nodes.OperationalOV-3Operational Information Exchange MatrixEssentialInformation exchanged between nodes and the relevant attributes of that exchange such as media, quality, quantity, and the level of interoperability required.OperationalOV-4Command Relationships ChartSupportingCommand, control, coordination relationships among organizations.OperationalOV-5Activity ModelEssentialActivities, relationships among activities, I/Os, constraints (e.g., policy, guidance), and mechanisms that perform those activities.  In addition to showing mechanisms, overlays can show other pertinent information.OperationalOV-6aOperational Rules ModelSupportingOne of the three products used to describe operational activity sequence and timing that identifies the business rules that constrain the operation.OperationalOV-6bOperational State Transition DescriptionSupportingOne of the three products used to describe operational activity sequence and timing that identifies responses of a business process to events.OperationalOV-6cOperational Event / Trace DescriptionSupportingOne of the three products used to describe operational activity sequence and timing that traces the actions in a scenario or critical sequence of events.OperationalOV-7Logical Data ModelSupportingDocumentation of the data requirements and structural business process rules of the Operational View.Table 2.3 –Supporting Framework Products – Systems ViewApplicableArchitecture ViewProduct ReferenceArchitecture ProductEssential SupportingGeneral NatureSystemsSV-1Systems Interface DescriptionEssentialIdentification of systems, system components, and their interfaces, within and between nodes.SystemsSV-2Systems Communications DescriptionSupportingPhysical nodes and their related communications lay-downs.SystemsSV-3Systems MatrixSupportingRelationships among systems in a given architecture; can be designed to show relationships of interest, e.g., system-type interfaces, planned vs. existing interfaces, etc.SystemsSV-4Systems Functional DescriptionSupportingFunctions performed by systems and the information flow among system functions.SystemsSV-5Operational Activity to System Function Traceability MatrixSupportingMapping of system functions back to operational activities.SystemsSV-6System Information Exchange MatrixSupportingDetailing of information exchanges among system elements, applications and H/W allocated to system elements.SystemsSV-7Systems Performance Parameters MatrixSupportingPerformance characteristics of each system(s) hardware and software elements, for the appropriate timeframe(s).SystemsSV-8System Evolution DescriptionSupportingPlanned incremental steps toward migrating from a suite of systems to a more efficient suite, or toward evolving a current system to a future implementation.SystemsSV-9System Technology ForecastSupportingEmerging technologies and hardware/software products that are expected to be available in a given set of timeframes and that will affect future development of the architecture.SystemsSV-10aSystems Rules ModelSupportingOne of the three products used to describe systems activity sequence and timing – Constraints that are imposed on systems functionality due to some aspect of systems design or implementation.SystemsSV-10bSystems State Transition DescriptionSupportingOne of the three products used to describe systems activity sequence and timing – Responses of a system to events.SystemsSV-10cSystems Event / Trace DescriptionSupportingOne of the three products used to describe systems activity sequence and timing – System-specific refinements of critical sequences of events described in the operational view.SystemsSV-11Physical Data ModelSupportingPhysical implementation of the information of the Logical Data Model, e.g., message formats, file structures, physical schema.Table 2.4 –Supporting Framework Products – Technical ViewsApplicableArchitecture ViewProduct ReferenceArchitecture ProductEssential SupportingGeneral NatureTechnicalTV-1Technical Architecture ProfileEssentialExtraction of standards that apply to the given architecture.TechnicalTV-2Standards Technology ForecastSupportingDescription of emerging standards that are expected to apply to the given architecture, within an appropriate set of timeframes.lies in the nature of the artifacts that are created during the development process.  In the MDA artifacts are formal models that can be understood by software engineering tools.  There are three principle artifacts associated with MDA.  They are the Platform Independent Model (PIM), Platform Specific Model (PSM) and the code itself.The PIM simply stated is a high level abstraction that is independent of any implementation technology.  The PIM allows for the modeling of the domain or business aspects of the desired software solution.  This model should be independent of implementation details such as information formatting technologies, languages, middleware, and Operating systems.  Through the use of a PIM one can begin to verify at an abstract level that the desired system is what the stakeholders want and can be realized to some arbitrary number of physical platforms. The PSM is derived from the PIM adding the necessary technical information that allows for generation and finally the deployment of the solution to a specific computing platform. The PSM must take into account the specific information formatting technologies, languages, middleware, and operating systems details.Also, the MDA concept of MOF and its associated profiling, which establishes translators or generators of the model for specific platforms, allows for the automated transformation of the model to actual executable code. The percentage of the transformation problem space that can be considered reliable is still being researched.Simulation Domain: BOMs and CMSEGrowing out of the High Level Architecture (HLA) M&S technology space, a Base Object Model (BOM) represents an independent aspect of simulation interplay that can be used as a building block in the development and extension of a simulation and/or interoperable environment. At its core, a BOM represents a design pattern that identifies an interaction and/or a trigger related to one or more object classes and its attributes. A BOM can also encapsulate additional meta-data and information such as behavior which gives it the status of a component.   In fact, BOMs are specifically identified in FEDEP as a potential facilitator for providing reusable object model components used for the rapid construction/modification of simulations and simulations spaces.The composition of individual BOMs for defining a simulation or simulation environment is used to produce what is loosely termed a Mega-BOM.  A Mega-BOM has associated with it the meta-data from each BOM, plus the dependency and interrelationships between those BOMs. The BOM concept is intended to influence four things within the M&S community:Substantative Interoperability – The application of XML and XML Schemas prescribed for BOMs provides a mechanism for defining and validating context, and facilitates understanding of the data being exchanged. Furthermore, the flexibility offered by BOMs allows for greater application of simulation interoperability with other domains.Reusability – The meta-data cataloged within a BOM such as intent-of-use, historical use, behavioral information, visual information, security, and accreditation information, will facilitate greater reuse of components.Composability – BOMs will facilitate the ability to rapidly compose simulations and simulation environments both statically (design time) and dynamically (at run-time).Adaptability – Mega-BOMs produced by BOM compositions can be used to represent the standard data exchange interface for systems and simulations. Adaptability is accomplished by deploying and applying the appropriate XML-based transformations that represent mappings between common BOMs within a Mega-BOM, by the receiving federate. With DMSO having established the Composable Mission Space Environment (CMSE) program, the Simulation Interoperability Standards Organization (SISO) community has begun to “see” the potential of what component technology, like BOMs, has to offer. The BOM effort is laying the foundation that will allow the architecture definition to smoothly transition into simulations that can then be transformed into fielded systems.Enabling Technologies: XML and Web ServicesThe eXtensible Markup Language (XML) provides a platform-neutral mechanism for declaring and representing data and meta-data. The intent of XML, as defined by WebPedia, is that “it allows designers to create their own customized tags, enabling the definition, transmission, validation, and interpretation of data between applications and between organizations.”  Specifically, XML allows data to be serialized into transmittable forms that are easily decoded on any platform. This provides the interoperability needed between different manufacturer’s platforms with differing software environments. XML-coding and decoding software are available on virtually every platform for every programming environment. XML is text based; this makes it easier to handle in low-tech environments (and when coupled with real-time compression algorithms keeps the actual size of the exchanged payload very small), and XML is a very flexible format that can be easily extended in unambiguous ways.XML, in addition to a few other core layers (see Figure 2.4), provide the basis for what is called web services.  Web services extend the concept of reuse to the application level. The Stencil Group defines web services as:  Loosely coupled, reusable software components that semantically encapsulate discrete functionality and are distributed and programmatically accessible over standard Internet protocols [3]. What this definition states is that web services are basically reusable software components. They allow developers to leverage the component-based model and reuse application nuggets created by others.  This reuse includes snapping the application nuggets together in unique ways as well as enhancing their capabilities as needed. Snapping can be easily achieved because web service components are loosely coupled, unlike typical application connections that require tight-coupling. This loose coupling can be achieved because web service components describe their own inputs and outputs in a way that other software can determine what it does, how to invoke its functionality, and what result to expect in return. Web services also make use of existing transport protocols like HTTP, allowing them to leverage existing infrastructure and comply with firewall policies. These foundational technologies are playing a key role in advancing the higher-level concepts discussed above (i.e., DoDAF, CADM, BOMs, MDA), as well as other related efforts such as the Simulation Reference Markup Language (SRML), web-based HLA, and the eXtensible Modeling and Simulation Framework (XMSF).The ApproachThe approach starts by utilizing the rich descriptions and meta-data collected and described within the DoDAF and CADM by the system architects, as communicated from the need-setters. They contain all of the information and data associated with DoDAF’s Operational, System, and Technical Views (see Tables 2.1 through 2.4). This architecture definition, or blueprint outline, becomes the initial baton which is then utilized by the system implementers to establish use cases, scenarios, and an initial simulation of the system. At this point the baton, which is now capable of being simulated (i.e., initially using Petri nets) can be evaluated for completeness and high-level correctness. As testing and validation measures permit, the baton migrates toward a constructive simulation captured as a UML-based PIM that is described via a set of BOMs that have been wired together to accomplish a specific purpose. This composition of BOMs, (aka a Mega-BOM), migrates toward a mature simulation that can interact with live systems, virtual systems and constructive simulations using HLA or other interoperable simulation architectures. As the baton is validated within this simulation environment by all stake holders, and consensus in the solution has been reached, the task of transforming the baton into a Platform Specific Model (PSM), or real system, occurs.  The key to this process is the paradigm shift from the “repeat line,” using baton handoffs, to a “model-centric” approach, in which everyone owns the baton and has first hand knowledge of it.  The simulation of the system is viewed as just a step in the development of the real system, such that upon completion, the baton contains both the simulation and the real system. The key that M&S plays is providing the driving force for a successful transformation from an architecture definition to an implemented system.    Architecture StageDiscovery of the architecture description from the expressed needs is the key task in the Architecture stage. Inputs from subject matter experts, users, program/project officers, system engineers, and other related stake holders is required to ensure that a complete architecture has been collected.  This collection includes any and all meta-data required to understand the decisions reached within the architecture description and to properly reflect the intent of the architects.  A framework, such as the DoDAF and its CADM, should be utilized to guide the formation of the architecture and the collection of its associated meta-data. Upon completion, the blueprint outline (or pieces of the architecture), being expressed in any of three views (operational, system, and technical) will be available, as well as its associated meta-data and initial componentization.Conceptual StageValidation of the architectural description and completion of the blueprint is the key task in the Conceptual stage.  During this stage, analysis is performed on the various architecture views to determine objectives, players and their respective connectivity and scenario(s).  This information can be effectively expressed by generating a set of Use Cases and related class diagrams that form the conceptual model of the architecture.  A conceptual model identifies “what” is to be represented and “how” it is to be represented [4]. This effort includes a quick-look analysis that supports early validation of the DoDAF Views (or similar architectural views) by performing a confidence check on the conceptual model representing the architecture description. Once validated, this conceptual model can then be used as the overall blueprint for all additional development.  The conceptual stage validation process is further accomplished by performing trigger analysis (i.e., Colored Petri nets [5]) to identify initial profiling of connectivity paths, load points, dead locks, and occurrence graphs.  This can be thought of as the initial validation of the baton (or model) and can provide the beginnings of the information needed to compute performance measures.Virtual StageThe key task in the Virtual stage is to represent selected aspects of the blueprint (i.e., systems, simulations, applications and databases) as patterns and potential components using Interface Base Object Models, which includes an XML Schema that is a derivative of the original HLA Object Model Template (OMT).  This will produce unique BOM XML documents with a corresponding BOM Schema that defines the elements (content and attributes) a BOM can contain, the order in which those elements appear, and the meta-data representing the component such as the scope, purpose, and “intent of use” [6].  At this level, the collection of BOMs forms the Mega-BOM describing the “data interface” ultimately needed for players and tools to connect and share information.  A player’s “data interface” is built based on this level of BOMs.  Simulation Execution StageUpon transition of the validated blueprint to a set of BOMs (i.e., the Virtual stage), the model moves into the execution stage. This stage provides the ability to perform a more mature simulation of the system. This might include a simulation that could be performed within a single platform and have initial behavior represented in SRML within the applied BOMs, or, it might include a Federation of federates (live, simulated, or mix), providing a more detailed and exact execution of the simulated space.The key task of the Execution stage is the successful instantiation and interoperability of the objects, triggers and interactions represented by the BOMs developed in the Virtual stage.   This includes instantiating the BOMs established in the Virtual stage, executing the scenario(s) laid out in the Conceptual stage, and collecting the results of the execution for use in the Assessment stage.Operationally, data will be shared from a player as XML information exchanged over the enterprise based upon the “data interfaces” established earlier (i.e. the mega-BOM).  A receiving player or tool will validate the incoming XML information based on the associated schemas identified in the incoming XML data.  Simulation Assessment StageThe key to the Assessment stage is the identification of shortfalls, gaps and deficiencies and duplicative efforts, unnecessary capabilities and unforeseen issues based on various scenarios.   The results of the Execution stage will be measured against the objectives identified in the Use Cases collected during the conceptual stage, which were based on the items identified in the DoDAF views.  Applicable tools can be used to support this analysis.  Based on the results, appropriate feedback will be applied to previous stages to adjust models as necessary to achieve a valid execution and/or an improved architecture description. Upon successful assessment, the set of architecture data, Use Cases, BOM componentization, other UML-based information, scenarios, measures, and other artifacts contained in the baton (or model) provide the meta-data and framework to transition the model into the MDA.Tactical StageAt this point, the simulated model is now ready to be transitioned into a specific model for a targeted platform. This effort would involve the generation of source code from the simulated model that would be compatible with the targeted tactical environment. MDA’s MOF and its associated profiles and generators can be applied to perform this action.Tactical Execution StageThe key task of the Tactical Execution stage is the successful instantiation and interoperability of the platform specific model developed in the Tactical stage.  This effort places the real tactical system into its real tactical environment. Environments such as the DEP/JDEP or other established system-of-system hardware-in-the-loop simulation environments can be leveraged to support this effort.Tactical Assessment StageLike the Simulation Assessment Stage, the results of the Tactical Execution stage will be measured against the objectives identified in the Use Cases collected during the conceptual stage, which were based on the items identified in the DoDAF views.  Measures such as those established by the TEMP-801 project, Battle Force Interoperability Requirements (BFIR) metrics, and SIAP metrics could also be leveraged. Applicable tools can be used to support this analysis.  Based on the results, appropriate feedback will be applied to previous stages to adjust models as necessary to achieve a valid execution and/or an improved architecture description.Summary	In summary, we have presented the need for a paradigm shift in the way in which tactical systems (and other systems and simulators) are defined and built. Our experience has shown that the current mechanism that relies on the “repeat” model – and the constant attention of super-heroes – must be changed. The development baton can no longer be handed from team to team, becoming more fragmented and farther removed from those who started the race, but must become a single team resource that everyone contributes to as the project moves through the development life cycle.Recent technology advances that lay the foundation for this paradigm shift include Web Services, XML, DoDAF, CADM, BOMs, and MDA. These elements, if properly integrated, can provide the “views” into the system such that all domains reference and utilize information contained within the same model. These together can establish a successful blueprint-to-simulation-to-implementation process.As the DoD drives toward its open architecture and network centric concepts, the shift in this paradigm from the “repeat” model to a single model-centric concept is critical to its success.References[1] C4ISR Architecture Working Group, DoD: “C4ISR Architecture Framework Version 2.0” 18 December 1997[2] S. Goss, P. Gustavson, L. Root: “The Use and Application of Modeling and Simulation for Architecture Assessment” ACS Defense, July 2002[3] The Stencil Group: “Defining Web Services” An analysis memo from The Stencil Group, June 2001[4] P. Gustavson, C. Turrell, P. Zimmerman: “Capturing Intent-Of-Use Meta-Data for the Conceptual Model – A Key to Component Reuse” SISO SIW, September 2003.[5] L. Alexander, L Wagenhals: “C4ISR Architectures I:  Developing a Process for C4ISR Architecture Design” George Mason University, Fall 2000[6] P. Gustavson, C. Turrell, P. Zimmerman, L. Root: “Conceptual to Composable: Driving Towards Rapid Development of Simulation Spaces” I/ITSEC, Nov 2003Author BiographiesLAWRENCE M. ROOT is co-founder of SimVentions, Inc. and has been involved in the broader modeling and simulation community since the onset of HLA. Larry has over 19 years of real-time tactical software and system engineering experience supporting Navy C4I combat systems and related analysis.  Most recently he has been involved in assisting the Navy in shifting its tactical combat system development paradigm from the current platform-centric approach to a battleforce-centric approach that supports and encourages the development of “composable components” that span missions and platforms.  As such, he supports the Navy’s contributions to Single Integrated Air Picture (SIAP), Force System Engineering Council (FSEC), and Open Architecture (OA). He has published several papers within SISO and is currently the Chair for SISO’s BOM Product Development Group (PDG).JERRY W. OESTERHELD has over 21 years of experience in software/system engineering with emphasis on real-time control systems. He has worked on a diverse set of programs consisting mostly of Command and Control systems, EW systems, and various Intel systems for the Navy, Marine Corps., and Army. Mr. Oesterheld is currently performing system-engineering activities for the Surface Navy Open Architecture (NOA), Littoral Combat Ship (LCS), and Cooperative Engagement Capability (CEC), and, is participating with the Single Integrated Air Picture (SIAP) engineering task force.   He has authored and taught numerous courses related to Ada83, Ada95, and C++. He holds a BS (80) from Mary Washington College and has taken numerous graduate courses related to software engineering.MARK MCAULIFFE is the Deputy Director for SMART Technologies at U.S. Army RDECOM Simulation Technology Center in Orlando. He is an IEEE Standards Associate and has been involved in Simulation Standards development for both DIS and HLA over the past 10 years.  For over twenty-five years, Mr. McAuliffe has been active in real time simulation reseach and development.Figure 3.1 – Blueprint – to – Simulation – to – Implementation ApproachFigure 2.4 – Web Services Technology Stack (Stencil Group 6/2001)