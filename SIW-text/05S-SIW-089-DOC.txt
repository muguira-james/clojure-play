An Agent Based Prototype for Model CompositionShon VickSean Patrick MurphyR. Scott CostWayne BetheaJohns Hopkins University Applied Physics Laboratory11100 Johns Hopkins RoadLaurel, MD 20723(240) 228-5000 HYPERLINK "mailto:Shon.vick@jhuapl.edu" Shon.vick@jhuapl.edu,  HYPERLINK "mailto:Sean.Murphy@jhuapl.edu" Sean.Murphy@jhuapl.edu, HYPERLINK "mailto:scott.cost@jhuapl.edu"scott.cost@jhuapl.edu, Wayne.Bethea@jhuapl.eduKeywords:Composability, Interoperability, Agents, ArchitectureABSTRACT: Automated composition of existing simulations has long been a goal in the Department of Defense (DoD) Modeling and Simulation (M&S) community, and for good reason.  The ability to automate the assembly of models into a composite for use within a simulation is incredibly attractive in terms of potential cost savings and other factors.  However, such composition is difficult for a number of technical and managerial reasons.  Although significant progress has been made over the last decade in achieving simulation interoperability, less progress has been made in the field of composability at the model level.  Universal agreement has not been reached on the definition of fundamental terms and definitions.  Although several researchers have started constructing a theoretical framework for the composition of models, the progress in developing prototypical systems to assist in automating composition lags behind. In this paper we present a prototypical architecture for automating the model composition process. 1. IntroductionThis paper describes work undertaken by the Johns Hopkins University Applied Physics Laboratory (JHU/APL) under the direction of the Defense Modeling and Simulation Office (DMSO) to engineer an architecture for assisting in automating the model composition process. While the complexity of military situations has increased, so too has the complexity of and the demand for simulation results in the Department of Defense (DoD) domain. Higher demand is often accompanied by shorter planning time frames that drive a desire to reuse components that make up such simulations. Motivated by cost and time pressures, users desire to compose models that already exist rather than modify components or develop them anew from first principles. The prototype facilitates model composition by finding model combinations that are semantically viable for use in a simulation that satisfies one or more user needs..  The system employs an agent-based architecture that uses a set of three separate ontologies to represent knowledge about the composition process from a different perspective. The User Intent Ontology (UIO) represents the semantics of the user’s intent in creating the composed system. The Model Composition Ontology (MCO) captures knowledge about the mechanisms for valid composition. The Model Definition Ontology (MDO) describes attributes and relationships of models and related data that are available from model and data providers [].The knowledge representations that formed the basis of the ontologies came from several sources; existing taxonomies and logical models, domain expert knowledge, and a DMSO-sponsored Workshop on Model Composability. This paper reports on the progress made in developing an architecture for model composition, it discusses the viability of the tools, techniques used in the prototypical architecture. Finally lessons learned from attempting model composition with an actual system and future directions for investigation are presented.1.1 Basic Definitions and ConsiderationsEfficiently composing such simulations from pre-existing components has proven difficult to automate for a many technical as well as non-technical reasons [].  In contrast significant progress has been made over the last decade in achieving simulation interoperability which is the first barrier to composing systems. This progress has largely been achieved through the standardization and use of middleware like the High Level Architecture (HLA), the Common Object Request Broker Architecture (CORBA) and other frameworks. Much less progress has been made in the field of composability (at any level) as even universal agreement on the definition of fundamental terms and definitions in the area of model composability has not been reached. While several researchers have started constructing a theoretical framework for the composition of models, the progress in developing prototypical systems to automate such composition lags behind.Although many researchers and several organizations have proposed varied definitions for the term “composability,” a general consensus has yet to be reached by the Modeling and Simulation (M&S) community.  Yilmaz has recursively defined the term as “the quality of being composable and means to be capable or worthy of being composed” [].  Petty has defined the term composability as “the capability to select and assemble simulation components in various combinations into simulation systems to satisfy specific user requirements” []. Another definition holds that composability is the ability to rapidly select and assemble components to construct meaningful simulation systems to satisfy specific user requirements. Composability includes the framework, body of knowledge, tools, techniques, and standards necessary to enable effective integration, interoperability, and reuse.No matter which definition is upheld, whether formal or practical, there are basic recurring themes and issues.  First, model composition assumes that there are smaller parts or components to compose into larger ones.  In order to build these larger entities from smaller ones the smaller entities must be able to exchange information, an ability often referred to as interoperability. The second consideration is that when smaller components are combined their combination must preserve the semantic integrity of the components. For example, if there are assumptions inherent but not necessarily explicit in the form of a model’s input or output that dictate its viable application, the use of that model must be consistent with these assumptions to be semantically viable. Suppose for example that we are composing an Anti Submarine Warfare (ASW) simulation. An automated composition system could easily determine that the ASW simulation is going to need an acoustic model. Suppose we are considering an acoustic model that does not accurately approximate wave propagation in the littoral for use in the composition, semantic integrity will be lost.  Finally, it is not sufficient for two models to be able to communicate in a semantically viable way. Semantic viability only says that components are assembled in a way that has meaning. It does not say whether this meaningful composition was the one intended by the user. 1.2 Context of Our InvestigationIf model composition involves the aggregation of components in a viable way, it is natural to define the term component. Just as with the term composition, there is no uniformly accepted definition for the term component. A noted researcher in the field of Component Based Software Design (CBSD) defines a software component as “a unit of composition with contextually specified interfaces and explicit context dependencies only. A software component can be deployed independently and is subject to third-party composition” []. Other researchers have defined components to be “nontrivial, nearly independent, and replaceable parts of a system that fulfill a clear function in the context of a well defined architecture. A component conforms to and provides the physical realization of a set of interfaces" []. Both of these definitions imply that components are well defined in terms of their interfaces and their context of use. In the general Model Composability problem these conditions are not always meet. So while the concerns in the composition of simulations and the general composition of software systems overlap in principle, the differences in practice dictate that these two topics are actually different areas of investigation. CBSD still has not yet become the mainstream software development technology [] and has been best applied only when the elements that form a composition as well as the item to be composed (conceptual components) are small and simple []. This is typically not the case within the Modeling and Simulation (M&S) community. Additionally CBSD supports only components engineered for composition while any viable automated Model Composition System (MCS) must deal with the huge body of existing models that already exist and were not designed with composition in mind. Many of these models do not have metadata to completely describe their input and output structures. A significant exception to this rule are models and simulations specially developed using the High Level Architecture (HLA) with it’s Federation Object Model (FOM) and Simulation Object Model (SOM) [] descriptions which offer significant information about those structures.2. Why Agents?Currently, the model composition process for many modelers and simulationists is often ad hoc and therefore problematic for at least three reasons. First, modelers often want to focus primarily on their own application area or domain of expertise. Thus, when selecting models outside of their domain of expertise, they may need to either rely on experts or be forced to make assumptions when picking models. This incomplete knowledge about whether a model was used in a simulation that was appropriate may result in inaccurate or invalid results. Secondly, even if the modeler has perfect knowledge about a model they may not know where to obtain a specific copy of it. Finally, model assembly can be a potentially time intensive process and model choice  can radically effect the degree of difficulty or time involved in that assembly.  Thus, the choice of an incorrect model may be quite expensive.  An automated or semi-automated composition system could address each of these issues.Collaborative software agents interact with each other to share information or barter for specialized services to affect a deliberate synergism. This resembles the relationship between simulationist and modeler. While each agent may uniquely speak the protocol of a particular operating environment, they generally share a common interface language or ontology which enables them to request specialized services from their other co-operating agents as needed.  The term ontology derives from logic and artificial intelligence, where it denotes the primitive concepts, terminology, and interpretation paradigm of a domain of discourse []. Such agents may perform logical inference and deduction like human modelers and subject matter experts (SMEs) in model selection or adaptation. Such SMEs have specific contextual domain knowledge that enable them to translate needs of a user to a model or set of models that are appropriate for the user simulation task.The software agent paradigm provides a useful framework for constructing a system for resource discovery in a distributed, heterogeneous environment. A system can be constructed with modular/autonomous components interacting at a high level, with a common language. The benefits of the agent framework include: facilitation of interest specification by user agents, need-based matching of agents via brokers, interoperability across platforms as well as easy integration of intermediary and support agents.3. Agent ArchitectureOur system consists of four types of agents: interface agents, mediator agents, composer agents, and gatherer agents.  Interface agents interact directly with the end user, and help to elicit a description of the desired model.  The mediator agents are responsible for validating requests, enhancing them, and providing feedback to the interface agent.  The composer agent determines and implements the appropriate search plan, and then tasks gatherer agents, which interface directly with model repositories.  Figure 1 depicts the relationships among these agents.The system also assumes the existence of basic support agents, such as directory services and management agents; these are provided by the Jade platform, in which this system has been developed. Jade is a Java based agent development platform which implements the Foundation for Intelligent Physical Agents (FIPA) standard and support for ontologies. FIPA [] was formed in 1996 to produce software standards for heterogeneous and interacting agents and agent-based systems. Once the user specifies a model requirement via a GUI, the requirement is passed by the interface agent to the mediator agent.  The mediator validates the specification, eliciting additional information where needed from the user. This validation process is accomplished by using the RACER deduction system []. Once the mediator agent verifies the completeness and validity of the simulation specification, the mediator agent passes it along to the composer agent.  The composer agent decides how a search should proceed, and engages gatherer agents to do the actual search. The gatherer agents must do a match, based on requirements and specifications, using appropriate ontologies. The pictorial summary of the message exchange between agents is captured by figure 2.4. OntologiesExisting repositories for model definitions, such as the DMSO Modeling and Simulation Resource Repository (MSRR) supply model definitions []. However this repository was created to be searched by a human operator using the provided keywords and text fields. The keywords are not uniform across the services and the text fields used to describe these resources are free text.  No explicit description of how the keywords may be linked together or how the free text may be structured or interpreted is given. Models are not detailed to the same level of specificity – some have lots of textual information, some almost none at all. The MSRR and other model repositories sources were not designed or intended for automated query so the information available is largely syntactic.  Unfortunately, automated model composition is facilitated by semantic content. Semantics could be described with an ontology. Ontologies can represent domain-specific information/knowledge and the relationships that are assumed to hold between these concepts. An MCS could leverage ontology languages and construction tools that are being used to construct the Semantic Web. The Semantic Web provides a common framework that allows data to be shared and reused across application, enterprise, and community boundariesAn ontology is often used to support the sharing and reuse of formally represented knowledge among systems.  As it is useful to define the common vocabulary in which shared knowledge is represented, an ontology becomes a specification of a representational vocabulary for a shared domain of discourse or reasoning made up of definitions of classes, relations, functions, and other object []. Thus an ontology is a mechanism for representing objects, concepts and other entities as well as the relationships that hold among them.  An ontology is a formal, explicit specification of a shared conceptualization. Conceptualization refers to an abstract mode. Explicit means that the type of concepts used, and the constraints on their use, are explicitly defined. Formal refers to the fact that the ontology should be machine-readable.  Shared reflects that the ontology should capture knowledge accepted by the communities that use it []. There are varying mechanisms for representing such knowledge.Our analysis showed that three ontologies within an integrated framework were needed to increase the semantic content and allow automation of the composition process. We have developed a Model Definition Ontology (MDO) that describes the output and input data elements for models, use restrictions and other metadata. Next we have developed a Model Composition Ontology (MCO) that describes the rules for composing models.  It describes not only what is semantically meaningful but also the potential costs of composition.  Finally, a User Intent Ontology (UIO) was formulated that describes user intent in terms of user interests, purposes and constraints. The emphasis in the first year has been on the creation UIO and MDO. The three ontologies are represented in over 400 classes, properties, and restrictions.  Figure 3 illustrates how the ontologies are used by the various agents.Each of the ontologies is encoded in the Web Ontology Language (OWL). OWL is derived from the DARPA Agent Mark-up Language and the Ontology Inference Layer (DAML/OIL) which is a standard for specifying and exchanging ontologies using a web-based representation.  OWL also contains an inference layer. The popular ontology building tool Protégé works with Owl and was used in the creation of our ontologies.  There are three variants of OWL. The first variant, OWL Lite, supports those users primarily needing a classification hierarchy and simple constraints. For example, while it supports cardinality constraints, it only permits cardinality values of 0 or 1. The OWL inferencing tool works with this variant. The second variant is called OWL DL. OWL DL is so named due to its correspondence with description logics, a field of research that has studied the logics that form the formal foundation of OWL, and supports those users who want the maximum expressiveness while retaining computational completeness. Owl Lite also has a lower formal complexity than OWL DL. OWL Full is meant for users who want maximum expressiveness and the syntactic freedom of RDF with no computational guarantees. Our ontologies make use of OWL DL []. As such, the RACER logic reasoner was used for consistency checking of user specification as noted above.While Protégé was used in the construction of the system ontologies, a mechanism was needed to interact with the ontology as the agents run. This was accomplished through the use of Jena which is a Java framework for building Semantic Web applications. Jena provides a programmatic environment for OWL, as well as including hooks for a rule-based inference engine such as RACER []. 5. How Does Our Work Relate to Service Oriented Architectures?Each of the agents could be viewed as providing a service that could be provided in the context of a Service Oriented Architecture (SOA) via web services. To understand the notion of an SOA we must first understand the notion of a service.5.1 Web ServicesThe term web services is used in a host of ways but usually refers to the technologies that allow distributed applications to connect on a network. Services are connected using web services. A service is the endpoint of a connection. Services are supported by a service provider and are used by a service consumer. A service is provided by some type of underlying application that supports the connection offered. The combination of service consumer and service provider makes up a SOA.  While the diagrams depicting middleware and web services would have similar structure, they are in fact quite different. With the middleware approach there may be different semantics in the data sources, there is no semantic translation, nor is the exchange constrained by any set of standard definitions for services. The web service approach solves each of these shortfalls by having a directory entity where a consumer can find a standardized definition of a service. 5.2 Using a Directory to Mediate the Consumer and ProducerThe directory mediates the interaction between service producer and service consumer in a five step process. The first step in the process is for a service provider to tell the directory that it can provide a service. After this step, the service consumer can ask the directory who provides that service. In step 3 the directory sends back to the service consumer the information about a service provider and it service. In the fourth step the service consumer contacts the provider of the service using information it got from the directory. In the fifth and final step of the process, the service returns the data associated with the service in response to the consumer request.5.3 The Structure of Messages between Consumer, Producer and DirectoryEach of the five types of communication is encoded as in SOAP (originally standing for Simple Object Access Protocol). The http protocol is used to transport SOAP messages. SOAP acts as an envelope for exchanging service messages.5.4 Model Composition in an SOASo how can a model composition system work within this framework? What are the services, who provides them, and who uses them? The services will include providing a composed model. The provider of this service is the Composition Provider through the Model Composition System. Another pair of services is provided by the data and Model Repositories who provide models and data to the composition to the Composition provider. Repositories however must be provided with their models and data. This happens through a submission service provided by the Repository for the Model or Data provider. 6. Observations and Future DirectionsAn overview of web services within an SOA has been presented illustrating the fundamental ideas and technologies. Next these ideas were shown in the context of the process of doing Model Composition. Our initial system has been implemented in a centralized agent based system. Future implementations of the system would have those agents interact within the context of an SOA. From figure 4, we see that the challenge still is formalizing what is still a non-standard semantics while the communication and distributed processing mechanisms are relatively more mature, stable and well defined.References	[] 	Reference from ADS05[]	Paul K. Davis and Robert H. Anderson (2004). “Improving the Composability of DoD Models and Simulations”, The Journal of Defense Modeling and Simulation,Vol.1, Issue 1, April 2004 Page 5-6,  The Society for Modeling and Simulation International[]	Yilmaz, L. and Ören T.I., 2004, “Infrastructure for Simulation Model Composability: Basic Concepts to Advance Theory, Methodology, and Technology,” A Position Statement on Model Composability prepared for the DMSO Model Composability Workshop, Sept 2004, Johns Hopkins University Applied Physics Laboratory, Laurel MD.[]	Petty, M., 2003, “Computational Complexity of Selecting Components for Composition”, Proceedings of the Software Interoperability Conference, 03F-SIW-073, (Fall)[]	Component Software: Beyond Object-Oriented Programming, by Clemens Szyperski, Addison-Wesley, 1997[]	The Rational Unified Process Made Easy Per Kroll, Phillipe Krutchen. Addison Wesley[] 	[Bartholet  2004] In Search of the Philosopher’s Stone: Simulation Composability Versus Component-Based Software Design, Robert G. Bartholet, David C. Brogan, Paul F. Reynolds, Jr., Joseph C. Carnahan, Proceedings of the Fall 2004 Simulation Interoperability Workshop[]	[Bartholet  2004] []	Creating Computer Simulation Systems: An Introduction to the High Level Architecture by Frederick Kuhl, Richard Weatherly, Judith Dahmann,[]	Towards Metaprogramming: A Paradigm for Component-Based Programming, Communications of the ACM, Vol. 35 No. 11, November 1992, Pgs 89-99[]	www.fipa.org[]	Description of the RACER System and its Applications, Volker Haarslev and Ralf M¨oller University of Hamburg, Computer Science Department Vogt-K¨olln-Str. 30, 22527 Hamburg, Germany, see www.sts.tu-harburg.de/~r.f.moeller/ papers/DL-2001-Racer.pdf[]	http://www.msrr.dmso.mil/[]	http://ksl-web.stanford.edu/KSL_Abstracts/KSL-92-71.html[]	http://www.cmswiki.com/tiki-print.php?page=Ontology[]	http://www.w3.org/TR/owl-features/[]	 HYPERLINK "http://jena.sourceforge.net/" http://jena.sourceforge.net/Author BiographiesSHON VICK is a Computer Scientist at the Johns Hopkins Applied Physics Laboratory. He did his undergraduate work at Rutgers University where he studied Economics and Mathematics and recevied a BA degree Cum Laude in 1980. In 1987 he receivied a MSc in Computer Science from the Johns Hopkins University. He has published in  diverse areas of Computer Science from Agent Based Computer Autonomy and Intelligent Systems, to Automated Planning and Scheluding to Software Engineering. Mr Vick is also an Adjunct Faculty Member in the Electrical Engineering and Computer Science Department at the University of Maryland Baltimore County.SEAN PATRICK MURPHY is a Biomedical Systems Engineer at the Johns Hopkins University Applied Physics Laboratory.  In 1999, he received degrees Magna Cum Laude in honors electrical engineering and mathematics at the University of Maryland College Park.  In the summer of 2002, he completed his Masters degree with honors in Applied Biomedical Engineering at the Johns Hopkins University.  R. SCOTT COST is a Senior Computer Scientist at the Johns Hopkins University Applied Physics Laboratory.  His research interests include software agents, information retrieval, and bioinformatics.  He holds a Ph.D. from UMBC, an M.S.E. from Johns Hopkins University, and an A.B. from Colgate University, all in Computer Science.  Dr. Cost holds faculty positions at UMBC, UMUC and DeVry University, and is a member of IEEE (Senior), ACM, AAAI, and PMP.Composition and adviceResults compilationSearch planningSpec. validation and enhancementGUIGathererAgentComposerAgentMediatorAgentInterfaceAgentDecompose requirements, formulate search plan and engage search agents.  Compose models.Validate requirements, and engage a composition agent.Assist user in formulation of modeling needs.SearchAgentsModelRepositoriesModelRepositoriesSearch*AgentsComposerAgentInterfaceAgentMediatorAgentFigure 3 – The Use of the Ontologies by the AgentsGatherer Agent fetches models and model definitionsComposer Agent constructs compositions using the MCO and instructs Gatherer Agent to return model informationMCOMDOFigure 1 – Agent framework.Figure 2 – Agent Message PassingUIOModel  ProvidersGatherer  AgentComposer AgentUser Mediator  AgentUser Mediator Agent  uses the UIO to refine query for models and interacts with the Composer Agent EMBED Visio.Drawing.11  Figure 4