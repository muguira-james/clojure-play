Advanced Data Collection and Analysis Tools for HLA FederationsDon-May LeeWilliam Bryan HigginsJerry FeinbergBruce StalcupChristopher JohnsonLitton/PRC1500 PRC DriveMcLean, VA 22102703-556-2720, 703-556-1867703-556-2911, 703-556-1307703-556-2913 HYPERLINK mailto:lee_don-may@prc.com lee_don-may@prc.com,  HYPERLINK mailto:higgins_william@prc.com higgins_william@prc.com HYPERLINK mailto:feinberg_jerry@prc.com feinberg_jerry@prc.com,  HYPERLINK mailto:stalcup_bruce@prc.com stalcup_bruce@prc.com HYPERLINK mailto:johnson_christopher@prc.com johnson_christopher@prc.com Keywords:HLA, VV&A, Analysis, Distributed Simulation, Data Collection, Smart Product Model ABSTRACT:	Distributed modeling and simulation (M&S), including the application of the Department of Defense’s High Level Architecture (HLA), is rapidly becoming capable of supporting higher fidelity applications.  These include the design and test of complex systems, analysis of manufacturing and life cycle costs, advanced training, and C4I planning.  To enhance customer payoffs from high fidelity simulation exercises, developers must be able to collect and analyze detailed simulation data.  Only then can they use the simulation successfully in evaluating and adjusting system designs, testing system performance in synthetic environments, evaluating tactics, and building life cycle business strategies.The analyst’s task in high fidelity simulations is quite complex and requires detailed data collection.  First, he must have data for verifying the proper interactions between exercise federates, i.e., to confirm that the overall simulation is interacting as expected, and therefore producing trustworthy results. Second, he must access the data required for in-depth analysis and optimization of the exercise results.  Finally, he must collect data in response to changing requirements and situations while interfering as little as possible with the running exercise.  The analyst’s constant goal is to prove that a high fidelity simulation exercise reproduces, as accurately as possible, the “real world”, and then to be able to extract, manipulate, and use data just as though it were coming from real life tests.Two specific problems face the high fidelity M&S analyst: (1) most current data collection tools, such as data loggers, can collect only the “public” information interchanged between the federates as specified in the HLA Federation Object Model (FOM), and not the “private” data contained within the individual federates, and (2) most analysis tools permit data manipulation only after the exercise is concluded and not during “runtime.”   To support the high fidelity M&S analyst, Litton/PRC has developed and demonstrated the SimdicatorTM Toolkit, a unique software tool providing dynamic data collection and analysis strategies.  It overlays existing HLA Federations and provides advanced data collection of both public and private data, either during or after a simulation run.  Furthermore, the SimdicatorTM Toolkit can pipe this data into specialized exercise analysis tools.  Because of its unique technological approach, the SimdicatorTM Toolkit provides a powerful analysis capability enabling verification and confident application of HLA-based, high fidelity distributed simulations.IntroductionToday’s world of defense procurement is under increasing pressure to improve the quality of major new systems, while reducing the cost and time to deliver and operate them.  Modeling and simulation is the crucial technology for answering these demands.  The Navy’s SC-21 Modeling and Simulation Master Plan [1] requires the following:“To support the design, development, manufacturing, training, and operations for the 21st Century Surface Combatant by applying an integrated set of modeling and simulation covering an entire spectrum from engineering component models to campaign simulation.  Appropriate elements of the model set will be fully interactive with other models across all warfare/mission areas to replicate SC-21 performance in a realistic Joint Warfare Forces environment.”Existing “lower fidelity” simulations have been used successfully and economically training operators to recognize and respond to varying battlefield stimuli.  These simulations are more focused on depicting battlefield situations than they are in portraying a complete, high fidelity representation of system operations and interactions. The SC-21 requirement is for simulations that can represent system and environmental details down to the finest level of physical detail. Only in this way can modeling and simulation (M&S) fulfill the requirements for accurate inspection and projection of system capabilities, life cycle costs, manpower requirements, et al. As we develop models and simulations to support this high fidelity goal, we need to integrate disparate systems into ever-larger federations, interoperate these models, and collect and analyze data.  This approach builds on the legacies of the SIMNET [2], ALSP [3, 4], and DIS [5] interoperability standards.  However, these frameworks were not designed to handle the interactions required for high fidelity.  The current approach is the Defense Modeling and Simulation Office’s High Level Architecture (HLA) [6].  Its goal is to provide a framework which allows more discrete control of  participating models, time-stepped execution, and guaranteed data exchange.  By extension, the HLA capabilities allow for the federation and control of high fidelity, engineering-level models.  However, the HLA framework does not supply the ability to collect, analyze, and display detailed public and private data in a timely manner.  Thus, we need the SimdicatorTM Toolkit, described below, as an HLA enabler for high fidelity simulations.Advanced Data Collection and Analysis Tools for HLAOur mission as HLA federation developers is to analyze and formulate successful designs, tactics, and business strategies in this high fidelity world.  To succeed, we must prove that a high fidelity simulation exercise reproduces, as accurately as possible and in engineering-level detail, the “real world.”  We find ourselves with two fundamental needs – we must be able to:collect all the data for verifying proper interactions between exercise federates, and access data for further analysis and optimization of the exercise in “real-time.”HLA does provide some capabilities along these lines.  By its design, the HLA data interchanged between its component models (federates) is carefully planned during the creation of the Federation Object Model (FOM).  The actual data interchange occurs via the HLA’s Runtime Infrastructure (RTI).  Since data passes through the RTI via publish and subscribe mechanisms, a data logger needs merely to collect this information and archive it into a database.  Once there, the data may be retrieved by the analyst for further study.However, the RTI handles only the interchanged data passed through it, and not the “private” data maintained internally by the federates.  For example, in a missile/target interaction, the target’s signature and the missile’s detection call (yes/no) are published.  However, the precise missile signal excess on the target is usually private.  It is precisely this private data that is vital for answering issues related to high fidelity operations, or even for basic Verification and Validation (V&V) of the system. The analyst must be able to access this private information to produce the required analysis.Also, collecting data using the HLA FOM is inflexible since it cannot quickly support changes. For example high level users will observe an exercise and then ask questions which cannot be answered from the initially “planned” data interchange.  We are then left with the choice of either modifying the FOM to pass the newly required data via the RTI, or employing an advanced tool (like the Simdicator( Toolkit) to access the needed information.  Finally, in the real world of simulation exercises, quick-look analyses often take between three and six months to complete, and full reports take a year or longer.  This is an expensive, labor intensive activity.  Also, when these lengthy analyses are completed, the useful feedback can be limited because design engineers have finished their drawings, pilots and other military trainees have forgotten the intricacies of the exercise, and command decisions have been made rendering the analyses meaningless.  Thus we need tools that can filter through the massive amounts of data that are output from the federates, both while the exercise is running and after it is finished.  This permits analysis during the simulation and enables the delivery of important information to the involved federates “on the fly.”Although the RTI version 1.3 includes Data Distribution Management (DDM) Services, which can greatly reduce the amount of information passed between federates, the data loads within DDM regions can still be overwhelming to an analyst, especially when the federation is large.  Therefore, analysts still need advanced analysis tools to further filter and collect even more specific information on federates within a region or across regions.In summary, our experience in real-world situations indicates that there are four main reasons why the HLA FOM/RTI approach is insufficient for satisfying all of the high fidelity M&S analyst’s needs:the RTI accesses only the interchanged, or “public”, data passed through it, and not the “private” data maintained internally by the federatesthe HLA FOM “planned” approach is inflexible and cannot quickly support changes to the initial designthe analyst often must wait until the exercise is completed to access the data base of interchanged informationthe volume of interchanged data is so high that an analyst can be overwhelmed (even within HLA’s publish and subscribe environment).Requirements for HLA Advanced Data Collection and Analysis ToolsThe previous section has emphasized the need for new HLA data collection and analysis tools.  As noted, these tools must be flexible, controllable, and able to collect and analyze data in “real-time” during an HLA-based simulation exercise.  Looking more deeply, we need to be able to collect and analyze all the details of model interactions since, at a minimum, every interaction must be checked as part of the V&V process.  To attain our analysis goals, we must be able to collect both public data published over the network and private data stored within each federate.  Further, we must be able to take this data and make it readily available to the analyst so he can determine the performance and accuracy of the distributed simulation.We require tools that allow us to:verify proper interaction of federates (models) within the HLA federationverify standards compliancevalidate that the exercise is operating to customer specificationsconfirm that the exercise accurately represents real-world conditions to the degree requiredrecord discrete events within the exerciseanalyze, visualize, present, and playback those events to provide understanding of exercise resultsinterface with a wide range of commercial analysis applications, e.g. Mathematica, PVWave, MATLAB, Satellite ToolkitWe note that these new HLA tools must perform the above tasks not only for operator-in-the-loop, real-time exercises, but also for varying exercise “speeds.”  With HLA’s Time Management Object, the federation creators can decide at what speed (faster than real-time to slower than real-time) an exercise runs.  The analyst will need tools that operate reliably in all of these environments.The SimdicatorTM ToolkitWe have outlined formidable requirements for advanced data collection and analysis tools supporting HLA-based simulation exercises.  These tools must be flexible and easy to use while maintaining their core capability in data gathering.  To satisfy these requirements, Litton/PRC initiated an Internal Research and Development (IR&D) project, the SimdicatorTM Toolkit, focused on high fidelity modeling and simulation. The SimdicatorTM Toolkit is a versatile, powerful data collection and analysis tool.  It is built as a pure Java( application providing a significant measure of platform and system independence.  The design consists of a special control and monitoring infrastructure combined with two separate libraries (see Figure 1). The first library contains data collection agents that can gather information from exercise federates and their computer hosts.  The second library holds integrated COTS or specialized analysis tools. This SimdicatorTM Toolkit can gather data from public (published) data streams and from private (federate internal) files and return them as specified by the analyst.  (Of course, there are security issues involved – it cannot collect data for which access is specifically denied.) It can filter the data and provide an “on the fly” analysis of a current running simulation or it can send the requested data into an analysis tool for more detailed assessment.  Currently, SimdicatorTM Toolkit is compatible with both the DIS and HLA frameworks.  We are upgrading its infrastructure to take full advantage of new Java( version releases (i.e., JDK1.1.x and JDK1.2) as they become available.  We are currently applying the SimdicatorTM Toolkit to several different projects in system design and planning.    EMBED PowerPoint.Show.8  Figure 1The distinct advantages of SimdicatorTM Toolkit include its ability to:be overlaid on an existing HLA-based distributed simulation exercise (not needing to be a part of the initial simulation design)be easily reconfigured “on the fly”collect data, both public and privatemate the collected data with any number of tools for analysis, optimization, or V&Vbe set up and controlled through GUIs.An analyst uses SimdicatorTM Toolkit as follows (See Figure 2): EMBED PowerPoint.Show.8  Figure 2We now provide an example of the SimdicatorTM Toolkit in action in a gedanken HLA-based exercise.  We have constructed a large federation to test the combat performance of the next generation Navy surface ship, the SC-21, in a joint land attack scenario.  The HLA federates include surface ships, airplanes, helicopters, submarines, land-attack missiles, anti-ship missiles, air-defense systems, and land targets.  The environment is handled by an environmental server.  We have specified a FOM to interchange all of the basic information necessary to produce campaign-level, force-level, platform-level, and combat system-level measures of effectiveness.  We have carefully planned for as many questions relating to SC-21 performance as we can.In this HLA-based exercise, we represent the SC-21 by its high fidelity Smart Product Model (or SPM).  The SPM is the data structure which contains the known information concerning the ship and its interactions, including CAD/CAM/CAE models for the hull and structures, parts lists, manufacturing information, life-cycle support information, combat system performance, environmental interactions, communications capabilities, etc., all combined with system behaviors.  It is this SPM which is the key step towards implementing the SC-21 vision of designing, building, and operating a major system within a computer before bending metal.  For the purpose of our exercise, the SC-21 SPM is the SC-21 to all degrees of fidelity required.While this land attack scenario exercise is running, we discover that, in spite of efforts to reduce the ship’s  signature, a threat cruise missile has locked-on and has begun to home on the SC-21.  Consequently, the analyst must isolate and then identify the radar "highlights" that have enabled this threat missile evolution.  As soon as this is accomplished, modifications can be made to the SC-21 design (directly to the SPM!) and another test/simulation cycle can start. The FOM design specifies that the SC-21 publishes its radar cross section as a lookup table and that threat missiles subscribe to this table.  Using this together with geometric and kinematic factors, the threat missile federate determines radar target detection by calculating its seeker’s signal excess.  Because we anticipated this situation, the FOM also specifies that this signal excess be published.  So far, so good – if, in real-time, the analyst can determine the performance of the seeker by collecting all the necessary data (using a tool like the SimdicatorTM Toolkit.)  Now, though, the analyst digs more deeply into the situation.  The goal is to find out what generated that radar cross section highlight in the first place; only then can the designers take action to reduce the SC-21 vulnerability.  This requires using the SimdicatorTM Toolkit to collect data which specifies the geometry of the interaction at threat missile lock-on and to use integrated SPM agents to query the SPM database concerning private data representing detailed ship geometry and design.  This information can be correlated with the published lookup table to not only identify the radar highlight, but also verify the accuracy of the interaction. Moreover the usefulness of the SimdicatorTM Toolkit does not stop here.  Since this radar highlight is of utmost importance to the SC-21 designers, actions must be taken to reduce it. The SimdicatorTM Toolkit can probe the SPM to provide understanding of the design issues that led to the resulting radar highlight.  It can also probe the models describing the motion of the ship as a function of the sea state to determine what fraction of operating time such a highlight might be a observed by a threat.  We are currently investigating even deeper applications of the SimdicatorTM Toolkit to smart product models. This example illustrates the usefulness of an advanced data collection toolkit to an analyst for understanding high fidelity, HLA-based simulations.  We have already demonstrated some applications of the SimdicatorTM Toolkit at the American Society of Naval Engineers (ASNE) Virtual Prototyping Conference in November 1997, the Interservice / Industry Training, Simulation, and Education Conference (I/ITSEC–19) in December 1997, and at the Advanced Information Processing and Analysis Symposium (AIPA) in March 1998.  Two patents have already been applied for related to the SimdicatorTM Toolkit.ConclusionThe SimdicatorTM Toolkit represents a new class of data collection and analysis tools for HLA.  Its versatile and powerful capabilities are vital in the new world of high fidelity, engineering-level simulations.  The SimdicatorTM Toolkit’s ability to produce early preliminary results and to collect data reliably from both public and private sources demonstrably reduces overhead cost and turn around time.  In combination with an expert human analyst, the SimdicatorTM Toolkit can save hundreds of hours by quickly identifying the minute inconsistencies and small exceptions that must be fixed in the application of engineering-level, high fidelity, HLA-based simulations. References[1]	J. Jaensch & D.P. Mahoney: SC-21 Program Office, Naval Sea Systems Command, 21st Century Surface Combatant: Executive Summary, Modeling and Simulation Master Plan, Version 1.0, October 11, 1996.[2]	U.S. Army, Headquarters USA Armor Center, Ft. Knox, KY. Abrams Tank Block 2 Product Improvement Packages: SIMNET Evaluation Final Report. Ft. Knox, KY: USA Armor Center, November 1988. Report No. ACN 70670. [3]	MITRE Corporation, Aggregate Level Simulation Protocol (ALSP) Translator Specification, Draft, March 1993.[4]	Executive Council for Modeling and Simulation (EXCIMS), DOD, Aggregate Level Simulation Protocol (ALSP) Management Plan, May 26, 1993.	[5]	IEEE Standard for Distributed Interactive Simulation—Application Protocols, IEEE Standards 1278.1-1995.[6]	Defense Modeling and Simulation Office, High Level Architecture Management Object Model, Version 0.2, October 17, 1996.Authors’ BiographiesDON-MAY LEE is an Engineer in the Center for Applied Technology at Litton/PRC in McLean, Virginia.  She is currently contributing to the SimdicatorTM Toolkit IR&D and the Litton Cooperative Technology Growth Initiative on Smart Product Models. She holds an M.S. in Engineering and Computer Science from Penn State University and an M.S. and a B.S. in Applied Mathematics from Chun-Yuan University in Taiwan. WILLIAM BRYAN HIGGINS is an Assistant Engineer at Litton/PRC.  He currently serves as a member of the SimdicatorTM Toolkit IR&D team.  He is involved with a Litton Cooperative Technology Growth Initiative developing Smart Product Models.  Mr. Higgins holds an MSW from the University of South Carolina and a B.S. in Psychology from Virginia Polytechnic Institute and State University.  JERRY M. FEINBERG is a Senior Technical Fellow and Principal Technical Advisor at Litton/PRC.  He serves as a member of the Litton/PRC Modeling and Simulation Lead Team and is the Principal Investigator for the SimdicatorTM Toolkit IR&D project and the Litton Cooperative Technology Growth Initiative on Smart Product Models.  Dr. Feinberg holds a Ph.D. and M.S. in Mathematics, and an M.S. in Physics, from Stanford University, and a B.S. in Mathematics from the California Institute of Technology.  He is a member of Tau Beta Pi.BRUCE STALCUP is a Principal Computer Analyst in the Center for Applied Technology in McLean, Virginia. He is currently working as Lead Engineer on the SimdicatorTM Toolkit IR&D and the Litton Cooperative Technology Growth Initiative on Smart Product Models.  He is also involved in the analysis and performance assessment of various unexploded ordnance detection systems for the U.S. Navy.  He has over 25 years’ experience in signal processing, time series analysis and remote sensing applications.  Mr. Stalcup holds a B.S. in Physics from Georgia Tech.CHRISTOPHER H. JOHNSON is a Senior Technical Fellow and Principal Technical Advisor at Litton/PRC.  He was formerly a program manager at the Defense Advanced Research Projects Agency (DARPA) where he conceived and managed a set of programs in very-high fidelity M&S, including an effort to link M&S directly into a military planning system.  Prior to that, Mr. Johnson was a career Naval Officer, serving in a variety of training, acquisition and operational jobs in the information and C4I arena. Mr. Johnson holds an M.S. and B.S. in Electrical Engineering from Stanford University.  He is a member of Tau Beta Pi.