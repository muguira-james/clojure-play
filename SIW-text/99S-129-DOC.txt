The Combat Automation Requirements Testbed (CART) Program:  Improving the DoD's Requirements Process Through Inclusion of Realistic Operator PerformanceDavid G. HoaglandAFRL/HECIHuman Effectiveness DirectorateAir Force Research Lab937-656-7013dhoagland@al.wpafb.af.milEdward A. Martin, PhDAFRL/HECIHuman Effectiveness DirectorateAir Force Research Lab937-255-8072emartin@al.wpafb.af.milBryan E. BrettScience Application International Company4031 Colonel Glenn HighwayBeavercreek, OH 45431937-431-4390brettb@dayton.saic.comKeywords:CART, IMPRINT, human, operator, performance, Simulation-Based Analysis, SBA, HLA, HITL, requirements, cockpit, modeling, simulationABSTRACT: This paper discusses the research and demonstration activities being conducted by the Combat Automation Requirements Testbed (CART) Program within the Air Force Research Laboratory/Human Effectiveness Directorate.  CART will focus on demonstrating how human-in-the-loop (HITL) and constructive operator models and data can be integrated with Simulation-Based Acquisition (SBA) activities for the purpose of defining crew system requirements.  Utilizing the Army's Improved Performance Research Integration Tool (IMPRINT) human performance modeling environment, CART will provide High Level Architecture (HLA) interfaces that enable human performance models to interact with constructive models of systems.  A second extension will incorporate the ability to represent the goal-oriented nature of human performance.  Modelers and analysts will be able to define operator goal states and priorities that dynamically drive task networks based on changing states and events in the operator's environment.  Payoffs include improvements for the acquisition community to generate quantitative crew system requirements that support specific system and mission performance criteria.  For the warfighter, CART's technologies will result in interfaces that (1) are better matched to human capabilities and limitations, (2) are more usable and acceptable to aircrews and operators, and (3) that require much less redesign and re-development.  The paper will provide an overview of the CART and IMPRINT Programs, and discuss an approach for developing a "goal-oriented" human performance model.1.  IntroductionMilitary weapon systems are normally built to satisfy a set of requirements levied by the warfighter.  One of the primary problems facing cockpit designers during the design and development of control stations is an understanding of the effects of human performance on total weapon system effectiveness which, in turn, impact those requirements.  We know that all weapons systems are manned in some sense, yet how do we quantify the effectiveness of a pilot station in performance terms?Today, technical trades and assumptions which effect system performance are determined without adequate representation of human behavior, if at all.  Requirements for the total system are typically grounded in the warfighter's desired level of effectiveness (which include the human element) and needs to include operator performance during each step of the acquisition process – from analysis of alternatives prior to Milestone 1 up through full scale production.  Having this capability would provide credible, quantifiable parameters in which to predict the cockpit's contribution to the system in performance terms.  This paper describes some of the key elements and technical objectives of a new program in AFRL, called Combat Automation Requirements Testbed (CART), which seeks to develop and demonstrate just this needed capability.2.  Problem Definition2.1 Limitations to AcquisitionTo date, use of modeling and simulation to derive system requirements has focused on the physical components of systems (e.g., propulsion, sensors, weapons, RCS, etc.).  Sensor developers, for example, can be given "hard" numbers about the range at which targets must be acquired to provide time for weapon hand-off and launch to achieve a desired probability of kill.  Unfortunately, consideration of the cockpit as part of the system has been avoided in these requirements generation efforts, primarily due to the time and cost to acquire the necessary amount simulator assets, expertise, and facility space.  Hence, there are typically no requirements produced which, for example, tell a cockpit designer that "sensor controls and displays must enable the aircrew to detect a target and hand it to a weapon to at least Xkm or and with 90% accuracy."  These requirements can and should be quantifiably linked up to some overarching measure of effectiveness levied by the warfighter based on the mission, assets available, and type(s) of threats expected.Instead, the result is an acquisition process wherein weapon subsystems are developed apart from each other and then placed before a crew who must be specially trained for that equipment.  At Initial Operating Capability (IOC), the impact is questionable mission performance due to sub-optimum crew system design, then significant time and dollars must be expended to solve the problem.  As an alternative, acquisition programs are increasing their use of Human-In-The-Loop (HITL) simulation as a means of deriving more accurate system requirements.2.2 Limitations to Human-In-The-Loop (HITL) SimulationHITL simulation has become increasingly complex and costly, even for relatively simply projects.  Realism, infrastructure development, and connectivity with other simulation assets are factors which increase the up-front costs associated with establishing a new HITL capability robust enough to be used for future projects.  As a set of constraints, limited budget, time, and access to qualified pilot subjects inherently cause cockpit studies and trades to be restricted in terms of the amount of data collected and its applicability of results across different crew station concepts.  Fundamental issues such as different levels of vehicular automation, numbers of operators, and information transfer rate go unanswered because of the cost and complexity in setting up a complete HITL simulation program.  As a result, those conducting simulation programs are relying more heavily now than in the past on human modeling as a "poor man's" way to fold in human interactions and behavior into a program to provide added realism.2.3 Limitations to Human ModelingTo aid the acquisition, wargaming, and training community in their endeavors, modeling human behavior or performance is conceived of as a natural evolution of modeling components in simulation.  As an aside, the authors note a difference between behavior and performance.  Behavior could be considered "deep-rooted" within the operator and a combination of emotion, training level, background, personality, etc.  Performance could be considered a measurable term which can be observed and then readily modeled in the context in which an action occurred, i.e. strength, reach, vision, hearing.  While the differences may be more subtle than this simplistic breakdown, for the purposes of modeling, algorithms must be created based on what can be seen and measured, hence performance appears to be a term which lends itself to constructs of the modeling and simulation community and is used throughout this paper in this context.One of the primary inhibitors towards advancing integrated models of human performance is the lack of a single representation architecture suited to multiple users and needs.  In addition, different models today have separate architectures, underlying databases, and algorithmic approaches to increasing the realism of human representation.[1]  Like models of subsystems which contain various degrees of freedom and realism, human models must evolve to be more interoperable, either through an interface with an accepted protocol, i.e. the HLA, and/or must be more re-usable with components available to other organizations.  While the problem of using a human model outside of the context in which it was developed will continue to plague program managers for years to come, the National Research Council report on Modeling Human and Organizational Behavior:  Application to Military Simulations should be reviewed for more information.3.  The CART ProgramThe Air Force Research Laboratory is currently underway in determining the feasibility and practicality of a Combat Automation Requirements Testbed (CART) concept.  The technical scope entails a re-thinking of how crew system performance requirements should be established, recognizing the key role that humans have in developing and operating Air Force systems and equipment.  A recognizable state-of-the-art advancement is expected that will significantly improve the design integration state-of-the art, while making a significant difference to the warfighter.  A primary objective of the effort is to demonstrate a means for quantifying crew system requirements much like system and subsystem requirements are specified today. In order to be effective, the testbed must support activities, methods and procedures used to derive system requirements at present.  These include the Analysis of Alternatives, Mission Needs Statements and Operational Requirements Documents.  The product of these activities and documents are objective, quantified system, subsystem and technology requirements that are stated with reference to objective, quantified mission goals.  CART will extend the notion of quantitative requirements to the crew system and will establish the linkages from these crew system requirements to overall mission performance goals whereby the contribution of the crew system to overall mission effectiveness is well-understood, quantified, and the data underpinning that contribution can be used throughout the acquisition process.The following paragraphs provide detailed information concerning each of CART six tasks.[2]3.1 Task 1:  Crew System Requirements EstablishmentFor Task 1, the current acquisition process will be characterized for major, and less-than-major, programs, highlighting the nominal extent to which crew system requirements are analyzed and established.  Using an example drawn from a recent system acquisition program, the program will define how the availability of performance data from an Combat Automation Requirements Testbed can improve the requirements generation process and provide quantified crew system requirements.  A key baseline in this task will be assuming that SBA concepts will continue to flourish and be used to greater degrees than in past acquisition programs.  Team personnel will perform a cost versus benefit comparison to estimate the extent of improvement in the crew system requirements generation process and illustrate how these requirements would be reflected in formal system justification and requirements documents using a CART Testbed.  Overall, Task 1 will identify the "user" of CART and the environment and processes they use today with an eye towards advancing current methods.3.2 Task  2:  Human Modeling ArchitectureThe primary goal of this important step is to provide an authoritative representation of human behavior within a suitable, DoD accepted system modeling architecture.  To begin with, contractor and government teams will select a military simulation architecture and demonstrate extensions that will enable integration of a human performance models (HPMs) or model later.  Most likely, an USAF/XOC-approved legacy model or its follow-on development model (i.e. Suppressor, SWEG, or JIMM) will be used as the baseline modeling environment.[2]  A data collection capability will be provided from this selected architecture that enables computation of measures of effectiveness that predict performance at the mission and subsystem level.Concurrently, a human modeling architecture will be defined for the selected military simulation.  An HPM and an interface to the engagement level simulation will be developed that characterizes the flow of information to and from the human operator and provides the appropriate human control interactions to the constructive system model.  One of CART's technical challenges is creating a framework which allows HPM/models to pass data interactively with the constructive models and influence the results of the constructive simulation. Like the military simulation, data collection capabilities from the HPM/models will be developed that enable predictions of task performance and quantification of the expected human performance impact on system-level measures of effectiveness.  See Figure 1 for a representation of CART's initial framework.Figure 1 CART Initial Architectural Framework [3]At the conclusion of Task 2, an interface control document (ICD) for the human performance model(s) to the selected architecture will be generated as well as the process to verify and demonstrate the model adequacy for performing case studies in Task 5.3.3 Discussion of the Improved Performance Research Integration Tool (IMPRINT)IMPRINT, developed by the Human Research & Engineering Directorate of the U.S. Army Research Laboratory, is a stochastic network modeling tool designed to help assess the interaction of soldier and system performance throughout the system lifecycle -- from concept and design through field testing and system upgrades.  As a system design and acquisition tool, IMPRINT can be used to help set realistic system requirements; to identify soldier-driven constraints on system design; and to evaluate the capability of available manpower and personnel to effectively operate and maintain a system under environmental stressors.  IMPRINT is also used to target soldier performance concerns in system acquisition, to estimate soldier-centered requirements early, and to make those estimates count in the decision making process.  As a research tool, it incorporates task analysis, workload modeling, performance shaping and degradation functions and stressors, and embedded personnel characteristics data.  IMPRINT uses Micro Saint, an embedded discrete event task network modeling language, as its engine.  Task-level information is used to construct networks representing the flow and the performance time and accuracy for operational and maintenance missions.  For some analyses, workload profiles are generated so that crew-workload distribution and soldier-system task allocation can be examined.  In other cases, maintainer workload is assessed along with the resulting system availability. Also, using embedded algorithms, IMPRINT models the effects of personnel characteristics, training frequency, and environmental stressors on the overall system performance.[4]  A major enhancement to be provided under CART is the ability to interface IMPRINT with an external representation of a system using HLA's RTI.  This will permit acquisition teams to connect human performance models with the specific models used by their programs. Another important enhancement will be the addition of a goal-oriented performance component.  Current concepts of human performance recognize the goal-oriented nature of human performance and that many performance environments are characterized by multiple, competing goal states.  To be effective, human performance models must be able to represent the operation of these different goal states as they affect selection of tasks to be performed, task prioritization, workload, and task management schemes such as shedding and suspending task performance.  IMPRINT will be modified to add the ability to specify alternative goal states and rules for activating and prioritizing goal states and initiating and controlling task networks associated with those goal states.  This will enable users to represent complex, competing performance requirements that characterize modern weapon systems and capture the interaction of those requirements with the system of interest.3.4 Modeling EnvironmentThe architecture that CART will develop for integrating human performance models into engagement level simulations will be a hybrid of two approaches to human performance modeling: task network modeling and first principle modeling.  Task network modeling breaks the human performances of interest into a series of tasks that are depicted as a flow diagram.  This provides a representation of human performance that is simple and easily understood.  For each of the tasks in the network, performance is characterized in terms of critical dimensions of performance such as time, accuracy, and probability.  Thus, task network models permit the user to directly manipulate factors that will be used to derive crew system requirements.  While task network models offer simplicity of use, they are very limited at modeling specific human capabilities such as cognition and perception.  More sophisticated users may require the ability to model the underlying human performance capabilities that drive task performance.  For this reason, the task network models will be augmented by "first principle" models, which do provide high fidelity representations of human capabilities.  Essentially, tasks in the task network will make calls to the first principle models that represent the human capabilities required of the task.  The first principle model will extend CART's architecture across an HLA-compatible interface allowing greater acquisition of human performance data as required by a peculiar set of tasks, i.e. highly cognitive tasks requiring high degree of mental agility.3.5 Task 3:  Conduct Trade StudiesIn Task 3, two trade studies will be conducted to select the two most appropriate operational contexts.  Various military applications of interest will be evaluated, including but not limited too:  unmanned air vehicles,  information dominance, real-time information for precision strike, attack warning, command and control, intelligence/surveillance/reconnaissance, and mission planning.  Active acquisition programs within the different topic areas will be identified that have a significant crew system component and that are in an appropriate requirements phase, such as those planning to conduct of an Analysis of Alternatives (AoA).  Programs will be assessed in terms of their suitability for use in Task 5.  A structured evaluation process will be employed which jointly considers factors relevant to the case study objectives, including: the types of human performance to be represented in the HPM; availability of engineering level models for representing the system of interest; cost required to create the engagement level simulation; the availability of operational mission level simulators for exercising the human performances of interest; the availability of data required to generate Measures of Effectiveness/Measures of Performance (MOE/MOP) and human performance measures; and the cost and level of effort required to condition the operational mission simulator to support the case study.3.6 Task 4:  Real-Time Operational Mission Simulation Once the military application has been selected, an operational mission simulator will be prepared for mission level testing and will be modified to represent the system of interest in the study using the appropriate mission environment, threats, and other entities.  As part of this modification, the needed data collection capabilities will be added for computing the MOE/MOP/human performance measures required by the test plan.  To facilitate the HITL simulation, support materials necessary for aircrews participating as subjects will be generated to include a mission pre-brief and hardcopy mission materials (maps, frequencies, imagery, etc).3.7 Task 5:  Conduct Case StudiesBefore testing begins, an overall study plan will be developed appropriate to the selected case study and operational context.  The study plan will be generated consistent with an AoA process and include elements such as the test scenario(s), MOEs, MOPs, and human performance measures applied across engagement level constructive simulation and operational mission testing.  This approach provides comparability of results between HITL simulation and constructive simulation and to demonstrate how human performance impacts mission performance.Using Task 2 research results prior to execution of the constructive simulation, a mission decomposition of the required mission functions and tasks to be performed will be created to support the development of an extensive task network model.  This analysis will include the key elements of task performance (information and action requirements, human performance capabilities required), crew members that perform the tasks, and interfaces used in task performance.  All the activities will be accomplished necessary to prepare, conduct, and analyze the results of the constructive simulation interacting with HPM(s) developed and modified during Task 2.  Elements of this step include: conditioning and integrating the engineering level models to provide a system representation to be used in the engagement level constructive simulation; development of the HPM(s) for the tasks of interest; integration of the HPM(s) into the system simulation; performing test series in which engagement level simulations are conducted and data are collected on HPM(s), system performance, and “engagement” outcomes; and analysis of test results.  In the analysis of the results, CART will demonstrate how HPM effects influenced engagement outcomes.Following each constructive simulation, HITL mission level testing will commence using the simulation environment created during Task 4.  This testing will verify the constructive simulation predictions of human performance.  Subject runs using the operational mission simulation environment will be accomplished and data will be analyzed in terms of the MOE/MOP/human performance measures established.  The results will be used to demonstrate how crew performance influenced mission outcomes and also a correlation will be made between the HITL and constructive, engagement level simulation results using those same selected performance measures.  Crew system design integration alternatives will be identified based on the results of the case study simulations and methods for the purpose of estimating the risk and cost associated with those crew system design integration alternatives.3.8 Task 6:  Prepare Testbed DefinitionFinally, a performance specification for an Combat Automation Requirements Testbed will be developed which defines an implementable hardware architecture using state-of-the-art commercial components.  In addition, the specification will define the software which is in compliance with DoD’s High Level Architecture (HLA) and can integrate with the Run -Time Infrastructure (RTI) implementation of HLA.  As part of Task 6, a Simulation Object Model (SOM) and a Federation Object Model (FOM) that addresses data publication and subscription requirements will be created to ensure re-use and compatibility across other military simulations.  The FOM should specifically address data requirements for crew integration and quantification of human performance impacts on system level measures of effectiveness within a distributed simulation environment.  The specification will include the following design and performance elements: human performance parameters (collection and rationale); integration of specialized test equipment or instrumentation for human performance data collection; data analysis, archival and retrieval; software architecture; HLA compliance and RTI implementation; and a reconfigurable hardware architecture.3.9 Integration of TasksThe six tasks discussed above will be executed across a three year effort with the support of AFRL/HE, Science Applications, and Micro Analysis and Design.  The relationship of the six tasks is shown in Figure 2.Figure 2 Flow of CART's Six Tasks4.  SummaryAnecdotal evidence suggests a costs savings of ten to one when using modeling and simulation (M&S), and a five to one savings in labor, as compared to starting a HITL simulation study from scratch.  This data supports Simulation-Based Acquisition's call for the increased use of M&S across traditional acquisition phases. In summary, this paper has discussed the need to capitalize on this promise as well as address fundamental flaws in the way our military requirements are generated, i.e. without inclusion of the human.  In response to this deficiency, as well as to improve our representation of human behavior and integrate it into larger scale simulations, AFRL is moving ahead through execution of the CART program.  By modifying existing human architectures and simulation environments to be sensitive to the operator's interaction with his or her weapon system, CART will advance the state-of-the-art in the credibility and usability of human performance data and models for real-world military applications.5.  References[1] Pew, et al: Modeling Human and Organizational Behavior:  Application to Military Simulations, National Research Council, 1998[2] Brett, B.:  Statement of Work, Combat Automation Requirements Testbed (CART), 1998[3] Langbehn, S.: Air Force Analysis Toolkit: Legacy Model Transition Plan.  Briefing at USAF XOC Industry Day, 16 January 1998.[4] Hoagland, D.G.: CART Program Briefing to AFRL/HE, 10 December 1998[5] Army Research Laboratory, IMPRINT Web Site, See: http://www.arl.mil/ARL-Directorates/HRED/ imb/imprint/imprint.htm6.  AuthorsDAVID G. HOAGLAND received his Bachelor's of Aeronautical and Astronautical Engineering degree from the Ohio State University in 1985, where upon he immediately joined the US Air Force at Wright-Patterson AFB.  As a crew systems engineer working in the Aeronautical Systems Division, he was involved in the design, development, testing and acquisition of major weapon systems, namely the F-16 Fighting Falcon, the new 767 AWACs, the F-117 Stealth Fighter, and the E-3 JSTARs.  He moved to the Air Force Research Laboratory in 1995 as a plans and programs engineer, and quickly became the deputy program manager for the Crew-Centered Design Technology (CCDT) Advanced Technology Demonstration 6.3 program.  He has served on numerous unmanned vehicle initiatives in the Air Force, specifically as a member the 1996 Air Force SAB UAV Summer Study team; as a core team member in the DARPA/AFRL UCAV program; and as supporting member of the AFSPC Space Operations Vehicle team.  He currently serves as Program Manager for Combat Automation Requirements Testbed (CART) Program.EDWARD A. MARTIN is a biomedical engineer in the Air Force Research Laboratory's Human Effectiveness Directorate where he is currently Technical Director of the Combat Automation Requirements Testbed (CART) Program.  He earned the MS in Electrical Engineering from Syracuse University in 1971, and a PhD in Biomedical Engineering from the Ohio State University in 1985.BRYAN BRETT received a Bachelor degree in Psychology and a Masters degree in Experimental Psychology, both from the University of Florida.  He currently manages the Crew Systems and Simulation Business Area of SAIC's Aeronautical Systems Division.  He has experience as a senior analyst, principal investigator, and currently serves as Chief Scientist for the CART Program.