Latency Compensation in Distributed Network Centric Warfare RDT&EDr. Larry McKeeScience Applications International Corporation6200 Uptown Blvd NEAlbuquerque, NM 87110 505-830-7442 HYPERLINK mailto:Mckeel@saic.com mckeel@saic.comKeywords:HLA, distributed testing, latency, RDT&EABSTRACT: Defense Network (DNet) is a Naval Air Systems Command (NAVAIR) project to establish an RDT&E network of selected facilities for RDT&E applications and support of Network Centric Warfare (NCW) across NAVAIR.  The initial implementation integrates nine laboratories/ranges with both HLA interfaces and an integrated series of tactical communications links using a network of T&E open air ranges and simulation resources.  The first application of DNet was a distributed test in which the Multi-functional Information Distribution System (MIDS) was the system under test (SUT).The DNet Test Event evaluated the utility of using DNet to perform NCW RDT&E and involved a simulation of a counter-air/strike mission in which F/A-18 platforms used MIDS to communicate with other friendly aircraft via Link 16.  In order for all the distributed players to have a coherent and consistent picture of the battlefield scenario, it was necessary to compensate for latency effects.  This paper describes the latency compensation technique used, technical challenges in implementing the technique, and the technique’s overall effectiveness.1. OverviewThe Defense Network (DNet) is a Naval Air Systems Command (NAVAIR) project to establish a research, development, test, and evaluation (RDT&E) network of selected facilities for RDT&E applications and support of Network Centric Warfare (NCW) across  NAVAIR.  The initial implementation integrates nine laboratories/ranges with both High Level Architecture (HLA) interfaces and an integrated series of tactical communications links using a network of test and evaluation (T&E) open-air ranges (OARs) and simulation resources.  The initial nine laboratories/ranges constitute the collaborative environment composing the DNet Network, as shown in Figure 1.  The first application of the DNet Network was the DNet Test Event completed in June 2001.The DNet Test Event provided a checkout and validation of portions of the DNet infrastruture.  Interfacing the infrastructure to provide a testbed for the Multi-functional Information Distribution System (MIDS) system under test (SUT) also provided direct support of NCW concepts by exploring the capabilities of the F/A-18 aircraft to link to other aircraft and ship systems.  The existing infrastructure (nine NAVAIR laboratories) with the added capabilities of the DNet Network was used to evaluate the benefits and utility of distributed RDT&E and its application to NCW concepts.The DNet Test Event involved a simulation of a counter-air/strike mission in which F/A-18 platforms use the MIDS.  Distributed testing techniques were used to link various simulations representing air and surface platforms to each other and to weapons simulations.  Execution control messages, entity state data, Identification Friend or Foe (IFF) messages, and weapons interactions were exchanged via the RTI (1.3NG-v3.2).  Tactical links (i.e., not using the RTI) were used for tactical message traffic, including Link 16 and Link 11.  These techniques allowed a mission-level evaluation of the MIDS SUT to be performed.The federation used for the DNet Test Event consisted of 10 simulation/range federates, a test control federate, a logger federate, and up to 22 viewer federates.  The usual federation size was about 30 federates. 2. DNet Test Event DescriptionThe battlespace simulated in the DNet Test Event was divided into two areas by a Forward Edge of the Battle Area (FEBA).  The blue force was on the west side of the FEBA and the red force on the east side.  This scenario was a day 20 engagement, where all air defense had been destroyed but there were still some red air forces operational. Geographically, the scenario was centered near the China Lake, California area. When the mission began, a P-3C was in orbit about 20 miles west of the FEBA, and an E-2C was in orbit southwest of the FEBA.  Two two-ship F/A-18C/D flights and an F-14 were also in orbit west of the FEBA awaiting orders.  Three civilian commercial aircraft were flying north and south, crossing the northern edge of the FEBA.  An AEGIS cruiser was located off shore.  The hostile forces had two flights of aircraft waiting for engagement assignments from their commanders.  One two-ship Mig-29 and one two-ship SU-27 interceptor flight were in orbit east of the FEBA.  The IFF systems operated as designed among all air targets, hostile and/or friendly.  The beginning of the mission is illustrated in Figure 2.  The starting positions of the entities are marked with crosses.  The scenario also included three Red reporting posts (RPs) which reported to a Red command post.The DNet Test Event scenario was broken up into three stages: Link-16 Orbit Checks – the aircraft maintained the orbits shown in Figure 2, and all Link-16 participants (E-2C, AEGIS, F/A-18s, F-14D) exchanged Link-16 messages, including surveillance checks, fighter-to-fighter checks, and AIC control.  The purpose was to exercise as many types of Link-16 messages as possible for the MIDS SUT.Counter-Air Mission – the Strike F/A-18 flight proceeded toward the enemy command post (ground target) and was challenged by the Migs.  The BARCAP F/A-18 flight left orbit to counter the Migs and defend the Strike F/A-18 flight.  This part of the scenario ended with the lead BARCAP F/A-18 engaging one of the Migs with an AIM-9 missile.Strike Mission – the F-14D left orbit for reconnaissance of the enemy command post.  Command post coordinates were transmitted from the F-14D via Link-16 to the E-2C and AEGIS and relayed by the AEGIS to the orbiting P-3C via Link 11.  The F-14D also sent a target image to the P-3C.  The AEGIS issued an attack order to the P-3C via Link 11, and the P-3C left orbit and proceeded to the designated waypoint for launching a Standoff Land Attack Missile – Expanded Response (SLAM-ER) missile.  At the waypoint, the SLAM-ER missile was launched, and it attacked the command post (ground target).  The F-14D then returned to the ground target for battle damage assessment.For the DNet Test Event, the scenario entities were represented as indicated in Table 1.Figure 2.  Initial ScenarioTable 1.  DNet Test Event Entity RepresentationsEntityRepresentationLocationP-3CMITL, HWIL simulatorP-3C AIP Lab, Pax RiverE-2CMITL, HWIL simulatorESTEL, Pax RiverStrike F/A-18C/D FlightFirst AircraftSecond AircraftMITL, HWIL simulatorConstructive (JIMM)AWL, China LakeACETEF, Pax RiverBARCAP F/A-18C/D FlightFirst aircraftSecond aircraftMITL, HWIL simulatorConstructive (JIMM)AWL, China LakeACETEF, Pax RiverF-14DMITL, HWIL simulatorWSIC, Pt. MuguCivilian Airline TrafficFirst AirlineSecond AirlineThird AirlineLive aircraftLive aircraftLive aircraftLand Range, China LakeSea Range, Pt. MuguATR, Pax RiverMig-29 FlightConstructive (JIMM)ACETEF, Pax RiverSU-27 FlightConstructive (JIMM)ACETEF, Pax RiverAEGIS CruiserConstructiveSea Range, Pt. MuguRed Ground TargetConstructive (JIMM)ACETEF, Pax RiverAIM-9HWIL simulationIBAR, China LakeSLAM-ERDigital simulationIBAR, China LakeMITL = man-in-the-loop					HWIL = hardware-in-the-loopJIMM = Joint Interim Mission Model			ESTEL = E-2C Systems Test & Evaluation LaboratoryThe DNet Test Event distributed architecture is shown in Figure 3.  There were ten simulation federates (AWL federate actually consisted of two federates – one for Shooter (BARCAP) F/A-18, one for Strike F/A-18).  The Test Control Center (TCC) consisted of four federates: Test Control Federate (TCF), HLA logger (hlaResults), 2-D viewer, and 3-D viewer.  Each simulation facility could also operate the 2-D and 3-D viewers as separate federates, so that up to 34 federates were possible.  The RTI was used to exchange formatted HLA messages for all non-tactical data exchanges (e.g., entity state data, federate health status, IFF messages, missile fire, missile detonation, execution control).  Tactical data were exchanged using dedicated links, not the RTI, and consisted of Link-11 messages, Link-16 messages, avionics (MIL-STD-1553) data between the Shooter F/A-18 and the AIM-9 HWIL labs, and SLAM-ER seeker video.  Target images from the F-14 were sent as JPEG images across the network.Figure 3.  DNet Test Event Distributed Architecture3. DNet Test Event ResultsIn preparation for the DNet Test Event, extensive integration testing of the DNet federation was performed from October 2000 through May 2001.  After completing integration, the DNet Test Event took place during the period 19-21 June 2001.  During this period, the complete scenario was executed in segments three times.  Findings from the testing are as follows:The DNet infrastructure provided a valid test bed for the MIDS SUT.  Platform entity state data exchanged in the test were valid truth data, and Link-16 terminals at the blue platform facilities (F/A-18, F-14, E-2C, AEGIS) permitted valid exchanges of tactical Link-16 messages.  Specifically, a MIDS SUT expert reported: Procedures were valid for MIDS/Link-16 testing.Data transfers were error-free.The F/A-18 responded correctly to received Link-16 J-messages.Link-16 messages were valid and representative of actual lab and flight test data.The DNet infrastructure, including the network, was reliable.  Each scenario segment took up to two hours or so, and the federation was able to function correctly for this relatively long period six times.HLA message losses over the DNet network were negligible.  All interactions used reliable transport, and there were no losses of these.  There were occasional losses of best effort messages (entity state and federate health check), but these were usually less than 0.3% of the total messages published.  Use of dead reckoning by subscribing federates ensured that entity state dropouts had an insignificant effect on entity behavior.HLA message traffic resulted in negligible network bandwidth utilization.  Even though 18 entities were simulated, implementation of a dead reckoning errorpublication rule kept entity state message publication rates below 1 Hz overall.  Typical peak HLA message data rates were less than 50 kbps total.  In contrast, test communications data rates were about 400 kbps, and video required about 4-8 Mbps.Test control and monitoring techniques were very effective.  Federate health check messages were exchanged between all federates at a 1-Hz rate, and these were monitored in real time at the Test Control Center (TCC) in China Lake.  The TCC also had a 2-D display which was used for real-time scenario adherence verification.  The network status was also monitored in real time during the test.4. Latency CharacteristicsLatency values for the HLA messages varied significantly depending on the message type, the publishing federate, and the subscribing federate.  Examples are in Tables 2 and 3.  Table 2.  Average Health Message Latency (ms) for Run #2 on 11 June 2001Publishing FederateACETEFATRAWLIAWLVE-2CF-14IBARLRP-3CSRSubscribing FederateACETEF0.954.698.6100.651.575.099.876.954.1154.4ATR50.60.1119.2121.579.198.8121.2101.073.7179.2AWLI113.6141.90.533.0134.771.992.369.6138.4152.4AWLV116.1144.528.50.5137.474.294.772.0140.7154.6E-2C93.9121.9162.6165.70.1142.4162.9145.9119.0222.6F-1479.5105.660.962.8100.9-2.762.138.2105.9105.3IBAR147.0178.0125.0127.0174.0111.01.0110.0182.0205.0LR78.6107.453.755.499.237.458.40.1104.1118.2P-3C108.1136.9177.7179.7127.8155.5177.2159.50.6236.0SR294.2321.2273.4272.1316.0244.1260.6249.7288.40.1AWLI = Shooter (BARCAP) F/A-18 federate		AWLV = Strike F/A-18 federateTable 3.  Average Entity State Message Latency (ms) for Run #2 on 21 June 2001EntityLead    Mig-29ATR Civ AirShooter F/A-18Strike  F/A-18E-2CF-14AIM-9LR Civ AirP-3CSR Civ AirSubscribing FederateACETEF103.0277.5889.81621.852.072.980.21314.0231.8968.2ATR151.1223.1914.91644.876.098.4105.21335.7258.4992.8AWLI215.9364.4762.71511.6138.072.585.11307.6315.9964.5AWLV216.9367.0790.41479.4142.075.075.01310.6320.2966.2E-2C197.4345.1957.31691.010.0142.2--1383.2304.11036.4F-14188.9328.8849.71584.1102.0-4.445.51277.3283.2918.4IBAR285.0--946.01801.0----1.0--435.0--LR186.8330.5849.71581.7105.037.741.91237.0282.7931.4P-3C----972.41703.9133.0158.0----180.4--SR411.5546.01063.61796.9320.0238.7275.01495.0503.8785.7Table 2 gives latency values for the federate health check messages.  These were published by each federate, and the values reflect the combination of network transmission and RTI/subscribing federate processing latencies.  The values along the diagonal are for health messages logged at the publishing federate and are small (typically <1 msec).Table 3 gives latency values for entity state messages.  The timestamp for these is for a “time of validity,” the time when the hosted simulation actually generated the data.  Note that values vary over a wider range (<50 msec to nearly 2 seconds) than for the federate health check messages, reflecting the different manners in which the entity simulations generate entity state data (this is best illustrated along the diagonal; these are values for logging at the publishing federate and do not include network transmission and RTI/subscribing federate processing latencies).  Comparing Tables 2 and 3 also shows that the entity state message latency is higher than that for the federate health check, due to simulation processing before passing the data to the federate. Significant message-to-message latency variations were noted for the entity state messages at the subscribing federates.  This is illustrated in Figure 4.  This figure gives the latency for the lead Mig-29 logged at both the publishing federate (ACETEF) and one of the subscribing federates (IBAR).  Note that the latency at IBAR has much larger variations (standard deviation = 167.6 msec) than at ACETEF (standard deviation = 18.8 msec). 5. Latency CompensationLatency compensation of entity state data in real time was deemed to be necessary due to the large average latency values and the large sample-to-sample variations.  This was done by dead reckoning entity state values to current time at the subscribing federate.  All entity state HLA messages included a time-of-validity timestamp in the message header based on GPS time.  When a subscribing federate input the data into its simulation, the entity state values were dead reckoned to current GPS time using first order dead reckoning (based on velocity only; no dead reckoning of attitude).  The equations used were:XS = XU + VXU (t	VXS = VXU	(S = (UYS = YU + VYU (t	VYS = VYU	(S = (UZS = ZU + VZU (t	VZS = VZU	(S = (U WhereXS, YS, ZS, VXS, VYS, VZS, (S, (S, (S are values input into the simulation XU, YU, ZU, VXU, VYU, VZU, (U, (U, (U are from the last HLA entity state update(t is the difference between the absolute time stamp for the simulation frame and the time-of-validity timestamp for the last HLA entity state updateNote that acceleration is not accounted for in the dead reckoning equations, so that when acceleration is low (<0.1 g), latency compensation is very good.  This is illustrated in Figure 5.  The first curve shows the Mig-29 X-position versus time of validity.  Because acceleration is low, this curve is a straight line.  The second curve shows the X-position versus time of receipt at the IBAR logger (i.e., IBAR log time).  The latency at the logger varied from 194 to 295 msec during this time, so that the second curve is essentially the same as the first, but translated in time by about 250 msec (the sample-to-sample latency variations distort the second curve’s shape relative to the first).  The third curve shows implementation of the first dead reckoning equation for Xs.  Note that it nicely overlays the first curve, indicating good latency compensation.  Another way to look at the third curve is that it represents a projection of where the Mig-29 actually is at the time of receipt of each entity state message, rather than where the message says it is (at the earlier time of validity).  Without latency compensation, the error in position (vertical distance between first and second curves) would range from 43 to 65 m, and the time history of position would be distorted.Figure 5 also illustrates the low entity state update rates when acceleration was low.  Updates were published whenever the dead reckoning position error exceeded 5 m in any coordinate or the orientation error exceeded 5 degrees in any angle (as determined by the publishing federate from comparing the dead reckoning equations to truth data).  In this case, the low acceleration resulted in more than 4 seconds between updates.Figure 6 shows latency compensation when acceleration is relatively high (~2 g).  Because acceleration is high, the curve of Mig-29 X-position versus time of validity is parabolic in shape.  The curve of X-position versus IBAR log time has more distortion than in Figure 5 because of larger variations in latency (205 to 507 msec).  The curve for the dead reckoned position again shows good latency compensation, even though the acceleration was relatively high.  This is because the velocity values were relatively high, and the latency values were relatively low.  In this case, the correction for velocity (VXU (t) varied from 20 to 98 m, while a correction for acceleration (½ AXU (t2) would have only been about 0.5 to 2.9 m, less than 10% of the velocity correction. Figure 6 also illustrates the higher entity state update rates when acceleration was relatively high.  In this case, the higher acceleration resulted in about 600 msec between updates.Figure 7 shows latency compensation when both acceleration and latency values are relatively high.  In this example, the shooter F/A-18 is turning at a constant rate so that the value of AX  stays fairly constant, but VX  continually decreases and passes through zero.  The correction due to velocity starts at nearly 300 m and decreases to zero, while the correction due to acceleration stay about constant at 15 to 20 m.  The fourth curve in Figure 7 shows second order dead reckoning which includes acceleration effects according to:XS = XU + VXU (t + ½ AXU (t2	  Since acceleration is approximately constant, second order dead reckoning results in good latency compensation in this case.6. Other Latency EffectsDead reckoning is effective for compensating latency effects in entity state data due to the ability to predict location using velocity.  However, the latency of discrete events, such as missile launch and detonation, cannot be compensated in real time.  As an example, the P-3C had a velocity of 144 m/s at the time it issued the SLAM-ER Fire message.  The typical latency from the P-3C federate to the IBAR federate (host for SLAM-ER simulation) was about 100 msec.  This meant that the P-3C had moved about 14.4 m before the SLAM-ER actually launched.  In other words, the SLAM-ER was not launched at precisely the intended launch point.Latency of discrete events made it necessary for the IBAR to compute miss distance for the AIM-9 engagement, rather than ACETEF (the target federate).7. ConclusionUse of first order dead reckoning based on time of validity provided good latency compensation for entity state data by subscribing federates during the DNet Test Event.  This technique was also useful for smoothing out distortion of the entity state data at the subscribing federates due to message-to-message latency variations.  When latency values exceed one second, second order dead reckoning (which also uses entity acceleration) is needed to ensure small (< 5m) position errors.  The use of dead reckoning based on absolute timestamps also helps to ensure all federates observe a coherent and consistent scenario, even when there are significant differences in latency values for the various entities.Dead reckoning cannot compensate for latency effects from discrete events.  Discrepancies between federates can only be minimized by minimizing latency.   Author BiographyDR. LARRY MCKEE has 30 years experience directing and performing RDT&E programs in distributed testing, nuclear weapon effects, system survivability, neutral particle beam interactive discrimination, and high energy laser effects.  This experience includes 20 years as an Air Force officer with duties in management of advanced R&D programs in directed energy weapon technology, R&D leadership at the Air Force Branch and Division levels, development and instruction of advanced graduate courses, and technical direction of underground nuclear tests.  He joined SAIC in 1989 and currently supports the DNet program as Lead Analyst.Figure 4.  Lead Mig Latency (Run #2 on 11 June 2001)Figure 5.  Lead Mig X-Position Versus Time (Run #2 on 11 June 2001) – Low Acceleration, Low Latency Figure 6.  Lead Mig X-Position Versus Time (Run #2 on 11 June 2001) – High Acceleration, Low Latency Figure 7.  Shooter F/A-18 X-Position Versus Time (Run #3 on 8 Feb 2001) – High Acceleration, High Latency MIDS is a low volume Link-16 terminal designed for certain fighters including the F/A-18.