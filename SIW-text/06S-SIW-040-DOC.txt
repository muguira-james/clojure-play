Introducing the Federation Engineering Lessons Learned ExchangeDouglas FlournoyThe MITRE Corporation202 Burlington RoadBedford, MA  01730781-271-2774 HYPERLINK "mailto:rflourno@mitre.org" rflourno@mitre.orgThomas J. Pawlowski III, PhD The MITRE Corporation401 DelawareSuite 200Leavenworth, KS  66048913-946-1904 HYPERLINK "mailto:pawlowst@mitre.org" pawlowst@mitre.orgKeywords:Lessons Learned, Federation Engineering, HLA PerformanceABSTRACT:  Distributed simulation systems are being deployed successfully in growing numbers for a variety of purposes including testing, training, analysis, experimentation, and decision support.  Each successful federation faces a unique set of complex challenges spanning the early stages of federation design through development, integration testing, execution, and post-execution analysis.  Much can be learned from techniques already employed to address difficult needs in problem areas including federation performance, network configuration, object and thread modeling, runtime monitoring and control, and data collection.  Collectively these lessons learned represent an invaluable body of knowledge for assisting current and future federation efforts.  In order to capture these lessons and make them more accessible to federation engineers, the Defense Modeling and Simulation Office (DMSO) is providing the Federation Engineering Lessons Learned Exchange.  The Exchange is a website built using Sharepoint® Community of Practice software.  Once sufficient lessons have been captured, the Exchange is envisioned as an enabling mechanism for generating Best Practices guidance in particularly challenging focus areas such as federation performance.  This paper provides an overview of the site, including guidelines for (1) posting experiences to the site and (2) searching the site for lessons learned that relate to specific challenges.  This site is a growing knowledge base intended to provide cost- and time-saving guidance for distributed simulation engineers.  IntroductionOver the past decade federations of simulations have been used with increasing success to meet testing, training, analysis, and experimentation requirements.  But despite the emergence of middleware standards and federation development procedures, the practice of designing and implementing these federations remains complex and is fraught with hidden technical “gotchas.”  In order to collect and make available the hard-won knowledge gained by the simulation community, the Defense Modeling and Simulation Office (DMSO) is developing and hosting the Federation Engineering Lessons Learned Exchange, hereafter referred to simply as the Exchange.  In this paper we provide an introduction to the Exchange.  First, we will review the motivation behind the Exchange by discussing its conceptual origins in Section 2.  Then in Section 3 we will discuss challenges inherent to such a web-based undertaking.  Next in Section 4 we will review how the website has been designed, and in Section 5 how the content has been shaped, to address these challenges.  In Section 6 we present a performance-based thrust to employ the website to facilitate development of Recommended Practices guidance.  Finally, we solicit participation by providing instructions for gaining access, submitting lessons, and searching the lessons on the Exchange.OriginsIt has been roughly 10 years since the introduction of the High Level Architecture (HLA) for exchanging data between DoD simulations.  Much progress has been made since that time advancing the art and science of distributed simulation engineering—however, there remains a host of technical challenges at a level of detail and complexity that cannot be adequately captured in documentation such as Users’ Guides or middleware specifications.  Each time a new set of applications and tools are lashed together, new data paths and mechanisms reveal their own unique set of issues to overcome.  However, when distributed simulation professionals come together to share such experiences, common patterns emerge across federations.  One such meeting was the Federation Performance Workshop that occurred at the John Hopkins University Applied Physics Laboratory (JHU APL) in February, 2005.  After hearing two days of performance-related presentations, representatives from across the community identified “generation of Best Practices guidance for performance-engineering federations” as one of the top actions that would have the most impact on future performance engineering efforts.  This suggests that if such knowledge could be captured and offered across the community, much time, money, and frustration could be saved in developing, testing, and executing simulation federations.  Shortly after this workshop, DMSO assigned personnel to begin engineering a web-based mechanism for capturing and offering this valuable knowledge base.  ChallengesIn setting out to build a successful lessons learned website for the distributed simulation community, we faced the following challenges:Design challenges.  How should the website be configured with appropriate display features and underlying mechanisms to (1) motivate engineers to come to the site and post their knowledge, and (2) provide easy-to-use links and search capabilities so that engineers find the knowledge they need quickly?  How should access to the site be controlled?  What software structure should be used to minimize the effort necessary to perform updates to the site?  What metrics will indicate whether or not the site is successful?Content issues.  What metadata should be associated with the lessons as they are entered, in order to facilitate appropriate categorization of the lessons for efficient retrieval?  What lessons should be collected and entered into the site to form an initial “critical mass” of knowledge for introducing the site to the community?  When, if ever, should “obsolete” data be removed from the site?  Once the site is put into operation, how will additional lesson entries be screened for biased or sensitive content before posting?DesignEarly in our efforts to design our lessons learned website, we looked to other organizations with lessons learned collection experience to see if we could leverage their experiences and solutions in our design.  We found several active lessons learned website efforts, but one organization stood out in the maturity of their experience and completeness of their documentation:  the Society for Effective Lessons Learned Sharing (SELLS).   SELLS has been associated with a major lessons learned website supporting the Department of Energy (DOE) community for seven-plus years.  Along the way they developed a DOE Standard for capturing, posting, and searching the lessons on their website. [1] Their website employs a Microsoft FrontPage™-developed user front-end connected to a Microsoft Access™ database.  The website contains over a thousand lesson entries (1200 lessons as of mid-2002) and is maintained by a team of 2-3 engineers.  Many of their lessons are motivated by safety concerns in addition to cost-savings and technical solutions.Six years of DOE lessons learned website experiences are captured in the SELLS white paper, “Building a Better Lessons Learned Program.”[2] Here are a few of the recommendations from this report that we considered in the design of our Federation Engineering Lessons Learned site:In order to facilitate rapid retrieval of lessons relevant to a user’s problem, a specific category search mechanism is needed in addition to a general word search capability.  However, determining the best breakdown of categories is challenging, and these categories should be allowed to evolve over time as the content of the lessons in the system evolves.Over time, the materials collected on a lessons learned website provide a solid basis for Best Practices or “roll-up reports” in focused areas of special interest to the community.  “Push” mechanisms should be employed to more quickly distribute posted lesson information to others in the community who may need it.  For example, the DOE Lessons Learned Website allows users to subscribe to topic areas of interest; if lessons are posted to the site in those topic areas, subscribing users will receive immediate notification of this new material via email.The overall impact of a lessons learned website is difficult to measure.  Because of this, the DOE team experienced difficulty maintaining their funding level from year to year.  Eventually the DOE team was able to cultivate a sense of positive impact within the community by soliciting and posting success stories from their users.Choosing a software basis.  Though the DOE website has been successful employing a traditional development approach using FrontPage™ and Access™, we decided to explore more recent advances in collaborative and social computing technologies.  We considered the popular wiki approach but concluded that, considering the SELLS experience, we would require a more structured basis that would give us more control over our lesson content and lesson approval and posting processes.  So we explored “Community of Practice” website development products, and found several mature packages available.  We were attracted by the ready-made features these packages provide for data entry/retrieval and user authentication procedures.  This allowed us to develop an initial operating website quickly without having to do any database development work or custom web screen generation.  For our initial implementation we chose the Sharepoint™ Community of Practice software package.Engineering the lesson submission process.  A key to the success of the SELLS website is their lesson entry and approval process.  They found that a well-defined process encourages lesson input from members of the community while also providing some important checks and balances for managing posted content.Figure 1.  Lesson Learned Submission ProcessUsing the SELLS lesson submission process as a guide, we engineered the process illustrated above in Figure 1.  The process shows the steps that must occur from the time a federation engineering “lesson” is experienced until the knowledge is posted to the website.  Along the way, the process defines the interactions between lesson submitters, the website software system, and lesson reviewers.Once a potential lesson is observed, the lesson submitter logs in to the website system and completes the lesson entry template (see additional details on the design of this template in Section 5).  Submission of this template triggers the system to send an email to website Review Panel members notifying them that a new lesson is ready for review.  The reviewer checks that the lessons falls within a few general approval guidelines (also discussed below in Section 5).  At that point the lesson is either approved and posted to the website or returned to the submitter with comments.ContentLesson Entry Template.  The SELLS website experience identified that the information or “metadata” requested when a lesson is entered into the website system is critical to the organization of the lessons for later retrieval by interested parties.  This metadata provides the basis for classifying and cataloguing the data to support search mechanisms that can be much more effective than a basic word search capability. The lesson entry template employed on the Exchange is illustrated below in Figure 2.  Most of the template need not be filled in unless the fields apply to the lesson being submitted.  Only the following categories of information are required:Submitter’s Name and EmailLesson Title, Statement and DiscussionHowever, additional fields are provided that, if applicable, will help users of the site to quickly identify knowledge that applies to their particular challenges.  These optional data fields include:Categorization by steps in the FEDEP processCategorization by RTI service groupCommon types of performance issuesData Collection & Analysis knowledge categoriesFederation specifics (name, sponsor, use case area, time management strategy employed, middleware version, etc.)At the bottom of the template are mechanisms for attaching supplementary documentation or linking to related information available on other websites.Figure 2.  Lesson Entry TemplateLesson Approval Guidelines.  Before a lesson is approved for posting, it is checked for adherence to the following general guidelines:The knowledge should be gained from actual federation development or execution experience, and not merely a restating of material already provided to the community in documentation such as IEEE specifications or User/Programmer Manuals.The lesson should not be overtly oriented toward the marketing of a particular product.The lesson should be generally constructive in nature, citing solutions to problems, where possible, rather than focusing on negative aspects of a particular software application or tool.In addition to these general guidelines, lesson statements and attached documentation must also be approved for unlimited public distribution before posting to the Exchange.Preliminary Sources Identified.  To provide an introductory set of lessons on the Exchange, the website development team is entering a number of lessons identified by the community.  The sources of these first posted lessons include:A variety of previously published conference and/or journal papers documenting the experiences of specific federationsPresentations from the HLA Performance Workshop held at the Johns Hopkins University (JHU) Applied Physics Laboratory (APL) in February of 2005RTI “tuning” recommendations for specific RTIs that are commonly used across the distributed simulation communityDMSO’s report on transitioning the HLA to the IEEE 1516 specification, which contains the experiences of several federation transition cases across the community.Toward Best PracticesIn addition to providing a wealth of specific experiences that can help users solve particular technical problems on a case-by-case basis, the SELLS has found that websites like the Exchange can provide a solid basis of community-wide material for Best Practices or “roll-up” reports focused on areas of high interest to the community.  One such area of interest in the distributed simulation community is federation performance.  Although the community has made great strides in federation performance over recent years, the complexities associated with understanding and troubleshooting performance issues continue to challenge federation engineers.  We must ask what experience-based knowledge that has been gained by federation teams over the years has not been collected and made available to the community-at-large.Supporting the HLA Performance Recommended Practices (HPRP) Study Group (SG).  In order to address these ongoing performance challenges and work toward documenting recommendations, the community has formed the HPRP SG.  The Exchange will be one mechanism used by the HPRP SG to collect the performance-related knowledge that exists within the community.  The HPRP SG can use this body of knowledge to begin to form recommended practices documentation in areas of performance engineering that have been addressed by multiple federations across the community.Additional areas for potential recommended practices “roll-up” reports.  In addition to federation performance, there are many other distributed simulation community-wide challenge areas that the Exchange could help address through knowledge collection and distillation.  Some examples are:RTI transition.  Transitioning federates and federations from one RTI version to another is a common occurrence in today’s simulation engineering environment.  Some transitions are more challenging than others.  Collecting and documenting the growing body of RTI transition knowledge could save significant time and money for future transition efforts.Mixed middleware federations.  More and more federations involve not only HLA connections between simulations, but also DIS and TENA connections as well.  The “art & science” of executing these connections effectively represents another growing area of knowledge to be documented and made available to the community.Object Modeling.  Along with the flexibility and freedom of HLA object modeling come the interoperability challenges of reconciling multiple, nonstandard data interfaces.  Over the years, many lessons have been learned regarding techniques for managing the open-ended nature of HLA object modeling, including employing reusable Base Object Models (BOMs).  Collection of these and other object modeling experiences would be invaluable to future HLA efforts. Data Collection and After-Action Review.  Collecting and documenting the results of federation executions is another complex area that does not lend itself to explicit coverage in specifications and manuals.  The Exchange could help uncover this body of knowledge by tapping the experiences of simulation professionals who have learned the “tricks of the trade” regarding extracting the most out of saved data.Join and Participate!To register to participate in the Exchange, follow this 2-step process:Step 1: Request a new account. Go to the URL:  HYPERLINK "https://partners.mitre.org/accountsetup/new/default.html" \o "https://partners.mitre.org/accountsetup/new/default.html" https://partners.mitre.org/accountsetup/new/default.html. Once there, you’ll be required to enter information. After you’ve submitted this information, your account will need to be verified by email using the security answer. Enter your email address. Set up a password. Enter a security question and answer (such as your mother’s maiden name). Fill in the name of the extranet community, HLA Lessons Learned, and the email for the owner,  HYPERLINK "mailto:pawlowst@mitre.org" \o "mailto:pawlowst@mitre.org" pawlowst@mitre.org.Step 2: Validate your email address with your security answer.  You will receive an email with a link to verify your email address. You will be asked to provide the answer to your security question. Once you have successfully done this, the registration process is complete. At this point, your account will be processed and ready for you to use in a few days.  Once registered, the site can be accessed by entering your username and password as prompted at the URL  HYPERLINK "https://partners.mitre.org/sites/hla_ll" https://partners.mitre.org/sites/hla_ll.  All the instructions and guidelines necessary to enter and retrieve lessons from the Exchange are displayed on the homepage.As more users register and participate in the Exchange, the body of knowledge will expand rapidly.  As recommended by the SELLS community, an “alerts” capability is provided on the Exchange to help users keep abreast of these developments.  Once you enable alerts for your account, you will receive immediate notification whenever lessons are added to the Exchange.  The ability of the Lessons Learned Exchange to help federation engineers will be determined by how many distributed simulation professionals contribute to and solicit knowledge from the website.  Help us to build this site into a hub of time- and money-saving engineering guidance by registering and participating online!References[1]	Society for Effective Lessons Learned Sharing (SELLS) Website,  HYPERLINK "http://tis.eh.doe.gov/ll/" http://tis.eh.doe.gov/ll/. [2]	Charles F. Miller, William F. Steinke.  Building a Better Lessons Learned Program, Idaho National Engineering and Environmental Laboratory, INEEL/EXT-02-00426, April 2002.Author BiographiesDOUGLAS FLOURNOY is a Principal Simulation and Modeling Engineer within the Center for Acquisition and Systems Analysis at the MITRE Corporation in Bedford, Massachusetts.  Doug’s simulation interoperability experiences include a variety of assignments involving DIS, HLA, and TENA-based solutions.  He also has experience in human behavior and process modeling, simulation-to-C4I system interfacing, and user interface prototyping.  Doug holds a Bachelor of Science Degree in Mechanical Engineering from the Pennsylvania State University and a Master of Science Degree in Operations Research from the George Washington University.THOMAS J. PAWLOWSKI III, is a Principal Operations Research Analyst for the MITRE Corporation. He graduated from the U.S. Military Academy at West Point in 1972 and was commissioned as a Second Lieutenant in the Field Artillery. His military career covers 26 years with assignments in numerous locations including Hawaii, Germany, Colorado, and Saudi Arabia. His final assignments were at Fort Leavenworth, Kansas, where he retired as a Colonel. His military education includes diplomas from the Air Command and Staff College, the Army’s Command and General Staff College, the Air War College, and the Army War College. Dr. Pawlowski also has a Masters degree in Operations Research from the Naval Postgraduate School and a PhD in Industrial Engineering from Georgia Tech. His work has focused on several areas including command and control systems, modeling & simulations, tools to support military training, and executable architectures. Dr. Pawlowski has worked for MITRE since 1998 at MITRE’s Fort Leavenworth site.