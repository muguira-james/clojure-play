Establishing Simulation Standards for Military Vehicle ComponentsSteve RoweGlenn BeachKevin TangBenjamin MichaelRyan O’GradyCybernet Systems Corp.727 Airport Blvd.Ann Arbor, MI  48108srowe@cybernet.com, gbeach@cybernet.comRakesh PatelEric JochumU.S. Army TACOMElectronics Architecture TeamAMSTA-TR-R/MS 264Warren, MI 48397-5000Rakesh.patel@us.army.mil, eric.jochum@us.army.milKeywords: Component, Vetronics, Standard, VSIL, SimulationABSTRACT: The process of creating a new tactical vehicle for the United States Armed Forces is a complex, expensive task.  Even relatively modest modifications to existing vehicles, such as adding armor to a personnel carrier, must be carefully tested.  Because developing a prototype vehicle is prohibitively expensive, the Army has relied increasingly on simulation to verify designs before committing them to a physical prototype.  The current lack of a standard set of simulation tools has led to the proliferation of vehicle component simulations that are not interoperable.In recent years, the Army has commissioned the development of the Virtual System Integration Laboratory (VSIL), which is to provide a set of standards for TACOM’s simulation capabilities.  These standards consist of an encoding standard for simulation components, interface specifications for controlling and receiving data from a simulation, and interface specifications for how models are configuration controlled and deployed.  At the heart of the VSIL system is a library of standard vehicle components called the Reference Architecture.  The Reference Architecture will allow systems engineers in different program offices to design vehicles using known, well-tested components that are all interoperable.  This will lead to a savings of effort, cost, and development time.This paper discusses some of the key challenges faced by the team that is developing the Reference Architecture and VSIL implementation for TACOM.VSIL SummaryA new system implemented in combat vehicles, goes through several phases, namely design, proof of concept, through simulation where possible, validation of the design, and finally the implementation.  This is true in non-combat systems as well.  During the proof of concept and validation phases, sometimes a hardware prototype is built and assembled in a real lab.  This can be costly, because the hardware has to be purchased and external suppliers or in-house personnel have to be engaged, depending on the situation.  Furthermore, whether the prototyping experiment is successful or not, the prototyping cost cannot be recovered.  In addition, it can be difficult and expensive to make design changes in a hardware prototype to compare alternative designs or to rectify any design flaws discovered later.To address the above issues, the Virtual System Integration Laboratory (VSIL) was developed.  The VSIL is a software system residing in one or more computers or workstations, which can be interfaced from remote places, if necessary, through the an internet connection.  The VSIL is a complete simulation environment with tools to support storage/retrieval of models (the Repository), execution of models (the Simulation Execution Engine), and analysis of data (Performance Analysis and Measurement).  By requirement, the models are created from components, which can be either discrete-event models or dynamic system models of various components of the system or sub-system being studied.  Once the complete system is built, it can then be simulated to study the overall system behavior and functionality.  Design changes can be easily made with the VSIL and thus this tool will lead to extensive cost savings and can be actually used as a universal commercial tool as well, since its scope is not limited to only military applications.Reference ArchitectureIn this paper, the term “Reference Architecture” (RA) refers to the set of interface specifications for each of the components that are used to build a virtual system.  A model that is referred to as a “reference architecture component” is a model of a particular vehicle component with an interface that is compliant with the appropriate RA specification.  The design of the RA was driven by a number of goals related to the original TARDEC requirements, and are covered in section  REF _Ref109124763 \r \h 2.1.RA design goalsThe original RA goals were initially specified in a request for proposal from TARDEC: The components can be considered to be computer based “Lego™-like” system building blocks, and can be easily dragged by mouse and then connected together to assemble the complete system.This goal was central to the design of the RA, and in fact was the reason that the RA was developed at all.  The need for components with interfaces that are generic enough to be arbitrarily interconnected, and yet still maintain their identity as specific, functional vehicle components, is extremely challenging.Figure  SEQ Figure \* ARABIC 2 - A Reference Architecture Component in its Test HarnessThe design needed to be flexible enough to allow the components to be customized and enhanced as needed.  Examples of common changes are changes to the fidelity of the component model, changing values of operational parameters within a component model, and creation of a new component model within an existing category (e.g., a new battery).Other, equally important goals included:Functionality - Reference architecture components must be functional; a RA battery model must have the interface necessary to support whatever internal implementation of battery behavior that the engineer who created it needed.  Thus, careful consideration needs to go into the selection of inputs and outputs on the interface.Hierarchical – Component models reside within a “family tree” to promote ease-of-lookup by vehicle designers.  For example, the vehicle electronics (vetronics) category contains trees of components with roots at “Power Generation and Management” and “Data Control and Distribution.”Interoperable – Components can be developed and executed in a variety of simulation environments, including the High-Level Architecture (HLA), Distributed Interactive Simulation (DIS), Java 2 Enterprise Edition Enterprise Java Beans (J2EE/EJB), Matlab™, and Simulink™.  The VSIL must accommodate each of these environments.  Because of its interface-oriented nature, the RA actually allows components to be developed as native, stand-alone services in any language.Support Legacy Models – The creation of the new system must not cause the extensive body of previously developed models to become obsolete.  This drove the interface-based approach to the RA, since a legacy model can be “wrapped” with an RA-compliant interface with little additional effort.Scalable – Vehicles are made up of a great many components.  For a high-fidelity simulation, the ability of the system to scale is crucial.  The VSIL supports scalability by allowing systems, sub-systems, or even individual components to reside on their own dedicated server.Each of the foregoing design goals has its own set of challenges.  The following section highlights some of these, and discusses the solutions that we employ to overcome them.Challenges and SolutionsWhen we set about designing the reference architecture, the first question was “where do we start?”  The VSIL development team mainly consisted of software developers. While very skilled at creating complex software applications and integrating them with hardware, the team lacked the vehicle systems engineering domain expertise necessary to create the hierarchy of vehicle subsystems, let alone the individual components within those sub-systems.  We overcame this challenge by working closely with TARDEC engineers, by reverse engineering existing component models, and by performing iterative development on the RA.  After the architecture was agreed upon, we created a reference implementation of each component type and then integrated those components into a larger model for verification.  This test integration was very important, as it showed us shortcomings in the interface immediately.The second great challenge we faced, once the component hierarchy had been developed, was deciding what functionality needed to be exposed on the interface for each component within the RA. We leveraged the information gained through the interaction with the TARDEC engineers to determine the required information for each component type. This included the information that the system would need from the component (outputs from model) and the information that the component would need from the system (inputs to model). In effect, these interface definitions described the relevant properties for each component type. This led to component generalization within a type. For example, any component that is classified as a battery will have the same interfaces as any other battery.When we began to populate the RA family tree for the data processing and distribution category, we discovered another challenge: some RA components were not only derived from more basic components (i.e., the “is-a” relationship), but also contained references to components as part of their structure (i.e., the “has-a” relationship).  For example, a single-board computer has one or more processors.  Our initial representation of the RA only supported inheritance, so we were forced to go back and examine how to add association relationships.  Our first approach was simply to use the name of the RA class of the contained attribute as a data type within the definition.  This was clean, but failed to support multiple contained attributes of the same type.  That is, we could represent a computer containing a CPU, but we couldn’t represent a computer containing two CPUs.The next challenge was a requirement for variable model fidelity. That is, the system needed to support simulations at various levels of detail. For example, in some cases a simulation may only require a simple effects based component model. However, in another simulation, where the component is a primary concern, we may need a full-physics based model that includes environmental effects. The current design handles this challenge by specifying required model interfaces (through the RA) to which all instances of a model must adhere. Then we create multiple models of the same component but with various levels of fidelity. Each model instance can be exchanged with any other model instance due to the shared interfaces. The last major challenge was extracting proper performance from the resulting simulation. Many vehicle systems or subsystems will be complex combinations of many components. Therefore, one can anticipate that any system trying to simulate every component within a complex system will execute slowly. To maintain an acceptable level of performance, we have implemented the following concepts:Distributed simulation: The component models can be divided between multiple computers. The core system supports data passing between the machines.Select proper level of detail: We analyze each component included in the system simulation to determine the appropriate level of detail necessary to achieve the goal of the simulation. By limiting the complexity of each component, we can limit the overall system complexity while still accomplishing the simulation goals.Select proper timing granularity: We set the timing granularity for the simulation to the maximum value to achieve the desired performance. Select proper execution model: We set the execution model (such as continuous time, discrete time, etc.) based on the requirements of the system. In the future, we anticipate that the greatest challenge of all will be acceptance by the user community.  After all, a standard isn’t a standard if people don’t use it.  In order to mitigate this, we are continuing to actively work with the Army and are showing the VSIL and publicizing the development of the reference architecture at trade shows and conferences.  We are also making every effort to make the architecture robust and open to attract engineers to it.ExamplesIn this section, we discuss two of the RA components from the Data Control and Distribution vetronics category.CPUThe central processing unit (CPU) is one of the key components of the Computer Model Reference Architecture subsystem.  When we began designing the CPU, we researched different types and made a list of what attributes were shared by all CPUs.  The list, which evolved into the RA for the CPU, included million instructions per second (MIPS), power requirements, and instruction set, as well as the attributes it inherited from its parent models.The CPU had to be able to receive instruction requests, but, as in a real system, they couldn’t come directly from processes.  This required the design of an operating system (OS) component.OSOSes are highly variable, yet they still all have the same basic function: to allow communication between processes and hardware.  This resulted in a highly simplified, open-ended OS, with a designator for whether it is hard real-time, soft real-time, or non-real-time.  Functionally residing within the OS are the processes, or Computer Software Configuration Items (CSCI). CSCICSCIs are the software of the system.  Usually the layer between the human and the hardware, they perform routines that the OS in turn translates to instruction requests and forwards to the CPU.  CSCIs have attributes such as instructions requested, number of contained CSCs (sub-routines), and time to process.InteractionThe interaction between the CPU, OS, and CSCI is designed to model the method that an actual computer integrates the CPU, OS, and executing software.  The CPU, which resides on a single-board computer (another RA component), communicates with the OS.  The information communicated includes CPU attributes, scheduling information, and instruction requests.  The CSCIs reside within the OS.  The OS can contain an arbitrary number of CSCIs, which receive communications from hardware via routing functionality within the OS.  The CSCIs execute instructions, which the OS aggregates and sends on to the CPU.  Once the CPU finishes processing the instructions, a message is returned to the OS containing how many instructions were processed.  The OS then updates each CSCI, which modify their instruction queue accordingly.With each of the components, the attributes that are defined act as guidelines to the creation of components that are usable in other systems, but are still flexible.ToolsThe VSIL system is about 20% infrastructure supported by 80% tools.  There are tools to support automatic generation of data points, creation of wrappers for RA components, laying out display screens, checking models into and out of the repository, configuring “what-if” runs, and a number of other tasks.  In this section, we highlight two of the tools that benefit directly from the interface-based approach to the reference architecture.Data DisplayThe Performance Analysis and Monitoring (PAM) functionality of the VSIL requires both real-time display of simulation data and historical display.  The Simulation Execution Environment (SEE) publishes selected data points whenever they change, at which point the PAM displays take over.We used OpenSim™, a product previously developed under a SBIR contract for the Air Force, to create our displays.  This was a good fit because the data structures for OpenSim™ and the RA are both defined hierarchically.  This allows us to generate OpenSim™ configuration files directly from models built with RA components.  These configuration files then make it possible for the user to create display screens by drag-and-dropping data values of interest onto the display using the OpenSim™ ViewBuilder tool.  The ViewBuilder has a palette of display widgets that includes text boxes, X-Y charts, strip charts, gauges, and icons (see  REF _Ref109446552 \h Figure 3).  In the context of the kinds of testing the VSIL is used for, the ability to rapidly create display screens is very important.  Many of the displays created are being made for the purpose of just a few simulation runs, and it does not make economic sense to spend much time writing specialized PAM code that is then going to go unused.  Also, because the RA component interfaces are stable, one can build PAM displays for a particular RA component (e.g., a CPU) using the defined RA interface, and then re-use that display for CPU display in future simulations.Figure  SEQ Figure \* ARABIC 3 - An Example PAM DisplayConfiguration and ExecutionBecause one of the main goals of the VSIL project is the ability to perform trade-off analysis using the RA pluggable components, we are developing a tool to facilitate this ( REF _Ref109449677 \h  Figure 4).  Dubbed the Virtual System Editor (ViSE), this tool will combine all of the pieces of the VSIL system into an integrated development environment (IDE).  The user will select and load from the repository a schematic model containing slots for RA components.  The user will then populate the slots with appropriate RA components via drag-and-drop.  The tool will prevent components from being dropped into incompatible slots, and will do a variety of other error checking.  Once a number of components have been selected for trade-off analysis, the user will deploy the model directly into the SEE for execution.  The data will be displayed via user-defined PAM displays, and/or recorded for off-line analysis.  The system will automatically handle the multiple simulation runs for each different configuration, and provide data from each configuration for side-by-side analysis. Figure  SEQ Figure \* ARABIC 4 - A Preliminary ViSE ScreenConclusions and Future WorkThe creation of an interface-based, component reference architecture is a challenging task.  However, the benefits gained by having it are great, and far outweigh the difficulties.The VSIL will ultimately be integrated into an Internet-based web environment, so that several people can interactively work on the same design from geographically different places, leading to better collaboration and also to easier and early implementation.  The initial focus of this work is on the development of components for the electrical system (e.g., the electrical power system architecture containing the wiring harness, actuators, sensors) of a vehicular platform.  We continue to extend the scope of the VSIL RA to other subsystems beyond the electrical system, thus leading the way to a complete "virtual vehicle build”.  In addition, we plan to add the ability to use actual hardware components of a system, thus creating a Hardware-In-The-Loop (HITL) environment.References[1]	 “Army Technology – Stryker 8-Wheel Drive Armoured Combat Vehicle” http://www.army-technology.com/projects/stryker/specs.htm[2]	 Lovaszy, Steven: “Compasrison of Bradley M2A2 and M2A3 Using Janus” Naval Postgraduate School, Monterey California, 1996.[3]	 “Army Weapons and Equipment: Tracked Vehicles” Army Magazine Green Book, 2002.  http://www.ausa.org/www/greenbook.nsf[4]	Salter, Margaret S.: “Bradley Fighting Vehicle M2/M3 A3: Training and Soldier System Observations”, ARI Research Note 2001-06, U.S. Army Research Institute for the Behavioral and Social Services, 2001.Author BiographiesSTEVE ROWE is Chief Software Engineer at Cybernet Systems Corporation in Ann Arbor, MI.  He is the technical lead for the development of the VSIL software suite.  His software development experience spans over 20 years in diverse fields such as space flight control systems, industrial automation, scheduling/resource management systems, artificial intelligence systems, and robotics.  Recent work has included the creation of the OpenSim™ toolkit, the Simulation Execution Environment of the VSIL, and interfacing technologies for HLA, J2EE, and DIS.RAKESH PATEL is the Electronics Architecture team leader at TARDEC, RDECOM.  He is leading the advancement of the electrical power management, electrical power distribution, system networking, computing hardware, system operating environment, virtual system integration laboratory, and Vetronics reference architecture related technologies.  His engineering work experience at TARDEC spans fourteen years in the following programs: Advanced Mobile integrated Power System (AMPS), Vetronics Technology Integration (VTI), Vetronics Technology Testbed, Common Ground vehicle Architecture (CGA), Future Combat System, Wolverine, Grizzly, Future Scout and Cavalry System, Vetronics Open System Architecture, and WSCOE.  He holds his BSEE from University of Illinois and MS in EE/CS from Oakland University.  He is a member of the Army Acquisition Corps.KEVIN TANG is a Research Engineer at Cybernet Systems in Ann Arbor, MI.  He co-wrote the first draft of the IGV Component Architecture Standard, an interoperability standard based on emerging Vetronics taxonomies developed with TARDEC.  He implemented soft component models and designed schematics for the power management and computer subsystems of the Bradley Fighting Vehicle (M2A3).  He received his BSE in Computer Engineering from the University of Michigan.RYAN O’GRADY is a Research Engineer with Cybernet Systems Corporation's R&D division. While working on a Virtual Simulation Integration Laboratory (VSIL) for the U.S. Army-TARDEC, he created a Java-based repository interface that allowed for the easy storage and deployment of VSIL runs. He also developed many of the MoML components that were used for testing and demonstration. He is currently involved with the creation of a tool that utilizes generalized layouts to allow “drag-and-drop” creation and testing of systems.  He holds a BS in Computer Engineering from the University of Michigan, Ann Arbor.ERIC JOCHUM is a computer engineer on the Electronics Architecture at TARDEC, RDECOM.  He is technical lead on the system operating environment, virtual system integration laboratory, electrical power management, and computing hardware. His engineering work experience at TARDEC spans six years in the following programs: Advance Mobile integrated Power System (AMPS) and WSCOE.  He holds his BSE in computer engineering from Western Michigan University. He is a member of the Army Acquisition Corps.BENJAMIN MICHAEL is a Research Engineer with the Research and Development group at Cybernet Systems Corporation.  He is the primary developer of the user interface extensions made to OpenSim™ to support the Performance Analysis and Measurement (PAM) of the VSIL software.  He also serves as the subject matter expert on intelligent ground vehicle architecture at Cybernet.  Mr. Michael holds a BS in Computer Engineering from the University of Michigan, Ann Arbor. The Virtual System Integration Laboratory (VSIL) was developed under Small Business Innovative Research (SBIR) contract DAAE07-03-C-L151 for the United States Army Tank-Automotive Research, Development and Engineering Center (TARDEC), a division of the Tank-Automotive and Armaments Command (TACOM). Matlab™ and Simulink™ technical computing software are products of The Mathworks, Inc.Figure  SEQ Figure \* ARABIC 1: VSIL System Architecture (Reference Architecture Role Circled)