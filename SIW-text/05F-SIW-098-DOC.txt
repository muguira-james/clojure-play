Reflections on "The Challenge of Environmental Data Interoperability on the Global Information Grid" by Dobey and Eirich (05S-SIW-133)Dr. Dale D. MillerLockheed Martin Simulation, Training & SupportAdvanced Simulation Centers13810 SE Eastgate Way, Suite 440(425) 957-3259Bellevue, WA 98006 HYPERLINK "dale.d.miller@lmco.com" dale.d.miller@lmco.comDr. Paul A. BirkelThe MITRE CorporationMcLean, VA HYPERLINK "mailto:pbirkel@mitre.org" pbirkel@mitre.orgKeywords:interoperability, environmental data, Global Information Grid, GIG, logical data model, data architecture, data fusionABSTRACT: Dobey and Eirich identify that the operational objectives of the Net-Centric Operations and Warfare (NCOW) environment necessitate a move away from the status quo of authoritative data producers generating standard data products – to a myriad of producers and consumers exchanging and fusing environmental data of varied timeliness, currency, quality, data dictionaries, data models, and physical formats using the Task-Post-Process-Use (TPPU) paradigm. The traditional approach of using point-to-point translators between individual data producers and consumers does not scale well to meet such objectives. They propose an interposed mediating software layer and associated normalized logical data model (LDM) to which any producer’s data content may be mapped (ideally losslesssly) and from which any consumer’s data content may be extracted (also, ideally losslessly) using appropriate mappings. However, there are some practical issues that must be addressed before this theoretical framework can be used to achieve the “come as you are” model for environmental data exchange on the GIG, even assuming steady improvements in the capability of the GIG Enterprise Services – including mediation. We identify recent relevant work from a variety of disciplines that have a bearing on the achievability of the Dobey and Eirich proposal, discuss several aspects of their proposal that we believe do not fully address important real-world constraints, and suggest several areas of additional analysis that should be undertaken.IntroductionDobey and Eirich  REF _Ref108346297 \r \h [1] identify that operational objectives of the DoD Net-Centric Operations and Warfare (NCOW) environment necessitate a move away from the current Task-Process-Exploit-Disseminate (TPED) paradigm in order to support greater timeliness of data sharing on the Global Information Grid (GIG). The underlying trade is an increase in data timeliness against smaller data sets of varying pedigree, quality and “authoritativeness”. NCOW envisions a myriad of producers and consumers exchanging and fusing environmental data of varied currency, quality, data semantics (captured in data dictionaries and data models), and physical formats using the Task-Post-Process-Use (TPPU) paradigm.The traditional approach of using point-to-point data translators between individual producers and consumers does not scale well to meet such objectives. In particular, the GIG Enterprise Services (GES) include a proposed capability – a mediation service – to facilitate such data interchange. Leveraging the GES services framework, Dobey and Eirich propose an interposed mediating software layer and associated normalized logical data model (LDM) to which any producer’s available data content may be mapped and from which any consumer’s required data content may be extracted using appropriate mappings and equivalences.Dobey and Eirich reflect upon the history of databases and data processing since the 1960s, citing W. H Inmon and Clive Finkelstein on information engineering and J. F. Sowa on the history of ontologies from Aristotle to the present. Not mentioned are a variety of more recent efforts from a variety of disciplines that have a bearing on the achievability of their proposal. These include the:Army Geospatial Data Integrated Master Plan (AGDIMP), National Geospatial-Intelligence Agency’s (NGA) Geospatial Intelligence Database Integration (GIDI) as a means to integrate two previously stovepiped data production systems, Multilateral Interoperability Programme (MIP) C2IEDM, a mechanism and data model for the exchange of command and control data (including environmental data), andActivities of the Army Battle Command, Simulation and Experimentation Directorate (BCSE) Geospatial Data/Information Focus Area Collaborative Team (GDI FACT) in defining the concepts of the Reference and Data Requirements EDMs (REDM and DREDM) as well as the emerging Defense Geospatial Community that is charged with using a Community of Interest (COI) based approach to address the challenges of GIG-based data interoperability.Given little attention in the Dobey-Eirich paper are issues of geospatial accuracy to support fusion and conflation, requirements for (machine parsable) metadata, and translation of similar concepts at differing levels of aggregation or specificity. Finally, policy issues, which could prove to be the greatest impediment to achieving their vision, are not addressed at the proactive, pragmatic level.The main thesis of the Dobey-Eirich paper is exciting, if not yet compelling. Every environmental concept populated with instance data by a data producer would be decomposed into atomic concepts through a mediating layer. A consumer of the data would reassemble those atomic concepts into information relevant to the consuming system. They give the following analogy:Another analogy for this transfer might be a chemical reaction, where atoms contained in molecules from one or more substances are exchanged and reassembled into molecules of one or more different substances. The common interchange hub provides a mechanism wherein the disassembly and reassembly of data objects can take place. In this paper, for conciseness we coin the acronym MLEDM to refer to the Dobey and Eirich “mediating layer environmental data model”.Dobey and Eirich recommend a three-schema architecture  REF _Ref108430115 \r \h [2] as their design approach. In this approach, each data producer and consumer has a schema. Data interchange is accomplished by mapping the data items in accordance with the concepts specified by the producer schema to data items in accordance with the concepts specified by the schema of the mediating layer and then again to data items in accordance with concepts specified by the schema of the consumer. In order to support lossless data item transformation (producer to consumer) the mediating layer must therefore be capable of representing the semantic content of all data producers and consumers.Central to the paradigm proposed by Dobey and Eirich is the use of the environmental data (items or data set) once converted to the consumer’s semantic frame of reference. They discuss both overlaying the new environmental data items on an existing base data set and fusing the new data items with the base. While the latter is certainly more in keeping with the tenets of NCOW, the utility and use of the transformed data (namely, overlay or fusion) by the consumer is actually independent from the mediating layer interchange mechanism. The utility of the transformed data, however, may be significantly affected by the data mediation process.Recent Relevant EffortsU.S. Army Geospatial Data Integrated Master Plan (AGDIMP)In future military operations it is envisioned that the U.S. Army and Marine Corps will continue to be the primary on-ground forces and therefore in the best position to acquire timely, accurate, “ground truth” environmental data – the constantly improving capabilities of National Assets notwithstanding. The Joint community has identified the requirement for a Joint Geospatial Enterprise Services (J-GES) in the context of the GIG and GES to support advanced operational capabilities, such as those to be fielded as part of the Future Combat Systems (FCS), as regards their geospatial data requirements.In preparation for the FCS, and the J-GES, the Army has developed a geospatial master plan  REF _Ref108421031 \r \h [3], which states:In the future, geospatial data must be moved forward to the AO [Area of Operations] and into the network-centric operations where subject-matter experts and Soldiers can become involved in creating, updating, verifying, analyzing, and making information available to commanders and Soldiers on a real-time basis with near-continuous updates. In the future, each Soldier can potentially become a sensor and provide real-time updated geospatial and related data to the common operational picture (COP) and to the National Geospatial-Intelligence Agency (NGA). This process is predicated on the development of standards (from NGA) that will allow two-way data flow between the AO and geospatial databases at data repositories.While the AGDIMP envisions the same myriad of producers and consumers using net-centric principles of full data distribution as do Dobey and Eirich, it proposes to assert considerably tighter controls on the warehousing, validation and integration (fusion and conflation) of the data through the development of standards by the National Geospatial-Intelligence Agency (NGA).Key recommendations include the development of a Joint geospatial data dictionary, a Joint geospatial data model, a standard ontology, and an integrated, end-to-end geospatial process. This system-of-systems process must include a responsive capability to identify requirements for new geospatial data, the capability to make real-time, on-site geospatial data updates which can be rapidly and broadly disseminated, and the ability to improve operational tempo by anticipating consumers needs based on their mission posture using “brilliant push” dissemination of “change only” data.The AGDIMP recommends development of machine-readable metadata standards for fitness-of-use determination and development of policy, procedures and standards for geospatial data management, including fusion and conflation, filtering, transformation, etc. Moreover, the AGDIMP provides specific implementation task recommendations for not just the near term (FY 05-07), but also the mid-term (FY 08-13) and long-term (FY 14-20). While the AGDIMP makes no specific mention of a mediating layer EDM or a three-schema architecture, such a approach can be inferred from (1) the AGDIMP’s acknowledgement that no set of geospatial standards and services can simultaneously meet the requirements of all communities of interest and (2) the recommendation that a Joint geospatial data dictionary and data model be established. However, no mention is made of reducing the data dictionary and data model to atomic components.Geospatial Intelligence Database Integration (GIDI)In order to integrate a heterogeneous COTS-based data production environment, NGA used a three-schema architecture in the GIDI, a GIS-based enterprise environment that integrates existing NGA databases and production tools  REF _Ref108419278 \r \h [5]. Initially fielded in 2002, and in continuous use and extension since that time to include support for Homeland Security requirements, the data model for the mediating layer is the Geospatial Intelligence Feature Database (GIFD).No attempt was made to decompose the GIFD schema into atomic elements; rather an existing DoD-standard geospatial data dictionary was extended (as needed) to define a single schema that spanned the Agency’s customer requirements – the DGIWG Feature and Attribute Coding Catalogue (FACC) data dictionary. All legacy and current NGA vector data sets that are based on the (extended) FACC can be imported into or exported from the GIDI common data store.For performing instance data mappings between data production and value-adding environments/tools and the GIDI, as well as to/from customer environments, a commercial tool is used, the SafeSoft Feature Manipulation Engine (FME)  REF _Ref108419773 \r \h [6]. Individual mappings were developed through manual analysis by domain experts, facilitated by the use of the FACC common data dictionary.Multilateral Interoperability Programme (MIP) C2IEDMThe Multilateral Interoperability Programme (MIP), established in 1998, provides another interesting use-case of the three-schema architecture by actual reduction to practice  REF _Ref108412268 \r \h [4]. Here, the mediating layer EDM is the Command and Control Information Exchange Data Model (C2IEDM), and, instead of producers and consumers, the applications interchanging (and interoperating with) data are national Command and Control Information Systems (C2IS). While Dobey and Eirich limited their consideration to environmental data, the C2IEDM is concerned with all command and control information, which includes at least some types of geospatial and other environmental data.While a key foundation of the Dobey-Eirich approach is the atomistic reduction of all environmental concepts, the C2IEDM makes no attempt to do so. In a recent analysis, Schmitt  REF _Ref108412268 \r \h [4] discusses many practical implementation issues which apply equally well to the Dobey-Eirich proposal. In particular, he states:In order to ensure true semantic interoperability, far-reaching modifications to the core of national C2ISs are necessary rather than just the addition of mapping adapters as new interfaces to the existing systems.Subsequently, Schmitt strongly advocates against allowing producing or consuming applications direct access to the data model of the mediating layer. Rather, he proposes a data access stack with multiple abstraction layers (relational view, object oriented view, normalized view, business objects view), each with access control and notification services. We suggest that a similar methodology be considered for incorporation into the Dobey-Eirich MLEDM framework.GDI FACT REDM, DREDM and the MLEDMIt is instructive to compare the proposed Dobey-Eirich mediating layer of atomic environmental data concepts to the REDM and DREDM  REF _Ref108412327 \r \h [7]. The DREDM, or Data Requirements EDM – constructed as an adjudicated union of constituent EDMs – losslessly supports the representation of any environmental concept of any of the constituents. Opportunities for harmonization are afforded, and taken as/when practical; however, uniqueness of the syntactic representation of a concept may not necessarily be achieved. Nevertheless, this is the approach that has been, and continues to be, successfully used for geospatial data interchange by the NGA GIDI.The REDM, or Reference EDM, is intended to express the common semantics to support the closely-coupled “deep” interoperation of multiple systems – which may be diverse, are often real-time, and are generally consumers rather than producers of environmental data. The Dobey-Eirich MLEDM is a similar concept, aimed at the (ideally lossless) exchange of environmental data from producers to consumers. What are the advantages of the MLEDM over a DREDM for the exchange of actual instance data items? Were it possible to develop an MLEDM for which any environmental concept has uniqueness of representation as a combination of atomic elements, then the MLEDM approach would offer an attractive elegance. Moreover, mappings from producer composite concepts into the MLEDM or from the MLEDM to consumer composite concepts may be easier to define and less lossy than analogous mappings to or from the DREDM, which itself is largely comprised of composite concepts.However, if a data producer produces a composite data element which has semantic differences from a composite data element desired by a consumer, the MLEDM, no matter how perfectly formed, cannot overcome the resulting lossy transformation of instance data items. This is discussed further in section  REF _Ref108521368 \r \h 4.Summary of EffortsSome lessons to be learned from examining these recent relevant efforts include:The Army has established a technical vision and recommended policies to foster the interchange and interoperability of geospatial data in the context of the GIG and GES (i.e., the AGDIMP). While advocating a Joint geospatial data dictionary and data model, it does not envision success as predicated on atomic decomposition of the geospatial conceptual domain.The three-schema architecture has been implemented previously in the environmental domain and has aided in the interchange and interoperability of environmental data (e.g., the GIDI). However, it has been successful while operating “above” the atomic level.Rigorous normalization of a rich data model is a complex undertaking (e.g., the C2IEDM).No previous effort (of which we are aware) has attempted to develop or leverage an environmental logical data model comprised entirely of irreducible (atomic) conceptual elements.Recognizing the broad scope of “geospatial” on the GIG, and the complexity of the undertaking to enable net-centric data exchange in accordance with DoD Directives, the Joint community is in the process of establishing the Defense Geospatial Community (DGC) comprised of a family of coordinated COIs to address issues in geospatial data exchange. These include: aeronautical, hydrographic & littoral, topographic, urban, imagery, METOC, facilities & environment, and geoanalysis data.Mappings and Semantic NuancesIn their section entitled “Implications for the GIG”, Dobey and Eirich state:Another potential tradeoff exists in cases where use of a mediating layer does not provide for a lossless representational match for a source data item. In this case, there may be nuances of semantics that are lost in the translation.In our experience, such loss of semantic nuances are likely to be much more prevalent than the papers’ authors assume. For example, in a discussion held by the SISO EDCS Product Development Group (PDG), many believed there were nuance differences between encoding a lighthouse as a lighthouse feature versus a building feature with a building function attribute value of lighthouse. If one agrees with this (regarding which opinions vary), then practically every decomposition of an environmental concept into its atomic parts will lose semantic nuances, at least to some beholders.On the issue of mapping complexity, Schmitt  REF _Ref108412268 \r \h [4] states (in the context of the C2IEDM):The required mapping rules can be very complex in practice. In particular, this holds in cases in which there is no clear 1:1 mapping of concepts. For instance, n attributes of the ODB [operational data base] might have to be mapped onto m attributes in the C2IEDM where the attributes may be distributed over several entities.Many examples of nuance differences can be seen in the mappings between concepts in FACC 2.1 and the EDCS 4.0, which have been developed by the SEDRIS Core Team. Consider the following example:FACC 2.1Code/Label FACCDefinitionEDCS 4.0 LabelEDCS 4.0 DefinitionAB010: Wrecking Yard/Scrap Yard An area or site engaged in the wrecking, dismantling, storage, or resale of discarded products. (See also AB000)recycling_siteA site engaged in the wrecking, dismantling, storage, recycling, and/or resale of discarded or scrap products.Many people would agree that the two human-understandable designators, “Wrecking Yard/Scrap Yard” and “recycling_site” conjure up quite different mental images. However, the two definitions are very similar, with only “recycling” added to the EDCS definition. Most people would agree that the definitions have a subtle difference, and the mappings between them would not be lossless. Even if these definitions were decomposed into atomic conceptual elements (e.g., “wrecking site”, “dismantling site”, “storage site”, “recycling site”, “resale site”), if a data producer produced an instance of a “Wrecking Yard/Scrap Yard” and a data consumer desired a “recycling_site”, the mapping of the data instances would be lossy. A different semantic would necessarily be assigned to the same data item when represented in the two different systems/applications. This is undesirable from an interoperability standpoint.Losslessness and CorrectnessAll lossless mappings are correct, but the converse is false. For example, the mappingriver ( watercourseis correct (because every river is a watercourse, or so we do stipulate here) but lossy (because some watercourses are not rivers), whereas the mappingrecycling_site ( AB010: Wrecking Yard/Scrap Yardis incorrect, because some “recycling_site”s are not “AB010: Wrecking Yard/Scrap Yard”s (e.g., a site at which household recycling is separated into paper, aluminum, steel and glass fails to meet the definitional criteria). In general, from the perspective of (instance) data item transfer, a mapping from the specific to the general is lossy but correct (the data item remains a valid instance of the more general concept), while the reverse mapping is lossy and incorrect.Dobey and Eirich discuss disaggregating producers’ data schemas into fine-grained component concepts, then reliably recombining them into a different set of aggregated elements for the user. Their analogy to a chemical reaction is intuitively appealing, in part due to the reaction’s conservation of mass and reversibility (albeit under the appropriate thermodynamic conditions). However, when viewed in the context of losslessness and correctness, the analogy is flawed.Let us generalize the example in the previous section. Let v, w, x, y and z represent normalized atomic environmental concepts, let A and B represent producer and consumer EDMs respectively, and let k be an instance data item. The symbol ( represents the disjunction (OR) operator. By the definition of the MLEDM, v, w, x, y, z ( MLEDM.(To understand the generalization, the reader may wish to think of these variables as:vSite engaged in wreckingwSite engaged in dismantlingxSite engaged in storageySite engaged in recyclingzSite engaged in resalekA point feature data itemAFACC 2.1BEDCS 4.0.)Example 1. First, we recast our example of the previous section. Here we haveConcept[v ( w ( x ( z] ( A,For all k in Concept[v ( w ( x ( z], k ( A Concept[v ( w ( x ( y ( z] ( B, andFor all k in Concept[v ( w ( x ( y ( z], k ( B.Assuming no perfect match exists in B, we have the optimal conceptual mappingConcept[v ( w ( x ( z] ( Concept[v ( w ( x ( y ( z],(meaning that) For all k in Concept[v ( w ( x ( z], k ( Bwhich is lossy but correct. (The inverse mapping would be incorrect.)Example 2. Assume Concept[w ( x ( y] ( A and Concept[x ( y ( z] ( B. Is it reasonable to mapConcept[w ( x ( y] ( Concept[x ( y ( z]And therefore that For all k in Concept[ w ( x ( y], k ( B ?This mapping is both lossy and incorrect. Nevertheless, given an equal likelihood assumption that the data instance is one of w, x, or y, when applied to actual instance data this mapping will be correct 2/3 of the time. Unless some better mapping exists, it seems useful to collect this mapping, especially if metadata on the mapping (and explicitly attached to the resulting transformed instance item) were included to quantify its degree of correctness or losslessness.How does the mediating layer help to identify the optimal mappings? The next example addresses this.Example 3. Assume Concept[x ( y] ( A. To what does it map in the MLEDM?Since Concept[x ( y] ( MLEDM (because [x ( y] is a composite concept), we have two choices:Concept[x ( y] ( Concept[x], orConcept[x ( y] ( Concept[y].Both are incorrect. The best we could do would be to assign probabilities to each of these mappings, e.g., p(Concept[x ( y] ( x) = 0.5, andp(Concept[x ( y] ( y) = 0.5.One can then imagine defining probabilities of possible mappings from the MLEDM to B and selecting among all possible composite mappings that with the highest probability. Of course there is no guarantee that there will be a unique composite mapping of highest probability.The analogy between chemical reactions and aggregate entity mapping breaks down because molecules are the conjunction of their elements and aggregate environmental features are the disjunction of possible specific types. When two hydrogen and one oxygen atom combine to form one H2O molecule, both sides of the equation still have three atoms: two hydrogen and one oxygen. But an aggregate feature instance is only one of the possible feature types comprising the feature description. Hence, at the instance level, there really are no building blocks to disassemble and recombine.Since the Dobey-Eirich mediating layer requires “atomic” level concepts, it is unclear how inherently aggregate concepts (e.g., built-up area) would be represented, and how the system would be capable of simultaneously representing the same general concept at multiple levels of aggregation or specificity. We propose that relationships, defined at the logical data model level and enforced at the feature (data) instance level, are key to such multi-resolution representation (see  REF _Ref108517164 \r \h [10]).Normalization – the Silver Bullet?On the issue of data model normalization, Schmitt  REF _Ref108412268 \r \h [4] states a requirement for transforming all data to a canonical form, gives examples of redundancies and ambiguities in the current C2IEDM, and states that:The MIP community is continually improving the model but there will always be some unresolved problems.That such an important interoperability program (which has been ongoing for 7 years) with participation of 26 nations and organizations continues to strive to normalize its data model points to the difficulty in so doing.Most readers would intuitively agree that the English language (e.g.) is far too complex to normalize in the sense that every expressible semantic can be reduced to a unique canonical form. As with the building / lighthouse example above, ascertaining nuance differences between similar concepts is often in the eye of the beholder. Ambiguity is ubiquitous in natural language, and the mathematical modeling of it is a current area of research in computational linguistics  REF _Ref108431289 \r \h [8].Normalizing a logical data model of fine grained, atomic environmental concepts should be much more tractable than normalizing a natural language, especially if each environmental concept has a rigorous, well-defined definition. Let us consider the concept of river, which our initial inclination would have been to specify as an “atomic concept”.The EDCS 4.0 defines river as “A natural flowing <WATERCOURSE>; a river or stream”. FACC 2.1 similarly defines “River/Stream” as “A natural flowing watercourse”. Sowa  REF _Ref108433237 \r \h [9] (referenced in  REF _Ref108346297 \r \h [1]) makes some interesting observations about how the French might interpret these definitions, due, he suggests, to the fact that in France the major rivers flow into the Altantic or the Mediterranean, while in the United States, major rivers like the Ohio and the Missouri flow into the Mississippi. Sowa observes:…[a] more complex pattern for the senses of the English words river and stream and the French words fleuve and rivière. In English, size is the feature that distinguishes river from stream; in French, a fleuve is a river that flows into the sea, and a rivière is either a river or a stream that flows into another river. In translating French into English, the word fleuve maps to … a subtype of the English type river. Therefore, river is the closest one-word approximation to fleuve; if more detail is necessary, it could also be translated by the phrase river that runs into the sea.So what is the point of this seemingly frivolous discourse? There are several. First, life experiences color our interpretation of words, and, no matter how precise and rigorous the definitions in an environmental data dictionary, not everyone will agree on all nuances of their meanings. Second, because of this, reducing a rich environmental data model to fifth (or even third) normal form is a monumental task which may not even be possible. And third, objective determination of the atomicity of environmental concepts may not be possible. Is river an atomic concept? How about river that runs into the sea? Or stream that runs into a river? There are many issues here that will require thought and adjudication by thoughtful people to arrive at a good solution for a particular context. But there is probably no perfect one.Nevertheless, we fully agree with Dobey and Eirich that reducing ambiguity and redundancy in a logical data model is desirable. Environmental Data Fusion – the Achilles’ heel?Dobey and Eirich envision an approach in which new producer data – which may be timelier, higher resolution, more (or even less) spatially accurate than data already residing in the environmental system database – is fused into the system database as it is produced and distributed. They state:…the most powerful approach to data interoperability is that of data fusion…. unless multiple versions of the system database are maintained, the system database will always, in some way, reflect the information extracted from the producer data. However, there is no discussion resolving geospatial positioning errors in data, even relative to the statement:Even small differences in spatial referencing (most often caused by failure to transform spatial coordinates to the same spatial reference frame or by using lossy coordinate transformation algorithms) can cause significant errors in data overlay,…With the prevalence of robust GIS systems today, it is much more likely that spatial referencing differences are due to positioning errors in the source data than to spatial reference frame transformation inaccuracies. In our opinion, by far the most difficult problem in geospatial data fusion is that of conflating geospatial features from multiple sources, especially where they use different data dictionaries and unique extraction/collection criteria for the “production” of feature (instance) data items from “raw sensor data” (e.g., imagery).In particular, the assumption that environmental data fusion is best achieved by geospatial registration may be a poor one. For example, despite high positional accuracy the decision to capture an observed phenomenon as a CULVERT, a small BRIDGE, or simply as an EMBANKMENT by different systems/applications is likely to overwhelm any fusion process attempting to unify these disparate characterizations into “one thing” based on georegistration. In 2002, the AMSO (now BCSE) Environmental Database IPT (EDB IPT) (now the GDI FACT) spawned a secondary IPT focused on terrain database generation processes, the DB IPT. Its mission was to develop a consensus architecture addressing the full range of Army M&S (and C4ISR systems, including FCS) environmental data set requirements. The DB IPT developed a number of position papers, including one on geospatial source data, which stated  REF _Ref108586460 \r \h [11]:The vast number of problems associated with fusing multiple data sources together is a daunting task. A number of research programs have attempted to solve this problem using conflation techniques and have experienced only modest success.The AGDIMP  REF _Ref108421031 \r \h [3] is slightly more optimistic regarding a solution to this problem:Consequently, a number of data sources may need to be fused and/or conflated. The process of fusion and conflation can be labor-intensive and error-prone, but the effective use of appropriate systems engineering principles can address these challenges.Dynamic Semantic Content Update Implications of the GIGThe schema of the MLEDM is a function of the constituent producers’ and consumers’ schemas. However, to support the operational objectives of the NCOW environment, new data producers or consumers should be able to publish or subscribe new environmental data dynamically. This would require updates to the MLEDM to ensure lossless representation of any new environmental content. Ideally, the MLEDM would autonomically extend itself to accommodate the dynamic requirements of the producers and consumers. This is a fundamental tenet of NCOW, for which we believe there is no near- (nor probably intermediate-) term solution.Consider, for example, a consumer (attempting, e.g., to solve some infamous murder case), declares a requirement for the geospatial feature shoe_footprint, with attributes manufacturer and model and the required attribute values bruno_magli and lorenzo, respectively. If these environmental data are not already incorporated into the MLEDM, how is the consumer able to specify this requirement? Likewise, if a data producer’s satellite image or plaster impression produces a bruno_magli footprint, how is s/he to publish this information to yet unknown subscribers?We posit that the current state of the art in natural language understanding is insufficient to support this consumer/producer semantic interchange autonomically; for the foreseeable future, an analyst will be required to mediate this process (as was the experience in the NGA GIDI). This leaves a human in the loop at a central hub of the data interchange design process, which is contrary to the design philosophy of the GIG but for which we see no reasonable alternative given available technology. Conclusions and RecommendationsWe have established the following conclusions:The three-schema architecture proposed by Dobey and Eirich is tried-and-true and is also well established in the environmental domain (section  REF _Ref108520133 \r \h  \* MERGEFORMAT 2).Mappings from aggregate concepts to aggregate concepts are inherently problematic, usually lossy and often incorrect (sections  REF _Ref108521347 \r \h  \* MERGEFORMAT 3 and  REF _Ref108521368 \r \h  \* MERGEFORMAT 4).The analogy between chemical recombinations and aggregate-level data model mappings is fundamentally flawed; chemical recombinations are represented by conjunctions of atomic elements, while aggregate environmental concepts are represented by disjunctions of their atomic building blocks (subtypes) (section  REF _Ref108521368 \r \h  \* MERGEFORMAT 4).Reduction of a logical data model to a maximally normalized (“nth normal”) form, while feasible and often advised in applications such as banking, inventory control and shipping and receiving, is much more problematical for a real-world environmental data model, which ultimately suffers the ambiguities, redundancies and nuances of natural language. Even significant programs such as the MIP C2IEDM have been unable to eliminate all ambiguities and redundancies and have been unable (yet) to guarantee the mapping of an arbitrary environmental concept to a (unique) canonical form (section  REF _Ref108521483 \r \h  \* MERGEFORMAT 5).Data fusion and conflation is a much more difficult problem than Dobey and Eirich appear to suggest. Both their approach and the AGDIMP assume this difficult problem will be solved. We believe that the problem is currently intractable for many data sources, but the trend toward higher geospatial accuracy may help to alleviate this – minimally by reducing the Circular Error Probable (CEP) associated with sensor inputs to the process (section  REF _Ref108521527 \r \h  \* MERGEFORMAT 6).The GIG should allow communication of all information of interest, to all interested parties, all the time. When a producer or consumer needs a new semantic concept, can it autonomically publish its description? We think not; human analysts will be employed in this regard for some time (section  REF _Ref108522198 \r \h 7). This issue is not addressed in the Dobey-Eirich paper, but we believe it relevant nonetheless.While these reflections on the Dobey-Eirich paper do not find agreement on all of their points, much is to be learned from their analyses. We embrace the three-schema architecture, but only as a fall-back temporizing solution where the system-of-system scope can not be determined and suitable subject matter expertise is available to “work the details”. We note that the nascent COI-approach to data harmonization in the DoD subscribes to this same philosophy of putting the “community members” is a room and forcing them to work out and document the semantics of their domain in order to support lossless data interchange on the GIG. GES mediation services can implement the results, but hard work to reach agreement remains the necessary precondition.As we suspect others are, we are enticed by their vision of a realistic environmental data model based exclusively upon (perceivably) irreducible concepts. While we agree with the AGDIMP proposition that different communities of interest require different environmental data models, were all COIs to (at best subjectively) reduce their EDMs to the Dobey-Eirich “atomic” level, then the aggregate concept mapping problem would be greatly diminished, and the Dobey-Eirich MLEDM (and its three-schema architecture) could achieve its vision. Aggregate concepts can and should still be included in environmental data models to support multiple levels of resolution, but only with explicit, instance-based relationships to the irreducible subtypes of which they are comprised.How should our community (including both M&S and C4I) proceed? In our opinion, the government should sponsor research in the area of environmental data model atomization, data model mappings between data models of different COIs, and the computational aspects of linguistics modeling. (This is not to mention the research required for geospatial fusion and conflation.) Were this research to achieve positive results, the vision of Dobey and Eirich may someday be achieved. AcknowledgmentThe authors thank Virginia Dobey and Peter Eirich for their thought-provoking paper which has given rise to this, and hopefully many future, productive exchanges on the important topic of the interchange and interoperability of environmental data on the GIG.ReferencesV. Dobey and P. Eirich, “The Challenge of Environmental Data Interoperability on the Global Information Grid”, Simulation Interoperability Workshop, Paper No. 05S-SIW-133, March, 2005.D. Jardine, “ANSI / SPARC Database Model (Three-Schema Architecture)”, Proceedings of the 1976 IBM SHARE Working Conference on Data Base Management Systems in Montreal, Quebec, North-Holland Pub. Co., New York, 1977.Department of the Army, Battle Command, Simulation and Experimentation Directorate, “U.S. Army Geospatial Data Integrated Master Plan”, HYPERLINK "http://www.amso.army.mil/agdimp"http://www.amso.army.mil/agdimp, June 13, 2005.M. Schmitt, “Integration of the MIP Command and Control Information Exchange Data Model into National Systems”, 10th International Command and Control Research and Technology Symposium, McLean, VA, June, 2005.D. Hampel, “Geospatial Intelligence Database Integration (GIDI)”,  HYPERLINK "http://www.sedris.org/stc/2004/pp/gidi/index.htm" http://www.sedris.org/stc/2004/pp/gidi/index.htm.Safe Software, “FME Suite”,  HYPERLINK "http://www.safe.com/products/fme/index.php" http://www.safe.com/products/fme/index.php.D. Miller, A. Janett and M. Nakanishi, “Environmental Data Modeling: REDM, DREDM, Ontology and Metrics”, Simulation Interoperability Workshop, Paper No. 03S-SIW-132, March, 2003.M. Cooper, “A Mathematical Model of Historical Semantics and the Grouping of Word Meanings into Concepts”, Computational Linguistics, Vol. 31 No. 2, June, 2005.J. Sowa, “Building, Sharing, and Merging Ontologies” (available from John F. Sowa’s website:  HYPERLINK "http://www.jfsowa.com/ontology/ontoshar.htm" http://www.jfsowa.com/ontology/ontoshar.htm).D. Miller, A. Janett, J. Nordstrom and P. Birkel, “Multiple Resolution Terrain Features”, Proc. Simulation Interoperability Workshop, Paper 00S-SIW-101, March, 2000.J. Turner, D. Miller, K. Donovan, R. Moore, S. Gifford, “Position Paper for Source Data Collection – Database IPT”, Northrop Grumman –TASC Contract No. ASTSC-99-003, Nov. 8, 2002.Author BiographiesDR. DALE D. MILLER manages the Bellevue, WA office of the Advanced Simulation Center of Lockheed Martin Simulation, Training & Support (LM STS). He has contributed to the areas of data modeling and data model frameworks, NGA product evaluation, terrain database development for modeling and simulation, distributed simulation, real-time computer graphics, computer image generator design, adaptive filtering, image processing, feature extraction, machine vision, optical character recognition, the residue number system and abstract algebra. During the last five years, Dr. Miller has been involved in ontologies and environmental data modeling. He led the LM STS Environmental Data Modeling (EDM) development, first for WARSIM, then for PEO STRI’s OneSAF Objective System, and most recently for the Army Battle Command Simulation and Experimentation Directorate’s Geospatial Data/Information (GDI) Focus Area Collaborative Team (FACT). Dr. Miller received his Ph.D. in Mathematics from the University of Washington in 1976.DR. PAUL A. BIRKEL is a senior principal scientist for The MITRE Corporation. He currently provides technical support to the NGA in multiple standards areas including registers, feature data dictionaries and catalogues, logical data models, portrayal, and modeling and simulation (M&S), as well as support to external standardization activities in the Digital Geographic Information Working Group (DGIWG), International Hydrographic Organization (IHO), and International Organization for Standardization (ISO). He also provides technical support to the Battle Command Simulation and Experimentation Directorate (BCSE – AGDIMP, GDI FACT), and Army programs (FCS, PD CTIS, ABE). Digital Geographic Information Working Group While in accordance with Joint doctrine Meteorological and Oceanographic (METOC) and Geospatial Information and Services (GI&S) data are considered to be under the purview of the J2 intelligence community, they are also important components of situational awareness in the J3 C2 community. We note, however, that a building constructed to specifically serve as a lighthouse is different than a building that happens to be functioning as a lighthouse at a point in time. We note that the brevity of both the FACC “name” and the EDCS “label” appear to do injustice to their related concepts. We note, e.g., that two molecules of water (2 H20) have markedly different properties than two molecules of diatomic hydrogen (2 H2) plus one molecule of diatomic oxygen (O2). Converting one to the other has militarily-significant impact. Consider that in the scope of the GIG it is reasonable to expect that the different mission perspectives for diverse participants interested in “trees” that may contain snipers, represent camouflage and concealment, result in movement impediments (mechanized and afoot) and may simultaneously represent construction materials for hasty defenses or an abatis results in a significant divergence in opinion regarding what distinguishes a “tree” from a “bush”, or a “wood” from a “thicket”. Once a data item has been collected in accordance with one of these concepts it can not be “transformed” to a correct instance of another. It is possible that the scope of the GIG may be simply much too broad to do so. The EDB IPT was chaired by the director of the U.S. Army Engineer Research and Development Center, Topographic Engineering Center. However we note that substituting dependency on semantic match with the assumption that no two bodies may occupy the same space simultaneously may not prove to be a viable approach to the data fusion challange.