Verification and Validation in an Agile Environment: Implications for Software Quality Assurance and TestingDavid Borowsky, Major USAFJoint SIAP System Engineering Organization (JSSEO)1931 Jefferson Davis HighwayCrystal Mall 3 Suite 1142Arlington, VA  22202703-602-0043 HYPERLINK "mailto:david.borowsky@siap.pentagon.mil" david.borowsky@siap.pentagon.milAlberto CalvoVictor Skowronski, PE, PhDNorthrop Grumman Information Technology55 Walkers Brook DriveReading MA, 01867781-205-7449 HYPERLINK "mailto:alberto.calvo@ngc.com" alberto.calvo@ngc.com  HYPERLINK "mailto:victor.skowronski@ngc.com" victor.skowronski@ngc.comKeywords:Agile Methods; Verification, Validation, and Accreditation, VV&A, Testing; Model Driven Architecture; Quality Assurance; Joint Distributed Engineering Plant (JDEP); distributed simulation; Tactical Battle Management and Command and ControlABSTRACT: The Joint Single Integrated Air Picture (SIAP) System Engineering Organization (JSSEO) has the mission of overcoming critical operational limitations in capabilities for effective Joint Theater Air and Missile Defense (JTAMD). In support of this goal, JSSEO is developing an Integrated Architecture Behavior Model (IABM). The success of this JSSEO program depends on rigorous Quality Assurance (QA) and testing for effective verification and validation (V&V), which must nevertheless be compatible with the realities of adopted MDA and agile practices. Agile methods emphasize short fast development cycles and informal personal communication among the team over formal documentation. However, proper QA and testing require traceability based on written records. Additionally, such documentation becomes critical when it is the foundation of a simulation Verification, Validation and Accreditation (VV&A) process that requires transparency for independent peer reviews and product evaluations by outside organizations. Therefore, JSSEO has had to reconcile the need for formality in QA, testing, and simulation VV&A with the speed and informality of agile methods.This paper describes the IABM development process, JSSEO’s approach to V&V and QA, the challenges the JSSEO QA and Test Team has faced in reconciling the need for formal V&V with the use of MDA and agile methods and how it has resolved these challenges. IntroductionThe Joint Single Integrated Air Picture (SIAP) System Engineering Organization (JSSEO) has the mission of providing the engineering design for the DoD’s Joint Theater Air and Missile Defense (JTAMD) integrated architecture (IA). This architecture will serve as the foundation for giving the Joint warfighter a clear understanding of the battlespace and the confidence to engage air and relevant ground targets within the full kinematic range of US and coalition weapon systems. The SIAP will meet JROC validated Theater Air and Missile Defense (TAMD) and Combat Identification (CID) capstone requirements and support the spectrum of offensive and defensive operations by US, allied, and coalition partners in the airspace within a theater of operations [1]. JSSEO is analyzing current, as well as planned and programmed, Joint Tactical Battle Management and Command and Control (BMC2) capabilities to identify policy and acquisition actions necessary to achieve the IA sufficient to enable SIAP capabilities among theater tactical operating forces. In support of this goal, JSSEO is developing an Integrated Architecture Behavior Model (IABM) (see Reference  REF _Ref60470709 \r \h [9]). The IABM enables the implementation and integration of common processing algorithms across diverse computing platforms, which will then be deployed in Tactical BMC2 systems for greater efficiency and interoperability than is currently possible today. The first increment of the IABM is expected to be delivered and available for integration into deployed systems in Fiscal Year 2005.JSSEO is following a development approach that is based on the Object Management Group's (OMG) Model Driven Architecture (MDA)  REF _Ref60471680 \r \h [10] and on the use of agile methods for modeling and development  REF _Ref60472984 \r \h [1]  REF _Ref60472988 \r \h [6]  REF _Ref60472990 \r \h [7]  REF _Ref60472992 \r \h [8]. MDA emphasizes the use of graphical tools, such as the Unified Modeling Language (UML), and the development of a system design prior to coding. Agile methods emphasize informality in the development process and rapid feedback to ensure that the user’s needs are met.For testing the IABM, JSSEO has adopted the Joint Distributed Engineering Plant (JDEP) Technical Framework for simulations as its primary means for performing required pre-deployment test and evaluation of IABM functionality and performance. JDEP is a Department of Defense (DoD) program that is focused on providing common, reusable sets of distributed simulation components that adhere to industry standards for interoperability, particularly the High Level Architecture (HLA). The JDEP Technical Framework is managed by the Joint Interoperability Test Command (JITC), a component of the Defense Information Systems Agency (DISA). JSSEO is using the JDEP Technical Framework to develop both Hardware-in-the-Loop (HWIL) and constructive simulations to stimulate the IABM and to record the response of the IABM for analysis.  IABM development is therefore dependent on the proper Verification, Validation and Accreditation (VV&A) of these simulations. The use of a simulation, such as JDEP, to test the IABM provides two advantages to JSSEO. Testing can be performed early in the development cycle, even before full functionality has been developed, to identify defects before their resolution becomes costly. Testing with a simulation can also exercise the IABM under conditions more severe than any that might be encountered in actual use. This later advantage is especially important to Verification and Validation (V&V), so that users have assurance that the IABM can perform its function in their environment under any conditions that might be encountered. Validation during a Live Exercise typically can only address a limited set of operational conditions.  Therefore, simulating such environments is the preferred way to provide this assurance.The success of the JSSEO program depends on rigorous Quality Assurance (QA) and testing for effective V&V. The documentation from the V&V process will be particularly important because organizations outside JSSEO must evaluate the IABM prior to its deployment in the field. This V&V must be performed in the context of MDA and the agile practices adopted by JSSEO, however. Agile methods emphasize short fast development cycles and informal personal communication among the team over formal documentation. Proper QA and testing require traceability based on written records. JSSEO has had to reconcile the need for formality in QA, testing, and simulation VV&A with the speed and informality of agile methods.This paper describes the IABM development process, JSSEO’s approach to V&V and QA, the challenges the JSSEO QA and Test Team faced in reconciling the need for formal V&V with the use of MDA and agile methods, and how they resolved these challenges. IABM Development Process and ArtifactsThe MDA describes the creation of a Platform Independent Model (PIM), which uses only concepts from UML and does not incorporate any computing platform- dependent features (such as the Operating System). The PIM cannot be executed, however, until it has been implemented in a specific computing environment. The resulting program is called a Platform Specific Model (PSM). The PSM instantiates the abstractions of the PIM and links them to the resources of the computing environment that are needed by the model. REF _Ref62620523 \h  \* MERGEFORMAT Figure 1 shows the principal activities and artifacts generated by the use of the MDA process for the IABM development. This process is repeated on a fixed interval, or Time Box.  At the end of each Time Box a new version of the model is delivered. The first several Time Boxes used an 8-week cycle, but current Time Boxes are based on longer Time Boxes (nominally 12 weeks).  This iterative, incremental model build approach is intended to generate useful functionality quickly, which can then be verified and demonstrated to users for guidance on future builds.The IABM Development is carried out by three teams:Architecture TeamModel Development TeamTest Team.The Time Box development process starts with Requirements Setting and Analysis & Design (A&D) activities, which are done by the Architecture Team working with Subject Matter Experts (SMEs). Time Box Requirements are documented in the Requirements Description Document (RDD), which is peer reviewed. Test Scenarios are also typically developed at this stage by the Test Team.   SHAPE  \* MERGEFORMAT Figure  SEQ Figure \* ARABIC 1 IABM Development Processes and ArtifactsImplementation of the design is done using the Kennedy Carter iUML © tool and results in the generation of a Platform Independent Model (or PIM). Unit testing is conducted by the developers during this process, followed by integration and limited testing of the various model domains.. From the PIM a Platform Specific Model (PSM) is generated, followed by using automatic C and C++ compilers to generate the executable code. The Test Team then conducts model verification tests at both domain and system levels.  Any generated discrepancy reports are stored and tracked in a defect database for resolution in a later Time Box.The current Time Box process takes approximately 12 weeks, with the architecture work (Requirements Setting and A&D) taking 4 weeks. These are the green boxes in Figure 1. The development work (Implementation and Integration & Test) taking the next 6-7 weeks. These are the gray boxes in Figure 1. The V&V work takes 4 weeks, but overlaps the last 2 weeks of the Integration & Test period to ensure the Test Team understand the Time Box and to minimize duplicative testing. These are the orange boxes in Figure 1. Once the Architecture Team is done with setting the requirements for the ith Time Box, they begin on the (i+1) Time Box.  Thus the Time Box cycle results in concurrent work on three Time Boxes--while the Architecture Team is on the ith Time Box, the Development Team is implementing the (i-1)th Time Box, and the Test Team is testing the (i-2)th Time Box. The end product of the Time Box development and test process is a PIM, which JSSEO will provide to the military Services as well as Industry to incorporate into the development of future combat systems. However, it is not enough to provide just a PIM. JSSEO will also provide the V&V tools and results so the Services and Industry can understand how the PIM was tested and to compare the results they get when the create their own combat—system PSMs and conduct test and evaluation on them.IABM Verification and Validation (V&V) The IABM V&V process is shown in Figure 2. The Test Team that conducts this V&V is organized by the following functions:Test Development: develops cases, which are detailed descriptions of the inputs, interactions and expected results of a test, as well as the test scenarios, which are the actual input/output data.  This group interfaces with the Requirements and Architecture teams, as well as SMEs, to develop both domain and system-level tests for each Time Box. A domain-level test assess the interactions between components of the IABM, whereas system-level tests assess the functionality and performance of the entire model   EMBED PowerPoint.Show.8  Figure 2	  IABM V&V ProcessTool Development: develops the equipment used to execute the tests, based on IABM requirements and feedback from the testers (Table 1).  The main focus is on development of the JDEP Technical Framework test environment.Test Execution: develops and executes test procedures based on the test cases and scenarios for each Time Box. They report test results from both the domain and system-level tests and record any problems in the discrepancy database.There is constant communication (on a daily basis) with the rest of the development team (i.e., Requirements, Architecture, Implementation and Integration) throughout the testing process.  IABM V&V consists of several types of tests to assess functionality, performance, conformance to standards and specific interfaces. The majority of tests are based on constructive (or digital) simulations using the JDEP Technical Framework; with some HWIL and Live exercise tests to provide validation with real-world combat systems.Verification of the IABM is the process of ensuring the model works as the architects and developers intended. It must answer the question “Did they build the thing right?” This is answered primarily through unit and domain-level testing, as well as use of QA methods and a Configuration Management process. Validation is the process of ensuring the model has the functionality and performance necessary to meet requirements. It must answer the question “Did they build the right thing?”  This is accomplished primarily through system-level testing.JDEP Technical Framework simulation tests Almost all of the IABM V&V will be conducted with simulations developed based on the JDEP Technical Framework (see Figure 3). The use of simulations allows testing the IABM under controlled conditions, as well as allowing us to create conditions more severe than will conceivably be seen in actual real-world operations. Figure 3	  JDEP Technical Framework Infrastructure BuildInput data for the JDEP-based simulation tests consists of track files, which specify the locations over time of various combat platforms. The files come from three sources. The first is based on the JSSEO Common Reference Scenarios (CRS). The scenarios provide an operationally representative employment of multi-Service combat systems and enemy forces, based on Defense Intelligence Agency (DIA)- threat data and approved by representatives of DoD operational commands [12]. The second source for track files come from the potential users of the IABM, who provide data from live exercises or other developmental efforts.The third source are “engineering track files”, developed by the Test Team in the proper format for JDEP testing. These track files may not be operationally realistic but they exercise the IABM in ways that stress weak points in its algorithms.  Simulation output is sent to a display for viewing by testers and to a database from which it can be extracted and analyzed. The output of each simulation will be analyzed using the IABM Performance (SIAP) Metrics [3]:Completeness: The air picture is complete when all objects are detected, tracked and reported.Clarity: The air picture is clear when it does not include ambiguous or spurious tracks.Continuity: The air picture is continuous when the track number assigned to an object does not change.Kinematic Accuracy: The air picture is kinematically accurate when the position and velocity of each assigned track agree with the position and velocity of the associated object.ID Completeness: The ID is complete when all tracked objects are labeled in a state other than unknown.ID Accuracy: The ID is accurate when all tracked objects are labeled correctly.ID Clarity: The ID is clear if no tracked object is labeled with conflicting ID states.Commonality: The air picture is common when the assigned tracks held by each participant have the same track number, position, and ID.These metrics have been formally defined by JSSEO and validated by the JTAMD community, and therefore form the basis for the validation of the IABM.In between the input files and the assessment of the output data is the simulation test environment. JSSEO is working with JITC to develop a set of re-usable digital simulations called the Infrastructure Builds (IBuilds), as well as planning and executing a HWIL simulation with the IABM [4]. As seen in Figure 3, the IBuilds consist of separate sensor, communications, navigation and Identification Friend or Foe (IFF) federates. The CRS is published to the other federates by the CRS-Driver. Future upgrades to the federates will add fidelity and capability to the IBuilds. This federation of digital simulations will undergo a VV&A process in parallel with the IABM development. This is a formal process [5], which includes a documented Test Plan/V&V Plan, Test Readiness Report/V&V Report and Accreditation Authority (who is the JSSEO System Engineer).The next level of testing for the IABM uses the JDEP Technical Framework with HWIL simulations of weapon systems. Such testing supports the validation aspect of IABM testing because the stimulation comes from actual tactical system hardware or Service-accredited sensor simulations.  The federation which will be used for IABM testing will include HWIL simulations of the E-2C, PATRIOT and AEGIS combat platforms.  This federation will also undergo a VV&A to ensure it can be used to assess the IABM maturity and real-time performance.These federations can be flexibly and cost-effectively altered to introduce proposed changes to systems and evaluate their effects in the integrated systems environment. The next level of testing for the IABM uses the JDEP Technical Framework with Hardware-in-the-Loop (HWIL) simulations of weapon systems. Such testing supports the validation aspect of IABM testing because the stimulation comes from actual tactical system hardware or Service-accredited sensor simulations. Table  SEQ Table \* ARABIC 1 Sample Tools and Components being used for V&V TestingTool/ComponentFunctionDescription Common Reference Scenario (CRS)Input dataThe CRS are sets of input data that define operationally realistic scenarios. Target_GeneratorGenerates Input DataThe target generator creates ground truth tracks from a user-friendly specification CRS_ABGenerates CRS format filesCRS_AB converts the output of target generator to CRS format BGSIMGenerates input dataCreates radar measurement, IFF, and Link-16 input data. The data cannot currently be used in JDEP but might be with different federates Common Reference Scenario Driver (CRSD)JDEP SimulationCRSD publishes ground truth data read from CRS format files First Order Sensor (typical of JDEP simulation federates)JDEP SimulationConverts ground truth data to sensor detection and measurement HLA ResultsJDEP SimulationCollects data published during the simulation and stores in databaseHLA EvalExtracts output dataExtracts data collected by HLA Results and formats it for analysis programs Performance Evaluation Tool (PET)Analyzes dataPET computes performance metrics listed in Appendix Live ExercisesThe final type of testing for V&V of the IABM involves using the IABM real-time during a live exercise.  The IABM will share one or more sensors with an operational system participating in the exercise. The parallel operation of the IABM and a live system will allow comparison of the behavior of the two systems. It will also demonstrate IABM capabilities to system operators and developers.3.3  Tests by other OrganizationsJSSEO has also coordinated with JITC so they will conduct Independent V&V assessments of the IABM, as well as Military Standards conformance testing. The Navy, through their Program Executive Office Integrated Weapons Systems (PEO IWS) office, has established a test and evaluation program of the IABM using their Open Architecture Test Facility. JSSEO has also established a risk reduction program where interim Time Boxes are provided to several contractor evaluation teams to assess the model’s performance and the overall development and test program. Figure 4 Sample QA Metrics for IABM Development4.0 The Role of QAThe role that QA plays in the IABM agile process is also less formal than with traditional methods. QA’s role focused on four areas through the first year of IABM development:Selection and reporting of meaningful QA metrics to JSSEO management for monitoring the overall effortTailoring and facilitating peer reviews that do not impose a heavy documentation and schedule burden on the agile processCapturing Lessons Learned for continual process improvement and identification of risk areasContinually assessing the development process in order to institute process improvements.4.1 QA MetricsJSSEO management relies heavily on QA metrics to monitor progress and the speed of the development process. Metrics that reflect the size, effort and quality of the model are being developed and data to quantify these metrics are extracted from the PIM. An initial set of Quality Metrics has been established for the IABM development effort and are listed below and shown in Figure 4. Model Size: measured by the number of iUML artifacts created (i.e., domains, classes, attributes, methods, lines of code)Effort: measure the staff hours devoted to developmentDefect Density: measure the number of defects per line of codeSoftware Size: number of physical and logical lines of C or C++ code generated by the iUML tool.Performance: number of measurements per second, new track starts per unit of time, and other metrics being defined to measure capacity, throughput, and processing performance.New metrics to measure performance of the PIM, such as capacity of the model to process sensor measurements and form tracks, reliability (i.e., rate at which software failures occur), and other metrics are being defined based on the following criteria:What goals does the model need to achieve?What set of objectives does management need to manage the development?What questions need to be answered to give management the confidence that the model is meeting these goals?What relevant metrics can be easily and quickly measured to provide indicators of progress toward achieving the stated goals?4.2  Peer ReviewsPeer reviews have proven to be an effective mechanism to ensure quality in the software product. For the IABM agile process, peer reviews take a slightly different form. They are conducted on a more ad-hoc, less formal basis than reviews conducted in more traditional software development programs.  Reviews are conducted at three points in the Time Box cycle: Requirements Definition Documentation (RDD) reviewsDesign ReviewsCode Reviews.These reviews are conducted by members of the software development team and feedback is offered at the time to the author of the artifact being reviewed.4.3  Lessons LearnedLessons Learned are captured at the end of each Time Box. These lessons relate to development process, work environment, test, requirements and other areas. They are captured by meeting with each of the development teams. The lessons are documented and briefed to JSSEO management along with improvement recommendations. These lessons become an effective feedback mechanism for JSSEO management action.A case in point was the determination of the appropriate length of the Time Box. At the start, the duration of a Time Box was set at 4 weeks. However, after the fourth Time Box, it became clear that the concurrency of the activities (requirements setting, architecture definition, design, coding and integration and test) along with the allocation of staff to these activities became problematic, resulting in near chaotic conditions. The Time Box schedule was re-defined by lengthening it to 8 weeks and creating two staggered 4-week segments within each Time Box comprised of an Analysis and Design (i.e., requirements and architecture definition) segment and an Implementation (model development, coding and integration and test) segment. By staggering these two segments a new version of the model is delivered nominally every month, and the scheduling of tasks and allocation of staff to these tasks become smoother. Modifications to this process continue.  More recent Time Boxes have increasing complexity and the Time Box process is now nominally 12 weeks. QA Function as CatalystWith an agile approach to development, the traditional imposed discipline of detailed schedules, formal reviews, and comprehensive documentation at every step are relaxed. Instead, needed communication among the team occur much more informally and frequently as the situation dictates. This added flexibility allows the team to act more dynamically and adjust quickly as circumstances change. This flexibility, however, comes at a cost in that management can easily lose track of what everyone is doing and where problems may be developing.  The QA function at JSSEO has played a role of facilitating communications across the team by constantly monitoring all activities at a high level and looking for symptoms of inconsistency in understanding or action. QA often can help resolve such issues by getting different team members together to clarify their views or can raise more difficult issues for additional management attention and resolution.  Most notable was the early identification of a disconnect between the Architecture Team and the Test Team during the requirements definition phase.  Testers simply were not involved in the process, resulting in confusion and differing interpretations of the requirements between the architects and the testers.   This problem was addressed by dedicating two testers to work with the Architecture Team throughout their process to ensure testing concerns were considered and that the requirements would have a common interpretation between the two teams.V&V ChallengesChallenges from MDA DevelopmentIn Model Driven Architecture (MDA)  REF _Ref60646254 \r \h [10]  REF _Ref61669145 \r \h [11], subject matter experts (SME) describe how an application must function to the modeler, who then encodes this information in a model using the Unified Modeling Language (UML). For an agile methodology, requirements are generally stated as Use Cases  REF _Ref60472990 \r \h [7]. Use Cases differ from classic requirements in that they describe how the system responds to some stimulus rather than specifying a capability of the system.Ideally, the requirements that drive this process would also drive the development of test cases and scenarios. However, they are usually at too high a level to be used as the only input for test case generation  REF _Ref60472990 \r \h [7]. The Test Team has found that they generally need assistance in interpreting a requirement in order to develop a test cases and  scenarios. JSSEO has made a number of SMEs available to the test group to aid in creating test scenarios.JSSEO has also found that some requirements do not readily fit into the stimulus-response model. Performance requirements, in particular, are better represented in the traditional manner, i.e., “The system shall manage a minimum of X tracks.” A mixture of the two types of requirements statements may be the most useful method for capturing the requirements under MDA.A secondary problem of MDA is the lack of pre-existing infrastructure in the PIM. For example, the Action Specification Language (ASL) cannot parse a string variable. This type of infrastructure, which already exists in more mature environments, must be designed and built in the MDA environment. The result is that the early development cycles were required to concentrate on building infrastructure and there was a lack of functionality that could be tested. This lack of functionality in early releases is more of a challenge when combined with agile methodologies, which emphasize customer feedback. With little functionality, there is little for customers to review.Challenges from Agile MethodologyAgile methods emphasize the following four values  REF _Ref60472988 \r \h [6].Individuals and interactions over processes and toolsWorking software over comprehensive documentationCustomer collaboration over contract negotiationResponding to change over following a plan.On the surface, there appears to be a conflict between the underlying values of agile methodology and the needs of verification and validation (V&V). V&V must be repeatable. A process can be made repeatable, but interactions between individuals can rarely be repeated exactly. Software is not considered to be “working” under V&V until it has been proved to work. Furthermore, the proof must be documented, so in some sense, the documentation is a requirement for working software, and is also needed by the user for independent V&V.Customers can and do collaborate with developers in the V&V process. The collaboration needs to be more formal than usually seen in agile methodologies, however, because V&V requires greater emphasis on documentation than agile software development.Following a plan is important with V&V. This insures that all the requirements are tested and that they are tested in the manner necessary to validate the software. Changes carry the risk that a vital check may be omitted.The conflict does not completely disappear but it becomes less acute when validation of the model is viewed as one of the products of the JSSEO effort. Agile processes are now applied to the development of the V&V of the IABM. This turns out to be an advantage, because the test framework is being developed at the same time as the IABM. Agility allows the test group to modify the framework as the requirements change. The process being used to develop the JDEP Technical Framework federates is also a challenge to the agile approach. This is partly because the federates are not being developed “in-house” by JSSEO but through JITC.  Maintaining close communication across agency boundaries is more difficult than doing so within a single organization. There is also a question of different agency priorities; JITC’s focus is on the reuse of software being developed. Since JTIC conducts tests using many remote sites rather than the local network that JSSEO will use, the requirements for the federation being developed are different for JDEP than they are for JSSEO. A formal method (a Configuration Control Board) is being implemented to insure that the requirements of one agency do not crowd out those of the other without both agencies being aware of it and approving it. Agile methods are therefore being applied to this process by allowing the JSSEO test team to adapt tools and develop test cases as necessary. More traditional methods are being used, however, where there is a need for traceability or clear definitions of responsibilityTest Team IntegrationTesters must be carefully integrated into an agile project  REF _Ref60472992 \r \h [8]. Testing occurs earlier in the development cycle than it does under more traditional methods, and testing is done as part of each development cycle.  Additionally, it has taken time to build a cohesive, multidisciplinary team.  The Test Team members typically have experience in one or two areas such as modeling, coding, testing, sensors or Tactical Data Links, but need to be knowledgeable about all of them to be effective. The use of short development cycles is a particular problem for the testers. As each cycle starts, developers request new test cases from the test group to test the functionality being developed in the new cycle. This puts the test group into a reactive mode, where they are designing and building test cases as new design features emerge from the developers. This can only be partially solved by reusing test cases. Most of the time, testing new functionality requires new test cases. A better solution has been to involve individuals from the test group early in the definition of new functionality for each development cycle. This allows the test group to develop the necessary test cases in parallel with the development of code, rather than reacting to it.CommunicationThe emphasis on oral communication over written documentation has also created some problems for the test group. In some cases, the requirements for a function exist only as a conversation between a designer and a coder. Not being a party to the conversation, the testers cannot write tests for these requirements.  . Recently, the QA Team has tried to use a process of more formal documentation to insure that requirements are properly captured, starting with conducting Peer Reviews of the RDD and including members of the Test TeamAvailability of Subject Matter ExpertsA key principle of agile processes is to use early and frequent customer feedback to check the correctness of design decisions  REF _Ref60472990 \r \h [7]. The IABM is at an early stage of development and has no formal customers. In these circumstances, the test group becomes a proxy for the customer  REF _Ref60472984 \r \h [1]. This requires testers to think beyond the formal requirements and imagine how the end users will actually use the IABM.The shift from tester of requirements to customer proxy is not easy to make. Testers come from a variety of backgrounds and few have experience as users of systems like the IABM. Contact with subject matter experts (SME) is required to help the testers learn how the system is used. Unfortunately, the ability of JSSEO to provide time with SMEs has been uneven. SMEs are required for most stages of development and their time must be managed between the needs of many groups.  JSSEO has begun several risk reduction efforts that will address this issue, such as interim Time Box deliveries to outside organizations who are willing to conduct testing in parallel with the JSSEO efforts.Challenges due to Program ComplexityThe nature and scope of the IABM development program is highly complex. This complexity arises from various factors:The fact that IABM attempts to capture the behavior of a “system of systems” that does not exist physically, but it is created when all the constituent systems (e.g., Patriot, AEGIS, AWACS) play together in a theater of operations, makes testing of the IABM very difficult and risky. The fact that the JSSEO “stood-up” a software development group composed of Government and industry personnel from various backgrounds and skill sets, establish a new computing network and software development environment from the ground up, establish QA and Test processes along with documentation, acquire and train the team on UML and more importantly nurture a collaborative work environment presents many challenges for the test and QA activities. Additionally, there is the concurrency of the IABM development and test process, the technical complexity of achieving a SIAP capability, and the political environment facing a DoD-wide, Joint Service development activity.As a result of these factors, a significant degree of “chaos” at the outset (during the first few months) was experienced, processes were created and dropped, continuous adjustments were made to these processes, QA and V&V plans were created, reviewed and modified as processes were changed and lots of brainstorming sessions were held among the staff to ensure fluid communications of ideas.  In response to this, the JSSEO organizational structure was changed to allow for a dedicated QA lead (Government person) and adding full-time QA staff.Challenges to QA FunctionOur experience to date has shown various impediments for the effective application of the QA function in an agile environment. The principal impediments experienced are:Getting the deliveries on time is paramount. Modelers are under a time-driven process facing deliveries at the end of each Time Box that little else gets attention, i.e., peer reviews, process, documentation, lessons learned, etc. Use of light, ad-hoc processes creates a perception among the developers and management that there is no or very little time to perform formal peer reviews of model artifacts, which is one of the principal tools in the QA discipline.Lack of pre-established, validated requirements  places a greater demand on the development team, and when coupled with the concurrency of activities (software requirements, architecture design, coding, integration and test) in the Time Box environment, further impede the formal accomplishment of QA activities. The stand-up of a model development organization from the ground up, coupled with the technical complexity of the task creates “chaos” at the outset, with lots of trial and error, and a continuous adjustment of processes. QA’s role in this initial phase is more of a facilitator and catalyst than its more traditional role of insuring established processes are followed and product quality is assured.ConclusionThe JSSEO is using agile methods to develop the IABM, which has impacted the QA and Test methods in several ways.  The JSSEO experience over the first year of IABM development and testing indicates some limitations of agile methods in highly complex projects. In general, the agile methodology can be consistent with a formal Verification and Validation process. It is necessary to focus on the products of that process, however, in order to understand how an agile process can be tailored to provide proper V&V. Agile methods can be used at most stages of the process, from developing tools and test cases to analyzing results. The key is flexibility and constantly adjusting processes, products and the organizational structure as the project matures.References“Joint Single Integrated Air Picture System Engineering Organization (JSSEO) Management Plan”, 7 January 2004Rothman, J., “Testers Shine on Agile Projects,”  HYPERLINK "http://www.stickyminds.com" http://www.stickyminds.com, November 17, 2003 JSSEO Technical Report 2001-001, Single Integrated Air Picture (SIAP) Attributes, June 2001 and JSSEO Technical Report 2001-003, Single Integrated Air Picture (SIAP) Metrics Implementation, October 2001Youmans, B., Talbot, J. and Karoly, S., “Integrated Air and Missile Defense Distributed Simulations: Reaping the Benefits of Lessons Learned”, SISO Paper 04S-SIW-0511,March 2004 JSSEO Technical Report 2003-006, “VV&A Guide for M&S”, April 2003Williams, L. and Cockburn, A., “Agile Software Development: It’s about Feedback and Change,” IEEE Computer, Volume 36, Number 6, June 2003, pp 39-43.Lippert, M. et al., “Developing Complex Projects using XP with Extensions,” IEEE Computer, Volume 36, Number 6, June 2003, pp 67-73Cohn, M. and Ford, D., “Introducing an Agile Process to an Organization,” IEEE Computer, Volume 36, Number 6, June 2003, pp 74-78“IABM Configuration 05 Description Document,” Version 1.0X.1, dated November 18, 2003Object Model Group web site,  HYPERLINK "http://www.omg.org" http://www.omg.org Mellor, S. and Balcer, M., “Executable UML, A Foundation for Model-Driven Architecture,” Addison-Wesley, 2002Byrd, E., Douthett, R., and Ellermann, A., “Common Reference Scenarios (CRS): Cornerstone of Single Integrated Air Picture (SIAP) Capability Analysis,” Proceedings of the Spring 2004 Simulation Interoperability Workshop, Arlington, VA, April 2004AcknowledgementsThe authors would like to thank Grady Campbell from the Software Engineering Institute for his comments to this paper, and Harry Ampagoomian, Test Director for his hard work in defining and establishing the Test organization in JSSEO. We also like to acknowledge Mr. Steve Karoly, Chief of Analysis and Test Division, and Captain Jeffrey Wilson, Technical Director, for their guidance and support.Author BiographiesMaj. DAVID BOROWSKY, USAF is Chief of Modeling and Simulation for the Analysis Division of the Joint Single Integrated Air Picture (SIAP) System Engineering Office (JSSEO). His specialty is the development and fielding of military C4ISR systems.  He has a B.S. in Mechanical Engineering from Rensselaer Polytechnic Institute and a M.S. in Aerospace Engineering from the University of Texas at Austin.ALBERTO CALVO is a senior operations research analyst in the Defense Enterprise Solutions Division of Northrop Grumman Information Technology. He has extensive experience in developing and testing simulation models of military logistics applications. Mr. Calvo earned a B.S. in Industrial Engineering from Northeastern University and an S.M. in Operations Research from M.I.T.  DR. VICTOR SKOWRONSKI is a senior engineer in the Defense Enterprise Solutions Division of Northrop Grumman Information Technology. Prior to joining Northrop Grumman Information Technology in 1996, he did research in solid modeling at Rensselaer Polytechnic Institute, where he also earned a PhD in Computer Engineering. Victor earned a M.E. and a B.E. in Electrical Engineering from Stevens Institute of Technology. He is a licensed Professional Engineer in New York and Massachusetts. PAGE 13PROCESSARTIFACTSPeer ReviewsSystemTestingTest CasesDefect DBPSMTime Box nthDeliveryPIMPSMTest ScenariosCollaboration DiagramsClass DiagramsUseCasesRDDV&VIntegration & TestCode Generation (automated C, C++)Implementation(Development Teams)Analysis and Design(Domain Architecture)RequirementsSettingRequirementsSettingAnalysis and Design(Domain Arch.)Implementation(iUML tool)Code Generation (automated)Integration & TestV&VRDDUseCasesUnit TestingUnit TestingIntegrationTesting