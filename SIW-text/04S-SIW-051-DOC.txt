Integrated Air and Missile Defense Distributed Simulations: Reaping the Benefits of Lessons Learned04S-SIW-051Elisabeth A. YoumansSystems Planning and Analysis, Inc.4900 Seminary Rd., Suite 400Alexandria, VA703.578.5696eyoumans@spa.comJayne E. TalbotVirtual Technologies Corporation5510 Cherokee AvenueSuite 350Alexandria, VA703.333.6231jtalbot@virtc.comSteve KarolyJoint Single Integrated Air Picture System Engineering Organization (JSSEO)1931 Jefferson Davis HighwayCrystal Mall 3, Suite 1142Arlington, VA703.602.0043, x204Steve.Karoly@Siap.Pentagon.milABSTRACT: Distributed modeling and simulation events like those conducted within the Joint Distributed Engineering Plant (JDEP) Technical Framework are becoming more widely used for a variety of defense applications.  From Single Integrated Air Picture (SIAP) engineering-level performance assessment to time sensitive strike to large-scale simulated wargaming, warfighter capabilities are being assessed using a distributed simulation environment.  Although individual tests are unique, each test event shares a core process that can benefit from lessons learned from other distributed tests.  This process includes defining requirements, planning, integration testing, verification and validation, maintaining configuration management, on-site test execution, follow-on analysis, and reporting. Other lessons that address test configuration issues, such as the strengths and limitations of High Level Architecture (HLA) vs. Distributed Interactive Simulation (DIS), can help test planners choose the right simulation components and tools for their own applications.  Proper training of existing staff and knowing when to bring in the external expertise can also increase the likelihood of a successful test.  The goal of applying these lessons is to develop a high-quality, robust, and scalable Integrated Air and Missile Defense (IAMD) distributed simulation environment within the JDEP Technical Framework.  Maturing this distributed simulation capability will enable the Services to more cost-effectively field IAMD-capable systems that meet JROC-validated operational requirements in a timely manner.This paper reviews lessons learned from distributed simulation tests conducted in 2002-2003 and proposes a process by which future testers can benefit from lessons learned to avoid repeating what can sometimes result in costly mistakes and delays. Along with lessons learned, the paper discusses the importance of recognizing and capturing the leave-behinds from a test to prevent reinventing the wheel for follow-on test events.OverviewThe Joint Single Integrated Air Picture (SIAP) System Engineering Organization (JSSEO) mission is to institute a disciplined joint system engineering process to address and resolve interoperability problems in the implementation of the Joint Data Network (JDN).  They are also responsible for developing the SIAP component of the Theater Air Missile Defense (TAMD) Integrated Architecture (IA), hereafter referred to as the IA.  The IA is expressed as Operational Views, System Views, and Technical Views, drawing upon the DoD Architecture Framework as its foundation [1].  To realize the IA, JSSEO is developing an executable model, hereafter referred to as the Integrated Architecture Behavior Model (IABM), using a Model Driven Architecture® (MDA®) approach that unambiguously demonstrates Joint Battle Management Command and Control (BMC2) functionality and dynamic system behavior among all TAMD nodes in the battlespace [2].As part of the system engineering process, the IABM will undergo a rigorous test and evaluation process to ensure that the IABM and the algorithms contained within are operationally effective and suitable for both distributed system (networked) and standalone system operations.  It will also be verified to ensure it meets JROC-validated capstone and operational requirements.  To achieve these goals, JSSEO, in collaboration with Defense Information Systems Agency (DISA), Joint Theater Air Missile Defense Organization (JTAMDO), Joint Interoperability Test Command (JITC), and Director, Operational Test and Evaluation (DOT&E), is developing a high-quality, robust, and scalable Integrated Air and Missile Defense (IAMD) distributed simulation environment that will have sufficient fidelity to address developmental and integration issues while simulating the real-world operational environment acceptable to the operational test community.   This test environment, called the Infrastructure Build or IBuild, is based on the Joint Distributed Engineering Plant (JDEP) Technical Framework. It will use operationally representative Common Reference Scenarios (CRS), Service-approved operational models of sensors and weapon systems, and high-fidelity models of communications terminals and environmental effects. In keeping with JDEP principles, the components of this environment will have applicability to a wide range of distributed simulation environments, and therefore will be reusable beyond JSSEO analyses.This paper discusses how lessons learned from JSSEO’s use of JDEP Hardware in the Loop (HWIL) analyses have influenced the development and testing of the IBuild modeling and simulation environment.  This paper will present some of the differences between the IBuild environment and the JDEP tools used in JSSEO’s HWIL events. It is the authors’ intent to show how the IBuild will evolve to meet the needs of JSSEO’s IABM testing efforts and become seamless with the JDEP HWIL test environment.The remainder of Section 1 provides background information on JSSEO’s mission and the JDEP Technical Framework.  Section 2 provides a brief discussion of the IABM and the IBuild development. Section 3 describes the JDEP HWIL analysis efforts that have been conducted to date and those underway and in the planning stages.   The process by which lessons are captured and specific lessons learned are addressed in Section 4.  This section also discusses how lessons are applied to subsequent analysis efforts and to the IBuild development.  Section 5 identifies the significant leave-behinds from the JDEP HWIL efforts.  The final section summarizes these findings and discusses the challenges in moving to a distributed modeling and simulation testing environment.JSSEO ApproachDahmann and Wilson [3] provide an excellent summary of the history and rationale behind JSSEO’s approach to improved tactical BMC2 capability within the aerospace warfare mission area.   In their paper, they describe the need to “build a complete and correct, prescriptive model of the tactical battle management and command and control functionality for aerospace warfare.”  It is JSSEO’s intent that this model be used not only for improved communication to industry on what is required from such a system but also to support objective analysis of the correctness and completeness of the model itself.  To achieve JSSEO’s goals, a MDA® approach is being used.  Details on this approach can be found in [3].JDEP BackgroundJDEP is a DoD initiative created to support system engineering of distributed systems. It is intended to support developers who engineer interoperability into systems, testers who evaluate interoperability among systems, and warfighters who assess operational capabilities. JDEP provides resources in the form of a standard technical framework, reusable simulation tools, and a standard development process to facilitate the development of federations.   As in many initial JDEP applications, the JDEP Technical Framework was used in conjunction with HWIL facilities to enable JSSEO to conduct system analysis, integration, and testing of fielded systems.   In addition to analyzing existing systems, it is JSSEO’s intent to use JDEP to address the challenges of engineering complex distributed systems early in their life cycle. JSSEO intends to examine the impact of design and development options before making costly implementation decisions.  To do so, JDEP will need to provide the resources to stimulate software in the loop (SWIL) or digital system representations, like the IABM.  Upgrades to the JDEP Technical Framework are underway to support the these requirements and JSSEO is playing a major role in the development and testing of the improvements to ensure that the stimulation environment is sufficient for testing the IABMCentral to JDEP is its standardization of a common technical framework which allows for the reuse of components and capabilities from one event to another.  The JDEP Technical Framework is based on the High Level Architecture (HLA) as well as open industry standards and standards-based commercial software and tools.  Further details on the history and JDEP vision can be found in [4].IABM Development and TestingJSSEO’s test and evaluation strategy is to “test for success;” that is, to provide tools to developers and program offices that will enable systems to pass Operational Test and Evaluation (OT&E) the first time.  In order to accomplish this, one of JSSEO’s goals is to improve the JDEP Technical Framework to provide engineering fidelity for Development Test and Evaluation and realistic operational conditions for OT&E preparations.  Another goal is to provide a certified IABM to the developers and program managers. This section provides a top-level overview of the development of the IABM, the JDEP Technical Framework improvements, and briefly discusses conformance and operational testing of the IABM.IABMIABM development is being conducted at JSSEO using an iterative, incremental development method or time box approach.  Time box testing is an iterative test process that uses test cases based on known IABM performance requirements. Executable code containing a prescribed set of joint tactical battle management command and control functionality is delivered with each time box. Service Subject Matter Experts (SMEs) evaluate the test findings and work with the test teams to address any issues they identify.  Changes are then incorporated and tested in the next time box. IABM functionality will be released for Service testing and used at the end of FY05.  This release version is designated as Configuration 05. Expanded capabilities will be available in the Configuration 07 IABM release. The Services are working with IABM developers to ensure a smooth integration process for Configuration 05 and Configuration 07 deliveries of the IABM. JDEP Infrastructure BuildAs mentioned in Section 1.2, JSSEO is involved in the program to provide the necessary improvements to the JDEP Technical Framework with the specific goal of providing a realistic test environment for the IABM.  This program known as the Infrastructure Build effort or IBuild uses a phased approach to develop new capabilities.  Upgrades to the JDEP Technical Framework will include the addition of specific components, or federates, that represent communication systems such as radios and Link 16 networks, sensors such as a representation of the AN/SPY-1 and the AN/TPS-59, and environmental effects such as multipath or absorption. Another significant element of the IBuild is the development of a basic component, called the model engine, used as the underlying structure for the new federates.  The model engine provides an interface to the HLA environment to support rapid integration of new models into the IBuild. It is an attempt to “hide” infrastructure integration as much as possible from the model developer so that time is not spent on integration rather time is spent on refining or developing the model.  Tools, such as graphical user interfaces, auto code generators and flexible FOM interfaces, are part of the model engine and facilitate in rapid integration into IBuild. An initial capability, IBuild 001, has been delivered and tested.  Figure 2.2 shows IBuild 003 capability planned for completion in FY04.  It is the intent that these new components be available to the distributed simulation community to use in other test events.   SHAPE  \* MERGEFORMAT Figure 2.2  IBuild 003+ Federation Additional features, federates and tools are being added in a phased development approach. Ultimately, IBuild will be a federation where the IABM is the system under test (SUT) and the environment is realistic enough to evaluate the IABM functionality.  Lessons learned in the IBuild development, though still in progress, are presented along with the lessons learned from the other federation development activities on which JSSEO has embarked.Conformance and Operational TestingAs described above, the JSSEO goal is to first test using the JDEP Technical Framework to ensure that the IABM and the algorithms contained within are operationally effective and suitable for both distributed system (networked) and stand-alone system operations.  This would then be followed by operational testing.  The objective is to ensure the results of these testing efforts are consistent.   To achieve this goal, the JDEP Technical Framework must have sufficient fidelity to assist the IABM in acquisition program development and integration engineering, and must be capable of simulation the real-world operational environment with the fidelity acceptable to the Operational Test (OT) community.  Success depends on coordinating IABM development with JDEP Technical Framework and infrastructure improvements, keeping the OT community involved in JDEP Technical Framework and infrastructure improvement planning, and working with Service acquisition programs to identify appropriate candidates for IABM implementation and JDEP Technical Framework testing. The Joint Interoperability Test Command (JITC), in coordination with JSSEO, is supporting the JDEP Technical Framework development for conducting IABM conformance testing. This support includes planning, documenting, and standards conformance and verification and validation (V&V) testing of the computer program implementation derived from the IABM targeted for the JDEP environment. Conformance to MIL-STD 6016 will be executed for Configuration 05 and Configuration 07 releases of the IABM.  After the IABM Configuration 05 release receives conformance certification, Service program managers will implement it into their combat systems’ tactical computer programs.  The resulting computer program will contain that systems’ complete set of functionality comprising the common Joint functionality provided by the IABM as well as the additional system-specific functionality not provided by the IABM.  The JDEP Technical Framework is expected to be used extensively to provide a realistic test environment that supports Industry integration efforts.  After sufficient development testing is completed, Operational Test Agencies (OTAs) will conduct OT events for final graduation testing.  Test Activities, Past, Current and Proposed In an attempt to characterize the problems with the generation of a SIAP for BMC2, JSSEO planned a series of HWIL analyses using the JDEP Technical Framework to gather performance information about the individual systems participating in the generation of a SIAP.  The first three analyses are referred as “pilot” events because of the anticipated learning process with the application of JDEP to these analyses.  In each event, the analysis objectives are the same and a different HWIL system is placed under test.  These analyses were done in a standalone mode such that all components participating in the event were co-located. The pilot events serve to collect important system data as well as provide stepping stones toward more complex analyses requiring more complicated test architectures.  JSSEO plans to integrate all three systems in one analysis event which will require that the JDEP Technical Framework be distributed across the three HWIL facilities.  Planning for this event has begun and the event is to take place in late fiscal year 2004.E-2C HWIL Pilot AnalysisIn November 2002, the E-2C System Testing and Evaluation Laboratory (ESTEL) conducted the first HWIL pilot test using the JDEP Technical Framework The E-2C mission computer (MC) served as the system to be characterized or SUT. The E-2C was represented by the Group II (C) variant, commonly known as the Hawkeye 2000.  The supporting federation included a scenario driver of red and blue aircraft, the E-2C radar simulation/stimulation suite, an IFF response server for blue aircraft, and a simulated Link 16 network.  There were several objectives for this test event. The first primary objective was to examine the sensitivity of injected translation and rotation biases in the E-2C radar and platform navigation models on E-2C SIAP performance.  The second primary objective was to evaluate the sensitivity of host-to-host time synchronization biases on E-2C SIAP performance.  As mentioned above, a secondary objective was to evaluate the use of the JDEP Technical Framework as the JSSEO standard infrastructure for real time systems analysis.  A description of the test [5] and the follow-on lessons learned [6] provide detailed information on this particular test event.PATRIOT HWIL Pilot AnalysisIn October 2003, the Software Engineering Directorate (SED), in support of the Lower Tier Project Office for PATRIOT, completed a similar pilot test.  While the objectives were the same as the E-2C pilot event, some details of the test event differed, such as the inclusion of theater ballistic missiles (TBMs) in the scenario and the use of higher resolution ground truth air track data. The common JDEP components, processes and framework evolved during this event to address these differences.  Lessons learned during the E-2C pilot event also factored into the changes made in the JDEP components and processes.  As a result, the JDEP Technical Framework continued to mature and the “leave behinds” were greater.  This paper captures some of the lessons that are being applied to subsequent analysis events using JDEP.AEGIS HWIL Pilot AnalysisLockheed Martin of Moorestown, NJ, in conjunction with Northrop Grumman, is working with JSSEO to conduct the third HWIL pilot test.  The SUT is the AEGIS combat system and the test objectives remain the same as the E-2C and PATRIOT test events.  They are in the final stages of the planning and expect the runs for record to be executed in March 2004.  As with the PATRIOT pilot, common tools and components have been carried from the E-2C and PATRIOT test configurations. This paper will attempt to capture some early lessons from the planning phases of this pilot event.Combined HWIL AnalysisJSSEO has begun the planning for their analysis in which each of the HWIL facilities will be networked and a series of analyses will be performed on the systems as they work together.   This event is referred to as the “combined event”.  JSSEO has several objectives for the combined event. Initially, they intend to rerun the pilot tests in a distributed mode to establish a baseline for each system as it is fielded today.  They will then test the integration of a new algorithm into each combat system, referred to as Corr/Decorr.  They intend to evaluate the affects of this proposed algorithm in each system as it operates in a distributed mode as they would on a battlefield.  The final objective of this test event is to add the IABM into the mix of HWIL systems running in a real time environment.  This is a significant step in the testing process of the IABM.  This objective will server to further the IABM maturity rather than collect data for specific analysis goals.The combined event objectives require the JDEP Technical Framework to operate in a distributed mode by networking the remote labs together and running the HWILs concurrently so that their interactions with each other can be measured and evaluated.  With that, the distributed requirement brings more complexity in the planning, development, testing, and analysis phases of the JDEP federation development process (FEDEP).  Lessons learned from the standalone pilot events have had a significant impact on the planning process for this combined event. Lessons Learned Application ProcessThe process for capturing lessons learned should involve representatives from each participating organization and should cover each step in the FEDEP including the post event activities.  The process that was used for E-2C is that lessons were captured throughout the planning, development and testing process.  Two sessions were set aside after the runs for record were completed.  In the case of the E-2C, a single day was divided in half.  For the PATRIOT event, it was done over several days.The first session was reserved for brainstorming only.  The time allotted was divided by subject area.  In the case of the E-2C event, it was done loosely by the FEDEP process as presented herein. Representatives from each organization assembled either in person or by phone.  It was important that this time be reserved to get the lessons experienced by all down on paper whether all in the room agreed or not. No debate or “word-smithing” was permitted. If there were a disagreement about a particular lesson, it was recorded and debate postponed to the follow up session.  If a lesson was raised that fell into two subject areas, it was recorded twice.  The second session was reserved for stepping through each lesson, debating it and coming to agreement on each lesson.  It was at this time that a rationale was added and verbiage to clarify to lesson for presentation purposes.  In the case of the E-2C, the lessons numbered close to seventy and there were many lessons learned about the process that subsequently affected the way that the PATRIOT event was conducted.Once lessons were finalized, resolutions are tracked in subsequent events to ensure lessons are not repeated.  A spreadsheet was managed by JITC to track lessons in all events.  Lessons were also posted to several websites so that those in the community might access them.  A knowledge base exists at JSSEO making these lessons available to the community as well.  Lessons from the JSSEO experiences are being fed back to the JDEP community through an active JDEP Working Group, a large forum sponsored by DISA, where JSSEO has the opportunity to interface with others in the testing community.  It is here that JSSEO has been able to both provide lessons as well as glean lessons from others experiences with JDEP.  One lesson that others are learning and that JSSEO reinforces in their experiences is to keep the objectives of the event as focused as possible.  JSSEO has had the luxury of well define experiment objectives which focus the developers and analysts toward achievable goals.  The multiple objectives being established for the combined event will need to be managed well to avoid participants from becoming crossed up in the process.  It is imperative that all parties involved share the same priorities for objectives.The federation development process for each HWIL test and the IBuild has been modeled on the IEEE-standard seven-step Federation Development and Execution Process (FEDEP) shown in Figure 4.1.  SHAPE  \* MERGEFORMAT Figure 4.1.  Federation Development and Execution Process (FEDEP)The FEDEP, by design, is a general purpose process.   JDEP has modified the process to meet the specific needs of JSSEO.  Details and rationale for these modifications are described in [4,6].   This paper lessons are presented in the framework of the modified FEDEP process.    Tracing Specific Lessons: Define Federation ObjectivesDefining the federation objectives was particularly challenging for the E-2C federation. Because the analysis was repeated, the process was less painful in subsequent events.  It was JSSEO’s desire to model all possible biases but with the differences in the radars, AN/APS 145 (two dimensional, scanning, airborne radar), PATRIOT radar (3 dimensional staring, stationary radar), and AN/SPY 1 (3 dimensional staring, ship-based radar) and differences in the platforms (air, ground-based and ship) and their navigation systems, not all biases have the same meaning to each system.  Coming to a common understanding between JSSEO and the system experts was a significant struggle in the E-2C event.  For the PATRIOT event, JSSEO and PATRIOT system analysts came to quick agreement as to the meaningful biases that could be introduced into their radar.  This was due in part to the early involvement of the PATRIOT team in the planning phases for E-2C.  They took part in the early E-2C discussions to determine the event objectives.  Unfortunately, there were delays in coming to agreement in the AEGIS event.  These were due in part to the limited participation that the AEGIS team had in early discussions over objectives.  There were also delays resulting from technical debate on the significance of the errors that JSSEO wished to induce and real world errors.  There were information exchanges on the errors resulting from ship flexure and discussions on how these might be modeled in the architecture.  Ultimately, it was decided that limitations in current models and lack of data prevented this analysis event from addressing these error sources despite the real world displacement errors created in the AEGIS combat system.There were lessons reported from the E-2C event on issues concerning the team structure particularly in the early stages of the FEDEP process.  The technical team was comprised of individuals with various roles and responsibilities from many different organizations.  JSSEO, owner of the analysis problem, has been the user. SIAP domain experts and analysts who understand the SIAP issues, attributes and systems were critical members of the team throughout the entire FEDEP process. Also included on the team throughout the process were the SUT domain experts and analysts.  It is important to distinguish these two types of domain experts.  The SIAP domain experts have knowledge of the SIAP vision, objectives and issues but may not have an understanding of the technical details of the SUT.  The SUT domain experts have understanding of how the SUT operates and its mission but not necessarily on its contribution or effects on SIAP.    The involvement of both has been critical to the success of these events.The team should also include individuals with federation development expertise and whose responsibilities include leading the federation development process and application of the JDEP Technical Framework.  Other members of the team include those responsible for the Common Reference Scenario (CRS), a common component maintained by Special Programs Center at the Joint National Integration Command (JNIC) for JSSEO.  It was essential that roles and responsibilities be clearly defined early in the test planning to give the right individuals the authority needed to complete assigned tasks.This collective team approach was embraced in each event with the composition differing slightly.  In the E-2C event, an individual with federation development expertise shared the leadership role with a representative from E-2C. It is believed that because of this and the fact that E-2C had significant HLA experience, the use of HLA was not a hurdle.  In the PATRIOT event, the event leadership lacked HLA expertise and coupled with the fact that the use of HLA was new to SED, their lessons concerning HLA were significant.  In the AEGIS event, there was limited knowledge of HLA at the leadership level and very little HLA expertise on the implementation team.  Building on the lessons learned at the PATRIOT pilot event, concerted effort was made by the JDEP team members to educate the developers and provide resources to aid in the development of the HLA-specific interfaces.Another element in the establishing of objectives was to consider the long-term vision when deciding who should attend.  Not only those from past tests, but also those who will participate in future tests could contribute to and learn from test planning activities.  This lesson was addressed with attempts to include E-2C technical staff in the federation development activities of PATRIOT.  Attempts were also made to include PATRIOT and E-2C representatives in the AEGIS federation development activities.  Not only would E-2C and PATRIOT experience help the AEGIS engineers, they could also help to gauge the repercussions of proposed changes to their own tools in preparation for the combined event.  Their continual involvement will help to make sure decisions are made with their systems in mind and will hopefully limit re-engineering in the future.For the IBuild, the team composition is very different because the SUT, IABM, is under development and not a fielded, well understood mission computer program.  The IABM is evolving with new functionality added in time boxes.  At the onset of the IBuild development, the staging of functionality for the IABM was still in the planning stages. This made the process for developing the stimulation environment challenging.  Additionally, there was an unintended division of expertise between the IABM development and the IBuild team which was not ideal but necessary given the timelines and development cycle for each.  The IBuild development started with a focus on supporting interfaces to the IABM only and with knowledge of existing systems to which the IABM would integrate.  Knowledge about systems such as the radars, the tactical radio interfaces, navigation systems, and identification friend or foe (IFF) systems were used to specify the IBuild capabilities and requirements.Tracing Specific Lessons: Conceptual AnalysisA significant lesson learned carried forward in all events was the need to conduct a conceptual analysis.  The conceptual analysis is an often overlooked step because it and its products are not well understood. A conceptual analysis attempts to describe what the real world elements of the analysis are and how they will get represented in the federation as a tool supporting the analysis.  It should be noted that the authors use the term “federation” very loosely for it is intended to convey the complete set of tools assembled for the analysis. It does not refer to only those tools connected through HLA. The conceptual analysis becomes a communication mechanism between the team members so that they may come to agreement on what simulation tools are required to support the analysis. Issues of scenario requirements, simulation fidelity, functional decomposition across tools or federates, and time representation are resolved during this phase.  If not done correctly, misunderstandings about what will be represented in the simulation tools can result and if discovered late in the process, can delay the analysis while the appropriate tools are assembled.Considerable time was spent conducting the conceptual analysis for the E-2C event because it was the first event.  It was documented and reviewed by JSSEO, the SIAP analysts and the SUT experts.  It was at this time that misunderstandings surfaced and plans were made to resolve the missing components.  Initially, the role that IFF played in the E-2C federation was downplayed by the SIAP experts because they understood the IFF to be critical to SIAP identification issues only and did not understand its role in the E-2C’s local track formation.  Creating a common understanding early in the development process avoided delays that may have resulted from the need to add missing functionality at the last minute.The E-2C conceptual analysis was reused for the PATRIOT and AEGIS events with some minor tweaks.  Subsequently little time was spent on this activity.  Interestingly, these conceptual analyses have also served as documentation for the tools developed and assembled to conduct the event.  It became a convenient way to describe the resulting federation for the events.A conceptual analysis is currently being done for the combined event as additional objectives, new scenarios and the distributed nature of the event will require a new analysis.  It will again serve as a communication device to ensure what is being developed will meet the analysis needs. Like the confusions that the conceptual analysis cleared in the E-2C event, the conceptual analysis was a critical step in the IBuild development.  In lieu of formal requirements, the IBuild conceptual analysis, created by the developers in this case, served to convey their understanding of what they understood should be developed. It also served as the basis for test plans when development reached that stage.  Tracing Specific Lessons:  Federation Design and Development Federation Object ModelA critical component in federation development is the development of the Federation Object Model (FOM).  For the E-2C federation, JDEP reference FOM was used as its starting point.  The reference FOM is a JDEP tool in which FOMs from existing programs, each focused toward system engineering or system interoperability, were merged with the hope of bringing the best practices from these programs together.  The reference FOM was modified to meet the needs of the E-2C federation.  This process was initiated by the federation developer and it was then reviewed and iterated with those developing the HLA interfaces to reach a workable FOM.  Attempts were made to avoid FOM development by a large group therefore a small group including the SUT experts was employed.  The intent in this FOM development process was to bring FOM development expertise to bear on the FOM design choices.  Changes to this process were suggested in the E-2C lessons learned and made for the PATRIOT and AEGIS events to add some structure to the process and include SUT expertise earlier in the process than in the E-2C event.  Federation AgreementsEqually important to specifying the FOM is the process for identifying federation wide agreements concerning how information would be exchanged.  For the E-2C event, this was an ad hoc process and agreements were captured in a brief presentation. Lessons coming from the E-2C event suggest that the agreements be documented fully and formally.The following are the set of federation agreements that have evolved for each of standalone events and will continue to evolve with the combined event.   They have been captured in a text document and are available through a website for all developers to access. The coordinate system is WGS-84.  Positional updates for aircrafts and missiles were published in an Earth Centered Earth Fixed (ECEF) reference format.   Originally in the E-2C event, the data was provided in an Earth Centered Inertial (ECI) reference which proved to be challenging to publish and overkill for the simulations that consume the data.  A change was made during the E-2C event and has remained unchanged for the subsequent events as well as the IBuild federation.For the E-2C federation, all attributes for the Platform class were mandated to be delivered at the same time to ensure that all data required to dead reckon position updates is valid for the associated timestamp.  For example, one can not update velocity in one time step and then orientation in the next.  This requirement is being reviewed for the combined event because it is recognized that this requirement does not take advantage of the flexibility in HLA to send only that data which changes and therefore reduce necessary bandwidth.  It is also recognized that for data extrapolation purposes by the consuming simulation (or federate), this requirement simplifies the process. For the events to date, bandwidth has not been an issue however, larger scenarios are being considered for the combined event.  The IBuild federation complies with this agreement however HLA bandwidth is an issue because considerably more data is shared through HLA for IBuild than in the HWIL events.  In HWIL events to date, the data shared through HLA consists of the scenario track data and IFF mode code updates.  Other data is shared over real world interfaces between real systems and emulators.  IBuild includes the same ground truth track and IFF data but also transports sensor detections, navigation system updates, and tactical communications data between systems, making the amount of data shared considerably more than HWIL events to date. Performance assessments are being conducted on the IBuild tools to address potential bandwidth issues.  At present, to ensure IBuild will be able to participate in an HWIL federation, IBuild implements this agreement.The ESTEL facility employed a combination of tools to synchronize the data shared across the federation. They made use of Network Time Protocol (NTP) to synchronize the system clocks of each machine in addition to using IRIG timecards where timing is a critical element in the process. For example, the CRSD application runs on a machine that has its own timecard. It does not rely on NTP as a source of time.  As the ground truth server, CRSD timing of the aircraft updates is critical. SED experience has shown that NTP is insufficient for synchronizing the processes in the supporting federation and they equip all machines with IRIG timecards.  The IBuild does not currently run in realtime and uses the time management services with the run time infrastructure (RTI) of HLA to synchronize the updates and events in the federation.  This difference between the IBuild and the HWIL event is significant and needs to be addressed as planning for the running the IABM with the IBuild in an HWIL event proceeds.JDEP as part of its definition of a technical framework, has selected the freeware supplied by the Defense Modeling Simulation Office (DMSO), also known as RTI1.3NGv6, as the RTI of choice.  Each HWIL event to date as well as the IBuild has been based on this version.  Support for this freeware is no longer available and JDEP is evaluating the commercially available RTIs that will meet the needs of the community.  There may be changes in what is used in coming events.In the E-2C and PATRIOT events, a standard RTI Initialization (RID) file was used.  There were some significant lessons learned about the RID file in the PATRIOT event that were applied to the AEGIS event.  Specifically, to avoid federate join order issues experienced in the PATRIOT event, initialization parameters in the RID file were changed.  Additional changes to the RID file were required in the AEGIS event to address issues arising from hardware platforms with multiple network interface cards.  Both these resolutions will be addressed in the creation of the RID file for the combined event.  In addition, a process is being defined to configuration manage the RID file and its distribution in the upcoming combined event.In each of the standalone events, the issues of distribution of time critical data has been minimized by the fact that these federations are run over a local area network in a single facility.  In the combined event, use of a filtering scheme in HLA, called data distribution management (DDM), will be used to address these concerns by limiting the data distribution across long haul networks.  Changes to the FOM, the RID file and federates are being made to effect this filtering scheme.  Implementation and testing of this DDM application is not complete to report lessons learned at this time. The IBuild currently makes use of DDM for data filtering between federates and has reported success however performance over long haul networks is not an IBuild mode of operation and provides no true lessons to the HWIL events.In each of the events, the agreements included the use of a standard for data marshalling.  All FOM data elements were required to be encoded using eXternal Data Representation (XDR) standard prior to being sent to the federation and must decode XDR data elements prior to use.  In the IBuild effort, the XDR requirement was adhered to by the primary software developer but in the cases of the IABM, the requirement was not met therefore when the IABM was testing in the IBuild environment, it did not work.  In hindsight, the IABM development team was not involved with the development of the federation agreements and overlooked their existence.  Federate DevelopmentWith the draft FOM, and draft federation agreements in place, federate developers proceeded to develop federates using their own software practices.  It was the goal to have developers come to the integration testing event with completed and tested federates.  Especially in the case of the E-2C event but also in the PATRIOT event, development and test was not completed prior to the integration and testing. Because of the dependency on the CRS and the common reference scenario driver (CRSD) and the degree of parallel development, several federates were not pre-tested.  Little development was required during the integration phases of the AEGIS event due to the maturity of the tools and the timelines for the event..  As a result, the time incurred for integration testing and V&V were less for the AEGIS event than either the E-2C or PATRIOT event.   PATRIOT interface development team had no HLA experience as compared with the E-2C team.  PATRIOT has a history of support of distributed simulation using Distributed Interactive Simulation (DIS).  As a result, PATRIOT’s implementation of HLA was not as smooth as E-2C’s.  While HLA expertise was part of the PATRIOT event team, their level of involvement was not sufficient to supply the missing expertise.  HLA training was provided but tight development timelines, geographically distributed teams, complexity of the PATRIOT radar simulation, and the flexibility in HLA caused delays in the implementation of a working HLA interface for the PATRIOT radar.   It should be noted that HLA provides tremendous flexibility in its application.  If implementers of HLA interfaces are not working closely together through the development of agreements, the resulting federation that may not operate as desired if at all.For the AEGIS event, more personalize training and HLA support was provided to their developers with the anticipation of avoiding the situation at PATRIOT.Tracing Specific Lessons: Federation Development, Integration Testing and VV&AA lesson learned from the E-2C Pilot test was the need for more formal V&V processes for federates and the federation.  The results from the E-2C event were considered valid despite the fact that the individual tools and simulations were not exhaustively validated.  Had there been an issue with the resulting data, it may not have been known until after the runs for record were completed and the analysis begun.   In recognition of the fact that this is not acceptable, JSSEO developed a V&V process that was first applied to the PATRIOT HWIL pilot test [7].  The process is shown in Figure 4.1. Figure 4.1 JSSEO V&V ProcessJSSEO’s V&V process accounts for the need to validate each component individually and then repeat the process once components or federates are integrated into the federation. This two stepped approach is necessary because once integrated, the sources of input data may change, mechanisms for timing may be different, or hardware may differ from the stand alone operation.  If possible, it is preferred to have individual components V&V’d at their home station prior to integration where others are not affected.  It is recognized that this may not be possible and the operation of the components in the federation may differ extensively and require significant V&V in the context of the federation.  Here once again, the authors use the term federation to mean a complete collection of distributed federates, simulations, emulators, HWILs and tools assembled to produce data. The term, federation, is not intended to be exclusive to the HLA components only. V&V of the entire federation to meet analysis objectives requires the involvement of experts for each component, HLA expertise, and most importantly SIAP analysis expertise.  It is with the SIAP analyst’s participation that the determination as to whether the federation will produce valid data is made.  This is a laborious task and one that can not be avoided or done lightly.  One observation made is that many of the same steps required to integrate the federation are repeated in the V&V process.  Therefore, plans for integration can be used as a guide for the V&V process.   One critical lesson learned from the PATRIOT event was that the V&V of a federate may not always carry forward to the next event, event if the second event shares the same analysis objectives. V&V must be repeated because small changes in the composition of the federation might result in differences that invalidate the previous V&V steps. A specific example includes the addition of TBM’s and the use of the full compliment of air tracks in the scenario in the PATRIOT event.  These additions caused issues not seen in the E-2C event.  Missing functionality in the Utility Player federate and throughput limitations in the Gateway Terminal Emulators (GTEs) were uncovered during the V&V phase of the federation development.  Without careful inspection required by the V&V process, some of these issues might have been overlooked until the analysis phase began. By this time, it would have been too late to fix and the data produced in the runs for record would have been useless.Interestingly, the lessons learned from the HWIL events did not feed to the IBuild as it entered this phase.  In the IBuild case, the need for unit testing, integration testing and V&V was not addressed sufficiently in the planning stages.  Requirements for V&V were not levied on the development team at the onset but it has been recognized as a significant milestone going forward.  Human resources for testing were not appropriately distributed among the development team. This coupled with a lack of formal requirements and the resulting complexity in the tools developed created deficiencies such that those saddled with the task of testing did not have sufficient knowledge of what needed to be tested to be efficient.  Time allotted for testing was not sufficient estimated and tools to support testing were non existent.   In lessons learned, it was reported that more than forty mouse clicks were required to start a run with the IBuild software and this did not include the creation of the configuration files used by each model.  Focus on the maturation in the usability of the IBuild software has become a focus for its next builds.Significant changes in the roles and responsibilities among team members were made as planning for subsequent builds takes place.  Better estimates in what is required to conduct testing, be it unit, developmental, integration, or verification and validation, have been made.  Goals for conducting V&V on the IBuild have been set for the development team which will require IBuild requirements to be detailed and testable.  Substantial development is being done to aid in the use and configuration of the IBuild software to combat the complexity in initializing and configuration managing runs.  Several organizations are taking part in the development of these test tools as each brings expertise to this problem.Tracing Specific Lessons: Test ExecutionSignificant strides were made in the test execution phase in the PATRIOT event to provide visibility into the federation while it is running.  These developments address specific concerns with the lack of viewers and tools that provided status while runs are being conducted.  This includes the data shared over HLA as well as the tactical data shared over Link 16.  Tools for the E-2C event were crude or non-existent.  PATRIOT used many of their existing DIS tools with DIS-HLA gateways to supply data to the tools in the correct format.  While introducing delays due to conversion of the data, once verified, the viewers served the test director well in providing him with visibility into the data exchange.  These tools were critical during integration, verification and validation phase, and during test execution.  Lack of proper tools remains to be a significant deficiency in the events although steps were made in the PATRIOT and AEGIS events to improve. Making viewers a critical federation component is necessary so that it is not an afterthought.Just as it was important to have dedicated personnel associated with each federate during integration testing, having the same support during the actual test execution is just as critical to ensure success of the test in the allotted time.One lesson learned from E-2C is to plan a back up week for runs for the record in the event that all the requisite runs for record are not made. The ESTEL personnel were fortunate to be able to make extra runs in following weeks to complete the data collection matrix.  PATRIOT and AEGIS made plans for backup testing in the event it was required.The IBuild has not yet been used in formal analysis where runs for record are required. Tracing Specific Lessons: Post-Event Capture of Lessons LearnedThe process used to capture lessons learned for the E-2C event was described above.  Lessons were captured throughout the planning, development and testing process and revisited in a separate session dedicated to capturing and documenting lessons learned.  Two sessions were set aside after the runs for record were completed.  In the case of the E-2C, a single day was divided into two sessions.  For the PATRIOT event, it was done over an extended period as explained further below.  Because the runs for record have not been made for the AEGIS event at this time, formal lessons learned have not yet been captured.In the case of the E-2C, being the first time through, the lessons numbered close to seventy.  There were many lessons learned about the process that subsequently affected the way that the PATRIOT event was conducted.  All organizations were represented and there was general agreement by those involved that the lessons were valid.The process for the PATRIOT event was not followed because not all organizations were represented when lessons were initially recorded.  As a result, there was disagreement on the lessons reported and the solution was to gather all parties together and revisit.  Lessons were reevaluated and the process was followed although over a longer period of time.It is recognized from this series of analysis events how critical the identification of lessons learned is to making the process of development better.  After the testing of IBuild 001, lessons were captured although the process as described herein was not followed.  Lessons were recorded from a variety of perspectives and not socialized together to produce a cohesive agreed upon set.  Despite this, JSSEO has assimilated the input and put forth a plan to address the lessons learned from their first delivery.  Improving the requirements definition and the testing process are primary focuses with subsequent builds. A formal requirements group has been assembled to provide clear direction for future builds base on the phases of the IABM. More time has been allotted to the testing process to include unit test as well. Additional resources, both technical expertise as well as hardware, are being procured to improve the testing process. Leave BehindsThe tools, federates, and processes that are created for each event are evaluated for their utility to future events.  In keeping with the JDEP vision, investments in the development or maturation of tools are made with the goal of being reusable.  There are several investments that are being made by JSSEO and by JDEP to create federates, tools and processes that are applicable to a range of applications beyond the air and missile defense arena.JSSEO realized that the quantification of the SIAP is a necessary step towards improving the SIAP. JSSEO has identified a core set of attributes associated with a SIAP, which provide the joint community with a common point of departure for quantifying and assessing a SIAP.  JSSEO has worked to define the attributes rigorously [8] and to identify the metrics associated with their definitions [9].  Examples of SIAP attributes include completeness, commonality, kinematic accuracy, and clarity.  Metrics drive the data collection plans developed for each event and the calculation of the attributes using a tool called the Performance Evaluation Tool (PET) form the basis of the analyses.  It has been demonstrated in the HWIL events to date that the use of the SIAP attributes, metrics and the evaluation tools has provided a standard framework for conducting analysis.  Originally, these tools were designed for live test events and were extended to support HWIL events.  IABM will provide the ability to support both the collection metrics and the calculation of attributes.  It should be noted that others conducting analysis outside the JSSEO have made use of the attributes and metrics for their studies. A similar thought process was used in the creation of the CRS, a Defense Planning Guide-based operational context for SIAP assessments [10].  Like the attributes and metrics, the use of the CRS provides a consistent scenario for evaluating current performance and proposed improvements.   The E-2C event was the first HWIL event to use the CRS.  Many lessons resulted and were fed back to the Special Projects Center to correct.  Suggestions such as the inclusion of aircraft orientation data, corrections to flight profiles, and locations of aircraft fed the next versions of the CRS.  The CRS has matured with each HWIL event conducted.The intent behind the IBuild is to provide a number of components to the distributed community. As was mentioned in Section 2.2, the model engine was developed to support the integration of models into the IBuild environment. The model engine serves as the starting point for the HLA interface for the models, the Link 16 communication server and the AN/TPS-59 radar.  The model engine is a general tool and will enable developers to integrate rapidly to not only IBuild but to HLA as well.  It can input a FOM an automatically generate source code that can be used in the interface. An example where others are using it outside the IBuild is the Defense Modeling and Simulation Office (DMSO).  They are using the model engine as the basis for an environmental server capability under development.  The model engine is a significant leave behind for the distributed simulation community.Other significant leave behinds will be the models of the AN/TPS-59 model and the AN/SPY 1 model underdevelopment by MITRE and Lockheed Martin, respectively.  These radar representations will be able to stimulate the IABM with radar detections and efforts are being made to run these in real time.    Plans to separate the detection process into radar signal emission, target observables, two-way propagation of the radar signal and environmental effects, radar receiver, and radar system processing are being implemented in these models.  This will enable the use of common accepted representations of environmental effects and target observables across each radar simulation so that these representations are consistent across the radar models.SummaryJSSEO is applying JDEP to their HWIL analysis and evolving it to it to address the testing of the IABM through the development of the IBuild.  Lessons learned from the use of JDEP in the HWIL environment are being tracked through a series of JSSEO HWIL events and the IBuild development process.  This paper describes some of the lessons learned in the HWIL analyses events and how they may be applied to the IBuild development process.  Capturing lessons from other venues provides further opportunity to refine the test approach and execution.References:[1] DoD Architecture Framework, Version 1.0, 9 Feb 2004.[2] High Level Operational View One (OV-1), Revision 2, 15 Aug 2001 [3] “MDA and HLA:  Applying Standards to Development, Integration and Test of the Single Integrated Air Picture Integrated Architecture Behavior Model”, Dahmann, Judith and Wilson, Jeffery, 2003 Fall Simulation Interoperability Workshop:  Paper , September 2003.[4] Dahmann, Judith S. and Clarke, Richard. “Joint Distributed Engineering Plant Technical Framework: Applying Industry Standards to System-of-System Federations for Interoperability,” Simulation Interoperability Workshop, September 2002.[5]  JSSEO E-2C Test Plan (2002, October). Arlington, VA: JSSEO. [6] “Single Integrated Air Picture E-2C HWIL Federation—A JDEP Pilot” Talbot, Jayne and Rock, Mary,  Simulation Interoperability Workshop; Paper 03S-SIW-12, March 2003.[7] V&V Process Technical Report 2003-006. (2003, February). Arlington, VA: JSSEO. [8] SIAP SE TF Technical Report 2003-029, Single Integrated Air Picture (SIAP) Attributes, Version 2.0, Aug 2003 (DTIC ADA 397215).[9] SIAP SE TF Technical Report 2001-003, Single Integrated Air Picture (SIAP) Metrics Implementation, Oct 2001 (DTIC ADA 397225).[10] SIAP SE TF Technical Report 2002-003, SIAP Common Reference Scenarios (CRS), July 2002(DTIC ADA 405057).ELISABETH A. YOUMANS is a Principal Staff Engineer and Program Manager for Joint Single Integrated Air Picture (SIAP) System Engineering Organization (JSSEO) Test Planning and Execution Support, Systems Planning and Analysis, Inc. in Alexandria, VA.  She has nine years of experience in various aspects of military systems analysis and program support.  She holds a B.S. in Physics from the College of William and Mary, an M.S. in Electrical Engineering from San Jose State University, and a Ph.D. in Aerospace Engineering from Virginia Tech. JAYNE TALBOT is a Group Manager at Virtual Technology Corporation (VTC) where she leads a technical team in support of JDEP’s Infrastructure Manager at the Joint Test Interoperability Command (JTIC).  Prior to joining VTC, Jayne was with The MITRE Corporation for ten years where she was the Aggregate Level Simulation Protocol  (ALSP) Project Manager.  She has worked for the Army’s Night Vision Lab and the Environmental Research Institute of Michigan on infrared system modeling and development.  She holds a BS in Electrical Engineering from the University of Virginia. STEVE KAROLY is the Test and Analysis Division Chief for the Joint Single Integrated Air Picture System Engineering Organization (JSSEO), formerly the Single Integrated Air Picture System Engineering Task Force (SIAP SE TF).  For the last twenty years, he has been working joint interoperability issues for the Naval Sea Systems Command.  He holds a B.S degree from the United States Merchant Marine Academy (USMMA) and is undergoing a Master of Science program in Systems Engineering at Johns Hopkins University.  Steve is also a Commander in the United States Naval Reserve (USNR). 