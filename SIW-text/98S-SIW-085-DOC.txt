Prescribing Fidelity and Resolution for CMMSFurman HaddixApplied Research LaboratoriesUniversity of Texas at Austin10000 Burnet RoadAustin, TX 78758512-835-3500furman@arlut.utexas.edu Keywords: Conceptual models, Fidelity, Modeling, Resolution, Simulation, Validation, VerificationABSTRACT: The problem of describing an appropriate level of fidelity and resolution for Conceptual Models of the Mission Space (CMMS) is open. This paper addresses ways that system purpose and desired capabilities can drive the development of prescribed requirements for CMMS. Specification of  metrics for describing existing CMMS and for guiding Warfighter SMEs in the acquisition and documentation of the Military Operations Mission Space (prescribing new CMMS) is discussed. The issues addressed are primarily with respect to cognitive descriptions, and include, in addition to fidelity and resolution, scope, and completeness from the aspect of simulation developer utility. Scope is generally determinable from system or user requirements and has to do with the number of models (in the large) required for a simulation. Fidelity is the most difficult to define because it applies to many aspects of a problem. For example, a simulation could be high fidelity with respect to communications, yet  be low fidelity with respect to movement and assessment of casualties. This means that an accurate statement of fidelity with respect to key areas must be multi-dimensional. Resolution is straight-forward, dealing with level of decomposition into components, subsystems, etc. Although completeness for purpose, the level of detail required by developers, is not well understood, the primary variable is the sophistication of the developers in the problem domain. Scope, fidelity, resolution, and completeness may be described prescriptively or descriptively, that is, prescriptively applying to an assignment of work to be done; or descriptively applying to evaluation of a body of work previously completed.1.	IntroductionThis paper addresses a fundamental question for the Conceptual Models of the Mission Space (CMMS), namely, “How to specify ‘How much is enough?” “How much” issues are divided four ways: scope, fidelity, resolution, and completeness. Some or all of these “types” may be applied to simulations, models (entities or processes) within a simulation, interfaces, characteristics, or behaviors. Here, interface means the set of interactions between two models.This paper is one of a series addressing CMMS issues [1]-[5]. Basic CMMS concepts are discussed by Sheehan et al [1].1.1	MotivationRegardless of individual or program opinions on what the appropriate levels of scope, fidelity , resolution, and completeness are, or should be, it is very important that those who define these elements clearly communicate the appropriate levels to those who must produce the components of CMMS. This is very important since these four elements may be the most important cost determinants for CMMS development, which is itself a major factor in the cost of developing military simulation software. Why select scope, fidelity, resolution, and completeness? Fidelity and resolution are generally accepted terms and widely used. However, they do not completely cover the problem space of prescribing level of detail in CMMS. Scope addresses the size of the problem space that identifies the elements for which fidelity and resolution are to be applied. Simulation scope is a straightforward term, indicating the rough boundaries of the simulation and either implicitly or explicitly, the real world aggregate processes and entities to be included in the simulation. Resolution then indicates the degree of decomposition to be applied to these processes and entities. Fidelity indicates the degree to which they accurately represent corresponding real world processes and entities. Completeness is used to denote specification completeness, the concept being that from the developer point of view, the specification must include all of the military operations mission space information needed, independent of scope, fidelity, and resolution of the CMMS and resulting simulation. In other words, regardless of scope, fidelity, and resolution, the specification must have the information necessary for development. This information is what is necessary to create specification-based software, and is directed toward covering the domain-specific information that the developer can not and should not create, but instead, should obtain by reference to his specification.Recent work [3], [6], [7] in designing a reusable CMMS Library capability identified the problem of describing models built to different specifications for different uses. Along with V&V pedigree, previous use, etc., descriptions of fidelity and resolution help a potential user determine if a set of models addresses his needs. Production of CMMS introduces the more difficult problem of prescribing fidelity and resolution as guidance to knowledge producers. A review of the literature in this area identifies a thread of validating measures addressing the issue, “Does a simulation achieve the objectives of its development?” It is clear that this is a very difficult and complex problem; however, it is a finite problem, and it can be addressed practically.1.2	AssumptionsIn preparing this paper, it quickly became apparent that there were implicit assumptions underlying a great part of this work. In some cases, assumptions might be considered to be conclusions based on experience; however, rather than argue such points, they are categorized as assumptions, specifically:Scope, fidelity, and resolution of simulations is determined primarily by reference to user-defined requirements, not by simulation developer interpretation of what is appropriate. In other words, simulation scope, fidelity, and resolution are end user and program manager issues, not military operations subject matter expert (MO SME) or simulation developer issues.The level of effort in CMMS production is program-specific. Scope, fidelity, and resolution of CMMS produced at program expense are dependent upon program requirements.The alternative to consistent, relevant to current software development, CMMS is chaos in the simulation development process resulting from inconsistent CMMS specifications and inconsistent interpretation of the CMMS specifications.MO SMEs are capable of producing consistent, specification-compliant, relevant-to-software-developer-needs (CMMS) knowledge products.Models of human behavior are limited to most likely approximations; e.g., multiple representations, such as conservative, aggressive, and average, are not specified.2.	Related Work2.1 Fidelity and ResolutionSeveral papers addressed issues of fidelity and resolution at the last Simulation Interoperability Workshop, of which four are discussed here because of their relevance to this topic. Foster and Yelmgren [8] address fidelity in High Level Architecture (HLA) federations. They build an interesting six-layer data flow model; however, their model does not deal with deltas from reality for cognitive models. Schow et al [9] present an “incomplete” taxonomy of fidelity measures falling into eight broad categories, in support of the thesis that fidelity is not a simple metric. They advocate a weighted measure. They place high value on effective communication and publication of simulation fidelity metrics. However, cognitive models of military decision-making is not one of the eight categories. Their mathematical models suggest a precision that doesn’t yet exist in “higher level” cognitive models.Noragas et al [10] describe a methodology in current use, which refines the concepts enunciated in [8] and [9]. However, this methodology was developed for live simulations of low-level processes, and does not directly address the issue of fidelity in cognitive models. The primary concern here is descriptive, i.e., comparing fidelities of existing simulations.Gross and Freeman [11] classify fidelity measures into three types -- existence (of entities), attributes, and behaviors -- and articulate a process composed of aggregation, clarification, and simplification for developing models whose fidelity is measurable. They measure fidelity between HLA constructs and CMMS.Several common threads are seen in the above mentioned papers, including the following:Need for generally accepted categories of elements of simulation for fidelity measurement,Solutions that explicitly or implicitly disregard the fidelity of the underlying CMMS,Mathematical measures of fidelity,Concern with descriptive and validating aspects of CMMS, as opposed to prescriptive -- primary concern is V & V, rather than production specification, and Overall indices of simulation fidelity or fidelity/cost. Overall indices may provide a means for quickly applying a coarse filter to the issue of simulation fidelity; however, they should be used with care, because the same deviation could occur from a number of relatively insignificant deviations or from one substantial element. If that one substandard element is crucial to the simulation objectives, it may be prima facie disqualifying, while the large number of smaller deviations is tolerable. The use of an index is not a substitute for a careful review of the simulation in the context of the simulation requirements.2.2 Verification and ValidationToth [12] describes a general software development process error model in which Requirements Analysis (RA) comprises 5% of the investment, yet introduces in excess of 50% of the errors. Unfortunately, fewer than 20% of the (total) errors are subsequently discovered and corrected during RA. Indeed, most are not located until the implementation, test, and maintenance phases. One would expect that in modeling and simulation software development, the 5% and 50% figures would increase; however, without robust verification and validation, the 20% figure may not increase. Before verification and validation can significantly benefit the situation, there must be reference standards and consistent process in place.A commonly used set of DoD definitions [13] for verification and validation discriminates between the concepts as follows: Verification is “The process of determining that a model or simulation implementation accurately represents the developer’s conceptual description and specification. Verification also evaluates the extent to which the model or simulation has been developed using sound and established software engineering techniques,” and Validation is “The process of determining the degree to which a model or simulation is an accurate representation of the real-world from the perspective of the intended uses of the model or simulation.”Rothenberg [14] uses a slightly different definition of  data validation: “The assessment of data for their intended use by evaluating the methods by which data have been derived and comparing these values against independently-acquired values that are either known or best-estimates.Based on these definitions, what does CMMS support? Is CMMS a specification of functional behavior of simulations, or is it a description (benchmark) of the real-world? The second view is certainly more convenient for using mathematical models. However, in high-level cognitive models, treating CMMS as a reality benchmark has at least two serious problems: (1) The fidelity and resolution of CMMS descriptions varies greatly; and (2) Assuming a maximum tolerable deviation from the real world is permissible, that maximum deviation may already have been violated by the CMMS’s deviation from the real world. If this is the case, the issue of how much the “analytic” model varies from the CMMS is immaterial. A benchmark must  measure relevant characteristics of the reality it purports to benchmark; and it must measure those characteristics accurately. Engineering benchmarks are used with the assumption that the differences between the benchmark and reality are inconsequential. This is not the case with most models of military decision-making: The abstractions necessary to achieve practicality introduce large deviations. It is unlikely that any “benchmark” of military operations decision-making will be close enough to reality for the deviations to be insignificant.Rothenberg et al [15] represent validation as several activities, informally “validation by design, validation by construction, and validation by inspection.” This paper takes a consistent approach wherein fidelity and resolution for CMMS are planned prior to construction (design) as part of developing a focused context, CMMS construction follows clear guidelines, and no “surprises” result from post-construction inspection.3.	CMMS in the Software Development ProcessIn this section, a set of requirements for CMMS,  emanating from the software development process (SDP) is articulated. The Texel-Williams Process (TWP) [16] process semantics are used instead creating definitions for this paper. Herein, the view is taken that CMMS production is a coexisting process with the basic SDP, the two processes having multiple interactions when the additional demands on the SDP from modeling and simulation (M&S) are considered, particularly in the military operations mission space. CMMS production as used herein includes both gathering information, TWP describes a seventeen-phase object-oriented software development process: For this paper, six summary groups of phases will be used:Requirements Engineering -- TWP Phase 1;System Analysis -- TWP Phases 2-4;Software Analysis -- TWP Phase 5-6;Preliminary Software Design -- TWP Phases 7-9;Detail Software Design -- TWP Phases 10-11;Implementation & Test -- TWP Phases 12-16; andMaintenance -- TWP Phase 17.3.1	Requirements EngineeringThis phase in TWP is concerned with user requirement descriptions. In non-M&S software development, real world descriptions would be included here. Indeed, such an approach might be adequate for live and virtual simulations, but the sheer magnitude of the real world specifications in a constructive simulation demands separate handling. Well-formed user requirements ideally drive the construction or selection of CMMS. We will refer  to this as Phase 1 of CMMS (CMMSp1). This is the phase of CMMS driven by TWPp1 and providing information to TWPp2-4. As in TWP, CMMS phases do not require sequential performance. Considering models as components of a simulation, phase 1 could be performed for all models, or phases 1-3 could be performed for each model. For CMMSp1, there are two sets of issues which should be considered. What prescriptive requirements can be derived from the user requirements, and what should CMMS provide for the next group of TWP phases, System Analysis? Three guidelines for specification of information should enable SMEs to satisfy system analysis needs: simulation scope, model resolution, and major interface completeness. Completeness of other interfaces will be addressed in CMMSp2.Simulation scope should be known early in the Requirements Engineering phase. Simulation scope describes how big a part of the military operations mission space a simulation describes. At a particular level how far does the simulation go, e.g., how many functional areas are simulated, how many services are simulated. One simulation may deal with ground maneuver units only; another with all functional areas of the army, another with all services. One simulation may deal with O-6 command level issues, another with O-3 command level issues, another with engineering level issues.In describing simulation scope as a prescription used in building CMMS, an enumeration of the aggregate processes and entities to be covered in the simulation is the best way to minimize the risk of misinterpretation and consequent omission or irrelevant inclusion. Major interfaces and requirements for model resolution should be products of the TWPp1.Model resolution describes how far down the chain of command the model describes, e.g., to which echelon, to which system component. Scope has described the top level components; model resolution describes the degree to which they are decomposed. Model is used to mean a simulation component which describes an aspect of the domain which the simulation addresses. A simulation may be composed of one or more models.A prescription for model resolution has different concerns from those of simulation scope. Although clearly, all model components will be known when the CMMS is completed, the decomposition of mission space entities and processes can only be completed by the SMEs. However, they should have guidance as to how far to continue the decomposition. Examples are very helpful. Although “Divisions should have a company level breakdown” does not translate directly to the Air Force or Navy, it will convey more to most SMEs than “models should be of moderate resolution”. Similarly, with subsystems, knowing whether radio components should be modeled is a good guideline to the desired level of decomposition.Descriptive metrics may be counts, after the existence classification described by Gross and Freeman [11]. These are not useful for prescription, estimating the number of entities may be difficult. The number of entities becomes clearer as the process continues, allowing its use for in-process metrics, such as percentage complete for definitions, even though that may not be suitable for the identification phase. Percentage complete is based on the number of entities and processes which have been specified (defined) divided by the number of entities and processes which need to be specified (number identified). This is complicated by the possible discovery that in order to provide a satisfactory model representation, additional entity and process specifications may be required. In dealing with ordered production, a good metric may be the number of entity and process specifications scheduled for delivery compared to the number of entity and process specifications actually delivered.For CMMSp1, all major interfaces should be defined. The level of detail required should be clearly implied by the entities and processes to be modeled according to simulation scope and model resolution.3.2	System AnalysisThe System Analysis group of phases (2-4) of the TWP is concerned with developing a high-level system structure. TWP categories are the major components of this system structure, including static software architecture and use cases. CMMSp2 is driven primarily by CMMSp1, but is also driven by the Requirements Engineering and System Analysis TWP phases, as well. It supports the Software Analysis TWP phases.For CMMSp2, the additional information needed by knowledge producers concerns model fidelity, model completeness, and interface completeness. These correspond to the software analysis needs of the developers, which will include identification of process and entity behaviors and characteristics, and interface definitions. Note that these are also the elements that will be needed by CMMSp3, which is defined below.  Issues which should be decided for the SMEs include:Model fidelity (entity and process),Model completeness, andInterface completeness.A descriptive model (element) fidelity can be based on three major metrics: How many characteristics does the model element exhibit and maintain?To how many stimuli does the model respond? andHow many behaviors does the model produceIn order to prescribe fidelity, the order of the questions can be inverted, so that first one looks at the behaviors to be simulated, then to the events which trigger them, and finally to the characteristics needed to support the decision logic. Others may prefer to look at important interactions, and then the behaviors required to simulate them. An adequate prescriptive description may be an enumeration of the behaviors or interactions. The number of behaviors or interactions enumerated implies the events and the event complexity in order to stimulate (and simulate) the full range of behaviors. This is also an indication of the complexity of the state space (number of characteristics) required.For example, a desired behavior might be for a fire support element (FSE) to allocate fire missions to the fire unit least recently assigned. This implies that a fire request is the triggering event for this behavior. So the FSE must respond to the stimulus of a fire request. This also implies that the FSE knows which of its fire units was least recently assigned a fire mission. Thus, a characteristic of an FSE is knowing which are its firing units and when each was last assigned a fire mission. These characteristics must be maintained and appropriately updated although that was not a requirement as specified in this example.One approach to sound development includes a set of end user driven use cases. These use cases will treat the mission space area as a black box, but should describe what behaviors are expected in response to end user stimuli. These behaviors are the first level of behaviors as described above. We can then work through the first level of models enumerating stimuli and characteristics. The stimuli may result from other behaviors which in turn require stimuli. Eventually, all the necessary stimuli can be derived and tied back to an end user action. Mission space models should be valid whether stimuli come from end or other mission space models.For example, the end user should see a tactical response to an appropriate intelligence stimulus. In the joint task force (JTF) realm, the intelligence, depending on its source might flow from a national asset to the joint intelligence support element (JISE). Receipt of an intelligence report would trigger filtering and analysis behavior by the JISE. The results of the analysis would go to the JISE watch officer. Assuming it merited action, it would then be sent to the joint operations center (JOC). This process continues until the appropriate units receive their orders. At that point, execution of the appropriate tactical behaviors (potentially) begins. Thus, the initial set of behaviors might be relatively small, yet based on the entities and processes being supported, a relatively complex model might be specified. Of course, if that single use case was all that was required, simulation of JISE, JOC, J-3, and CJTF behavior would be unnecessary -- however, if there were many other behaviors to be simulated, providing the detail breakdown, not only makes the simulation behavior more realistic, it also makes it possible to obtain significant reuse of the model components.A related issue is the potential functional specialization of the simulation system. What areas of functionality are significant to the simulation’s purpose? Which of Maneuver, Communication, Engagement/Damage Assessment, Terrain/Weather Events, or Expenditure/Resupply are more important. A more detailed list is provided by Schow et al [9].The more important aspects of the simulation are identified as requiring higher fidelity. Answers to issues such as what mundane real world details are left out to reduce cost. and which are retained because of their significance. If communications is an important aspect of a simulation, issues, such the following should be addressed in the guidance to the SMEs: Do agents (persons, organizations) communicate abstractly, e.g., can the simulation send abstract information between organizations, or must it be formatted by a C4I device, encrypted, and transmitted over a communications network? Do agents use communication devices, such as multiple subscriber equipment (MSE)? andDoes the simulation consider characteristics of the communications media and electromagnetic environment in determining what elements were successfully communicated?Model completeness at this level is straightforward in that what is necessary is assurance that all entities, processes, entity-process intersections, behaviors and characteristics have been identified. In addition, the processes and entities should be described. This is still a less stringent requirement than that of interface completeness.Interface completeness must be addressed in order to assure CMMS utility as an important resource for system analysis. To be complete for system analysis purposes, the interface specification must contain logical descriptions of the elements to be transferred, and the semantics of interactions which should be defined by the SMEs. The syntax of the transfers and interactions may be defined by either the SMEs (CMMS) or the developers (not CMMS), depending on each element’s significance to the real world and to the simulation being developed. The difference is “Does the real world constrain the implementation?” Furthermore, SMEs shouldn’t waste time deciding which details they should provide and which should be provided by the developers. In other words, identify which elements are to be constrained by real world considerations and which are not.Descriptive and validating metrics primarily depend upon consistency of application and subjective perceptions of the real world. Perceptions of the real world come from experience and training; consistency of application as several of the cited authors have already suggested depends upon development of a standard methodology and standard taxonomy for describing fidelity and resolution and related concepts, an issue which could be addressed by SISO. A three-value descriptor (high, medium, low) of fidelity has been frequently used. Nouragas et al [10] describe a seven-value descriptor for importance of an aspect to a simulation, ranking from Least Important – 1 to Most Important – 7. One approach might be to use a multiple level description of coverage relative to the real world. For example, use cases described as a percent of possible use cases multiplied by average outcome coverage for the use cases. This measure suffers from high fidelity attributes or simulations having low indices, e.g., less than 0.2. Low fidelity simulations would score even lower. A practical approach might be to go back to the three-value scale but apply it to multiple aspects of each simulation. However, this approach will have low value added without consistently applied aspect definitions.3.3	Software AnalysisThe Software Analysis phases of the TWP are concerned with refinement of interfaces, identification of characteristics and behavior. From the CMMS view of software development, a robust specification of which entities, characteristics and behaviors should be included in CMMS should exist. The CMMSp3 is driven primarily by CMMSp2, but is also driven by the Requirements Engineering, System Analysis, and Software Analysis TWP phases. CMMSp3 supports the Software Design TWP phases. These phases continue to introduce more detail into the design through TWP Phase 11 – Method Design. CMMSp3 is the final phase of CMMS, and so the SME guidance must address the issues of fidelity, resolution, and completeness with regard to behavior and characteristics.Behavior resolution describes the number of stimuli types a behavior can process, and the number of output types  produced as a result of those stimuli. A descriptive measure compares these against the real world number of stimuli and outputs. Unfortunately, such a measure can be confounded by the relative importance of different behaviors, which is both significant and subjective. Prescriptive guidance is not a concern if the variance in the space has been substantially eliminated by the definition of model fidelity and interface completeness. Behavior fidelity addresses the accuracy of the linkage between stimulus and model output, as well as the accuracy of the model output. Note that since model fidelity has already been addressed at a higher level, this problem, by depending on model fidelity, becomes much easier to solve. A useful metric might compare results from stimuli of the CMMS models and real world. A descriptive metric can be developed based on how well the results from the CMMS model correspond to the real world results. Since the collapse of the option space due to model abstraction has been recognized in model fidelity, only the mapping of the CMMS results to the comparable real world results for modeled behavior must be considered. These spaces should be the same or similar size, so that the only issue is how refined the consideration of the mapping should be, e.g., should 100 data points (assuming availability) be considered or more or less?A prescriptive guide for the producer SMEs can be couched in the same terms, bearing in mind that an admirable lack of over-precision to an SME might receive a less laudatory reception from the V&V folks. An example prescription is “On the average within a trigger (stimulus) space, the mapping to behaviors should have an 80% correspondence with reality”. This means that for a trigger set, a reasonable expectation is that a random test of trigger-results mappings should score as 80% correct. Unfortunately, this means that if the real world responds to this trigger set with a single result 80% of the time, all that has to be done is specify that result 100% of the time and correctness is guaranteed 80% of the time. The acceptability of this inference is specific to the simulation objectives. A better specification might be to represent all behaviors with a more than a 1% occurrence (or other suitable criterion) and provide an 80% (or 90%) accuracy when compared with reality. Note that inaccuracy has multiple sources in this model:Significant and possible real world results may not appear in the CMMS,Accuracy may be lost due significant real word stimulating events not appearing in the CMMS, Errors in describing model stimulating events or behavior (results) may exist, andLinkages between model stimulating events and behavior may be specified incorrectly.Synthetic representations (SR) should be considered. By SR, we mean the science-based rules of computation for real-world phenomenon superimposed upon CMMS. For example, CMMS has specified the decision rules that a pilot will follow when confronted with an approaching SAM (surface-to-air missile). He will initiate a set of maneuvers for his aircraft. SR adds to his model algorithmic descriptions of aircraft capabilities. Whether this is a simple table lookup or a 6-DOF computation is an issue of congruency with the objectives of the simulation. The significance of this with respect to the current discussion is that decisions made in the SR area may also have a significant impact on overall simulation fidelity. Further discussion of this issue is beyond the scope of this paper.Behavior completeness is the property of descriptions of behaviors being adequate for developer use in the design of the simulation. These descriptions must provide information adequate for the design and implementation of the simulation. It is essential that all specification information regarding the military operations mission space be provided to the developers in order to satisfy two objectives of CMMS -- both of which are concerned with process control. One objective is to provide to the warfighters control over (the real world simulation aspects of) the system being built. The other objective is to facilitate the efficient development of the system. Providing control over the product to the warfighters is achieved by facilitating their creation and validation of a specification that captures the essence (at least with respect to one simulation) of the real world of defense operations. All of the specification will be read with great interest by analysts, designers, programmers, quality engineers, testers, and system integrators. Then, all of these agents of the SDP can concentrate on implementing that specification. What is not in that specification will likely be interpreted separately by/for analysts, designers, programmers, quality engineers, testers, and system integrators. Then each will concentrate on promoting his/her favorite interpretation. The resulting chaos will be not only ugly, but expensive.The major concern with completeness is providing the developers what they need, both to create their product and to demonstrate its compliance with its requirements. A common deficiency of knowledge products is lack of definition of decision-making processes. Many knowledge acquisition documents do a good job of providing the context for a decision; however, decision outcomes may be explicit, implicit, or not provided al all. The developer needs more from the defense operations mission space than the fact that three documents went through an officer’s hands and one was modified. The developer needs to know which are the (significant) data elements of the document. Which elements are modified by the current agent? Which elements should be used to update the agent's information concerning the world (the agent's state or perception of the current situation)? Which data elements are principal criteria for a specific decision (e.g., who to forward the documents to)? What is the cutoff for the enemy's ability to conduct offensive operations (e.g., 85% of combat effectiveness)? How does the military compute combat effectiveness (man by man, or battalion by battalion)? What "facts" (data elements) go into a combat effectiveness computation. How does the simulated object acquire the "facts" to make these calculations and decisions? When a task force commander allocates assets (e.g., planes) to different areas of warfare, what are his considerations? What are the key asset capabilities he considers (e.g., range, payload, TTT). What are the key areas of warfare concerns regarding asset allocation? Are the concerns different between Strike Warfare and Anti-Submarine Warfare? Are the concerns different between Strike Warfare and Fire Support? In the real world, by the time it is important for an officer to know these things, he has learned them. He's learned them so well that he may not think of them as part of the situation; they are just given. Unfortunately, the majority of software developers don't have the same insight into what goes into these decisions. Without an authoritative specification, the experience on which the developer relies may come from his Game Boy. Of course, every piece of information that an officer has doesn't need to be represented as part of his information base (state), just the pieces of information that the warfighter/SME believes should have an effect on the simulation behavior. Of course, what pieces of information should affect his behavior is a fidelity issue; however, once fidelity tells us how complex the decision-making model should be, specification of the elements required for that decision-making model is a developer-driven need. It is not an adequate specification to say that the commander provides Commander's Guidance. The developer needs to know what elements a simulated commander needs to put into Commander’s Guidance, and how to derive, develop, or calculate them. The developer also must know how these elements affect the recipient’s perception of the world around him and consequent decision-making. With regard to characteristics, the definitions provided by the HLA Object Model Template (OMT) [17] can be used. For fidelity, refer to attribute accuracy (Attribute Table). For resolution, refer to attribute or parameter resolution, although this information should be supplemented by datatype and possibly, enumerated and complex datatypes (Enumerated Datatype Table, Complex Datatype Table). For characteristic completeness, one may also reference the HLA OMT Attribute and Parameter Tables. Diving responsibility between SMEs and developers may depend upon the purpose of the simulation, and in some cases, the entity or process and characteristic under consideration.3.4	Software DesignThe TWP Software Design phases utilize the CMMS specification, characterized by appropriate scope, fidelity, resolution, and completeness. The products flow into the TWP Implementation and Test phases.3.5	Implementation and TestDuring implementation, the CMMS used as a basis for design is available for clarification and reference. The CMMS use cases provide a validated  test case basis.3.6	MaintenanceThe CMMS is used to understand system functionality throughout its life cycle, providing value added to maintenance programmers and exercise controllers.4.	Plan for Fidelity and ResolutionThis section organizes the preceding chapter’s material into a plan for prescribing CMMS fidelity and resolution and addressing scope and completeness.4.1	CMMS phase 1 – Supporting System AnalysisSME Guidance in CMMSp1 should address the following areas:		Simulation scope,		Model resolution, and		(Major) interface completeness.Although alternative guidance may be used, the cost of misinterpretation indicates a complete enumeration of high-level processes and entities to be modeled should be used to define simulation scope.Model resolution should be addressed by clear guidelines with ample examples of limits on resolution. Interface completeness guidance to SMEs should address two key issues, how much responsibility should SMEs take for syntax, and semantics. The semantics should be based on behavior and characteristic needs. Since these may not have been described yet, the interface specification may end up driving this aspect of model behavior and characteristics. A good outline of an interface specification is provided by the HLA OMT Parameter Table[17], bearing in mind the need to clarify the necessary division of syntactic responsibility between SME and developer.4.2	CMMS phase 2 – Supporting Software AnalysisSME Guidance in CMMSp2 should address the following areas:		Model fidelity (entity and process),		Model completeness, and		Interface completeness.Model fidelity guidance for SMEs should be in the form of an enumeration of primary results (behaviors) expected from user provided stimuli. This should be a derivative product from the end user use cases produced in TWP phase 1, Requirements Engineering. Additional guidance may be provided for secondary behaviors, i.e., behaviors needed to complete the circuit from stimuli to results, in the form of examples and/or guidelines. However, the enumeration of primary behaviors should exemplify the granularity desired in secondary behavior descriptions.Model completeness here is a generic guidance that all essential entities, processes, entity-process intersections, behaviors and characteristics have been identified. In addition, entities and processes should be defined (descriptions provided).A guideline for CMMSp2 interface completeness should follow the guideline used for CMMSp1. The significant difference is that now all (or most) interfaces should have been identified.4.3	CMMS phase 3 – Supporting Software DesignSME Guidance in CMMSp3 should address the following areas:		Behavior resolution,		Behavior fidelity,		Behavior completeness,		Characteristic resolution,		Characteristic fidelity, and		Characteristic completeness.Behavior resolution is largely derivative from model resolution, so little additional guidance should be necessary. Note that the distribution of behaviors across functional areas is a determinant of fidelity by functional area.Based on the assumption that the SME has a reasonable concept of the breadth of results from a particular behavior, behavior fidelity can be expressed as an approximate percentage of model behavior agreeing with real world behavior, e.g., 80%, 90%. Note that this is based on a specific behavior. The hit to fidelity for not representing all behaviors has already been taken. What this means is that if you represent 50% of the behaviors and the behaviors represented have an average of 80% fidelity (agreement with results in the real world), the simulation has 40% fidelity (or 40% fidelity for one specific aspect). Such a value might qualify as high fidelity, when compared with existing constructive simulations.SMEs must be led to behavior completeness. Most officers do not think of themselves as finite state machines, yet it is very likely that that is how the developer will represent an officer in a simulation. In the military operations mission space, events lead to behaviors and behaviors lead to events. In the simulation development space, events must be expressed as interactions or changes in characteristics. The developer must be told what events affect behaviors and/or decisions, and what aspects of organization perception are modified as a result of specific events or behaviors. This is a difficult concept to communicate, and it likely won’t take the first time.Characteristic resolution, fidelity, and completeness are well-covered in the HLA OMT Attribute, Enumerated Datatype, and Complex Datatype Tables [17].5.	ConclusionsA practical approach to prescribing fidelity and resolution in CMMS has been presented. Concepts for describing fidelity, resolution, and related concepts in CMMS and associated simulations are suggested. Many of the benefits claimed for well-formed software development processes apply to well-formed knowledge (CMMS) development processes, particularly when the knowledge development process is well-integrated into the software development process. Indeed, realization of some of the benefits of well-formed software development processes is dependent upon the quality of the knowledge development process and its products. 6.	ReferencesJack Sheehan, Terry Prosser, Harry Conley, George Stone, Kevin Yentz, and Janet Morrow, “Conceptual Models of the Mission Space (CMMS):  Basic Concepts, Advanced Techniques, and Pragmatic Examples”, 1998 Spring Simulation Interoperability Workshop. Fran Dougherty, Frederick Weaver, Jr., and Mike Cluff, “JWARS contribution to the DoD Conceptual Models of the Mission Space (CMMS),” 1998 Spring Simulation Interoperability Workshop.Tom Johnson, “Air Interdiction Conceptual Mission Space Model using the DMSO CMMS Toolset”, 1998 Spring Simulation Interoperability Workshop, Orlando, FL 1998. David Kendrick, Jack Sheehan, Mike Hopkins, and Lana Eubanks McGlynn, “Authoritative Data Sources for us in DoD Modeling and Simulation,” 1998 Spring Simulation Interoperability Workshop. Lance Obermeyer, Jack Sheehan, and Mike Hopkins, “Order of Battle Data Interchange Format and Data Access Tools”, 1998 Spring Simulation Interoperability Workshop.Dynamics Research Corporation website, http://www.orlando-drc.com/cmmsinfo.htmInnovative Management Concepts website, http://cmms-toolset.imcva.com.Lester Foster, Kevin Yelmgren: “Accuracy In DoD High Level Architecture Federations,” The 1997 Fall Simulation Interoperability Workshop.Greg Schow, Rhonda Freeman, Tony Lashley, Harvey Meier, Jeff Swauger: “Derivation and Publication of Simulation Fidelity Metrics,” The 1997 Fall Simulation Interoperability Workshop.Paula Nouragas, Norman Watts, Parimal Kopardekar, John Richards: “Fidelity Assessment in Aviation Related Simulation Applications,” The 1997 Fall Simulation Interoperability Workshop.David C. Gross, Rhonda H. Freeman: “Measuring Fidelity Differentials in HLA Simulations,” The 1997 Fall Simulation Interoperability Workshop.Kalman C. Toth: “Software Engineering Best Practices,” lecture notes, Simon Fraser University, 1997.Department of Defense: “Draft Glossary of Modeling and Simulation Terminology,” 1995.Jeff Rothenberg: “A Discussion of Data Quality for Verification, Validation, and Certification of Data to be Used in Modeling,” project memorandum, The RAND Corporation, 1997.Jeff Rothenberg, Walter Stanley, George Hanna, and Mark Ralston: “Data Verification, Validation, and Certification Guidelines for Modeling and Simulation,” project memorandum, The RAND Corporation, 1997.Putnum P. Texel, and Charles B. Williams: Use Cases Combined with Booch/OMT/UML: Process and Products, Prentice Hall PTR, 1997.Department of Defense: “High Level Architecture Object Model Template, Version 1.2,” 1997.Author BiographyFURMAN HADDIX is a research scientist at the Applied Research Laboratories, University of Texas at Austin.  For the past two years, he has participated in the development of a DoD-wide Conceptual Models of the Mission Space sponsored by DMSO, providing the opportunity to build upon his prior experience in modeling and simulation with lessons learned from major program modeling and simulation development. 