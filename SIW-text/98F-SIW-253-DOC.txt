A Scaleable Solution to support Support Multicast Distributed Simulation ExercisesGrant ShackelfordBBN Technologies1300 N. 17th Street, Suite 1200Arlington, VA 22209(703) 284-4735 gshackel@bbn.comLarry A. O’FerrallNaval Research Laboratory4555 Overlook Ave, SWWashington, DC 20375Code 5523Integrated Communication Technology(202) 767-5110oferrall@itd.nrl.navy.milKeywords:multicast, architecture, LAN, WAN STOW, RTI, bi-level multicast, QCBMRABSTRACT: This paper will presenpresentst the communications solution developed to support the STOW '97 ACTD. There has also been significant work done to implement and test additional capabilities of the communications architecture not implemented for the STOW ACTD. The end result is a documented and tested solution that could allow future HLA compliant exercises using IP multicast to co-exist with other users on an ATM based Wide Area Network. This solution provides almost transparent high-speed data encryption that is NSA approved for Secret level data and higher. This communications solution also provides for large numbers of multicast groups (demonstrated up to 3000 groups) and provides protected bandwidth based on bandwidth requirements of the scenario that can be adjusted in real time based on changing requirements. This paper focuses on the overall communications architecture and how the various components of that architecture work together to provide an integrated scaleable solution to secure distributed multicast exercises while minimizing the load on individual hosts.1. BackgroundSIMNET was the first networked simulation system used widely for training purposes within DoD. The SIMNET system was based on a set of protocols developed by BBN Systems and Technologies under contract to DARPA. All traffic was broadcast to all simulators, and the only concept of filtering pertained to exercise ids, which merely allowed more than one simulation exercise to run on the same LAN. The systems that ran the SIMNET protocols were originally not networked across a Wide Area Network. The SIMNET protocols worked on a principle of each system having perfect knowledge of all network events. It was up to individual simulations to do any filtering of network traffic, determine visibility, sensor detection, and other capabilities based on limited knowledge of the environment and situation. As the capabilities of the systems and the requirements for the simulations began to scale upward not only in size, but also in geographical scope, scalability problems began to arise with these protocols. As DoD began to rely on distributed simulation systems more for training, research, and development efforts, the requirement to scale up the number of systems involved in a single exercise became more important. As simulation evolved to mean DIS and now HLA-RTI, efforts have been made to address scalability from a protocol perspective. The Synthetic Theater of War (STOW) program was developed to address some of these scalability issues from different perspectives in an HLA compliant exercise implementing the RTI.The Real time Information Transfer and Networking (RITN) program was funded as part of the larger STOW program, specifically to develop a communications infrastructure to support these scalability requirements. The work being presented in this paper is the end result of work originally begun under the RITN program headed by Dr. Stu Milner at DARPA.2. IntroductionThe intent of the RITN program was to develop a scaleable communications infrastructure to support larger simulation exercises like those envisioned for the STOW program. The simulation community had already begun to embrace the concept of interest management for simulation exercises. The STOW architecture needed to support interest management by whatever means the RTI used to implement this capabilityusing mechanisms that were compatible with the RTI. The communications architecture needed to stay fairly generic and provide services to the RTI that applications could utilize without knowing the low level details. The communications architecture needed to be transparent to the user while offering an array of services that were available through standard mechanisms.  The use of IP Multicast was the agreed uponchosen method to support interest management at the communications layer. This meant that to support highly scaleable exercises and provide maximum flexibility for interest management, the communications architecture needed to support large numbers of multicast groups.3. Communications PrerequisitesRequirementsThere were several prerequisites requirements that influenced the decisions made in determining the makeup of designing the communications architecture that could support the STOW exercise. Each of these prerequisites requirements influenced individual decisions, but the set of prerequisites requirements taken as a whole, led to several key decisions.The first overriding prerequisite requirement was for a Wide Area Network (WAN) with greater bandwidth than previous WANs that supported simulation exercises could provide. A goodFor example, was the DSI which was limited to a theoretical maximum of 1.544 Mb/sec in either direction to a given site. This constraint was due to the use of T-1 tail circuits to provide a site with access to the DSI network backbone. The actual limitation data rate was somewhat less than 1.544 Mb/sec due to the limitations of encryption devices used on the network. This observation brings up an important issue, namely that the encryption device can be, and has been in the past a limiting factor to available bandwidth into a single site.Another prerequisite requirement of the communications infrastructure was scalability. It was important that the network infrastructure not have any built in limitations. This includes limitations in the ability of the network to scale upward to support larger exercises. Scalability limitations can be limits in terms of bandwidth, latency, multicast group support or number of sites.Support for unobtrusive encryption to at least the secret level was also a prerequisite requirement for the communications architecture. It is also important that the security requirements not place other limitations on the network and be able to scale up to support even larger exercises with higher speed communications links. Since there are more joint service level exercises and even multinational exercises being conducted, the encryption device should be widely acceptable to all of the US services and releasable to allied nations. The security prerequisite requirement for unobtrusive encryption also  implies that the encryption device does not place limitations on the network in other areas.The encryption device chosen to meet the above criteria also mandated the use of ATM in part of the communications infrastructure. Since ATM is being deployed widely within DoD as a WAN architecture, this also seemed to be a logical choice. Further, the encryption device also mandated the use of UNI 3.1 signaling. The prerequisite for an ATM WAN also allowed for the architecture’s ability to be able to co-exist with other communities of interest sharing a common backbone infrastructure.The HLA-RTI specifies use of IP multicast by applications to deal with the issue of interest management. Since interest management was primarily done by having each area of interest sent to a particular multicast group, there is also a potential for thousands of multicast groups to be required. In fact, the initial requirement was for the LAN at each site and the WAN infrastructure in between sites to be able to deal with 10,000 multicast groups.Since the WAN requirement for ATM is stipulated by the encryption device and other goals of the project, and the applications are expecting an IP multicast environment, there is an implied requirement to integrate these two environments fairly seamlessly. This particular prerequisite ended up being one of the more difficult challenges, requirements to meet due to the lack of commercial-off-the-shelf commercial -off the shelf (COTS) products that provided the required IP/ATM integration.The STOW program was also to be the first major DoD simulation program to work within the new HLA standard. Specifically, the STOW program would work with the developers of the first implementation of an HLA compliant Run Time Infrastructure (RTI). This prerequisite requirement primarily applied to the applications, which had to be written to communicate with the RTI as an application frame work that provided low level services to the applications. What this meant for the communications architecture was a set of services that the communications infrastructure would support.The communications architecture for STOW also needed to meet the general requirement of being manageable by standard network management systems. Early on in the program, it was decided that all of the major components would be manageable via the Simple Network Management Protocol (SNMP)SNMP. In some cases, this was carried to an extreme to allow SNMP to be the primary means of configuring and managing some devices.There were also a few requirements that became obvious from work done under the RITN program and earlier simulation programs that could probably fit into the category of requirements from previous lessons learned. Several additional requirements for the architecture grew out of previous design decisions and lessons learned during integration. These prerequisites additional requirements included the ability to filter individual multicast groups to the host level, use of resource reservation and a mechanism to prioritize certain simulation traffic to guarantee delivery, and an overall capability of the network to work without any degradation over a shared ATM backbone.	4. Component SolutionsThe various components that make up the STOW communications architecture are shown in figure 1. The next few paragraphs will describe each component of this architecture as well as the function of that component.FASTLANE: The KG-75 was one of the most significant enabling technologies to contribute towards the success of meeting the goals of the STOW architecture. For the first time since there have been Wide Area Networked simulations, the traffic encryptors were not a bottleneck, and in fact were not utilized anywhere near their rated capacities. Encryption with the KG-75 was nearly transparent, and the close cooperation with the developers and program management office for the FASTLANE with the STOW program office, clearly contributed to this success. The KG-75 did place a few requirements on the LAN site architecture. Specifically, the devices on either side of the encryption device needed to be UNI 3.1 network side devicescapable. Also, the FASTLANE in the release used for STOW will is not pass any ATM routing protocolsPNNI capable and is not SNMP manageable. These requirements were easy to solve with the use of IISP static routing on either side of the KG-75.Since the FASTLANE did not participate in the PNNI environment, the local sites used IISP connections and static routing to establish calls to and from the black side.  The remote management issue was dealt with by providing remote dial in access to the KG-75 management port via secure telephone units.Figure 1.QCBMR: The QoS Capable Bi-level Multicast Router (QCBMR) addressed the incompatibilities between the requirement for an ATM WAN and the requirement for an IP multicast network. The QCBMR also helped address the issues of resource reservation across the WAN and prioritization of traffic. The QCBMR performs these functions by mapping IP multicast into ATM VBR point to multipoint signaling, mapping RSVP into ATM VBR QoS parameters, maintaining state on the multicast group membership of all sites in an exercise, and managing the use of virtual circuits as a scarce resource across the ATM WAN. The QCBMR was developed by BBN Systems and Technologies in response to a BAA issued by DARPA with help from the Naval Research Lab (NRL). The intent of the QCBMR development effort was to provide an operational prototype that could support the requirements of the STOW exercise and serve as a proof of concept and reference implementation of the functionality required to meet the above criteria.MCED: The MultiCast Edge Device (MCED) essentially acted as a multicast filter that was capable of filtering multicast traffic on a LAN to the individual host level. In some of the preliminary work done by NRL and BBN, research indicated that a significant portion of the processor resources of each simulation host was spent dealing with processor interrupts caused by broadcast IP traffic. Additional research also indicated that most hoste Network Interface Cards (NICs) did not filter multicast traffic effectively up tofor the numbers of groups required to support the STOW exercise. It appeared that, in most of these NICs, the NIC hardware filtering worked effectively up to on a rather modest number of multicast groups and then began to treat any extra additional anything over a modest number of groups as broadcast traffic. The objective of the MCED was to filter the traffic sent to an individual host (or port on an ethernet switch) such that only traffic destined for that host, or multicast traffic for groups joined by that host were delivered to the host (or ethernet switch port). The actual implementation of this functionality used for the STOW exercise was the Cisco Catalyst 5000 ethernet switch. Cisco delivered a capability to filter to the individual switch port for up to 3000 multicast groups at a single site. The Catalyst 5000 utilizes the processing power of a Cisco router to maintain multicast group membership tables, so the Catalyst works with a Cisco router at each site.IP Router: The IP router used to support the STOW exercise was the Cisco 7507 router. The primary role of the Cisco was to deliver unicast traffic between sites on the network. As mentioned above, the Cisco router also supported the Catalyst 5000 by providing multicast group membership tables to the Catalyst via the proprietary CGMP protocol. Although there were tests done with the Cisco routers running MOSPF and delivering multicast traffic to the sites over the ATM network, this method was much less efficient than the QCBMR and was also unable to provide any type of resource reservation or traffic prioritization. The Cisco routers were also utilized as part of the Network Time Protocol (NTP) time synchronization hierarchy and performed this function flawlessly.ATM Switch: Driven in part by the faster encryption requirement and availability of the FASTLANE, ATM was chosen for the WAN technology.  Operational capabilities of the FASTLANE along with the type of ATM service being provided by the commercial carrier, required that two ATM switches be used at a site.  There was both a classified and an unclassified ATM switch at each site. The FASTLANE was placed between these two switches.  Due to the requirement by the FASTLANE to have an ATM WAN, there was a requirement for both an unclassified and a classified ATM switch at each site. The ATM switch that was used for the STOW exercise and subsequent testing done by the authors was the Fore ASX-200BX ATM switch. The LS1010 switch from Cisco was also tested and met all of the requirements, but was not chosen for this particular exercise. The ATM switch was required to have demonstrated interoperability with the KG-75 and be capable of supporting several ATM standards such as UNI 3.1, ILMI, IISP routing, VBR and UBR traffic support, and traffic shaping.The basic requirements for the ATM switch were to support the UNI 3.1 signaling protocol as defined by the ATM Forum.   The ATM switches on the unclassified side of the network also ran the ForeThought PNNI routing protocol due to the fact that PNNI 1.0 had not been finalized by the ATM Forum. The ATM switch was required to have demonstrated interoperability with the KG-75 and be capable of supporting several other ATM standards such as ILMI, IISP routing, VBR and UBR traffic support, and traffic shaping.  Due to the use of VBR SVCs and deficiencies in state of the art ATM NICs available to the QCBMR, the Call Admission Control (CAC) algorithm in the switches needed to be adjusted to support the SVCs being established.  On the link between the United Kingdom and the US, traffic shaping was implemented on the black ATM switch to control the traffic being placed on to the commercial network.  Traffic shaping was not placed on the CONUS side of the network due to less severe policing policies imposed by the commercial carrier.  However, in the future, shaping will be necessary in a shared environment.IGMP v2: As a software component of the architecture, use of the Internet Group Management Protocol, version 2 (IGMP v2) as defined in RFC 1112 is easily overlooked. However, the use of IGMP v2 provided a very important capability to the multicast capable devices in the network that would not have been available otherwise. Specifically, IGMP v2 provides for explicit leaves of multicast groups. The explicit leave function allows a host to send a message indicating that it is leaving a multicast group immediately. The alternative under IGMP v1 is for a host to merely cease to reply to group membership queries for that group until the time out interval is reached at which time the multicast router records that the host in question has left that group.	GPS Time server: In order to provide a robust network time service using the Network Time Protocol (NTP), several GPS Time servers were deployed across the STOW WAN. GPS time serverstimeservers were deployed at WISSARD, NRaD, NRL, and ARL-UT. The GPS time serverstimeservers allowed the implementation of a hierarchical distribution of time serverstimeservers and clients that had individual host machines synchronizing to a stratum 2 time server on the local LAN. The use of multiple GPS time servers geographically distributed across the WAN provided a robust accurate NTP service to the STOW sites.	5. Data Flow through the STOW ArchitectureAs an aid to understanding how the STOW communications architecture works as a system, it is probably most helpful to follow the flow of a packet sent by a ModSAF host at site A through the communications system. We will trace the packet through the system with two sets of pre-existing conditions. The first condition is when thea packet that has a destination address that is afor a multicast group that no other host at site A has joined. The second condition is when thea packet that is addressed to a multicast group that another host at site A is sending traffic to and has joined via an IGMPv2 join message.New multicast group: In the case of a simulation host that sends a packet with a destination address of a multicast group that no other host at site A has joined. Host 1 sends a packet destined for group 224.100.0.100, which shall be referred to from now forward as simply group 100. Host 1 at site A will make a call to the RSVP API built into the ModSAF application and make a reservation for group 100 based on host 1’s knowledge of how much traffic host 1 intends to send to group 100. Host 1 also sends an IGMP join message for group 100 since an entity on host 1 sending traffic to group 100 should receive any traffic sent by other entities to group 100.The Cisco router at site A receives the IGMP join message and makes an entry in the local group membership table maintained by the router. The router also sends this updated group membership information to the MCED (a Cisco Catalyst 5000) using the proprietary CGMP protocol. The MCED then makes an entry in it’s local group membership table to tell it to forward multicast traffic for group 100 to the port that is associated with host 1.The QCBMR at site A also receives the IGMP join message for group 100. The QCBMR first checks it’s local group membership table to see if any other host at site A has already joined this group. Since this is a new group join for site A, the QCBMR adds this entry to it’s group membership table and then immediately sends this update information for the global group membership table to the other QCBMRs using the QCBMR Bi-level Multicast Routing Protocol (BMRP).The MCED at site A forwards the actual multicast packet going to group 100 to the QCBMR at site A. The QCBMR uses the information previously looked up fromchecks the global group membership table to determine if any other sites have requested traffic addressed to group 100. If no other site has requested traffic for group 100, the QCBMR discards the packet and does not forward it to the WAN. If another site has joined group 100, the QCBMR will immediately forward the packet for group 100 to all sites using the default ATM virtual circuit (VC) that is set up at initialization to go to all other sites. The QCBMR at site A will also make an entry into a signaling table to set up a VC for group 100 traffic to go to sites that have joined group 100. This VC will either be set up as a new VC if current conditions permit, or the flow for group 100 will be added to an existing VC if there is already one that goes to the group of sites that have joined group 100.The ATM network will forward the traffic across the virtual circuit currently carrying the group 100 traffic. This VC will pass through the FASTLANE at site A and be encrypted. Then the packet will go across the public ATM infrastructure to the receiving site(s) and pass through the receiving site(s) FASTLANE to be decrypted. The traffic will then be passed through the classified ATM switch and forwarded to the ATM interface of the receiving site(s) QCBMR. Since this traffic was carried to all sites via the default VC, each QCBMR will look at it’s local group membership table and determine if a host at that site has joined group 100. If a host at that site has joined group 100, the QCBMR will forward the packet to the site LAN. If no host at that site has joined group 100, the QCBMR will discard the packet and it will never reach the site LAN.   Since the QCBMR is doing VC management and is trying to reduce the amount of signaling, it will sometimes use an SVC to deliver multiple multicast groups to a merged set of destination sites, thus allowing a small bit of traffic to be delivered to sites that have not joined that group.  This over delivery is held to a minimum and balanced with the delay and overhead of signaling a new call.Once the traffic has reached the LAN of a site that has one or more hosts that have joined group 100, the MCED at that site will forward the packet to the port associated with the hosts that have joined group 100.Additional packets to a group already joined: In the case of packets sent to a group that has already been joined by a host, there are a few differences in the data flow through the system. Host 1 at site A sends additional traffic to group 100. If the simulation traffic being sent to group 100 has changed, a new RSVP reservation will be sent with the new traffic flow characteristics, otherwise the previously sent reservation is still used. The MCED at site A forwards the traffic addressed to group 100 to the QCBMR. The QCBMR at site A checks its local VC lookup table to see if the flow for group 100 has been moved off of the default VC. If this flow has been moved to another VC that only goes to the sites that have requested traffic for group 100, the QCBMR forwards the traffic to that VC. If the flow for group 100 has not been moved yet, the QCBMR will forward the traffic on the default VC. The QCBMR will also queue up a signaling request to move this flow to another VC if this request is not already pending.The ATM network will carry the traffic for group 100 across the ATM network, through the sending side FASTLANE. The sending side FASTLANE will encrypt the traffic and forward it through the network to the FASTLANEs at the receiving site. The receiving site FASTLANEs will decrypt the traffic and forward it to the ATM switch at those sites. The ATM switch at these sites will forward the traffic to the attached QCBMR ATM interface.The receiving side QCBMR will look up group 100 in its local multicast forwarding table to determine if a host at that site has joined group 100. If a local host has joined group 100, the QCBMR forwards the traffic to the site LAN via the MCED. If not host at the local site has joined group 100, the QCBMR discards the traffic and it never reaches the site LAN.Once the receiving site MCED has received the traffic, it forwards the traffic to the port associated with the host that has joined group 100.6. ResultsAs a result of the STOW communications architecture, the STOW exercise was supported with a reliable stable communications infrastructure that supported up to 3000 multicast groups, provided reliable timely traffic delivery, used network resources efficiently, and did not limit the scope of the STOW 97 Exercise. The communications architecture was transparent to the applications. The appearance to the applications was that of a large multicast LAN with per host multicast filtering.IP to ATM bi-level multicast was used to provide the performance characteristics of an IP multicast LAN across an ATM WAN. The bi-level multicast implementation along with the use of IGMPv2 provided rapid join and leave times across the WAN for multicast groups. The bi-level multicast implementation in conjunction with the MCED provided support for thousands of multicast groups and multicast group filtering to the site level and the individual host level. The use of IP to ATM bi-level multicast also introduces a capability to do resource sharing across an ATM backbone. The multicast IP network can be overlaid onto an existing ATM WAN infrastructure without placing an extraordinary burden on the ATM infrastructure. This ability enables future sharing of a common ATM WAN infrastructure between multiple communities of interest while still providing the simulation exercise with guaranteed bandwidth reservation for real-time traffic. Bi-level multicast also intelligently uses bandwidth to forward traffic to other sites across the WAN only when necessary or efficient.Based on data collected during the STOW exercise, on average less than 30% of the traffic on a given site LAN was delivered to the WAN. Also, on average, 95% of the traffic arriving at a site across the WAN was requested by a host at that site. These statistics combined demonstrate the efficient use of costly WAN resources by the STOW communications infrastructure.Data collected during STOW 97 as well as data from tests done after the STOW ACTD indicate that the current architecture is clearly scaleable to support larger exercises. As with the STOW ACTD, there will probably be additional problems and challenges encountered in scaling the architecture up to higher levels, but there are no fundamental limiting factors that prevent this architecture from scaling to support exercises on a much larger scale than the STOW ACTD.The FASTLANE encryption was clearly an enabling technology. The use of FASTLANE allowed the STOW exercise to be conducted at the Secret level with no degradation in communication services due to the encryption. Considering that the STOW ACTD was conducted using release 1 of the FASTLANE, relatively few problems were encountered and those were typically dealt with quickly by the developers. Use of the FASTLANE also allowed the use of a public ATM infrastructure in the middle of the network. This is a significant gain in that it permits the procurement of ATM backbone service with a “buy buy-what what-you you-need need-model”. It also encourages the future use of a shared public ATM infrastructure that supports several different communities of interest.7. Lessons LearnedThe resource reservation capability of the STOW architecture was not utilized during the conduct of the STOW ACTD. The reason behind this was insufficient time to test the RSVP implementations on the various platforms and the possible interactions of the resource reservation process with other network processes. Subsequent testing proved that this was probably a wise decision based on the time constraints at the time. This testing proved that the implementation of RSVP available at the time was not very robust and did not deal well with maintaining reservation data for large numbers of multicast groups. We were able to implement a work around by filtering the RSVP messages within the MCED and sending that data to the QCBMR only. This filtering prevented individual hosts from trying to keep reservation status on hundreds of multicast data flows. This work around proved satisfactory for the testing performed after STOW, but is clearly not a good long term solution. Also, the RSVP capability in the version of the RTI used for STOW is not currently planned to be included in future version of the RTI. For these reasons, we believe the simulation community should be involved in the IETF efforts to widely support resource reservation for IP protocols.Use of the NTP for simulation hosts as well as network devices proved to be a useful tool in troubleshooting problems occurring across the network. We also demonstrated the ability to keep hosts scattered around seven sites distributed across the US synchronized within a few milliseconds of each other. This was done using a total of four GPS based NTP receivers at a cost of about $2000 each. The value added by having a reliable source of time synchronization was well worth the expenditure.There are still many problems that exist with both hardware and software implementations of multicast support. The requirement for an MCED was based upon research which indicated that without per host filtering, a good portion of the processor utilization of each host on the network was dealing with network traffic not required by that host. Although the Catalyst 5000 used for STOW was limited to 3000 multicast groups, performance improvements are anticipated that would increase this number significantly. The requirement for an MCED would not exist if the NIC hardware in each host were capable of filtering on multicast addresses. In this case, the MCED would merely need to be an ethernet switch with no multicast filtering abilities.The QCBMR work done to support STOW was basically a reference implementation of bi-level multicast with some added functionality to support this capability. Although fully functional, some problems and performance issues were not resolved and thoroughly tested until after the ACTD. Some of the QCBMR functionality is ground breaking work, specifically in the area of VC management. The hardware used for STOW was a COTS low cost Intel based platform. Between the QCBMR, which used NetBSD on Intel hardware, and the Linux on Intel hardware platforms used as ModSAF hosts, STOW demonstrated that less expensive PC hardware in conjunction with Open Source operating systems could be used effectively in simulation. NRL has a goal to pursue a commercial implementation of the QCBMR functionality in a COTS product. STOW demonstrated that there is a requirement for this functionality.8. ConclusionsThe STOW communications architecture clearly demonstrated a scaleable system to support IP multicast traffic over a WAN supporting thousands of entities and thousands of multicast groups. Given the amount of work and funding put into the development of this communications infrastructure, it seems evident that this work should be considered for use in support of other large scale simulation programs.The ability to support simulation exercises on a shared backbone will eliminate the need in the future for stove pipe networks to support the simulation community. Additional work should be done by the simulation community in the areas of resource reservation and multicast scalability to allow continued improvement in these areas to speed progress towards a goal of a common communications infrastructure that can support multiple communities of interest.Author BiographiesGRANT SHACKELFORD is a Network Engineer with BBN Technologies in Rosslyn, VA. He has been working in the simulation field for over 10 years and worked on the original SIMNET contract for BBN. He was a member of the Real-time Information Transfer and Networking (RITN) project which developed the network technologies for STOW. LARRY O’FERRALL is an Electrical Engineer at the Naval Research Laboratory in Washington DC.  He is involved with IP and ATM research in the Integrated Communications Technology Branch.  He was a member of the Real-time Information Transfer and Networking (RITN) project which developed the network technologies for STOW. 