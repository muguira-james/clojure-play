GUIDELINES FOR CONDUCTING VV&A OF SIMULATION FEDERATIONSRichard  BernsteinPeter EirichDale PaceRandy SaundersJohns Hopkins University Applied Physics Laboratory11100 Johns Hopkins RoadLaurel, MD 20723-6099240-228-8723, 240-228-7264, 240-228-5650, 240-228-3861 HYPERLINK "mailto:Richard.Bernstein@jhuapl.edu" Richard.Bernstein@jhuapl.edu, Peter.Eirich@jhuapl.edu  HYPERLINK "mailto:Dale.Pace@jhuapl.edu" Dale.Pace@jhuapl.edu,  HYPERLINK "mailto:Randy.Saunders@jhuapl.edu" Randy.Saunders@jhuapl.eduKeywords:VV&A, HLA, FEDERATIONSABSTRACT: This paper proposes a set of guidelines for conducting Verification, Validation, and Accreditation (VV&A) of High Level Architecture (HLA) Federations and other distributed simulations.  The rationale for developing VV&A guidelines is based on the increasing use of HLA, and the need for focused attention that is experienced-based and which may not apply to standalone model or simulation VV&A experiences.  The set of guidelines proposed here is based on conducting VV&A of several HLA Federations and other distributed simulations.  Although theoretical and conceptual aspects are drawn upon and considered, the guidelines proposed below are derived primarily from specific programs and promulgated as the most important focus areas when conducting VV&A of HLA Federations.1. IntroductionThe rationale for developing a set of VV&A guidelines for HLA Federations is based on the increasing use of HLA, as evidenced by the increasing development of HLA federations, the transition of HLA to an IEEE standard, and the general understanding in the M&S community that the development and use of simulations requires VV&A efforts to insure credibility of those simulations.Much VV&A work to date has been oriented toward theoretical and policy aspects of how VV&A should be conducted.  This paper, on the other hand, focuses on proposing a methodology for VV&A that emphasizes the undertaking of specific operational tasks.  There are many guidelines for how VV&A efforts should be planned.  In addition, simulation and HLA Federation development guidance is applicable to VV&A.  However, this paper defines guidelines which are specifically focused on HLA Federations and which are based on performing VV&A on numerous specific federations, simulations, and mixed simulation/Hardware-in-the-Loop test tools.  The guidelines are summarized and compared with VV&A of standalone models or simulations in the table at the end of this paper.2.0 VV&A Guidelines for HLA Federations2.1 Each federate in an HLA federation should have its individual VV&A activities documented to allow evaluation of a federate’s compatibility with and appropriateness for the federation.VV&A activities for an overall Federation should be separate from the VV&A activities for the internal aspects of each federate.  If incorrect results are output from a federate and are used as inputs to another federate, the ultimate results produced by the federation will likely be incorrect as well.  Typically it is not practical to conduct a comprehensive V&V of individual federates, and often an acceptable substitute for a comprehensive VV&A is for the user or developer to certify that the federate is appropriate in the context of the federation.2.2 Each federate in an HLA federation should be expected to publish all the data and subscribe to all the data required for the analytical or training objectives of the federation to be accomplished, and VV&A of the federation should be based on the evidence of cause and effect of the simulated entities and interactions.To fully verify and validate a federation, it is necessary to make test runs in order to demonstrate that the output elements published by each federate were generated by the logically correct inputs - either from the same federate or a different federate - produced earlier in time.  Verifying and validating the existence and correctness of the cause and effect of inputs and outputs is necessary to provide conclusive proof that the federation is operating correctly.  Often it is not possible to develop that kind of evidence because federation developers do not design their Federation Object Model (FOM) and federation to allow the Run Time Infrastructure (RTI) to transmit sufficient data for recording.  In undertaking VV&A of several existing federations, it has been discovered that insufficient data is published to prove that the federation has been implemented correctly.  In cases in which insufficient information is available, a federation may be inferred or judged to be correct to a limited degree based on a limited set of test data, but correctness cannot be unequivocally demonstrated.  The following steps are required to demonstrate that a federation design has been implemented absolutely correctly:Identify the sequence of events which support the analytical Measures of Effectiveness (MOEs) and Measures of Performance (MOPs) that the federation is required to generate;Design software thread-based FOM objects and attributes and identify desired cause and effect relationships;Design data flows and enable these data flows through Data Management and Data Distribution Management and document these in the Federation Execution Planners Workbook (FEPW);Define acceptability criteria for how close the simulation results need to be to real world systems and capabilities based on the simulation’s intended use;Design test runs, using a data collector to define what object instances are required to be published and subscribed to by the appropriate federates;Run the federation and analyze the data output by the RTI so that it can be verified and validated;Check the output object attributes to insure they are consistent with the federation’s conceptual model and other requirements;Insure that the timing of the outputs are correct with respect to inputs of the appropriate object instances, and are within the stated acceptability criteria for timing.Of the steps above, the first four are generally considered the responsibility of the simulation developer.  (The fourth step, definition of acceptability criteria, can also be the responsibility of the user or sponsor for a given exercise.  Often, the VV&A agent assumes this responsibility.)  The last four steps are generally considered the responsibility of the verification or validation agent.2.3 Interactions between live and simulated forces, or virtual and simulated forces, should be treated as interactions between simulated forces.The HLA was designed to facilitate the development of distributed capabilities not only for simulation purposes, but also for the integration of live and virtual forces with simulated forces.  This poses a challenge for VV&A.  Live and virtual forces cannot be verified or validated in the same way that simulated forces can be verified and validated.  A challenge in a live/virtual/constructive federation is how to apply VV&A to the live or virtual component.  Typically, the live or virtual component in a federation falls into one of three categories:Human-in-the-Loop (HITL) simulator in which a human operator can take one of several authorized actions.  A typical example would be a distributed simulated battle management interoperability exercise which includes human participation, and which any action a human operator is able to accomplish is permitted by the real world C4I equipment;A “free play” exercise, in which a human controller has freedom to control live or simulated forces;A real system under the control of its human operator(s).In all these cases, rules of the exercise may govern a human operator, controller, or commander’s actions.  Generally, HITL actions or free play command activities will be part of the rules of a specific use of a Federation.  These rules are generally determined by the developer or user.  Hence, HITL and live operator actions should be assumed by a VV&A agent to be valid for purposes of the federation’s use, unless the federation developer or user specifically wants human performance to be assessed as part of the VV&A effort.2.4 Infrastructure verification must be accomplished to demonstrate the operational correctness of the federation.Infrastructure verification is the verification that the components of the distributed federation operate correctly.  This verification considers different aspects of the federates than model verification or data validation consider.  Model verification checks that the simulation software design is a faithful representation of the desired conceptual model.  Infrastructure verification is an engineering analysis of the design of the computer system which will run the simulation to check that execution bottlenecks or communications technologies will not introduce anomalous behaviors that would not be expected from stand-alone assessment of the simulation software.  As any engineering analysis, it needs to identify which resource types impact performance and assess the level at which the design provides them.  Typical resources in a distributed federation include the following:Power;Cooling;Processor time;Memory;Storage;Storage bandwidth;Network bandwidth;Network latency;Federation throughput.These resources are typically allocated in three categories:Simulation (producing useful results);Overhead (lost to the operating system, HLA RTI, networking stack, hardware controls, routers, hubs, encryption gear);Spare or margin (covering the expected variance of the other two categories and available to support expansion.)As part of sound engineering practice, these allocations are compared to values measured under the expected operating conditions.  It is essential that the measured operating conditions cover the full expected range to avoid federation failure at times of maximum load, initialization, or other edge situations.  Measuring only average, steady state, or worst case situations has produced spectacular failures in the past.  Table 1 shows an allocation matrix for a typical small federation.  Tables such as these serve to make infrastructure verification results easier for people not familiar with V&V to understand.ResourceAvailableSimulationOverheadMarginPower5,000 KVAN/A40%60%Cooling10 tonsN/A70%30%Processor Time (per processor)60 sec/min (100% availability)50%30%20%Memory5,000 MB3,000 MB500 MB1,500 MBStorage400 GB350 GB10 GB40 GBStorage Bandwidth100 MB/sec20 MB/sec<1 MB/sec80 MB/secNetwork Bandwidth900 Kb/sec400 Kb/sec300Kb/sec200Kb/secNetwork Latency35 ms max.20 ms12 ms3 msFederation Throughput900 ops/sec99%<1%N/ATable 1. Representative resource allocation matrix showing the planned uses of federation system resources.The issues associated with allocation of many resources are not unique to federated simulations.  For example, engineers considering environmental factors like power or cooling do the same analysis for a simulation federation that they would do for any distributed processing application.  Infrastructure verification goes beyond these basic computer engineering considerations.  It is necessary, but not sufficient, for a simulation federation to run without hardware problems.  Infrastructure verification also considers simulation unique resources that are allocated to assure the federation as a whole meets the requirements of the intended use.  The most significant resources with unique federation characteristics are:Network bandwidth margin;Host computer CPU margin;Network transfer latency;Federation throughput.Network bandwidth margin is the unused percentage of available bandwidth on each communications link in the distributed network.  The total network loading for a federation is an acceptable measure of bandwidth usage to collect due to the networking protocols, cost/staff constraints, and data collection capabilities available.  Bandwidth margin that is always greater than zero (indicating that the entire bandwidth of the link is not being used) is a typical acceptability criterion.Host computer CPU margin is defined as the unused percentage of available CPU or processor capacity on each of the computers in the federation.  The total CPU loading for federation data is an acceptable measure of computer margin to collect due to the operating systems characteristics, cost/staff constraints, and data collection capabilities available.  Margin greater than zero (indicating that the entire node CPU capacity is not being used) is considered acceptable.  Computer processor margin values of zero (indicating that the host CPU is completely utilized) are acceptable provided that the 100 percent utilizations do not exceed defined time periods that are federation-specific.Network transfer latency measures the time delay between two federates that need to share a piece of information.  The total latency is defined as the time that passes from when the first federate sends a changed attribute, the first RTI relays it across the network to the second RTI, until the second RTI makes it available to the second federate.  Acceptable data transfer latency is therefore a function of process scheduling, event processing time and data transfer time between interacting simulations.  It is noted that some of the factors contributing to acceptable latency are themselves a function of processor speed, simulation task scheduling, and algorithms designed for required update rates, independent of data transfer times.  Acceptable data transfer latency may also be different for various interactions between simulations and functionsFederation throughput measures the ability of a federation to provide the entire set of interaction data (object attribute updates and interactions) from the set of sender simulations to the set of receiving simulations in a timely manner without loss.  Acceptable data transfer throughput for a federation is the ability of that federation to not drop data while maintaining a “real-time” scenario state advance, assuming a coordinated system-wide time base, with each simulation scheduling and processing events with acceptable latency relative to the time base.  Acceptable throughput is achieved if all messages sent by the federation are received and processed by the intended recipients while the desired (“real-time”) state advance is maintained.  Indications of unacceptable data throughput are as follows:Not maintaining a real-time state advance for the scenario;Exceeding acceptable bandwidth margin;Exceeding hosts’ acceptable CPU margin;Simulations not receiving messages for attribute updates and interactions that are subscribed to and are actually sent by another simulation.2.5 Federation Integration Must be ValidatedJust as the infrastructure must be validated to assure that the connections between federates are correct and individual federates must have V&V to assure they correctly represent their objects, the integration of each federate into the federation must be validated.  A federation expresses its integration requirements in two forms.  The static federate interfaces are defined in the FOM, objects, interactions, and the related data elements.  Verification of each call to the RTI is needed to assure that the data types and values provided match the FOM definitions.Beyond the static interfaces defined in the FOM, federations define dynamic interface requirements in the Federation Agreements Document (FAD).  However, these English language rules must be enforced by people.  This is an expensive and inaccurate approach when compared to computer automation.  Data logging tools exist today to monitor and record HLA traffic.  Future federations will have access to a significant data stream through federate-independent interfaces.  Analysis of that data stream can produce test measurements at a reduced cost.  Development of measurable federation agreements is essential to maximizing the benefits of these techniques.  Measurable federation agreements depend on concepts which should be validated in the conceptual model.Where measurable federation agreements exist, a data logger database can be used to produce validation results in a batch process.  After the data logger is running, the federation executes a series of relevant and representative runs.  Each measurable rule is represented as database criteria and all logged event records not matching the criteria are flagged as “errors” to be investigated.Where federation data flows are documented (as suggested in section 2.2 above) a similar data logger approach can be employed.  The logger is used to capture scenarios matching the key “sequences of events” (2.2a).  The resulting database can be expected to show causal links following the data flows (2.2c).  If absent, the data path is not working, and, if present, they provide additional criteria for systematic measurement and investigation of anomalies.2.6 Data Validation Must be ConductedData validation means that all simulation input data should be compatible and consistent in scope, fidelity, and granularity among all the federates in a federation, and also should be compatible and consistent with the objectives of the federation and/or a specific use of that federation.  A typical data problem encountered in VV&A projects is the use of data items at incompatible levels of data aggregation, e.g., use of data in one federate that is appropriate for a theater or campaign level simulation while another federate uses data appropriate for a mission or engagement level simulation.  Another frequent data problem concerns the use of inconsistent and conflicting environment and terrain data by different federates.The source of data may (or may not) be important for the VV&A of some federations.  Typically, for DoD systems, System Program Offices (SPOs) are authoritative data sources for their system, while the intelligence community is an authoritative data source for threat systems and capabilities.  Depending on the objectives of the federation or its intended use, a SPO or intelligence community component should validate the “goodness” of the data used for a particular purpose.A related issue to the quality of system parametric data is system operational data.  Typically, operational capabilities may be different from parametric system data.  Parametric system data is usually test and evaluation or design data, and may represent the best capabilities of a given system under optimal conditions.  Operational data may address how well that system works in specific tactical situations.Typically, system capabilities depend on a combination of the algorithms embedded in the simulation with the input data sets.  The algorithms and data must be considered together.Data validation must include data hard-coded into federates, and federates should “expose” this data.  Especially in the case of legacy simulations, a federate may be designed so that some data values (e.g. weapons system performance parameters) may be either hard-coded into the source code, entered manually by the user through a graphical user interface, or read in from a file with a non-standard, not easily parsable, file format.  Often, such legacy simulations will not have been designed to “report out” these internal or user-entered data values in a readily usable format.  In such cases, the reconciliation between federates of the data values used can be difficult, and can lead to the latent discovery of data problems, very late in the debugging cycle for the federation.  For example, variances among federates in coding the unique entity enumerations in a campaign or mission simulation can be difficult to track down solely by observing the simulation results and looking for abnormalities.It is important, if at all possible, to incorporate into each federate being added to a federation, the ability to “dump” into a readily-processable format (e.g. a spreadsheet) all the critical internal parameters that describe the objects being simulated.  Time for this additional programming work should be allowed for in the overall development schedule.  In this manner, the consistency of the object identification data and the performance characteristics being employed can be verified by the VV&A agent and the model developers prior to the initiation of federation testing.  More of the test and debug time will then become available for functional testing rather than data consistency checking.  Such a “parameter data dump” capability should be designed into all new simulations that are intended to be HLA compliant.  The VV&A agent should be prepared in advance with tools and software as needed to rapidly compare the parameters being employed within federates.2.7 When practical, the federation test results should be benchmarked or anchored to authoritative data or results from authoritative models or simulations.The most desirable kind of model validation is comparison of model results with data collected in real-world tests.  However, comparing model results to live test data is often not possible or practical because of limited test data, non-representative test configurations or scenarios, insufficiently accurate or non-existent test instrumentation, and difficulties in comparing stochastic test data to deterministic model data.  Comparing model results to test data works best for engineering models; it is more difficult and frequently impossible for highly aggregated models, such as mission- and campaign-level models.  There can be special difficulties in comparing federation results to real-world tests if the federation is intended to examine systems and capabilities which have no real world analogue.  Typically real-world data, especially for an entire federation, are not available in these cases.  However, it may be possible to compare parts of federation results to real-world data.In the absence of appropriate real-world data to use in federation VV&A, standards to which individual federates (and sometimes the entire federation) should be validated may be appropriate.  Such a standard, often called an “anchor” or “reference model” can consist of the following:Authoritative data derived from a requirements document or other authoritative source;Operational or developmental test data;Results from a model or federation of higher fidelity;Results from a model or federation of similar fidelity but which represents a greater degree of confidence;A Concept of Operations description for a future system or capability.Although validating a model to a reference standard is a useful technique, the VV&A agent and model developer or user must agree on the following:That the reference model and the model to be validated against the reference model are appropriately similar;That the input data and scenario(s) are consistent enough for a valid comparison;That the number of runs for both the reference model and the model to be validated must be sufficiently large for an acceptable statistical conclusions;That measurement points for the anchor and model to be validated must be defined and must be substantially comparable, although the nomenclature as defined in each model may be different;That measurement metrics must be defined.  A typical example is whether mean or median should be used as a measurement of central tendency;That acceptability criteria must be defined to determine how close the reference model and model subject to validation must be for either a build, a particular intended use, or a class of intended uses.One situation in which federations cannot be practically anchored is when federations that have no real-world equivalent are developed.  These types of federations can be and have been validated nevertheless and generally fall into two categories:Federations that represent plausible but hypothetical real-world event using existing systems;Federations which represent capabilities that do not exist today, but which may be developed in the future.In both cases, federation VV&A requires a greater reliance on SMEs than would normally be desirable.  In such cases, it is important that SMEs be used appropriately, according to the guidelines discussed elsewhere in this paper.  In particular, SMEs may be used to determine whether the hypothetical real world events or future capabilities are plausible and reasonable.2.8 When conceptual models are not available, a VV&A project can utilize other materials but if an acceptable substitute is not available, the VV&A effort will be incomplete.Elsewhere, this paper noted the importance of conceptual models.  However, frequently federation developers do not produce appropriate (or any) conceptual models.  In that case, alternative products, such as systems engineering documents may be substituted.  All of these types of products may be developed with many different types of tools, or may be simple block diagrams created from a drawing program.In all cases, both the quality of conceptual models or appropriate substitutes that are used for VV&A can limit the confidence or utility of the federation.The following table shows how standard systems engineering products can be used as substitutes for conceptual models.Systems Engineering ProductPrimary Validation ApplicationFunctional Block DiagramValidate federation design functionsPhysical Block DiagramInsure object and attributes in FOM are exchanged correctlyData Flow DiagramInsure information passed in a federation design is logicalArchitectural DesignInsure components in a federation are logical and consistent with federation designs and can be demonstrated as being correct with a data logger3.0 SummaryThis paper has proposed 8 guidelines for conducting VV&A of HLA Federations and other distributed simulations.  The guidelines are based on best practices identified after numerous VV&A projects, and it is recommended that these 8 guidelines be used as the primary guidance to be supplemented with other sources in conducting VV&A of Federations and distributed simulations.Table 2. Use of systems engineering products for Federation validation.Key VV&A CriteriaStandalone Model or SimulationFederation1.   Federate VV&ANecessary for VV&AEach federate should have a completed VV&A effort as a separate effort from the overall federation VV&A.2.   Publishing and subscribing of objects between federatesNot applicableData logger should record intra-federate and inter-federate objects attributes if they are important in terms of cause and effect actions pertaining to the intended analysis.3.   Interactions between live, virtual, and simulated forcesRarely applicableHLA is designed to facilitate the integration of live and virtual forces with constructive simulations, and this is frequently done.  Because this integration is typically part of the objective of the federation, live and virtual forces generally should be treated as correct from the standpoint of the federation developer’s objective.4.   Infrastructure verificationRarely applicable or of minor importanceCritical to verify that the federation speed and accuracy are implemented correctly.5. Federation integration must be validatedNot ApplicableUse of data loggers to supplement human checking is necessary for accuracy.6.   Data validationIssues are similar to HLA Federations except that there is no issue of mismatches between data sets of different models.Differences in fidelity, granularity, and accuracy must be accounted for between different data sets used by different federates.7.   AnchoringTypically, reference standards are more readily available for a standalone model or simulation than for a federation.Because federations allow developers and analysts more freedom to simulate systems and capabilities for which there is no real-world equivalent, additional complexity is added in defining an appropriate real-world standard.8.   Conceptual model validationInterface issues between models do not exist.Interfaces between federates must be validated and verified as correct and this represents an additional level of complexity.Table 3. Summary of VV&A guidelines for federations compared with standalone models or simulations.Author BiographiesRICHARD BERNSTEIN Is a Senior National Security Analyst at the Johns Hopkins University Applied Physics Laboratory.  He has over 20 years experience in military intelligence analysis, and in verification, validation, and accreditation of simulations.  He received his B.S. degrees in Economics and International Studies from The American University and is pursuing a M.S. in Systems Engineering at the Johns Hopkins University Whiting School of Engineering. PETER L. EIRICH is a member of the Senior Professional Staff at The Johns Hopkins University Applied Physics Laboratory (JHU/APL), where he works on projects involving Simulation Based Acquisition environments and on procedures for the Verification, Validation, and Accreditation of models and simulations.  He received BS and MS degrees in Electrical Engineering from MIT in 1970, an Engineer’s Degree in Electrical Engineering from MIT in 1974, and an MS from MIT’s Sloan School of Management in 1974.  Previously, he has been active in the development of both US and international standards, with leadership roles in advisory groups, working groups, and technical committeesRANDY SAUNDERS is a Senior Staff Engineer at the Johns Hopkins University Applied Physics Laboratory.  He has over 20 years of experience in the design, implementation, and integration of high-fidelity simulations for military and business customers.  He received his M.S. degrees in Engineering from Harvey Mudd College in 1980 and in Computer Science from the University of Southern California in 1985.  Mr. Saunders has been involved in distributed simulation standardization since the first DIS Workshop, both DIS and HLA standards committees, and as the initial Technical Area Director for the Real-Time Platform Reference Federation Object Model (RPR FOM).  He joined APL in 2001.DALE PACE is a member of the Johns Hopkins University Applied Physics Laboratory Principal Professional Staff.  Dr. Pace was co-chair for the Foundations for V&V in the 21st Century Workshop (Foundations ’02).  He was co-chair of the MORS SIMVAL 1999 Workshop and is a member of the DMSO VV&A Technical Support Team.  He is involved in a standards committee on V&V in computational solid mechanics for the American Society of Mechanical Engineers (ASME) International.  He led Summer Computer Simulation Conferences in 1991 (Program Chair) and 1994 (General Chair).  He was an initial co-chair of the VV&A Group for the Distributed Interactive Simulation (DIS) Workshops and as an initial member of the Simulation Interoperability Workshop (SIW) Conference Committee.  He also taught in the graduate technical management program of Hopkins’ Whiting School of Engineering from the mid 1980s to the mid 1990s.  The term “theoretical” includes both ideas based on conceptual analysis of VV&A and ideas based on general VV&A experience not directly related to specific examples.  APL’s role in VV&A of models and simulations has been both as a developer of policy, a technical direction agent, and a validation lead for many specific models, simulations, and federations.  This paper emphasizes the technical aspects of VV&A of HLA federations.   Two of the more comprehensive guidance documents which are also useful for conducting VV&A are the VV&A Recommended Practices Guide (RPG) and VV&A Overlay to the HLA Federation Development Process (FEDEP), both published by the Defense Modeling and Simulation Office (DMSO), and available on the DMSO web site.  As used in this paper, “federation” refers to HLA federations only, not to simulations linked by other technical approaches.  However, as will be noted, many of the lessons learned apply to non-HLA linked simulations.  The multi-part definitions of verification, validation, and accreditation used in this paper are taken from the DMSO VV&A web site:Verification:The process of determining that a model implementation and its associated data accurately represent the developer’s conceptual description and specifications.The process of determining that a model or simulation faithfully represents the developer’s conceptual description and specifications.  Verification evaluates the extent to which the model or simulation has been developed using sound and established software and system engineering techniques.Validation:The process of determining the degree to which a model and its associated data are an accurate representation of the real world from the purpose of the intended uses of the model.The process of determining the fitness of a model or simulation and its associated data for a specific purpose.Accreditation:The official certification that a model, simulation, or federation of models and simulations and its associated data are acceptable for use for a specific purpose. This paper uses the term “VV&A” collectively to describe any or all of the tasks that a VV&A agent would perform.  (The VV&A agent does not formally perform accreditation, but does help the accrediting authority prepare for it.) PAGE  PAGE  9