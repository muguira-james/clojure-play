How To Plan A Creditable Virtual ExperimentDon P. HodgeArmy Materiel Systems Analysis Activity392 Hopkins RoadAberdeen Proving Ground, MD 21005410-278-6540dhodge@amsaa.army.milKeywords: test design, scenario development, analysis, verification, validation, accreditationABSTRACT: The availability of high fidelity manned simulators capable of operating in a virtual environment provides a new set of tools to look at the "Human" component of a system.  Since this is a relatively new technology, organizations that want to use this type of modeling system face a number of major challenges over and above those normally required to conduct a study using a model or simulation. This paper provides a brief overview of seven major topics  that are necessary to successfully conduct a creditable virtual experiment. The recommendations presented in this paper are drawn from the lessons learned during the conduct of the Anti-Armor Advanced Technology Demonstration (A2ATD) program and the second Virtual Identification Experiment (VIE-II).1. IntroductionThis paper will review many of the elements that are critical to the successful conduct of a virtual experiment.  The target audience for this paper is the individual or group who has the responsibility for planning the conduct of a virtual experiment. While many of the major elements needed for a successful virtual experiment are covered, this paper is not intended to provide a comprehensive checklist of all of the elements needed to conduct such an experiment.  There are seven major focus areas addressed in this paper. These areas are the analysis plan, creditability issues, test personnel availability, scenario development, test design issues, risk management activities, and experiment command and control. While these elements are presented sequentially, in the order that they are first encountered, there is in fact a tight coupling between each focus area. Because of this close interaction, the actual planning and preparation processes will be accomplished in an iterative fashion.  Some of the opportunities for focus area interaction will be highlighted in the discussion of each area. This paper makes two assumptions about the nature of a virtual experiment.  The first is that the experiment is intended to examine one or more issues that relate to either man-machine interface questions and/or human decision making questions. If the primary purpose of the experiment is not related to some human in the loop type question, other means of analysis should be considered. The reason for this is provided in the section that discusses "Test Design" issues.   The second assumption is that the experiment is intended to compare and/or contrast at least two alternatives.  2. Analysis PlanThe development of an analysis plan should be the first step in the planning process. This step is probably the most critical of all of the planning elements since the decisions and requirements developed during this phase of the planning process will impact almost all of the other planning phases. There are ten distinct elements or steps in developing an analysis plan. The first step in developing an analysis plan is to define the critical issues of the experiment. That is, what are the primary questions that need to be answered? The second step is to decide if the experiment will only use the results from the simulators or if the results from the Computer Generated Force (CGF) units will also be used. This decision will impact the development of the VV&A plan.  The third step is to identify the figures of merit that will be used to distinguish between your alternatives.  The fourth step is to determine or define how these figures of merit will be calculated, and to identify all of the data needed to calculate these figures of merit. The fifth step is to identify all other data that may be of interest in explaining the results of the experiment, (i.e., data that may help explain why the results turned out the way that they did). The sixth step is to determine if the data that are needed for all of the identified calculations exists and are available in the simulation system.  If you can not capture the data needed for a given figure of merit, you will need to redo the third step in the analysis plan process. The seventh step is to determine how these data elements will be collected and stored. This step is critical if the virtual system to be used includes legacy systems that still use Protocol Data Units (PDU's). If your virtual system is HLA compliant, this step can be completed during the development of the Federation Object Model.  The eighth step is to identify how the collected data will be converted into something that can be analyzed. The ninth step is to determine how the raw data will be manipulated for analysis.  Will you use a database management system, a spreadsheet, or write specific programs to extract the information of interest from the raw data. The tenth and last step is to develop plans for a "Quick Look" data analysis system.  This system will be used during the experiment to do an initial quality control check of each trial of the experiment to determine if the data that are of interest were in fact present in each trial and that these data were captured. 3. Accreditation PlanTypically, when the results of the virtual experiment are provided to a decision-maker, an accreditation letter accompanies these results.  This letter, from an authority recognized by the decision-maker, states that the results from the virtual experiment are creditable and/or believable, not creditable, or are believable within some defined context. The very first step in the creditability determination process is to find out who will act as the accreditation authority for the virtual experiment. That is, who is responsible for making the determination that the virtual experiment is â€œgood enough"?  The accreditation process is directed at providing this individual the information that is needed to make this determination. The process that leads to this determination can be divided into pre-test and post-test activities. The pre-test activities are focused on determining the "structural" creditability of the virtual system.  That is, are all of the representations of the real world present, in sufficient fidelity and realism, to make the results from the virtual system believable? It is in this part of the accreditation process that we find the traditional validation and verification activities. Accreditation plans focus on this part of the process. The post-test phase involves an assessment of how the experiment was conduced. If the experiment was conducted as planned then the post-test phase is a pro-forma review.  If there were problems with the conduct of the test, this phase of the accreditation process takes on more importance. From a risk management perspective, it is preferable to make two creditability determination presentations to the accreditation authority.  If possible present the results of the structural creditability portion to the accreditation authority before the conduct of the virtual experiment.  It is very embarrassing, not to mention very expensive, to have the accreditation authority refuse to accredit an experiment due to problems with the structural creditability of the virtual system. The second phase of the creditability determination can occur simultaneously with the presentation of the results derived from the experiment. When trying to determine that the structure of the virtual system is creditable, the first step is to identify those elements of the virtual system (i.e., simulators and CGF) that are critical to the question being studied. It should be noted that not all of the elements of a virtual system are critical to the study question. As an example, in the Virtual Identification Experiment II (VIE-II) the primary question to be answered was "would the addition of the Battlefield Combat Identification System (BCIS) reduce the occurrence of fratricides over and above the reduction expected from the use of the Force XXI Battle Command Brigade and Below (FBCB2) system?" The elements that were determined to be critical to the VIE-II study question were the representations of the FBCB2 and BCIS systems, and the ability of the simulator image generators to provide realistic views of the virtual world.   Formal V&V efforts were undertaken on these elements.  Other elements of the virtual system, such as the ability of the simulators to move over the virtual terrain, were not considered important to the VIE-II study question.  Those elements of the virtual system that were not considered critical to the study question were not included in the formal V&V program.  Once the critical elements (i.e., typically algorithms and or models) have been identified, the next question becomes to what level have these elements been validated? For the purposes of this paper, validation is defined as the process of determining how well a model or algorithm matches the real world.  If critical elements of the virtual system are based on models/algorithms that have good credentials and are widely accepted, the effort becomes focused on developing a plan to verify that the performance of these algorithms/models in the virtual system matches the performance of the reference algorithms/models. If the models/algorithms have not been validated, the effort becomes one of developing a plan to determine the level of validation of the critical elements.  Note that this validation effort can be conducted independently of the software in the virtual system, but if it is conducted independently, you must also conduct the verification activities discussed earlier in this paragraph. Once the critical elements have been identified, and appropriate verification and/or validation plans have been developed, the next step is to determine what data these models will need. Any identified data will need to be obtained from an authoritative data source where the data source certifies that the data are both appropriate and correct for the conditions being tested. The last step in the structural creditability process is to determine if you have any fair fight issues. The question of a fair fight comes up when you have different models/modeling systems performing the same function in the virtual system.  Examples of where fair fight issues might arise include planning to use the results from both the CGF units as well as from the simulators, or where you have two different simulators representing competing concepts.  If the potential for fair fight questions is present in the virtual system, you must develop a plan to examine the critical elements of the competing systems and determine that one system does not have an unfair advantage over the other system. 4. Test Personnel AvailabilityIn a virtual experiment you will need qualified personnel to crew the simulators and to operate the various SAF stations.  The number of persons needed will depend on the number of simulators that will be available and the size and complexity of the scenarios to be used.  For U. S. Army troops, the request for troops should be prepared at least nine months before you will need the troops. Requests that are made with less than six months lead-time need Chief of Staff of the Army approval. Once the specific troops that will support the experiment have been identify, it is very beneficial to keep them informed about what is going on and allow them to participate, as much as possible, in the planning process. You will need to determine if the crews have the necessary training and/or proficiency with all of the systems to be used in the virtual experiment. If the crews have not been qualified on the equipment to be used in the experiment you will need to develop a training program to address this deficiency.  It is best if the proponent of the equipment in question develops the training program.  In the case of the VIE-II, the military crews were unfamiliar with the FBCB2 system. For this experiment the Project Manager for the FBCB2 program developed a specific training program to support the requirements of that experiment.  The requirement for additional training will impact your test design plan and test schedule.   Even if your test crews have been trained on all of the equipment needed in the experiment you will still need to provide time for the test crews to become familiar with the simulators. In addition to the test crews that will man the simulators, you will need to obtain the services of qualified personnel to operate the SAF stations and/or advise the SAF operators during the conduct of an experiment.  You can use former service members in these positions. 5. Scenario DevelopmentWhen developing a scenario to be used in a virtual experiment there are a number of steps that need to be undertaken. The first step is to identify all of the tactical conditions that are of interest and/or are critical to the question being studied. Once these tactical conditions have been identified, you need to select a scenario that will provide these conditions and that has been officially approved by the appropriate authoritative source. In the case for U. S. Army scenarios, this authority is the Training and Doctrine Command (TRADOC). Results from a virtual experiment that use a non-standard/non-approved scenario will be suspect. Because most official scenarios involve large forces (i.e., hundreds of systems) and large battle areas (i.e., hundreds of square miles), you will need to develop vignettes (i.e., a smaller scenario either based on or extracted from the larger scenario) that are tailored to the size needed for the virtual experiment. You need to ensure that the resulting vignettes are tactically sound and doctrinally correct. As you develop each vignette you will need to decide where to place the simulators and determine what role they will play. Note that you will need to develop several vignettes for each test condition. The need for this will become evident in the discussion of test design issues. The vignettes need to be large enough and long enough to provide the necessary tactical situations, but no larger and no  longer than necessary. The time it takes to exercise a vignette will impact the number of test replications that can be conducted in a given time. The next step is to translate each of these vignettes into the detailed vehicle paths, task sequences, timing requirements and sets of decisions needed to input the scenario in the CGF part of the virtual system. This includes developing instructions and/or scripts for the SAF controllers/role players. The last step in the scenario development process is to develop a set of operations orders that will be used by the troops who will be involved in the virtual experiment.  While the use of knowledgeable retired or former service members can be beneficial in developing and/or refining the scenarios to be used in the virtual experiment, it is recommended that you obtain input from active duty military personnel. These persons should be currently serving in positions that involve the development of official scenarios or involve the preparation of real world operations orders. The use of active duty military personnel provides an avenue for checking that the vignettes to be used in the virtual experiment are tactically sound, doctrinally correct, and in step with current military thinking.6. Test Design ConsiderationsBecause virtual experiments involve humans making decisions, the possibility exists that the results between individual trials will vary widely. In order to account for this variability, you need enough samples to determine if there are major trends in the results.  While it is theoretically possible to determine a trend with three data points, it is recommended that you plan on acquiring at least five replications or trials for each test condition that was identified earlier. Obviously more replications are desirable in order to increase the sample size you have available for analysis. As will be discussed in the next paragraph, other constraints usually preclude the ability to conduct these additional trials. The number of test conditions times the number of trials for each test condition defines your desired test sample size. The next question involves determining the number of trials that can be conducted during the experiment. The initial figure is based on the number of trials that can be conducted in a day, times the number of days that you have access to the test facilities and access to the test crews. At a minimum, this number will be reduced by the time it takes to familiarize the test crews with the equipment to be used in the experiment.  If the test crews require additional training before the start of the experiment, the number of available days will be further reduced by the number of required training days. After these familiarization/training issues are taken into consideration, the remaining number of trials will become the upper figure for the number of trials that will be available for the experiment. If the desired test sample size exceeds the number of test trials available, the number of test conditions to be investigated will need to be reexamined and a reduced set of conditions selected.  Once the test sample size has been determined, the next step in the process involves developing a test execution matrix.  When developing a trial matrix, several factors need to be kept in mind.  First, the test crews will gain in proficiency as the test progresses. Practice does make perfect. If you schedule all of your trials for one condition during the first half of your experiment and all of the trials for the other condition in the second half of the experiment you will bias the results of the experiment. Second, test crews will learn a given scenario if that scenario is presented to them several times.  This learning or ability to anticipate what will happen next may also bias the results of the experiment. This scenario familiarization phenomenon is the reason it was suggested to develop a number of different vignettes for use in a virtual experiment. It is beneficial to vary the scenarios that are used to keep this scenario learning to a minimum. The last issue is to identify the minimum critical set of trials needed to answer the basic experiment question(s).  There is always a good possibility that one or more events will occur to disrupt any experiment. These disruptions usually lead to lost test trial opportunities. It is always a good practice to develop a contingency plan to account for any test disruption.7. Risk Management ActivitiesThere are a number of activities that need to occur before the experiment starts in order to manage or mitigate those factors that pose a risk to the successful conduct of the experiment.  Some of these need to occur early in the planning process and some will occur just before the start of the for record trials.  The first issue that needs to be completed is the identification and definition of all of the essential criteria that must be met before the virtual experiment can begin.  An example of the entrance criteria that were developed for the Anti-Armor Advance Technology Demonstration (A2ATD) program is shown below.The thing to note from this chart is that all of the elements that have the potential to negatively impact the conduct of the virtual experiment should be included in the final review. If any of these critical elements have not been successfully met, you should not proceed with the experiment.  This implies that these criteria should be reviewed on a periodic basis before the expected start of the experiment.  Critical elements that are not on track can be given additional attention.  The final review should be conducted before the expected start of the experiment to allow sufficient time for any final corrective actions to be taken. The timing for this final review should be based on the results of the previous reviews. That is, if everything has checked out and is good to go, you can schedule the final review fairly close to the start date for your experiment. If the previous reviews have uncovered problems, you will want to schedule the final review well before the expected start of the experiment in order to allow for any last minute corrections.  A major critical element of this readiness review process involves testing the functionality of the virtual system. As the various elements that make up the virtual system are developed, they should be tested to ensure that they are functioning correctly. Once all of the elements appear to be functioning correctly, it is vitally important to conduct a complete end-to-end test of all of the elements of the virtual system together. That is, conduct a sample trial where all of the elements (i.e., simulators and CGF) are present and functioning. This test needs to include both data collection and data reduction. As mentioned in the preceding paragraph, this full up test needs to be conducted early enough in the schedule to allow for time to fix any problems discovered by this test. 8. Experiment Command and ControlDuring the conduct of a virtual experiment, it is necessary for there to be one and only one test director.  The test director is the individual who is responsible for making all of the real time last minute decisions during the conduct of an experiment. Note that these decisions can have a major impact on the ability of the virtual experiment to produce useful results.  One of the critically essential tools that all test directors need is the ability to communicate, in real time, with all appropriate test personnel.  This communication system allows test personnel to inform the test director if something is going wrong in their area of responsibility, and this system allows the test director to inform all of the test personnel about any decisions being made about the conduct of the experiment. The last issue to be covered in this section is the requirement to develop trial exit criteria. That is, you will need to develop a set of criteria that will allow you to determine when a specific trial has met all of the necessary conditions and can be ended.Author BiographyDON HODGE is an Operations Research Analyst at the Army Materiel Systems Analysis Activity, Aberdeen Proving Ground, MD.  He is the senior analyst on the Advanced Simulation Team. His background includes serving as the Test Director for the second experiment in the Anti-Armor Advanced Technology Demonstration program as well as the lead analysts for the second Virtual Identification Experiment.