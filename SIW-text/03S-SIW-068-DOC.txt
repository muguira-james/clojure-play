Cluster Computing for Calculating Line of Sight over a Distributed TerrainGuy Schiavone,  School of Electrical Eng and Computer Science, University of Central Florida, Orlando, FL 32826Judd Tracy, Eric Tichi, Eric WoodruffInstitute of Simulation and Training (IST), 3280, Progress Dr. Orlando FL 32826ABSTRACT- The increasing need for more accuracy and realism in modeling and simulation applications requires the use of high-resolution synthetic environments. Currently, most applications that run on single processor computers lack both sufficient memory to support the size of the high-resolution data, and sufficient computational power to support the complexity of the required spatial operations. One possible solution to the problem is to develop new techniques for distributing both the terrain data and the spatial calculations using a cluster computer. In this paper, the problem studied is the calculation of line-of-sight (LOS) over terrain surfaces. This particular problem is important in many applications, for example, the limiting operation in terms of number of entities supported by computer generated force applications is often the determination of LOS between entities over the synthetic environment. An object-oriented approach has been devised to distribute the terrain and the LOS operations over a Beowulf cluster. Initial experiments were performed to study the time complexity and scaling of the operations employed.Introduction Our work in this area was originally motivated by the very high resolution synthetic natural environment representations and potential large number of entities required for successful implantation of Live-Virtual simulation. Live-Virtual simulation involves the real-time or pseudo-real-time (perceptual real-time) coincident “mirroring” of a live, real, world exercise for the purpose of simulating certain aspects of the live exercise.  One example of this application is the use of live virtual simulation for integration of simulating non-LOS weapons in a live training exercise.  Current training systems have shown to be effective in providing direct fire force on force training, but have inadequate provisions for realistic training that includes non-LOS weaponry such as Mk-19 and XM-29 Objective Individual Combat Weapon.  A successful implementation of a live-virtual tactical engagement training simulation will require both a very high resolution and high-fidelity representation of the real-world environment in virtual space, as well as the ability to track in real-time a potentially large number of entities operating in the field. The computing resources needed to support such large SNEs are clearly beyond a single-processor system.  Our approach is to investigate the use of relatively low-cost “Beowulf”-type clusters built from commodity hardware and running free software for use in processing large, high resolution synthetic environments a potential solution to the problem.     Another benefit of this research is the potential to improve the efficiency of intervisibility calculations, thereby increasing capabilities of CGF systems in terms of number of entities supported.  It has long been recognized [1] that one of the limiting factors in the number of entities that can be generated by a CGF system is the O(n2) entity-to-entity interactions that must be continually updated, with the determination of simple intervisibility being the dominating operation.  The SF-Express project [2] combined a variety of novel approaches and optimizations to ModSAF to increase the number of entities that could be simulated on high-performance computers, with an emphasis on reducing network bandwidth as a limiting factor by restricting communications to those between entities within defined interest classes.  The interest classes were defined, in part, by a simple cell-based geographic partitioning, termed “geographic interest cells” [3].  Although the main focus of this project was defining an interest-filtered communications architecture to reduce bandwidth saturation as a limitation in DIS simulations, another finding of particular importance in light of our present research was recognition of the importance of optimizing intervisibility calculations in ModSAF by Craymer and Ekroot [4].  Detailed analysis of ModSAF 2.1 execution profile showed that techniques for calculating intervisibility and related sensor calculations were the main cause of load imbalances resulting in unexpected poor performance.  Specific to ModSAF, it was found that inefficient processing of lists comprised of entities within a given Area-of-Interest (AOI) could be improved by using unordered lists in place of ordered lists.  The overriding conclusion reached by the authors was that “The principal barrier to scalability in ModSAF, excluding issues of data distribution, proves to be the computation of interactions between simulation objects. Intervisibility computations, route-planning, collision avoidance, and secondary damage from exploding ordnance are all examples of such interactions. This has obvious impact for sensor-processing code--one of the principal design goals should be to minimize such processing.”  Other motivations for this research are to improve performance in terrain analysis and visibility problems. Calculations for the high resolution analysis of tactically significant terrain features such as defensible positions, concealed avenues of approach, choke points, and intervisibility lines are all computationally-intensive tasks that can potentially  benefit from applications of cluster computing.  Richbourg et al describe the value of tactical operations planning via generation and use of visibility maps and smoothness maps on Digital Elevation Models [5] and over Triangulated Irregular Networks [6] as a metric for terrain fidelity.   In [5], it was noted that the calculation of a visibility map for an approximately 3.2 KM by 2.1 KM area at 1 M resolution took approximately 3 days of continuous processing time on a Sun Sparc II platform.   As an example of another application of terrain analysis, Marengoni  et al describe an algorithm to obtain observer placement for complete terrain visibility coverage  over a given area with a complexity of O(n3), where n is the number of polygon vertices [7].   An example of an algorithm for the generation of stealth path planning using harmonic functions is given in [8]   Line-of-sight communication and sensor network routing (see, for example, [9, 10] ), is another important application of terrain analysis.  A review of some algorithms for tactical route planning is given in [11].  An excellent overview of the tactical value of various types of terrain analysis, with an emphasis on perceptual issues, can be found in [12].Problem DefinitionThe problem we consider in this paper is to minimize execution time by parallelizing LOS calculations over a TIN by partitioning responsibility for both terrain and entities amongst the computational nodes of a Beowulf-type cluster.  We begin this research by considering the case of static entities on the terrain.  A simple illustration of the problem is given in Figure 1. EMBED Word.Picture.8  Figure 1 – Example of intervisibility between entities on a TIN terrain.Important considerations and design decisions we consider in the paper needed to accomplish this goal include:Terrain Partitioning StrategyChoice of Network Communication APIDynamic Load Balancing StrategyThere is a large body of related research concerned with TIN partitioning and parallel visualization techniques.  Parallel approaches to solving finite element methods (FEMs) seem to offer an analogous situation to the problem at hand.  FEM codes that employ adaptive meshing must consider the problem of dynamic load balancing (for example, see [13] [14]).  Unfortunately, there are some key differences between the requirements for parallel adaptive FEM problems and the terrain partitioning problem that make this research inapplicable to our problem.  First, the main computational task in FEM problems is that of solving sparse linear systems, so most of the benefits of parallelization are gained by parallelization of this task.  Secondly, the adaptive meshing (somewhat analogous to “dynamic terrain”) tend to be localized and recursive.  The significance of this difference is that global dynamic load balancing rather that diffusive techniques often are found to be more effective in the parallel FEM problem, whereas this conclusion probably does not apply to our problem.Despite the obvious need for determining efficient approaches to parallelizing processing of large, high-resolution SNEs, little research has been done in this area.  In the following, we briefly review two published approaches to the problem.In [15], Lui and Chan consider the problem of partitioning a virtual environment for immersive applications.  Some key points from the paper follow:Focus is more general – DVEs over WANs with mulitiple servers.  This introduces issues such as view consistency and associated synchronization issues that we need not consider for our problem.  Makes simple distinction between single server and multiple server architectures, and chooses the latter approach for scalabilityUses AOI to limit communication overheadConsiders the issue of “dynamic environment”, where environmental objects can be changed and manipulated.Allows for dynamic membership so that avatars may join and leave the distributed simulation at any time.Uses simple cell-based partitioning of the virtual environmentFormulates a cost function that includes computational and communication costs, and proves that the workload partitioning problem as formulated is NP-completeUses a rather complex three step procedure a) “Recursive Bisection Partitioning” (RBP) algorithm to determine an initial partition that reduces workload deviation (for load balancing) and interserver communication costs, b) layering partitioning problem that moves responsibility for an avatar to a different server to reduce the overall computation workload cost of the system ,c) communication refinement partitioning algorithm to reduce communication costs, formulated as a linear programming problem.Experiments assume equal AOIs for each avatar, do not consider spatial density variations in the virtual worlds, and consider the relative importance of the computation workload to be equal to the communication workload.The cost of dynamically changing partition strategies is not considered, since cost function does not include the cost of disk swappingThe paper presents small-scale virtual world experiments showing that the results of their approach give total system cost close to the optimal cost as calculated by an exhaustive search over all possible partitioning strategies, however, there are questions as to the experiments: It seems that the assumption in the experiments is that the spatial density of the “virtual worlds” is considered to be uniform, or perhaps the virtual environment experiments are simply simulated.. Oddly, in the experiments, no description is given of the virtual environments that were actually employed, or as to the workloads assumed for each avatar, or, if an actual virtual world representation was used, what its characteristics are (total number of polygons, density variation, urban vs. natural, etc).  It would seem that these “details” could have a large effect on the results obtained.  The second paper we review that seems particularly relevant is Niedringhaus [16].  In this paper, a diffusive dynamic load balancing algorithm is developed to support parallelization of large scale battlefield simulations.  Niedringhaus provides a good discussion and comparison of global vs. local (diffusive) load balancing strategies, and unlike approaches taken in parallel adaptive FEM, makes a case for the diffusive option.  A simple cell-based partitioning strategy is used, which seems particularly applicable to uniformly gridded polygonal terrains or simple DEMs. Load balancing achieved by exchanging responsibility for cells between neighboring computational nodes  An innovation of the approach is an algorithm for “anti-gerrymandering”, to prevent the evolution of long peninsulas and highly concave partitions that would have the effect of increasing communication overhead.  His approach seems to have had some good success, but his results still show some irregular partitions, perhaps suggesting the need for a hybrid dynamic load balancing approach employing an occasional global load rebalancing step.   As in all other work reviewed, an assumption of a circular AOI is used to simplify the problem.   In the remainder of this paper, we describe our own initial studies and experiments.Review of Terrain Partitioning StrategiesOne class of hierarchical representations is a quadtree. Quadtrees allow a representation of a 2-d space. The entire space is partitioned into four equally sized squares. The development of a quadtree follows a top-down approach.Figure 2: QuadtreeThe hierarchy is initialized by creating a single parent node (root) that represents the entire terrain. The subdivision of the root is obtained by inserting a central point and joining it to each midpoint of the four sides of the domain. Each step of refinement subdivides the domain into smaller regions. This recursion happens until a desired level of detail (LOD) is obtained. Several different schemes can determine when a threshold is met. These division thresholds provide a stopping criterion to prevent infinite recursion. The threshold can be either the tree depth or level of detail. Level of detail can be the triangle density associated with a division.This basic representation found in Figure 2 shows how areas of interest can be mapped into a quadtree structure. Some research has been conducted that overlaps two or more quadtrees. If the structures have similar content then memory can be shared between the two or more structures [17]. The nodes of the quadtree are references, which allow more than one quadtree to access the data easily. The algorithms that perform operations on these structures are oblivious to the fact that there may be two structures sharing information. Within these quadtrees, there can be a field(s) for temporal data. Queries can be made on the temporal data to reveal a change in the terrain. This can be useful in calculating erosion. This research can be applied to soil-landscape relationships that are fundamental to the soil mapping process. Topography is a significant reason for soil formation from the effects of erosion, drainage runoff, and solar exposure. Temporal data could make it possible to observe formation or depletion of an area. There is a goal of predicting space-time processes with an almost sense of exigency. Scientists are gaining empirical knowledge through satellite data and other geographic data of complex spatial-temporal process at multiple geographic scales [18]. The overlaying of the maps allow for a dynamically changing terrain that is efficient in memory usage. Only the changes are recorded and if the terrain has stayed constant then the quadtree will simply reference a previous version of the terrain database. The nodes can have the ability to forward chain and backward chain to other nodes [17]. This allows terrain to be exposed over a period of time.K-Dimensional TreesK-D trees are multidimensional binary search trees that organize vectors of numeric data to facilitate nearest-neighbor searches”. Consider a set of n points in k dimensional space. Let d1,d2,d3…dk denote the k dimensions. The root of the tree represents the entire volume like a quadtree. Take a dimension di and find the median of all the points within this region. Splitting a region yields all points in one-half to be of lesser value than the key and all points in the second region to contain the remaining points. The K-D tree has the same goal the spatial structures have, which is to decompose a space into smaller regions so that each space is occupied by a set amount of objects. Every non-terminal node will split by placing lesser key values on the left and greater key values on the right. A balanced tree will be built if all of the attributes are known ahead of time, thus the tree is optimized. An optimized K-D tree can be built in time of O (n log n), where n is the number of nodes in the tree. Access to any input object by position is relatively fast.Figure 3: K-D RepresentationBinary Space Partition TreesBinary Space Partitioning or BSP trees were originally formulated as a means of quickly and correctly sorting a set of polygons into a depth order, known as a visible surface determination algorithm [19]. A brute force algorithm would require O(n2) to compare every pair of polygon. A large scene comprised of 105 polygons would require 1010 operations. Rendering a scene from back to front the visual will occlude the correct polygons to produce the correct representation. Today, with the help of hardware that can z-buffer, the reverse is implemented with the z-buffer taking care of occlusion while the BSP takes care of overdraw. BSP trees divide a scene by bisecting it with a plane and sorting scene polygons into in front of and behind the dividing plane. Polygons are split into two if necessary, when they intersect the dividing plane. The resultant order data can be used to very efficiently sort the scene into ascending or descending order of polygons for any arbitrary position inside (or outside) the scene. This terrain model effectively represents walls of buildings or urban areas. It is easy to define these areas with a BSP. However, BSPs are not ideal for representing geo-specific terrain. Finding the optimal BSP tree is an NP-complete problem [20]. There are heuristics, however, to make the search close to real time. Figure 4: BSP representationBSP trees reduce the set of polygons that actually need to be processed at run time. Processing such as set operations, collision detection, and visible surface determination are a few tasks that benefit from BSPs. Algorithms that require O (n2) are reduced to O (n log n), and algorithms that require O (n) are reduced to O (log n) [21]. In the approach to partitioning SNEs there is the constructive and the visual aspect to the problem. The constructive is the focus of this study. How can we correctly store information and distribute that information to speed up queries? Initial Experiments – First IterationThe distributed environment (runtime) architecture used two main concepts, Contributors and a Distributor, setup on a local dedicated cluster. The lowest level of communication implemented uses the omniORB CORBA Object Request Broker (ORB), a robust, high-performance, multi-threaded CORBA 2 ORB, originally developed by AT&T Laboratories Cambridge. It is freely available under the GNU public license. The native transport is IIOP, and it comes complete with a COS Naming Service. It supports C++ language bindings. The Contributor acted as a resource pool for the line of sight calculation system. Each contributor maintained a collection of terrain objects, represented by a collection of triangles with precalculated side-normals for more efficient geometrical calculations. It is significant to note that the system was converted to the blitz++ template library for vector calculations [22].The Distributor relocated the leaves of a terrain quad-tree to the Contributors, in a randomized fashion to ensure better communication overhead distribution. A quad-tree was used, composed of bounded objects that maintained references to their children, herein composite terrain.Since the system followed object-oriented design principles, the Composite Terrain and Primitive Terrain (Leaf) objects were polymorphically substitutable. Both types shared a common interface that allowed querying for line-of-sight intersections along a given path. Independent of the size of the terrain density or other criteria that can influence the size/height of the quad-tree, the substitutability gained by polymorphism allows the architecture to theoretically scale well. (Note that this substitutability also applies to any spatial division pattern that appeals to both your imagination and logic.)The apex of this system’s design is that the details of the quad-tree (or your favorite flavor) are hidden behind a common interface at the root of the tree. This provides almost over-simplified access to this database through a lean object reference.An advantage of using CORBA was that the reference to this object is easily relocated over any compatible network—extending the simplicity of this scalable cluster application into accessibility. ResultsTo assess the performance of the system a test suite was created. The test suite consisted of small to very large terrain databases. Each of the terrains has associated with it several quadtrees. An application within the Distributor will query the database to find how many lines of sight per second the system can achieve. The hypothesis of the test is there will be an efficient speedup in terms of lines of sight calculated per second as more nodes are introduced into the system. To test the terrain, random terrains were generated through a program in the form of a regular grid. The program allowed the tester to determine the size of the terrain in terms of resolution and geological extents. Since the data was in the form of a regular grid, the quadtrees that represent the data will be balanced.Testing the DistributorThe first evaluation will be the distributor that houses the quadtree and is where all tree traversals take place. It also keeps track of all of the threads waiting for an asynchronous callback from the contributors. The tree traversals involve the geometric calculation to determine what subdivision a line intersects. The algorithm goes as followsConsider a line L and a “cube” CFor every side of C {	If L does not cross the side then quit	Else return the point of intersection is on the side}  CommunicationCommunication is a necessity in all distributed computing; the costs of it can be reduced but never circumvented. It also is the highest overhead in distributed computing, especially in Beowulf cluster employing fast Ethernet-based switching. The project was written using CORBA as a means for passing information between the contributor and the distributor. CORBA is known to be slower than other methods of communication but we felt that the advantages of CORBA in developing a clean, extensible architecture potentially outweighed the disadvantages. Figure 5: CORBA communicationThe system expanded more easily from using object oriented paradigms rather than procedural ones. Figure 5 shows a test of how a server implementing CORBA reacts to an increase of clients. Shown is the mean of three different runs of the program. Notice the sharp decrease from one client to four clients. This proved to be the largest bottleneck in the system. Another test was done to compare the CORBA communication latencies with a server written in C implementing raw sockets only send () and recv (). The client would send 12 bytes to the server, which would echo that same information back to the client.Figure 6: Socket CommunicationBoth show a similar downward trend as clients were added to the system. The socket implementation displayed a more stable performance in its downward curve. Overall, the sockets proved to handle more calls per node. Figure 6 shows the mean value for all clients involved with the program.Figure 7: Geometric calculationGeometric CalculationsThe last part of the system is the geometric calculation. The geometric calculation determines whether there is an intersection between the line and any of the triangles. Above, Figure 7 shows the processing speed of an increasing triangle density. As expected, this is linear with respect to the total number of triangles involved with the search. If there is intersection then the search immediately exits. The average case would be half of these values. Note that this graph will be different on different architectures since this is a depiction of how fast the CPU can calculate floating-point math.Lines of Sight per SecondThis is the combination of the distributor, network communication, and geometric calculation. This benchmark will give an overall performance rating that is common among similar systems. The first test was done with a database that consisted of 977,202 triangles. The following table represents the different quadtrees that were created with this database.The different quadtrees will have the associated triangle density and depth.3 deep (64 leaves) with a triangle density of 154884 deep (256 leaves) with a triangle density of 38725 deep (1024 leaves) with a triangle density of 968 Nine different input files were created that provided entity locations on the terrain. Every point would be compared to every other point in the file by a line of sight test. The locations were taken from the vertex file that represents the terrain to guarantee that they would be on the terrain. An average was taken from all inputs and charted to see the performance of the system. An arbitrary line of sight will have associated with it one or several nodes that will compute the line of sight. The system will not benefit when more nodes are added that are not involved with the calculation. Figure 8: Mean of First TestAn analysis of the system was created by taking the average result produced by the input files. The test will scale with the use of 2, 4, 8, then 15 nodes. Note, when the system was running with one node, it was running in serial. Figure 8 shows the mean eight different runs.The trees with a depth of three and four scaled nicely due to the ratio of leaves and triangle density. The quadtree with a depth of five showed a decrease in performance due to the overabundance of leaves that had only 968 triangles to compute. This quadtree had more network traffic plus the use of more threads, which resulted in the degradation of performance. This analysis suggests that there must be a minimum amount of computation for a node to do to outweigh the communication cost. The last test is a simulation of a very large dataset. This is done by inserting a loop that calculates the same set of triangles multiple times. The simulation will iterate over a leaf’s set of triangles one-half as long if there was an intersection. If there were no intersection, the simulation would iterate over the whole set of simulated triangles. Figure 9. Mean of second testThe size of the simulated database was 32 million triangles with several quadtrees associated with this data. The results in Figure 9 reveal the performance of the system with the addition of more nodes. The importance of a quadtree becomes very evident when dealing with large databases. The geometric calculation is the bottleneck in the 64-leaf tree. The 64-leaf tree does not divide the terrain enough, leaving the triangle density to high in the leaves. The 256-leaf tree is a little better but leaves much to be desired in terms of processed requests. The 1024-leaf tree is the most desirable in its scaling. The quadtree is providing the leaves with a proper amount of calculation to be done that scales. The most interesting is the 4096-leaf tree. This tree scales very sharply up at four nodes but then remains flat lined thereafter. The reason for this is the communication bottleneck. CORBA cannot “talk” to 8 or 15 clients fast to allow any more scaling after four nodes. There are too many leaves trying to communicate with too many clients.Additional OptimizationAn additional optimization technique that was tested is getting more efficient use of the L1 and L2 processor cache, which yields increases in performance. Figure 10. Grouped vs. Individual LOS checksFigure 10 shows a test that was conducted on grouping lines versus individual lines. We found that by processing multiple lines at one time offered a 250% speedup on certain architectures. The machines used during the test were dual Pentium II 350 MHz with 256 megabytes of RAM. The grouped check offers another optimization in that it lowers the amount of communication on the cluster. This will address the issue of a communication bottleneck. Future WorkFor the second iteration of our system, we are considering the following issuesLoad-BalancingDiffusive vs. Global repartitioningReconsideration of terrain partitioning strategies.Minimal disk accessRedistributing terrain vs. redistributing entities.Graph Theoretic Approach - Some speculationsUsing a graph theoretic approach would allow us to leverage off of the huge body of existing research in data clustering.Neglecting load balancing considerations, and the AOI simplification assumption, by constructing a graph with nodes defined as entities, edges defined as possible LOS between entities, and edge weights defined as Euclidean distances, by constructing the minimum spanning tree of the graph and deleting the remaining N largest edges, we have partitioned the graph into N subgraphs such that any terrain partitioning that contains exclusively one of the subgraphs will be a terrain partition that minimizes communication between computational nodes on the cluster. By constructing a graph as outlined above, but instead labeling edge weights as the number of polygons transversed in the 2D TIN projection, we have minimized the computational load due to LOS calculations  (Depending on tree traversal efficiency?).  This seems to be correct because it is more efficient to search for polygons in the terrain partition tree leaves, rather than having to traverse the tree to search in other leaves of the tree.Adding the AOI assumption simply puts additional constraints on the terrain partition described in (1).  That is, the terrain partition should be such that overlapping AOIs between subgraphs must be minimized by the terrain partition.  This implies that the terrain partitioning scheme that minimizes interprocessor communication when considering AOIs is related to the Voronoi diagram defined by the node AOIs.AOI Assumption – ComplexitiesFor the LOS problem, the AOI must encompass the viewshed, which is hardly circular in general.  Can advantage be taken of parallel viewshed generation techniques to obtain better results?Also, the simplistic AOI assumption neglects several factorsSensor differencesWeapons rangesInterest filteringVariable atmospheric visibility References[1] M. D. Petty, C. E. Campbell, R. W. Franceschini, and M. H. Provost. "Efficient Line of Sight Determination in Polygonal Terrain", Proceedings of the 1992 IMAGE VI Conference, Phoenix AZ, July 14-17 1992, pp. 239-253. 4, C. P. A. Fishwick[2] Sharon Brunett and Thomas Gottschalk, “ HYPERLINK "http://www.sisostds.org/doclib/doclib.cfm?SISO_RID_1000454" Scalable ModSAF Simulations With More Than 50,000 Vehicles Using Multiple Scalable Parallel Processors”, 1998 Spring Simulation Interoperability Workshop, March 9-13, 1998, Orlando FL, Paper 98S-SIW-181.  [3] Sharon Brunett , “Balancing the Load in Large-scale Distributed Entity-level Simulations”, Technical Report CACR – 163, May 1998, Center for Advanced Computing Research (CACR), California Institute of Technology, Pasadena, CA.[4] Craymer, L., and Ekroot, L, "Characterization and Scalability of SF Express/ModSAF", 1998 Spring Simulation Interoperability Workshop, March 9-13, 1998, Orlando FL, Paper 98S-SIW-190.[5] R. F. Richbourg, C. K. Ray, and L. W. Campbell, “Tactical Operations Planning As A Metric For Terrain Fidelity”, Proceedings of the 13th DIS Workshop on Standards for the Interoperability of Distributed Simulation, 18 - 22 September 1995, Institute for Simulation and Training, Orlando, FL, Paper 13-95-012.[6] R. F. Richbourg, C. K. Ray, and L. W. Campbell, “Terrain analysis from visibility metrics”, SPIE Proceedings Vol. 2486. Integrating Photogrammetric Techniques with Scene Analysis and Machine Vision II, 04/17 - 04/21/95, Orlando, FL, pp.208-219. Paper #2486-30[7] M. Marengoni, B. Draper, A. Hanson, and R. Sitaraman. ``Placing Observers to Cover a Polyhedral Terrain in Polynomial Time", Vision and Image Computing, 18(10):773-780, 2000.[8] S. Ravela, R. Weiss, B. Draper, B. Pinette, A. Hanson, E. Riseman. "Stealth Navigation: Planning and Behaviors," ARPA Image Understanding Workshop, Monterey, CA., Nov. 1994, pp. 1093-1100.[9] Prosenjit Bose and Pat Morin, “Online Routing in Triangulations”, Alok Aggarwal and C. Pandu Rangan (Eds.): Algorithms and Computation,10th International symposium, ISAAC 1999, Chennai, India, December 16-18, 1999.Proceedings. Lecture Notes in Computer Science, Vol. 1741, Springer, 1999, ISBN 3-540-66916-7, pp. 113-122.[10] Julius Goldhirsh, Wolfhard J. Vogel, Handbook of Propagation Effects for Vehicular and Personal Mobile Satellite Systems, NASA Reference Publication 1274, Second Edition, 2001.  Also available online at  HYPERLINK "http://www.utexas.edu/research/mopro/index.html" http://www.utexas.edu/research/mopro/index.html[11] John R. Benton, S.S. Iyengar, Weian Deng, Nathan Brener, and V.S. Subrahmanian, “Tactical Route Planning: New Algorithms for Decomposing the Map”, IEEE International Conference on Tools for AI, 1995, Nov. 6-8 1995, Herndon, VA, 268-277. Also in International Journal of Artificial Intelligence Tools (1996), Vol. 5, Nos. 1 & 2 (1996) 199-218.[12] Rachel Banks and Christopher D. Wickens, “Commanders’ Display of Terrain Information: Manipulations of Display Dimensionality and Frame of Reference to Support Battlefield Visualization”, Technical Report ARL-97-12/Army-Fed-Lab-97-2, Aviation Research Lab, University of Illinois at Urbana-Champaign, 1997. [13] Bart Maerten, Dirk Roose, Achim Basermann, Jochen Fingberg and Guy Lonsdale, "DRAMA: A Library for Parallel Dynamic Load Balancing of Finite Element Applications", European Conference on Parallel Processing, pp. 313-316, 1999.[14] Leonid Oliker, ``PLUM: Parallel Load Balancing for Adaptive Unstructured Meshes'', Ph.D. Dissertation, University of Colorado, Dept. of Computer Science, 1998.[15] Lui, J.C.S., and Chan, M.F., “An efficient partitioning algorithm for distributed virtual environment systems”, IEEE Transactions on Parallel and Distributed Systems, Volume: 13, Mar 2002, pp. 193-211.[16] Niedringhaus, W.P,  “Diffusive dynamic load balancing by terrain parcel swaps for event-driven simulation of communicating vehicles” Proceedings of the 28th Annual Simulation Symposium, April 9-13 1995  Phoenix, AZ, pp. 166-174[17] Theodoros Tzouramanis, Micheal Vassilakopoulos, Yannis Manolopoulos, “Overlapping Linear Quadtrees: a Spatio-temporal Access Method*” Proceedings of the sixth ACM Int. Sym. on advances in geographic information systems, November 1998.[18] Donna J. Peuquet, “Making Space for Time: Issues in Space-Time Data Representation” in Database and Expert Systems Applications 1999 pp. 404 – 408[19]C. Thibault and Bruce F. Naylor,"Set Operations on Polyhedra Using Binary Space Partition Trees",Computer Graphics,  vol. 21, no.4, July 1987, pp. 153-162.[20] Adam James, “Binary Space Partitioning for Accelerated Hidden Surface Removal and Rendering of Static Environments” PhD dissertation at University of East Anglia, August 1999.[21] Honors Self-Study, Henri Haki, Computer Science University of Stellenbosch  HYPERLINK "http://www.cs.sun.ac.za/~henri/advgfx.html" http://www.cs.sun.ac.za/~henri/advgfx.html[22] Blitz++ libraries, http://www.oonumerics.org/blitz/manual/Frames.html 