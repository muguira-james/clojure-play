Simulation Conceptual Model Role in Determining Compatibility of Candidate Simulations for a HLA Federation *Dale K. PaceThe Johns Hopkins University Applied Physics Laboratory11100 Johns Hopkins RoadLaurel, Maryland 20723-6099(240) 228-5650; (240) 228-5910 (FAX)dale.pace@jhuapl.eduKeywords: conceptual model, validation, fidelity, simulation ABSTRACT:  A simulation conceptual model is a simulation developer’s way of translating modeling requirements (i. e., what is to be represented by the simulation) into a detailed design framework (i. e., how it is to be done), from which the software, hardware, networks (in the case of distributed simulation), and systems/equipment that will make up the simulation can be built.  Suggestions about methods for developing a simulation conceptual model have been presented at previous SIWs and elsewhere -- such as in DoD’s updated Recommended Practices Guide (RPG) for simulation verification, validation, and accreditation (VV&A).  This paper will examine the role of the conceptual model in determining compatibility of candidate simulations for a High Level Architecture (HLA) federation.  Compatibility of federates (individual simulations) in a federation (distributed simulation) is a key element in validation assessment to determine if the federation can appropriately support the intended application.  Key conceptual model attributes needed for such assessment and ways to perform and document such assessment will be presented.  The paper will also provide practical suggestions for how to perform such validation assessments when there is inadequate information about one or more federates conceptual models, and will identify limits imposed on such assessments by that information lack.IntroductionSimulation verification (determination that the simulation satisfies its specifications, i.e., that “the simulation was built right”) and simulation validation (determination that the simulation is adequate for its intended application, i.e., that “the right simulation was built”) are essential for a correct and credible simulation.  This is true both for individual simulations (federates) and for distributed simulations, such as a High Level Architecture (HLA) federation.  Federation verification and validation (V&V) is based both upon the V&V of individual federates and the compatibility of those federates for the purposes of the federation.  Quality, well-document conceptual models for individual federates contributes significantly to determination of federation validity.The conceptual model is a simulation developer’s way of translating modeling requirements into a detailed design framework, from which the software, hardware, networks, and systems/equipment that will make up the simulation can be built, modified, or assembled  Descriptions of how to develop and document a simulation conceptual model exist in various places, such as [1]-[7]. The paper focuses upon how the simulation conceptual model contributes to determination of compatibility of candidate simulations (HLA federates) for a distributed simulation (HLA federation).  Compatibility of federates in a federation is a key element in validation assessment to determine if the federation can appropriately support the intended application.  For context, a paradigm of simulation development and a construct for the simulation conceptual model are presented.  Key conceptual model attributes needed for such assessment and ways to perform and document such assessment will be presented.  The paper will also provide practical suggestions for how to perform such validation assessments when there is inadequate information about one or more federates conceptual models, and will identify limits imposed on such assessments by that information lackParadigm for Simulation DevelopmentFigure 1 illustrates the interaction between development of system theories and simulation development.  System theories describe behavior and characteristics of real world entities and processes.  The system theories are developed by abstracting what has been observed. Such data are elaborated by hypotheses (some of which may be derived from simulation results).  Such theory is validated by its correspondence with system data (observations and experimental results) and enhanced by data from additional experiments and observations. Blue background indicates “real world” with relationships between experiments (observations), tests, and system theories (system theories incorporate system data).Yellow background indicates simulation world, with model development based upon system theories and simulation objectives.  Simulation development begins with a set of objectives, which should be based upon requirements that have been reviewed for consistency, completeness, clarity, and correctness – such review, V&V of requirements, is essential since many studies have indicated that the majority of software faults (also true for simulations) are a result of deficient requirements.  Figure 1 shows the conceptual model linking requirements and objectives (general what needs to be done) and specifications (detailed description to support design), with reality constraints from system theories.  Simulation Verification demonstrates that what is used in a simulation experiment is derived from validated requirements through simulation objectives.Specification Verification assures simulation specification and design represent simulation requirements and objectives as reflected in the simulation conceptual model.Implementation Verification assures simulation implementation (code) reflects specificationsSimulation Validation is tied to simulation experiment objectives/intended use, and is constrained by system theories and real world data. Failure to validate requirements adequately is a major problem in both real world and simulation world experiments.Conceptual Model Validation assures simulation concept which drives simulation specification and design fully reflects simulation requirementsOperational (Results) Validation assures simulation as implemented satisfies requirements and objectives, includes assessment of simulation inputsValidation demonstrates the simulation is appropriate for its intended use.  The referent for validation assessment includes real world system data where such exists and system theory (which may include results from other simulation) to supplement real world data as needed, all focused by simulation experiment objectives.  Statistical methods and other analysis techniques may be applied to increase the likelihood of proper assessment.Conceptual Model ComponentsA simulation’s conceptual model consists of three main components: 1) the simulation context, 2) mission space (representational aspects of the simulation in the simulation elements), and 3) simulation space.  Each of these is discussed briefly below – additional information is available in [1]-[7].  The relationships among these are illustrated by Figure 2.   EMBED PowerPoint.Show.8  1) Simulation Context.  The simulation context provides “authoritative” information about the domain which the simulation is to address.  Often the simulation context is merely a collection of pointers to sources that define behaviors and processes for things to be represented within the simulation.  Special care must be used when algorithms are taken from more than one source to ensure that sources do not employ contradictory assumptions or factors (e. g., different models for the shape of the Earth).  The information contained in the simulation context establishes boundaries on how the simulation developer can properly build the simulation.2) Mission Space.  The mission space of a conceptual model addresses representational aspects of the simulation and consists of all the simulation elements in the conceptual model.  A simulation element consists of the information describing concepts for an entity, a composite or collection of entities, or process which is represented within a simulation.  Assumptions, algorithms, characteristics, relationships (especially interactions with other things within the simulation), data, etc. that identify and describe that item’s possible states, tasks, events, behavior and performance, parameters and attributes, etc. comprise the simulation element.  A simulation element can address a complete system (e.g., a radar), a subsystem (e.g., the antenna of a radar), an element within a subsystem (e.g., a circuit within a radar transmitter), or even a fundamental item (such as an atom).  It can also address composites of systems (e.g., a ship with its collection of sensors, weapons, etc.).  A person, part of a person (e.g., a hand), or a group of people can likewise be a simulation element, as can a process such as environmental effects on sensor performance.3) Simulation Space.  The simulation space part of the simulation concept includes all additional information needed to explain how the simulation will satisfy its objectives.  Such additional information often addresses control capabilities intended for the simulation, such as pause and restart capabilities, data collection and display capabilities, and how data and simulation control factors can be entered into the simulation (by keyboard, by voice, by gesture or touch, or by feedback from other parts of the simulation).  Simulation space characteristics can range from identification of specific kinds of computing systems (hardware and operating systems that the simulation must run upon) and timing constraints so that real systems can be part of the simulation (such as hardware in the loop unitary simulations or involvement of live forces in distributed simulations) to simulation control capabilities described above.  Simulation space considerations are closely related to implementation issues.In conceptual model development, entities and processes are identified that must be represented for the simulation to accomplish its objectives.  This enumeration process is fundamental in conceptual model development.  It defines the level of detail and aggregation appropriate to support simulation requirements.  This determines whether a system (such as a radar) will be represented as a single entity, as a composite of subsystem entities (such as an antenna or receiver), or as a composite of composites of ever smaller entities (to whatever level of detail is needed for the purpose of the simulation).  This also determines the level of representation of human decisions and actions.  For example, in the movement of a platform (tank, aircraft, ship, etc.), will decisions and responses of all people involved (the crew) be represented implicitly as a single aspect of the movement control process or will each individual involved be represented explicitly (as in a tank simulator with a position for every member of the tank crew)?  Conceptual model decomposition determines simulation elements of the conceptual model.  This determines the scope of representation in the simulation and the discernible levels of the simulation.How characteristics of simulation elements are abstracted determines accuracy and precision of representation.  For example, movement can be represented at various levels of sophistication (linearly, with attention to acceleration, with attention to changing acceleration, with 3, 4, 5, or 6 degrees of freedom, etc.). Simulation fidelity is a complex function both of the scope and discernible levels of the simulation and of accuracy, precision, and other parameter quality characteristics.  A federate’s conceptual model should provide information indicating fidelity of representation of simulation elements within the federate.Conceptual Model DocumentationIt has been recommended that simulation conceptual model documentation employ the scientific paper approach and include the nine items listed below:  Conceptual Model Portion Identification; Principal Simulation Developer Point(s) of Contact (POCs) for the Conceptual Model (or part of it); Requirements and Purpose; Overview; General Assumptions; Identification of Possible States, Tasks, Actions, and Behaviors, Relationships and Interactions, Events, and Parameters and Factors for Entities and Processes being described; Identification of Algorithms; Simulation Development Plans; and Summary and Synopsis.  This list of items is functionally equivalent to the ten items in the generic content guidelines from IEEE/EIA 12207 for describing a planned or actual function, design, performance or process: -- the ten items are:  date of issue and status, scope, issuing organization, references, context, notation for description, body, summary, glossary, and change history [8].If the above approach is taken for conceptual model documentation, there information needed about the level of representational resolution, parameter accuracy, assumptions, etc. needed for assessment of federate compatibility and federation capability to support intended applications is much more likely to be available than would otherwise be the case.Issues In Determining Compatibility of Federation FederatesFour primary issues related to compatibility of federation federates are discussed below.Availability of Federate V&V InformationSince the mid-1990s, much progress has been made in verification, validation, and accreditation (VV&A).  The military services and Defense agencies have formalized VV&A policy, a number of V&V textbooks and other substantive works have been published, a Recommended Practices Guide (RPG) for VV&A has been developed by the Defense Department, professional societies have published or updated guidance for V&V of software and simulations, etc.  Unfortunately, documented V&V information for many legacy simulations is spotty and scarce; some legacy simulations also lack explicit documented conceptual models.  Information in V&V documentation varies from one simulation to another, and no standard format for V&V information enjoys wide use.  This is particularly true for conceptual model validation (or conceptual validation as it is sometimes called).  Consequently, it is likely that the V&V endeavor for a federation will encounter many lacks in federate V&V information.  The implications of this condition for federation validity and uncertainty are addressed later in the paper.It is also unlikely that adequate resources (time, money, personnel, etc.) will be allocated for federation V&V to allow federate V&V information deficiencies to be corrected fully in all cases.Coherence of Federate V&V InformationA federation may consist of federates of different generations of simulations, some legacy simulations of long standing and others newly developed simulations.  Some federates may be simulations that are a mixture of old and new approaches.  A federation may combine federates that use JOVIAL and FORTRAN constructs with ones that employ the latest object-oriented approaches, and some that may even employ modern formal constructs such as VDM++ or Z++.  It is sometimes very difficult to fully assess implications that may result from such combinations of approaches.  The V&V for some federates may be well documented while other federates are almost without V&V documentation.  A primary function of the federation V&V effort will be to organize federate V&V information so that it is coherent, so that more substantial and reliable V&V information is given more credence than scanty and less reliable V&V information.Adequacy of Articulation of Federation ObjectivesMany simulation applications, whether involving a single simulation or a group of simulations (such as an HLA federation), do not have adequate articulation of application objectives to establish clear and unequivocal validation criteria.  Accreditation criteria are the detailed subset of simulation requirements that have to be satisfied for a simulation to be valid.  It is often difficult to get sponsors and users of simulations to articulate such criteria early and in sufficient detail for them to serve as useful guides in V&V assessments.  This kind of articulation of federation (and federate) objectives should be both clear and testable (i.e., one can tell if an objective has been satisfied or not), critical for the application to be successful (i.e., non-essential items should not be included in this articulation of objectives), and directly related to the more important aspects of the application.Responsibility for development of (or at least approval of) such acceptability criteria rests upon those who have the authority to approve a simulation for the particular application (the Accreditation Authority), but often initial drafts of such criteria will be prepared by others.  Sometimes this will be done by the simulation development team, sometimes by the VV&A or V&V team, sometimes by a combination of these, and sometimes by others.Lack of Standards (or even guidelines) for Determining Required Federate/Federation Fidelity for Specified ApplicationsValidity for both federations and federates is a function of simulation fidelity.  Unfortunately, there are no widely accepted approaches for determining the fidelity required for a particular application.  There is not yet even a widely accepted approach for how to define and describe simulation fidelity, in spite of extensive efforts within the Simulation Interoperability Standards Organization Simulation Interoperability Workshop (SISO/SIW) to establish a common vocabulary and construct for simulation fidelity [9], much less agreement about how to measure or assess fidelity (especially for data sparse situations – and the question arises for data rich situations, “why not simply use the data instead of a simulation?”).The problem of how to articulate simulation fidelity requirements for different applications will become even more difficult as simulation usage crosses organizational and institutional boundaries, as it must if the vision of Simulation Based Design is to become reality [10].  At some point, it will be necessary for simulation fidelity to be able to be discussed in terms that are meaningful to those involved with computational science and engineering simulations, such as computational fluid dynamics (CFD) codes, to those who use simulations to study interactions within complex systems such as a guided missile, to those who are concerned about human interaction with simulations (such as the Federal Aviation Administration (FAA) concern about cockpit simulators and Department of Energy (DOE) concern about simulators of control rooms for nuclear power plants), and to those who want to use HLA federations in training senior military commanders for response to crises and for other operations.  Many challenges remain in this area since many communities use the same terms in different ways, and the technical foundations to facilitate use of common techniques in different domains are limited.Conceptual Model Role in Assessing Compatibility of Federation FederatesThis section has four main topics.  The first addresses the role of the federate conceptual model in determining federate validity.  Then we discuss the role of information from federate conceptual models in determining compatibility of candidate federates for a federation.  Next we discuss how to cope with federate information deficiencies in assessing federate compatibility and federation validity.  Finally we address federation uncertainties resulting from federate uncertainties.Conceptual Model Role in Federate ValidationA federate’s conceptual model provides the only rational basis for judgment about a federate’s validity for any set of conditions for which the federate has not been tested explicitly.  Consider the following illustration.  All three equations below pass through both the origin and the point (1,0), but they have different values for y at x = 0.5:  Without the equation and just knowing that the origin and (1,0) are on the curve, one can say little about what to expect for y when one interopolates, say at x = 0.5, and even less if one tried to extrapolate beyond the two points.  The value of y could be a positive number, a negative number, or some place in between.  A similar kind of situation exists for a simulation without an explicit (and well documented) conceptual model – one has little rational basis for judgment about simulation performance for any conditions other than those tested explicitly.  	[Eqn. 1]     y = x * (1-x)  	[Eqn. 2]     y = 0[Eqn. 3]     y =  x * (x-1)The assumptions of a federate’s conceptual model help to establish limits on valid applications for the federate.  Some algorithms are only appropriate for steady state conditions.  A conceptual model with such algorithms would show that simulation is inappropriate for use in transition situations.  Some algorithms become inappropriate under some conditions (division by zero is a classic example of an inappropriate exercise of an algorithm).  A federate’s conceptual model helps to clarify such and prevent misuse of the simulation.Potential for inappropriate reuse is a major concern to many.  Potential inappropriate reuse ranges from modifying user requirements in order to accommodate limitations of reused components to Ariane 5-like disasters, in which a satellite-launch costing more than half-a-billion dollars failed because of software reused from Ariane 4 [11].  The potential for inappropriate reuse applies as much to HLA federations as it does to reuse of simulation components (such as a requirement or specification, algorithm, or module of code) in building or modifying a federate.A federate’s conceptual model provides a basis for estimating the federate’s fidelity since the conceptual model should indicate the level of decomposition and aggregation employed in the simulation as well as indicate the accuracy of parameters in the algorithms used representing its subject.  These two considerations (level of decomposition and parameter accuracy) are important aspects of representational fidelity.6.2  Conceptual Model Role in Assessing Compatibility of Candidate FederatesIncompatibility among federates can stem from many factors, but the primary factors that cause incompatibility are 1) level of resolution/aggregation, 2) parameter accuracy, 3) timing considerations, and 4) assumptions.  Quality conceptual models for federates will provide information about these factors that might not be available otherwise.Federation developers have three basic options in regard to federate compatibility.  First, a federation developer can insist upon compatibility among federates.  This can be accomplished by restricting federation applications to the intersection of federate compatibility or by only considering candidate federates which are fully compatible with federation application objectives (assuming such are defined early enough in adequate detail).  Second, a federation developer can accept some level of federate incompatibility.  If this occurs, it is recommended that the federation developer follow the suggestions presented later in this paper.  Or third, the federation developer can ignore federate compatibility issues, which some would contend is often done.  When that happens, the federation user may be like the American traveler to Australia who discovered that the plug on his electric razor (or her hairdryer) could not fit into the electrical outlet – and even if an adapter to let the device get connect were employed, it still might not work since the voltage in Australia is twice the US norm (the razor or hairdryer might even be damaged).  Failure to consider federate compatibility puts successful federation application at risk.Level of resolution/aggregation.  It is common for different parts of a simulation to have different levels of resolution because the simulation would be too computationally intensive and/or require too many resources to prepare simulation inputs if all parts of the simulation were to function with the level of the greatest detail of any part of the simulation.  It has been common for simulations to address aspects of greatest interest in much greater detail than the aspects of lesser interest.  And when mixing of resolution levels is done with consciousness of which aspects can be treated more simplistically appropriately for intended applications, it is an acceptable and appropriate approach.  However, if the simulation’s application changes (as is likely when a federate appropriate for one federation is used in a second federation for which it may not be as appropriate) or if it is discovered that some aspects of the simulation which were treated simplistically in fact can have significant impact on simulation results, it may become necessary to redo parts of the simulation or to restrict the applications for which the simulation is appropriate.  Principles for assessing validity of multi-level resolution simulations are a subject of continuing discussion (and debate), as illustrated by Davis, Hillestad, et al in publications such as [12].A variety of compatibility problems exist when federates have different levels of resolution, especially when only relatively small numbers of different items (such as aircraft, personnel, network nodes, etc.) are represented by the simulation.  It is simply the old problem of someone drowning in a stream that averages only a few inches deep.  Many military studies have shown how potential outcomes of a military activity may be very different when the tails of distributed parameters are considered (instead of only using distribution means in the analysis).  In general, assessment of the acceptability of varied resolution within a simulation (whether a federate or a federation) is a judgment call that has much more credibility when examples exist of similar resolution variations in simulations whose correctness (or lack there of) is well established for applications like those intended for the current simulation. Parameter Accuracy.  If one federate uses a parameter with greater precision than another federate, there is potential for incompatibility in some applications.  For example, if one federate uses target signature as a parameter that varies with both aspect and frequency and another federate uses constant or only statistically varied signatures, the second federate would be incompatible for applications that had to account for a target’s signature that might vary with aspect and sensor frequency – a potentially significant consideration in assessments of multi-sensor tracking and other matters of concern for currently popular topics such as the Single Integrated Air Picture (SIAP).Timing Considerations.  When federates interact within a federation, timing can be a significant issue if one or more of the federates has stringent timing requirements (such as the real-time requirements imposed in live simulations that involve actual systems as federates).  Obviously simulations which can only process much slower than real-time, as is the situation for computational fluid dynamics (CFD) codes which often process far slower than real-time (perhaps taking hours of computer time to represent less than a second of simulated time), would be incompatible with real-time federates in a federation.  Anytime such incompatible parts (from a timing perspective) are used in a simulation, it must be done consciously with due consideration to implications for simulation results.This kind of timing incompatibility problem is different than the timing problem caused by the HLA run-time infrastructure (RTI) and networks connecting federates together.  That is more a consideration for federation execution than for comparison of federate compatibility.Assumptions.  Every simulation is loaded with assumptions, many of which may not be explicit.  Serious conceptual model validation reviews, and their functional equivalents, usually uncover unstated assumptions.  In fact, a noted simulation author (Averill Law) states that he has never been involved in such a review in which significant unstated assumptions were not discovered [13].  Development of a quality conceptual model will help a federate have a more comprehensive list of explicit assumptions that it employs.Federates may have incompatible assumptions.  For example, one federate may assume that two items (entities or processes) are independent while another federate may treat one item as dependent upon the other.  One federate may use algorithms that are only valid for steady-state conditions, and another federate may have algorithms that are oriented toward transition situations (vice steady-state conditions).  Just as was the case with compatibility of federates with different levels of resolution, assessment of the acceptability of different assumptions within the federates of a federation is a judgment call that has much more credibility when examples exist of similar assumption differences with a federation whose correctness (or lack there of) is well established for applications like those intended for the current federation.  The role of federate conceptual models is to ensure that comprehensive understanding of federate assumptions is available so that such assessments may be based upon knowledge and not ignorance.6.3  Federation Uncertainties Resulting from Federate UncertaintiesSome seem to believe that computers can perform magic.  Half-a-century ago, when most people who used computers were more thoroughly grounded both in computer programming details and in basic math and science than seems to be the case today for many in the current “user-friendly” era, an acronym was coined:  GIGO (for “garbage in, garbage out).  Sophisticated graphics and the emotional impact of immersion in virtual realities causes some to forget that GIGO still applies.  In general, uncertainties in one federate will cause similar uncertainty for the federation, even though other federates may not have that particular uncertainty.  Usually, the strength of a chain is limited by its weakest link.  A similar situation applies for federations.Adroit federation design may ameliorate the impact of federate limitations by structuring the federate so that such limitations have no significant consequences on the federation; however, Murphy’s Law still has to be faced.  At sometime, the federate imitations/uncertainties/etc. that the federation had been designed to prevent from impacting federation results will manage to have impact.Those who build things know the “penny wise, pound foolish” dilemma (sometimes called the “pay a little now, or pay a lot later” choice), and most understand that a design with quality parts usually provides more reliable and dependable performance than a fancier design without quality parts.If a federation involves federates without the information about federate level of resolution, accuracy, timing, assumptions, etc. that quality conceptual models provide, it will be difficult to ensure federate compatibility or to know all potential federation uncertainties/limitations that might result from federate limitations and uncertainties.6.4  Suggestions for How to Cope with Federate Information DeficienciesAt this point in time, it is more likely than not that there will be information deficiencies for at least some candidate federates for a federation.  V&V information, conceptual model documentation, etc. may be vague, incomplete, or even lacking.  There are only four basic options for the federation:  1) cause the information that’s needed to be developed, 2) seek different federates to replace candidate federates with information deficiencies, 3) accept uncertainty for the federation from federate information deficiencies, or 4) abandon the federation (just stop, change what the federation is expected to do, etc.).  The first option is generally the most desirable one, but in many cases, there will not be time or resources available to correct these information deficiencies before the federation needs to function.  The second option may not exist – there may not be alternate federates, although creative exploration of this option may reveal more possibilities than expected.  And the fourth option may be unacceptable to the sponsor and/or expected users of the federation.  Thus, we will focus discussion in this section on living with federation uncertainty caused by federate information deficiencies.First, the situation does not have to be permanent.  Federate information deficiencies can be corrected over time, even if they cannot be removed before initial use of a federation.  Long term correction of this problem should be a given for any federation.  However, this involves a commitment by leadership of the endeavor to ensure that adequate resources and attention are given to correcting such deficiencies.Second, potential impacts on the federation from federate information deficiencies should be explicitly described (even if it can only be estimated) so that federation users have advance warning about the uncertainties and so that reports from federation usage may bear appropriate caveats.  This may be something that federation leaders are reluctant to do since it exposes federation “warts,” and many in leadership roles only advertise (and may even exaggerate) capabilities.Third, in order to compensate for federate information deficiencies, test plans for the federation should include items specifically designed to calibrate the federation in areas that might be impacted by federate information deficiencies -- and additional data collection may be required for federation use to help in determining whether federation performance is acceptable or not.  Without this kind of “insurance,” use of a federation with incompatible federates is courting disaster.ConclusionThis paper identifies some federate compatibility issues and federation options when federates are incompatible.  However, this paper is frustrating in that it does not go far enough.  It has at most taken a “baby step” forward in dealing with the issues and does not provide the kind of detailed guidance that we all long for.  In part this is because we lack guidelines for assessing federate compatibility and appropriate responses to incompatibility that can be applied to the full spectrum of federation applications.  But in part also we have the problem of immature technology (and no comprehensive research program to enhance validation technology).  Before we can advance to the level of detailed guidance that we seek it will be necessary for advances in several technical areas, such as how to describe and assess simulation fidelity.Simulation interoperability has made great strides in the past decade, but more progress has occurred in the area of how to connect simulations together (sometimes called “technical interoperability”) than in how to assure that their connected activities achieve objectives (sometimes called “substantive interoperability”) [14].  Addressing federate compatibility is a key element in federation validation and is an essential ingredient in substantive interoperability.  Hopefully this paper has indicated some of the important roles that quality federate conceptual models can play in federation validation.References[1]  Special Topic:  Conceptual Model, in the updated DoD Recommended Practices Guide for VV&A [available at the DMSO website:   HYPERLINK http://www.dmso.mil] http://www.dmso.mil].[2]  Dale K. Pace, “Development and Documentation of a Simulation Conceptual Model,” Proceedings of the Fall 1999 Simulation Interoperability Workshop, March 15-19,1999.[3]  Dale K. Pace, “Conceptual Model Descriptions,” Proceedings of 1999 Summer Computer Simulation Conference, July 12-14, 199, Chicago, IL.[4]  Dale K. Pace, “Development and Documentation of a Simulation Conceptual Model,” 99 Fall Simulation Interoperability Workshop Papers, September 1999.[5]  Dale K. Pace, “Simulation Conceptual Model Development,” Proceedings of the Spring 2000 Simulation Interoperability Workshop, March 26-31, 2000, Orlando, FL.[6]  Dale K. Pace, “Simulation Conceptual Model Issues:  Development Methods (Part 1), Interaction with Simulation Requirements (Part 2), and Simulation Development Costs and V&V Costs (Part 3),” Proceedings of the 2000 Summer Computer Simulation Conference, July 16-20, 20000, Vancouver, British Columbia, Canada.[7]  Dale K. Pace, “Simulation Conceptual Model Development Issues and Implications for Reuse of Simulation Components,” 2000 Fall Simulation Interoperability Workshop Papers, September 2000.[8]  Reed Sorensen, “Software Standards:  Their Evolution and Current State,” CrossTalk:  The Journal of Defense Software Engineering, Vol. 12 No. 12 (December 1999), pp. 21-25.[9] David C Gross (redactor), “Report from the Fidelity Implementation Study Group”, 99S-SIW-167, 1999 Fall Simulation Interoperability Workshop Papers, March 1999.[10]  J. F. Keane, R. R. Lutz, S. E. Myers, and J. E. Coolahan, An Architecture for Simulation Based Acquisition,” Johns Hopkins APL Technical Digest, Vol. 21, No. 3, July-September 2000, pp. 348-358.[11]  John Doyle, “Virtual Engineering:  Toward a Theory for Modeling and Simulation of Complex Systems,” Appendix B of Volume 9 Modeling and Simulation, Technology for the United States Navy and Marine Corps 2000-2035:  Becoming a 21st-Century Force, Panel on Modeling and Simulation, Committee on Technology for Future Naval Forces, Naval Studies Board, Commission on Physical Sciences, Mathematics, and Applications, National Research Council – published by National Academy Press, Washington, D. C., 1997., pp. 158-159.[12]  Paul K. Davis and James H. Bigelow, Experiments in Multiresolution Modeling (MRM), RAND, 1998.[13] A. M. Law and W. D. Kelton, Simulation Modeling and Analysis, 3rd Edition (McGraw-Hill), 1999.[14]  Judith Dahmann, Marnie Salisbury, Phil Barry, and Chris Turrell, “HLA and Beyond:  Interoperability Challenges,” 99 Fall Simulation Interoperability Workshop Papers, September 1999.About the AuthorDale K. Pace, is a member of the Johns Hopkins University Applied Physics Laboratory Principal Professional Staff.  A specialist in operations research, systems analysis, wargaming and seminar gaming, scenario development, and defense analysis, Dr. Pace has been a major contributor to simulation verification and validation (V&V) ideas and serves as the Associate Editor of Simulation for Validation.  He was co-chair of the Military Operations Research Society (MORS) Simulation Validation (SIMVAL) 1999 Workshop and is a member of the Defense Modeling and Simulation Office (DMSO) Verification, Validation, and Accreditation (VV&A) Technical Working Group and its VV&A Technical Support Team, with particular responsibilities in the area of conceptual model development.  He is involved in a standards committee on V&V in computational solid mechanics for the American Society of Mechanical Engineers (ASME) International.  He also taught in the graduate technical management program of Hopkins’ Whiting School of Engineering from the mid 1980s to the mid 1990s.* The work reported in this paper was performed under sponsorship of the Defense Modeling and Simulation Office (DMSO), but its views are those of the author and should not be construed to represent views of DMSO or of any other organization or agency, public or private. EMBED Word.Picture.8  