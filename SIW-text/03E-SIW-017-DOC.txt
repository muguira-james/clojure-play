An “UnFair Fight” Assessment Tool for Distributed SimulationsRena ZhangTerry S. McDermottMike ButterworthConway A. BoltNorthrop Grumman Information Technology12000 Research Pkwy Suite 236Orlando, FL 32826407-243-2018, 407-243-2026, 407-243-2014, 407-243-2022 HYPERLINK "mailto:rzhang@northropgrumman.com" rzhang@northropgrumman.com,   HYPERLINK "mailto:Tmcdermott@northropgrumman.com" Tmcdermott@northropgrumman.com HYPERLINK "mailto:mbutterworth@northropgrumman.com" mbutterworth@northropgrumman.com  HYPERLINK "mailto:CBolt@northropgrumman.com" CBolt@northropgrumman.comKeywords:Fair Fight, Physical Model, Behavioral Model, Environment Model, Network PerformanceABSTRACT:  When two or more simulations join the same training exercise, the difference in the simulations may have an effect on the outcome of the training, that results in an “unfair fight” situation.  In a distributed training environment, “Unfair Fight” situations can originate either from network performance problems or from differences in the simulation models, which not only include physical models and behavioral models, but also environmental models.  Therefore, identifying “unfair fight” situations is not an easy task and it has been a major concern for both simulation Interoperability and Verification & Validation.  This paper presents the design of a test tool for automated fair fight analysis in distributed training environments.  The tool listens to the network data traffic and analyzes the data to identify “unfair fight” situations through the analysis of network performance, and the validation of physical models (e.g., weapon, radio, visibility models and sensor models), behavioral models (e.g., maneuver, engagement and concealment) and environmental models (e.g., terrain and weather). The implementation of the tool will use a centralized run-time data collection infrastructure with a generic data collector being its center and the Fair Fight analysis tool as a series of plug-ins.  In this infrastructure, any new tool can be added easily as a plug-in, and any existing tool can be replaced or updated.  INCLUDEPICTURE "http://www.opinionjournal.com/images/storyend_dingbat.gif" \* MERGEFORMATINET IntroductionWhen two or more simulations join the same training exercise, the differences in the simulations may have an effect on the outcome of the training, which results in an “unfair fight” situation.  In a distributed training environment, “Unfair Fight” situations can originate either from network performance problems, or from differences in the simulation models, which not only include physical models and behavioral models, but also include environmental models.  Therefore, identifying “unfair fight” situations is not an easy task and it has been a major concern for both simulation Interoperability and Verification & Validation.  The first step in identifying “Unfair Fight” situations is to come up with an accurate definition for “Fair Fight”.  According to the glossary given by the Army Model and Simulation Office (AMSO), “Fair Fight” is defined as: Two or more simulations may be considered to be in a fair fight when differences in the simulations’ performance characteristics have significantly less effect on the outcome of the conflict than actions taken by the simulation participants.   The questions here are how to detect and identify differences in the simulations’ performance characteristics, determine what the effects are, and how to decide whether the effects are significantly less or not?If the differences in the simulations’ performance characteristics have a measurably greater effect on the outcome of the conflict than actions taken by the simulation participants, this would indicate an unfair fight situation.  For example, two simulations using different line of sight algorithms or different terrain models may result in a situation that entity A in one simulation can see entity B in the other simulation, but B can’t see A. This is may be an unfair fight situation for B, depending on the other circumstances surrounding the time when this situation occurs.  However, even if the differences in the simulations’ performance characteristics have significantly less effect on the outcome of the conflict than actions taken by the simulation participants, does it guarantee a fair fight?  The answer is probably “no”, but why?  How can this be determined?The American Heritage Dictionary defines the word “fair” as “having or exhibiting a disposition that is free of favoritism or bias.”  With the case above, bias has been introduced for A over B.  The question is how was the bias introduced and by what system?  The different terrain models seem more likely than the different line of sight algorithms in this case.  More insidious is the possibility that small effects – or biases – perturb the results of individual engagements, sometimes giving advantage to one side, sometimes to the other, but average out over many exchanges.  Hence, the battle may have the correct result, but many individual lower level conflicts still can be “unfair.”  Worse yet, if this happens where live participants are involved, negative training could go undetected if the evaluation of success aggregates observations.Although the analysis of “Fair Fight” between CGFs involves many difficult issues, it is not as complicated as the analysis of “Fair Fight” among Manned Modules (MM)or between MM and Computer Generated Force (CGF).  In the case of MM versus CGF, fair fight analysis is more challenging since MM is constrained by the human interactions and CGF operates on data and predefined models.  ApproachThe difficulty in “Fair Fight” analysis starts with what is an unfair fight situation, how to identify it and what is the cause.  For example, if A and B have the same type of weapons but different munitions, and B’s munitions are significantly less powerful than A.  The outcome may indicate a fair fight for A even though A may in fact be at a disadvantage too.  Ideally, if A and B use the same weapon model and they spot each other at the same time, they would fire at each other at the same time, resulting in B being killed and A being damaged.  However, in the situation that A and B have different weapon models that results in a longer weapon loading time for A (unfairness to A), also B is experiencing network delay that he can’t receive the firing command before A fires at him, the outcome will be completely different. B will be killed and A will get away undamaged.   Here we have not only unfairness for B (network latency) but also unfairness for A (longer loading time from the physical model).  Therefore, even if it appears to be a fair fight, unfairness could exist for both sides, even for the winner A. In situations like this, our analysis tool still will analyze the unfairness from the winner’s point of view.  In order to detect these situations, our tool will examine the difference in the simulation physical models. In addition to the physical model, “Unfair Fight” situations can also originate from network performance problems, the differences in simulation behavioral models and environmental models.  Therefore, our tool will also consider these aspects for possible causes of “Unfair Fight” situations. Examining the differences of simulations’ physical, behavioral and environment models will not be achieved thoroughly by simply listening to the network traffic.  Some of the information about the models is simply not available through the network data.   Therefore, a methodical approach would be to introduce a bridge between the simulations and our tool to extract additional modeling data from the simulation.  There are two approaches to accomplish this.  The first is to set up a common analysis data model standard and let simulations provide the same set of data for analysis. OneSAF designed an initial Analysis Data Model (ADM) for this purpose. The analysis data is only targeted at AAR right now.  The second approach is based on the concept of composable models.  Model data can be easily extracted for analysis during the composition phase of the physical, behavioral and environmental models.   This is in the line of OneSAF’s concept of composable simulation.  “Fair Fight” analysis can be performed in two phases: pre-event and run-time. Pre-event analysis is targeted at certain aspects of the physical models, behavioral models, environmental models and network performance in order to eliminate as many potential “unfair fight” problems as possible before an exercise.  The difficult part of this phase is the set up for comprehensive testing.  For example, to test line-of-sight in the environmental model, an algorithm might be developed to move two entities spatially to discover all the different positions where only one of the entities has line-of-sight of the other.  In this case, the terrain representation in the environmental model plays an important role in determining those positions.  Pre-event analysis may be effective in eliminating many “Unfair Fight” situations, it can not guarantee to catch them all since the generation of all possible positions results in a combinatorial explosion of data positions to analyze.  Therefore, run-time analysis is also needed to detect “Unfair Fight” situations. Run-time analysis is triggered by possible unfair fight situations. When a possible unfair situation is detected, the analysis of that situation is executed. This analysis is performed from the network performance, physical, behavioral and environmental point of view.  In fact, both the pre-event and run-time analysis use the same analysis algorithms, the only difference is that the pre-event analysis has an extra piece for the test case generation, and the run-time analysis applies an unfair situation detection algorithm before the analysis.  In summary, to provide a feasible “Fair Fight” analysis, we propose the development of a test tool for automated fair fight analysis in distributed training environments that could be used for both pre-event testing and run-time analysis. The tool listens to the network data traffic and analyzes the data to identify “unfair fight” situations through the analysis of network performance using the network performance data, and the validation of physical models (e.g., weapon, radio, visual model and sensor models), behavioral models (e.g., maneuver, engagement and concealment) and environmental models (e.g., terrain and weather).“Fair Fight” AnalysisThe first step in fair fight assessment is to come up with a list of observable situations in a distributed simulation that may indicate the existence of an “unfair fight”.  Some examples of the situations include, but are not limited to:Entity B failed to detect A when it should.Entity B doesn’t respond (fire back or take cover) to Fire/Detonation data from entity A when it should.  A and B are firing at each other using similar munitions, however, A gets damage but B gets little damage comparing to A.A and B are similar type of vehicle, however, A accelerates or decelerates faster than B, and maneuvers better than B during turns.Obviously, this list could be quite long, and is unlikely to be complete very soon.  However, if it is based on observed phenomena as well as common sense analysis, it could fairly easily cover the widely recognized possibilities.The second step in fair fight assessment is to come up with a list of possible causes in a distributed simulation that may affect fair fight.  These causes will be derived from analysis of network performance data, physical models, behavioral models and environmental models.  The following is a list of some of the aspects that should be considered:Physical models:Weapon range and effectRadio communication modelTarget acquisition Visual sensor model (visibility range, FOV and LOS algorithm)Sensor model (Sensor range, FOV and detection algorithm, sensitivity, environment interactions, target signatures)Electronic countermeasures (EC)Attrition modelBehavioral modelDamage assessment (Combat damage, stochastic failures and deterministic failures)Mobility (Maneuver, velocity / acceleration, terrain and soil type)Collision Engagement (Rules of Engagement, rate of fire, and delivery accuracy)ConcealmentEnvironmental Model Location and orientationVisibilityTerrain representation Weather and atmosphere representationNetwork Performance PDU update rate PDU latency PDU error  PDU drop rateThe connection between the situation list and the cause list is to seek out correlations that might suggest analyses that would allow the detection of the situation and at a minimum, investigate or suggest potential causes.  Once some analyses are defined that would have potential benefit either in a pre-run or run-time application, the next step is to collect the data required for the analyses, and analyze the data for fair fight assessment (See  REF _Ref31011477 \h  \* MERGEFORMAT Figure 2Figure 2).  Data collection should gather all the relevant data and avoid redundancy.  It should support analysis of how unfair advantage or bias is introduced into distributed simulation events and provides a framework on which the effect of the unfair condition on the outcome of the event can be measured or evaluated.  There are two possible purposes for the data collection here: One is to collect data that will identify unfair situations; the other is to collect data that will identify the cause of the unfair situation.  It became clear to the authors that the data set for determining the existence (or possible existence) of an unfair situation was not necessarily the same data set needed to determine the cause of the unfair situation. The identification of the causes of the unfair situations is much more complicated and it involves model data and possibly human heuristic judgment of the situation.  The following table provides an analysis for “Fair Fight” between CGF systems.  It presents a list of “unfair fight” situations, data to collect in order to identify the situations, data available on the network and possible causes of those situation. (See  REF _Ref31099332 \h  \* MERGEFORMAT Table 1Table 1).  The table is color-coded based upon the author’s judgments about data availability.  The color green represents data that is already available on the network.  The color yellow represents data this is available in the models but not necessarily sent out on the network.  The color red represents data that is not currently available at this time either on the network or in the models. This list of issues also applies to the “Fair Fight” analysis between MM and CGF. However, “Fair Fight” analysis between MM and CGF will be different from the “Fair Fight” analysis in terms of the target acquisition and the behavioral model.  In CGF, sensor models restrain target acquisition, and predefined rules and doctrines determine behaviors, but in MM, image generator, image display and human vision greatly affect the target acquisition, and human interactions limit human behaviors.  REF _Ref30586478 \h  \* MERGEFORMAT Table 2Table 2 is a summary for “Fair Fight” analysis between MM and CGF from paper “SAF and Manned Simulators Correlation Issues in CCTT” by Henry Marshall et al.   ImplementationIn the envisioned tool, it is unlikely that a single effort will solve the problem.  By the nature of the fair fight issue, it is likely that an ongoing period of discovery will require addition of, and modification of analyses over a period of time.  Hence, the tool should be designed from the outset to accommodate this expectation.  We propose an infrastructure provides data collection and other services. The infrastructure will provide a plug-in interface so that data analysis tools can be implemented as plug-ins to the infrastructure framework (See  REF _Ref30566830 \h Figure 1Figure 1). The data collection would accept adaptation to collect specific types of data required by the analyzers.  This can be achieved by maintaining a list of the analyzers currently plugged in together with their data requirements.  The advantage of this infrastructure is that the centralized data collection will perform various kinds of data collection upon requests, and any additional analysis tool will be independently added or removed with little or no affect on the data collection and the other analysis tools plugged in.  Therefore, it offers a flexible and easy extendable way to add additional analyses or replace an old analysis tool with an improved one. EMBED Word.Picture.8  Figure  SEQ Figure \* ARABIC 1: Data Collection & Analysis Tool SuiteThe infrastructure might also provide other non-core services like logging for post-analysis, data routing services to allow tools analyzers to provide data to one another to avoid redundant processing.  The implementation should support both DIS (IEEE std 1278.1a-1998) HLA. It is vital to support both DIS and HLA based simulations. Though most new simulation work is focusing on HLA, DIS is still a widely used protocol, and many distributed simulations are incorporating both DIS and HLA simulations..  ConclusionThis paper has suggested a path to achieving useful analysis that could be applied to federations of distributed simulations to detect and identify potential causes of unfair fight conditions.  In addition, an implementation approach has been described that reflects the nature of the proposed approach and problem: that it will take a lot of individual, case-based analyses to complete a comprehensive tool. These analyses, in all likelihood, will need to be discovered and implemented over time.  Hence, the proposed implementation provides a framework for incorporating these changes.  If the described development path were to be undertaken, it might not be able to guarantee all unfair fight conditions would be brought to light – or eliminated.  But, it would provide a defined path to the systematic extermination of this problem, which is intrinsic to the combining of independently developed simulations into federations. It is likely that the unfair fight cases that are heard anecdotally would be the first to be addressed.  In this way, the most visible problems could be solved in a way that provides for a complete solution over time.References [1]  Edward P. Harvey : “AVCATT-A Fair Fight and CCTT Interoperability”, Proceedings of Simulation Interoperability Workshop, Fall 2000 [2]  Henry Marshall, Edward V. Chandler, Brian R. McEnany and John G. Thomas: “SAF and Manned Simulators Correlation Issues in CCTT”, Proceedings of Sixth Conference on Computer Generated Forces and Behavioral Representation, 1997.[3]  Henry Marshall: “CCTT SAF and Synthetic Environment Core Update 1998”, Proceedings of Seventh Conference on Computer Generated Forces and Behavioral Representation, 1998.[4] Robert F. Richbourg, Robert J. Graebener, Tim Stone and Keith Green: “Verification and Validation (V&V) of Federation Synthetic Natural Environments”.[4] Wesley A. Milks, Matthew B. Gerber, Wayne A. Lindo and Bob Burch: “An Analysis Data Model for Modeling and Simulation”, Proceedings of Simulation Interoperability Workshop, Spring 2003.Author BiographiesDR. RENA ZHANG currently is a senior member of technique staff at Northrop Grumman. She has been involved with simulation research and development in physical modeling, behavioral modeling, human performance modeling, case-based reasoning, HLA and simulation test tools for the past five years with SAIC and NG.  She also served as an IT consultant at IBM.  Rena received her Ph.D. in computer vision from University of Central Florida.DR. TERRY MCDERMOTT is Senior Scientist for the Simulation Technology Department of Northrop Grumman Information Technology.  He has over 20 years of experience in systems engineering and simulation including system and software definition and development, system performance simulation, software design, real-time and constructive simulation, protocol development, standards development, and project management.  Currently, Terry is contributing to development of systems for visualizing tactical situations, training exercise planning and coordination, and distributed test environments.  Terry received his doctorate in mathematics from the University of Southern California specializing in Functional Analysis.MICHAEL R.  BUTTERWORTH is a Principal Member of the Technical Staff for Northrop Grumman Information Technology Defense Enterprise Solutions in Orlando Florida.  He has 16 years experience in DOD virtual simulations including fixed wing, rotary wing and ground based simulators and trainers. He now leads the SNE Standards and Common Models Standards work for Distributed Mission Training (DMT) Operations and Integration (O&I).  He holds a M.B.A. in Management from Western International University and a B.S. in Computer Science from East Tennessee State University.CONWAY A. BOLT, III is a Software Technical Lead for Northrop Grumman Information Technology, Orlando FL and has been developing distributed simulation systems for nearly a decade. He holds a Master’s degree in Computer Science from The Johns Hopkins University.Figure  SEQ Figure \* ARABIC 2: Data Capture & Analysis CycleTable  SEQ Table \* ARABIC 1: Fair Fight Analysis Between CGFsUnfair SituationData to CollectData AvailablePossible CausesEntity B doesn’t respond (fire back or take cover) to Fire/Detonation data from entity A when it should.  Position of BFire/Detonation data from ALack of Fire/Detonation data from BLack of change in velocity of BFire/Detonation PDUs from AFire/Detonation PDUs from BEntity State from BB may not have line-of-sight to A.B may ignore PDUs from A.Evasion model may fail due to location of B (Terrain data around position of B).A and B are firing at each other using similar munitions, however, A gets damage but B gets little damage comparing to A. Detonation data from ADetonation data from BDamage data from ADamage data from BPosition of APosition of BFire/Detonation PDUs from AFire/Detonation PDUs from BEntity State from BEntity State from AA and B have different damage assessment models or damage assessment tables.A and B have different delivery accuracies for the same munition.A may model environmental effects that B does not.A and B are similar type of vehicle, however, A gets considerable less damage than B in a similar collision situation.Damage data from ADamage data from BCollision data from ACollision data from BEntity State from BEntity State from ACollision PDU for ACollision PDU for BA and B have different collision models.A and B have different damage assessment models or damage assessment tables.A and B employed similar type of weapon to fire at each other, but A’s rate of fire is different than B.Fire data from AFire data from BFire/Detonation PDUs from AFire/Detonation PDUs from BA and B have different physical models for the weapon, including slew rate, load time, and the lay time.Entity A loses radio contact with entity B when it should have radio contact.Position of APosition of BRadio input data for ARadio input data for BRadio PDU from ARadio PDU from BEntity State from AEntity State from BA may have a different propagation model.A may have a different terrain obscurance model.Entity B failed to detect entity A when it should.Position of APosition of BRange of system detectionFOV of BEntity A not in entity B’s Spot list when range < entity B’s detection rangeEnvironmental data such as fog, dust, or smokeExposure of entity ARate of detection of entity BFire event of entity ADust cloud for entity AEntity State from AEntity State from BFire/Detonation PDUs from AA and B have different LOS algorithms.A and B have different terrain / environmental models, so there might be a mountain top in B’s field of view but not in A’s.A and B have different visibilities caused by weather and environment models.A and B have different sensor models (range, field of regard, and field of view).A and B have different target acquisition algorithms.A and B are similar type of vehicle, however, A accelerates or decelerates faster than B, or maneuvers better than B during turns.Velocity/acceleration data from AVelocity/acceleration data from BVehicle type for AVehicle type for BTerrain material code for ground vehiclesStart Position of AStart Position of BEnd Position of A after time xEnd Position of B after time xEnvironmental data (wind, moisture, temperature)Entity State from AEntity State from BA and B have different mobility models.A and B have different terrain models, including soil type.A and B have different environmental models.Entity A suffers Stochastic Damage differently than entity B.Stochastic damage assessment from AStochastic damage assessment from BStochastic damage input data from AStochastic damage input data from BEntity State from AEntity State from BA and B have different damage assessment models such as component level vs. system level.Entity A suffers Deterministic Damage differently than entity B.Deterministic damage assessment from ADeterministic damage assessment from BDeterministic damage input data from ADeterministic damage input data from BEntity State from AEntity State from BA and B have different damage assessment models such as component level vs. system level.Entity A does not maintain target persistence when entity B does maintain target persistence.Position of APosition of BRange of system detectionFOV of BEntity A’s Spot list over time xEntity B’s Spot list over time xExposure of target entityEntity State from AEntity State from BSystem resource limitations.A and B are equivalent units in two different simulations, however, they behave differently during the execution of the same task.Task and sub-task hierarchy from entity ATask and sub-task hierarchy from entity BReaction task(s) from entity AReaction task(s) from entity BSituation awareness data from entity ASituation awareness data from entity BFire/Detonation PDUs from AFire/Detonation PDUs from BEntity State from BEntity State from ADifferent behavior models in terms of unit behavior, decision-making and C2.Different radio communication models within the units.Table  SEQ Table \* ARABIC 2: Fair Fight Analysis between MM and CGFCategoryMMCGFDamage AssessmentHigher FidelityLower FidelityCombat DamageComponent levelEven distribution, non-cumulativeStochastic DamageComponent levelSystem level (mobility, firepower, electrical/sensor)Deterministic DamageComponent levelLimited (thrown tracks, collision, out of fuel, out of ammo)MobilityHigher Fidelity, component level model, crew error, visual DB limitsLower Fidelity, system level model, no crew error, correlated DB inputTarget AcquisitionCrew limited, Visual DB limitsNVESD ACQUIRE methodology (computed)Target exposureCrew skill< 30% exposed ignoredRangeCrew skill, visual system load management limitsComputedRate of detectionCrew skillComputedMuzzle flash, smoke, dust cloudCrew skillIgnoredTarget contrastCrew skillSimplistic terrain typeBehaviorCrew skillPredefined by doctrine, miss less obvious targets, loses target persistenceRate of FireCrew skillComputed average crew proficiencyDelivery AccuracyCrew skillCenter of mass regardless of exposed target area Positive Bias   FILENAME MBFairFight03S-EURO-SIW1.docFairFight03S-EURO-SIW1.doc		8/8/02PAGE \# "'Page: '#''"  Page: 1I don’t understand this example.ResultsResultsSystem BSystem ANetwork DataPerformance DataScenario VignettePhysical Model EffectBehaviors Model EffectEnvironmental Model EffectNetwork Performance EffectConclusionsOutcomesBehaviorsCompare