Logically Correct Data Distribution Management in SPEEDESDr. Jeffrey S. Steinmansteinman@ca.metsci.comTuan Trantran@metsci.comJacob Burckhardt bjacob@ca.metsci.comJames S. Brutocaobrutocao@ca.metsci.comMetron Incorporated512 Via De La Valle, Suite 301Solana Beach, CA  92075(619) 792-8904AbstractThis paper describes the techniques that are used for distributing published data within the Synchronous Parallel Environment for Emulation and Discrete-Event Simulation (SPEEDES) Modeling Framework (SMF) and within the SPEEDES-based High-Performance Computing Run–Time Infrastructure (HPC-RTI) that is currently under development. First, this paper introduces the data distribution management (DDM) problem, with particular emphasis on challenges faced by the DoD simulation community. This is followed by a historical summary of other DDM-related work. Third, the SPEEDES DDM technical approach is summarized, referencing a number of detailed technical appendices. Fourth, Performance results are  provided to demonstrate scalability with large numbers of entities executing on large numbers of processors. The paper concludes by discussing future optimizations to the SPEEDES approach that will (1) further simplify programming interfaces for users, (2) extend current DDM functionality, and (3) improve parallel performance through the use of advanced distributed simulation synchronization techniques.IntroductionDDM is a concept well known by the parallel discrete-event simulation (PDES) community. In general, it refers to the mechanism used by a PDES engine to automatically marshal the appropriate user-specified data between entities as they move in and out of one another’s field of view. A “field of view” is a fully generic concept, defined by the user by way of plugging interest information into the engine’s DDM sub-framework for dynamic evaluation. From there, the engine uses this interest information from each entity to trigger when to send data from one entity to another.Since the late 1980s, it has been well known by the DoD PDES community that providing efficient DDM is critical for supporting scalable large-scale military simulations. This is because DoD simulations typically involve entities that can detect one another in a myriad of ways and are free to move about without a pre-defined communication topology. Solving this problem for large PDES military simulations has proved difficult, since the detections involve marshaling data between entities that are distributed to multiple processors where they are dynamically modeled through the processing of time-tagged events. Entities on different processors learn about each other and interact by exchanging time-tagged messages. The challenge faced by SPEEDES and all previous attempts has been to develop a DDM system within a PDES environment with acceptably low overheads, and with performance and memory profiles that scale as a function of the number of entities and processors.DoD simulations typically involve entities that detect each other with sensors. Thus, for such simulations, DDM systems primarily focus on determining which entities are detectable by which sensing entities. In general, entities can detect other entities while also being detected. Thus, (1) detectable entities are required to register information to the DDM system that describes their ability to be detected, and (2) entities with sensors are required to register information to the DDM system to describe their ability to detect other detectable entities. The DDM system determines which entities detect which other entities, and then coordinates the flow of data as required.In general, entities are able to move in a simulated three-dimensional space using complex equations of motion that can change at any time. Similarly, entities may also have complex detection capabilities that also change in time. Frequently, sensors require targets to be within a specified range for detection. Thus, proximity detection often becomes one of the most critical DDM requirements for military simulations. The SPEEDES DDM sub-framework described in this paper provides a solution to these difficult demands for scalable DDM in military simulations.BackgroundSince the late 1980s, the DoD distributed simulation community has developed various technologies to address the DDM problem. Four significant phases, shown in  REF _Ref454074463 \h Figure 1, have introduced new simulation technologies into the mainstream. EMBED Word.Picture.8  Figure  SEQ Figure \* ARABIC 1: Evolution of DoD simulation technologies.In the early days of SIMNET [11] and later in DIS [12,15], UDP/IP broadcast techniques were used to address the DDM problem by periodically disseminating information about entities within real-time distributed simulations. The broadcast approach was easy to implement and was somewhat robust. However, scalability problems quickly became apparent with this strategy. Message bandwidth and processing overheads were excessive when running large distributed exercises across networks. In the later years of DIS, IP/Multicast techniques were explored to reduce the message traffic over the network and reduce wasted time processing unwanted messages. These ideas eventually made their way into the High-Level Architecture (HLA) real-time community.Also in the late 1980s, NASA developed the Time Warp Operating System (TWOS) for the Army to model theater-level campaigns on parallel super-computers [7,10]. As the first optimistic parallel simulation engine, TWOS showed promise of achieving parallel speedup, while exhibiting many of the classic instability problems now better understood in optimistic computing. This work provided the fundamental breakthroughs that germinated the SPEEDES project.The SPEEDES project began at NASA in 1990 and brought flow control to optimistic parallel simulation strategies such as Time Warp [16,19,21]. SPEEDES also introduced generic interactive support for external systems. SPEEDES was primarily used to model global ballistic missile defense applications on parallel and distributed supercomputers. During this time, parallel proximity detection algorithms [18] were developed for logical-time simulations executing on parallel computers. The results of this work were encouraging and showed that it was possible to provide scalable DDM for logical-time simulations using optimistic time management algorithms as long as flow control techniques were provided to throttle optimism.In the mid-1990s, the technologies of DIS and ALSP [24] were brought together by the Defense Modeling and Simulation Office (DMSO) [5] to launch the High-level Architecture (HLA) [3,4,20,29] with the primary goal of supporting interoperability between disparate DoD simulations. In 1996, the Architecture Management Group (AMG) unanimously voted to adopt the HLA standard. Industry and academia are currently developing other RTIs that follow the standardized interface specification. The ushering in of the High-Level Architecture brought focus to the DDM problem by standardizing the concept of interest regions in multidimensional routing spaces [1,2,13,23].While HLA was becoming a standard in 1996 [6], concurrent development of the SPEEDES Modeling Framework (SMF) simplified the use of optimistic time management techniques for software developers while promoting integration and reuse of models through a common simulation engine. Although initial DDM development focused on real-time simulations, research efforts such as SPEEDES showed that time-managed DDM was possible [8,9,27]. In particular, SPEEDES offered an approach for DDM that would directly support applications using the SMF and users of the future HPC-RTI in an interoperable manner.By the year 2000, the SMF will be fully extended to provide a high-performance simulation engine for parallel simulation applications and an interoperable RTI for HLA that executes on High-Performance Computers (HPC-RTI). This final product will allow parallel simulations and HLA federations to interoperate on high-performance computers with complete support for time management across all HLA services, including DDM.Technical Approach REF _Ref454152568 \h Figure 2 provides a high-level picture of the SPEEDES DDM infrastructure. Each entity that wishes to participate in DDM must inherit from an HLA Base-Class Object that supports the mirrored HLA functionality in SPEEDES. This base class provides Object Management, Declaration Management, and Data Distribution Management services to entities.Figure  SEQ Figure \* ARABIC 2: High-level DDM scheme in SPEEDES.Detectable Simulated Entities map their published attributes into the object proxy infrastructure during construction. Object proxies provide the data caching mechanism that allows entities to have local access to other entities’ exportable attributes. A complex SPEEDES messaging system handles updating object proxies automatically. For more information on object proxies, see Appendix A [22]. Because all exportable attributes are provided as C++ classes, operator-overloading techniques detect when their values change. This allows SPEEDES to automate the reflection of attributes to remote entities when they are modified.Detectable Simulated Entities define their publication regions to register their ability to be detected. Similarly, Sensing Simulated Entities define their subscription regions to register their ability to detect other entities. The HLA Base-Class Object coordinates both publication and subscription regions with the Distributed Hierarchical Grids (HiGrids) where region-overlap computations are done in parallel. For a more detailed description of HiGrids, see Appendix B. The HiGrids report Object Handles back to the publisher indicating which subscribers need the publisher’s proxy. The HLA Base-Class Object condenses the subscriber information into several distribution lists and then coordinates the discovery and removal of object proxies. Later, when a publisher updates one or more of its attributes, a message is sent to each node requiring the update. Information is then locally disseminated to the subscribing entities.In order to reduce message traffic, all attribute-related object proxy messages are sent only once to subscribing nodes. The Local proxy Manager (LPM) receives object proxy information and locally distributes proxy pointers to subscribing entities on its node. By distributing pointers to the proxy instead of having each subscriber replicate the proxy, memory is significantly reduced.To reduce the possibility of the LPM becoming a rollback bottleneck, NLPM instances of this object are created. Each publisher uses its UniqueId and modular arithmetic to determine which LPM instance manages its proxy.	LPMId = Mod(UniqueId, NLPM)	(2)The LPM maintains a list of remote proxies (see the UML class diagram [14] in  REF _Ref454361571 Figure 3). For each remote proxy in an LPM, a distribution list of object handles is maintained for each locally subscribing entity. When an LPM first receives a remote proxy, the proxy is added to the LPM’s proxy list and then its initial local distribution list of object handles is constructed. Later, if another local entity subscribes to the proxy, its object handle is added in the LPM to the proxy’s local distribution list. In a similar manner, when an object no longer subscribes to the proxy, its object handle is removed from the proxy’s local distribution list. If a proxy’s local distribution list ever becomes empty, the proxy is completely removed from the LPM.Figure  SEQ Figure \* ARABIC 3: UML diagram of the Local Proxy Manager (LPM). The LPM maintains a list of remote proxies. For each remote proxy, a list of object handles is maintained for distribution of proxies to local entities.Sensor objects that participate in range-based filtering can optionally participate in the precomputation filtering algorithm provided by SPEEDES sensor components. Precomputations perform the final stage of range-based filtering by computing exactly when remote entities enter and exit the sensor’s true sensor range. Sensor components manage their own detectable proxy list. When a component’s detectable proxy list transitions from being empty to having one detectable proxy, the precomputation wakes up the sensor to begin scanning and forming detections. If the list ever becomes empty, sensor scanning terminates. This interrupt-driven approach eliminates sensor polling when there are no objects within the sensor’s field of view.Multidimensional Routing SpacesMultidimensional routing spaces provide value-based filtering in SPEEDES. Publishers define publication regions and subscribers define subscription regions in routing spaces. An entity can register more than one publication region, subscription region, or both. If a subscription region overlaps a publication region, then the subscriber will receive the publisher’s object proxy. This is shown conceptually in  REF _Ref444078720 \h Figure 4.Figure  SEQ Figure \* ARABIC 4: An example of a simple 2-dimensional routing space. This figure identifies two subscription regions and one publication region for three different objects. In this example, Object A will receive the proxy of Object C.SPEEDES provides four different kinds of routing space dimensions:Normal Dimensions – double precision values bounded by a minimum and a maximum. Users provide a low and high value to define regions.Theaters – bounded spatial regions using <latitude, longitude, altitude>. Users provide a position and a range to define spherical regions.Categories – integer-based values bounded by 0 and the number of categories - 1 for that dimension. Users specify integer values to define categorical regions.Enumerations – an enumerated list of string elements that users can employ for filtering. Specifying elements in the enumeration identifies multiple regions.Users bin each dimension according to specified resolutions to form cells. SPEEDES then distributes each cell using a scatter decomposition algorithm. By distributing routing spaces in this manner, region overlap computations are processed in parallel. Further binning on dimensions permits SPEEDES to manage regions having finer resolutions within a grid-cell. A hierarchical grid data structure manages each cell’s sub-dimensions.SPEEDES does not require using every dimension in a routing space when specifying a region. Regions with ignored dimensions are equivalent to specifying the entire dimension for that region. In other words, SPEEDES does not provide filtering on that dimension for that region since the region really does not care about that dimension. For efficiency, SPEEDES does not subdivide ignored dimensions into multiple bins for distribution across processing nodes. However, users can specify whether SPEEDES will subdivide such dimensions within a hierarchical grid-cell on a dimension-by-dimension basis. For in-depth information on routing spaces, see Appendix C.Range-Based FilteringApplications commonly employ DDM to support range-based filtering using theater dimensions. Theater dimensions are used to support geographical regions (see Appendix D for a detailed description.) The theater dimension automatically determines region overlaps. To do so, it uses each entity’s maximum sensor range and each entity’s maximum velocity to determine (1) region sizes and (2) how often regions require updating. Users can specify multiple theaters to support multiple conflict zones around the world. Theaters work differently from the other dimension types in that the union of all theaters automatically forms a logical dimension. A spatial region that does not intersect any of the theaters results in a NULL routing space region. REF _Ref454079295 \h Figure 5 shows an example of how physically correct range-based filtering is accomplished between a moving entity with a sensor and a moving detectable target. In this example, a simple {X, Y} routing space is used to define physical regions within the battlefield. The sensing entity’s field of view is modeled as a circular region and the target’s position is modeled as a point.Figure  SEQ Figure \* ARABIC 5: Range-based routing space example. Here, the routing space is decomposed into 15 grid cells, which are each decomposed into 4 partitions for finer resolution. Subscription and publication regions are mapped into the grid cells where overlap computations are performed by SPEEDES. If a publication region overlaps any part of a subscription region, the publisher sends its proxy to the subscriber. Subsequent updates to proxy attributes will be sent to the subscriber until the regions no longer overlap. At that point, SPEEDES removes the proxy from the subscriber.Because entities periodically register their subscription and publication regions at discrete points in time, regions must be artificially expanded to account for motion. Physically correct filtering requires (1) that the true sensor coverage of each entity always remains within its expanded subscription region and that (2) target positions always remain within their expanded publication region. The object proxy of the target is delivered to the sensor if its publication region overlaps any part of the sensor’s subscription region.In the worst case, an entity might unexpectedly change its trajectory at any time and move in any direction with maximum velocity. Therefore, regions are expanded symmetrically about their current position to account for this worst case. There is a relationship between how often an entity registers its regions, a lookahead value if used, how much to expand a region, and its maximum velocity. This relationship is described in equation (1).	(R = Vmax ( ((T+L)	(1)This equation implies that the expansion is proportional to the maximum velocity of the entity and the time between registering region updates. Frequent updates require less expansion, but may diminish overall performance because of the additional overhead involved in registering regions within the DDM system. It is important to update regions often enough to keep their expansions reasonably small, but not so often that region management overheads become excessive.Performance ResultsSPEEDES DDM was tested on a number of computers using different operating systems and compilers. Here, results taken on a 96 processor Origin 2000 are presented. Both the standard SPEEDES shared memory communications library and the Message Passing Interface (MPI) library were tested, but only the shared memory results are reported here. 1,000,000 entities were modeled with 100-kilometer sensor ranges. Each entity randomly moved about a global theater using great-circle trajectories with randomly generated velocities between 0 and 0.333 mach.One of the goals of this experiment was to have each object detect one other object on the average. To accomplish this objective, two more dimensions provided additional filtering. An ENUM dimension with five named values decreased the distribution of proxies by a factor of five. A normal DIMENSION provided the final filtering necessary to accomplish the goal.  REF _Ref455059590 \h Figure 6 shows a histogram of the number of proxies actually delivered during the execution of the simulation. The first column indicates that 167,700 publishers did not deliver their proxy to any subscribers. The second column indicates that 295,387 publishers delivered their proxy to exactly one subscriber. Etc., for subsequent columns. By fitting a binomial distribution to the histogram, it was determined that roughly 1.81 proxies were delivered on the average to subscribers. The fact that this measurement so accurately fits the binomial distribution indicates that detections were indeed random.Figure  SEQ Figure \* ARABIC 6: Histogram showing the number of proxies delivered. Perfect filtering would result in a binomial distribution fit where p=1/1,000,000 and q=(1-p). In this experiment, the binomial distribution best fits 1.81/1,000,000. This means that the filter allowed 81% more proxies to be delivered than optimal.The most important performance measurement is the simulation run-time.  REF _Ref455061416 \h Figure 7 plots the run-time of the simulation as a function of the number of processors and entities. The results show nearly perfect speedup when going from 32 to 96 processors.Figure  SEQ Figure \* ARABIC 7: Run-time of the simulation as a function of the number of entities and nodes.One of the critical measurements of scalability is the amount of memory required to run the application. This is shown in  REF _Ref455060456 \h Figure 8. As the number of processors increased from 32 nodes to 96 nodes, the total event and message memory consumed during the execution only increased by about 10%. This indicates that the memory usage and message passing overheads scale as they should.Figure  SEQ Figure \* ARABIC 8: Memory usage by event type.The critical path analysis calculated by SPEEDES measures the maximum possible speedup in a simulation’s execution. The critical path analysis assumes that each entity was modeled on its own processor and that there are no overheads in communication, rollback, or event management. It strictly determines maximum speedup based on application event processing and dependencies between simulation objects. The maximum speedup is determined by first finding the simulation object with the largest critical path CPU-time. Maximum speedup is calculated as the total CPU-time spent processing all events divided by the largest critical path time. The results are shown in  REF _Ref455062053 \h Figure 9. It is clear that there is a tremendous amount of parallelism in the application, and that the inherent speedup is somewhat insensitive to the number of nodes.Figure  SEQ Figure \* ARABIC 9: Maximum speedup possible according to critical path analysis. REF _Ref455063692 \h Figure 10 shows the number of events processed during the execution of the simulation by event type. The only application-level event in this experiment is the RadarScan event. It is encouraging to see that this event dominates the chart. However, this figure does not indicate how much time is actually being spent processing each event type.Figure  SEQ Figure \* ARABIC 10: Number of events processed by event type.The total amount of time spent processing each event type is shown in  REF _Ref455063969 \h Figure 11. Here, the results are not so encouraging. Despite the fact that the RadarScan event is processed an order of magnitude more frequently than the other events, the overheads for PublishGrid, SubscribeGrid, PublishSpace, and SubscribeSpace are quite high. These are the critical DDM events coordinating regions and grids. Future work may be required in lowering the processing overheads in these events. Still, the DDM mechanisms scale because (1) the grids are distributed, and (2) because grid lookups are done by entities in parallel.Figure  SEQ Figure \* ARABIC 11: Total amount of time processing events by event type.The average event processing times are shown in  REF _Ref455064347 \h Figure 12. The PublishGrid and SubscribeGrid events have the largest average processing times. It is interesting to note that the RadarScan event requires very little processing time. This is because the radar sensor model was developed with low fidelity. One would expect that if the fidelity of the sensor were increased, the relative amount of time spent in application code would also increase, thus providing more parallelism. Furthermore, when track-fusion is added to the simulation, even more application-level event processing will occur.Figure  SEQ Figure \* ARABIC 12: Average event processing times by event type.In summary, it is clear that the DDM mechanisms in SPEEDES are scaling nicely. The DDM algorithm itself executes almost perfectly in parallel. There appears to be some event-processing overheads that should be examined and reduced if possible. The SPEEDES DDM infrastructure provides services to applications in logical time, and with complete repeatability.Summary and ConclusionsThis paper discussed the techniques used to implement scalable and logically correct DDM in SPEEDES. The key technologies presented were:Object proxies and automating the reflection of attribute updates.Distributed hierarchical grids with multiple resolutions.Four kinds of routing space dimensions: THEATER, DIMENSION, ENUM, and CATEGORY.How to manage regions to provide physically correct DDM for range-based filtering.The measured performance of the algorithm shows nearly perfect speedup and scalability for very large scenarios. 1,000,000 entities were modeled on 96 processors. However, more extensive experiments ought to be performed. For example, it would be interesting to set up an experiment where objects are drawn together. Such an experiment will exercise the need for flow control in optimistic simulations.A number of optimizations are planned to further increase performance. These optimizations are:Reduce the overheads of several DDM-related events.Develop a query-reply mechanism to prohibit a simulation object from processing its next event when it knows that it is expecting a reply event with an earlier time tag (e.g., the information that is returned from grids to publishers).Automate lazy cancellation to roll events forward that have been rolled back when possible (SPEEDES supports lazy cancellation, but it is not automated) [17,25,26].Develop an event-reparation mechanism to allow events to repair themselves instead of being completely reprocessed after they are rolled back.The results of this work will provide a powerful DDM capability to all SPEEDES applications and for users of HLA. These algorithms will be integrated with the HPC-RTI that is currently under development. The end product will be a scalable infrastructure that provides complete support for the management of logical time.Appendix A: Object Proxies in SPEEDES DDMObject proxies allow entities to distribute published exportable attributes and methods in an automated manner to remote nodes. Subscribing entities on remote nodes receive the proxy of the objects they have subscribed to. In a sense, this is very much like extending the notion of private, protected, and public levels of encapsulation used in C++ by adding a new exportable access privilege. Object proxies, and how they relate to HLA services, is shown conceptually in  REF _Ref454163292 \h Figure 13.Figure  SEQ Figure \* ARABIC 13: Object Proxies and HLA services.An example of SPEEDES DDM helps to show how object proxies are used. Suppose that each entity in a simulation has a range-based sensor (e.g., radar), and that each entity moves in arbitrary motion that can dynamically change unexpectedly over time. Entities enter and exit each other’s sensor range over the course of the simulation execution. DDM must continually determine which entities are within each other’s sensor range, manipulate object proxy distribution lists, coordinate the discovery and removal of remote object proxies, and automatically coordinate the reflection of published attributes as they are updated.SPEEDES DDM supports three fundamental operations for object proxy messages. First, each entity must determine which other entities require copies of their proxy. Second, each entity must receive the proxies of the entities in which they are interested. Third, published attributes that are modified by an entity must be circulated to those other entities that are on its distribution list.Declaration Management (DM) and Data Distribution Management (DDM) services coordinate the various distribution lists managed by each publishing entity. The object proxy infrastructure then provides a foundation for the Object Management (OM) services. The distribution list algorithm automates object discovery and removal while the object proxy infrastructure automates the reflection of subscribed attributes when their values are changed. REF _Ref454168569 \h Figure 14 provides a UML diagram showing how object proxies relate to application entities. The base-class for all simulated objects that process events is the SpSimObj. The S_SpHLA class inherits from SpSimObj to support all of the object proxy and HLA-related services. Application entities (e.g., S_AppSimObj) that wish to participate in HLA must inherit from the S_SpHLA class. The S_SpHLA class creates an object proxy instance that will eventually point back to the published attributes of the application entity. The S_SpHLA class also manages a remote proxy list that is dynamically updated by the DM and DDM services for object discovery and removal. The attributes of remote proxies in this list automatically reflect any remote changes made by their owners over time.Figure  SEQ Figure \* ARABIC 14: UML class diagram of object proxies.Exportable attributes and methods are combined into a proxy that can be distributed to other nodes or to the outside world. Object proxies support a number of object-oriented concepts including encapsulation, inheritance, dynamic binding, aggregation, and arbitrary levels of containment. The exportable attribute types currently provided by SPEEDES are shown in  REF _Ref444056702 \h Figure 15. When updating exportable attributes, operator-overloading techniques automatically reflect the updates across the network to subscribers in logical time.Figure  SEQ Figure \* ARABIC 15: Object proxy attribute types.Special kinds of attributes, called Dynamic Attributes, compute values as a function of time. Dynamic attributes find use, for example, in providing an entity’s position. Instead of periodically sending out position updates to the rest of the simulation, a delivered formula calculates entity positions locally. Dynamic attributes can significantly reduce message traffic and improve simulation fidelity.Dynamic attributes are represented as a list of Dynamic Items. The extensibility of the dynamic attribute framework allows users to add new items to the library without requiring modifications to the general infrastructure. Dynamic items store a start time and an end time. Dynamic items are linked together in a dynamic attribute list and have consecutive start and end times {[t0, t1], [t1, t2], [t2, t3], ...}. To obtain a dynamic attribute value at a given logical time, the dynamic attribute class first searches through its list to find the dynamic item containing the correct time interval. Then the dynamic attribute invokes a virtual function that the inherited dynamic item provides to compute its value. The dynamic item list need not be homogeneous. This permits linking together different types of equations, providing a fully general mechanism to compute predicted values.The S_SpHLA class maintains a distribution list for the entities that require its object proxy.  REF _Ref398119494 \* MERGEFORMAT Figure 16 shows an example of a randomly moving object that delivers its object proxy to new nodes using the E_SendProxy event. The object proxy is packaged into a message that is delivered to a corresponding S_ProxyMan object on each processor. The E_DeliverProxy event reconstructs the object proxy from the message and stores it in the S_ProxyMan object. A pointer to the object proxy is then delivered to each subscribing S_SpHLA entity using the E_DeliverPtr event.Figure  SEQ Figure \* ARABIC 16: SPEEDES diagram showing the delivery and automatic reflection of updated attributes in a test simulation of randomly moving objects. The E_TestRanMot event periodically changes proxy attributes that are circulated to subscribing entities.Through operator overloading, the object proxy software in SPEEDES automatically knows when an attribute has been modified as events are processed. Whenever an attribute is modified, the update is recorded in a list that is maintained by the proxy. The E_ScheduleUpdate event is automatically scheduled to occur immediately after the current event has completed. This event packages the modified attribute values into a message that will be delivered to each node in the S_SpHLA distribution list. The E_ReflectAttributes event makes a clone of the object proxy in the S_ProxyMan where it replaces the old object proxy. Then, new proxy pointers are locally distributed to the appropriate S_SpHLA entities using the E_TouchProxy event. This approach ensures that all reflected updates are coordinated correctly in logical time.Appendix B: Multiresolution Hierarchical GridsAs a first step in the DDM sub-framework process, SPEEDES first subdivides multidimensional routing spaces into grids in order to support distributed processing of regions. This provides scalability to the region overlap computations. Each grid is further decomposed into finer resolutions with the recursively-defined Hierarchical Grid (HiGrid) tree data structure. Each node in the tree maintains a list of subscribers and publishers. A pyramid resolution scheme determines overlaps between publishers and subscribers registering in the HiGrid at different resolutions.An example showing how a three-dimensional HiGrid decomposes into a tree with multiple resolutions is provided below.  REF _Ref454441552 Table 1 describes a multi-resolution HiGrid having X, Y, and Z dimensions.Table  SEQ Table \* ARABIC 1: HiGrid example with three dimensions, X, Y, and Z. For each dimension, a minimum, maximum and list of resolutions is given.DimensionMinimumMaximumResolutionsX10020050, 25Y400700100, 50Z608010, 5, 2.5 REF _Ref454442463 Figure 17 and  REF _Ref454441286 \h Figure 18 show a recursive view of a HiGrid object. Notice that at the root, the full range of each dimension is given. When a region is inserted into the HiGrid, the tree first traverses down the X-Subtree until it reaches the closest X resolution. Then, the Y-Subtree is traversed until the closest Y resolution is found. Finally, the Z-Subtree is traversed. It is likely that a region will overlap multiple nodes in the tree. Sensible choices for dimension resolutions should be used. It would be wasteful to insert a large region into a HiGrid using a very fine resolution. In the other extreme, when inserting small regions, one should use fine resolutions to improve filtering efficiency. Figure  SEQ Figure \* ARABIC 17: Top-level view of a sample three-dimensional Hierarchical Grid Data Structure. The root node contains the full range of each dimension. The HiGrid recursively decomposes into finer resolutions.It is instructive to step through an example of how a publication or subscription region is inserted into a HiGrid. First, remember that regions may overlap multiple HiGrids. The region must be inserted into each HiGrid. In this example, assume that the region is specified as follows:X = 160-180 with resolution 22Y = 580-640 with resolution 54Z = 64-67 with resolution 4Starting at the top, the root node’s implicit X resolution is 100, which is too large. So, the HiGrid moves down into the first node of the X-Subtree with X interval 150-200. Once again, the X resolution is too large so it moves down deeper in X. Now the right resolution is found. Because the X region covers two nodes (the 150-175 interval and the 175-200 interval), the insertion splits into two paths. For each path, the insertion algorithm works on the Y dimension until the closest specified resolution is reached. In this case, the 550-600 and 600-650 Y intervals are used without requiring further traversal down the Y-Subtree. For each of the Y nodes, the Z-Subtree is traversed. The 60-65 and 65-70 nodes closest match the desired resolution and interval range for Z. Therefore, eight total nodes insert the id of the entity specifying the region. These nodes are summarized in  REF _Ref454448157 Table 2.Table  SEQ Table \* ARABIC 2: HiGrid nodes required for inserting the X, Y, Z region in the previous example.X IntervalY IntervalZ Interval150-175550-60060-65150-175550-60065-70150-175600-65060-65150-175600-65065-70175-200550-60060-65175-200550-60065-70175-200600-65060-65175-200600-65065-70Figure  SEQ Figure \* ARABIC 18: Subtree breakdown for a three-dimensional Hierarchical Grid. Each node in the Subtree maintains both a publication list and a subscription list of entities to manage regions with different resolutions. Subtrees inherit the intervals and resolutions of their parents for missing dimensions. The number of potential nodes in this tree is very large. The nodes are created/deleted dynamically by the HiGrid as needed to save memory when publishers or subscribers are added or removed.Distributing Grids to ProcessorsEach dimension in a routing space is binned and can optionally be used to distribute the space into grids. If none of the dimensions set their Distribute flag in Spaces.par to true, then the entire space will be represented in a single grid. Otherwise, portions of the space will be distributed to different processors to support parallel region overlap computations.  REF _Ref454346267  \* MERGEFORMAT Figure 19 shows an example of a routing space with two dimensions, X and Y.Figure  SEQ Figure \* ARABIC 19: An example of distributing a simple routing space with two dimensions, {X, Y}. Four different decompositions of the space into grid cells are possible, depending on the Distribute flag settings in Spaces.par. Each grid is then further decomposed locally into finer hierarchical resolutions as specified in the Spaces.par file.This example assumes that the X dimension is bounded between 0 and 300, with top-level resolution of 100, and that the Y dimension is bounded between 1000 and 5000 with top-level resolution of 1000. Four distribution possibilities arise:X and Y are not distributed. The entire space is mapped into one grid.Space 	=	{X=[0,300], Y=[1000,5000]}Only the X dimension is distributed resulting in three grids.Space 	=	{X=[0,100], Y=[1000,5000]}(	{X=[100,200], Y=[1000,5000]}(	{X=[200,300], Y=[1000,5000]}Only the Y dimension is distributed resulting in four grids.Space	=	{X=[0,300], Y=[1000,2000]}(	{X=[0,300], Y=[2000,3000]}(	{X=[0,300], Y=[3000,4000]}(	{X=[0,300], Y=[4000,5000]}Both dimensions are distributed resulting in twelve grids.Space	=	{X=[0,100], Y=[1000,2000]}(	{X=[100,200], Y=[1000,2000]}(	{X=[200,300], Y=[1000,2000]}(	{X=[0,100], Y=[2000,3000]}(	{X=[100,200], Y=[2000,3000]}(	{X=[200,300], Y=[2000,3000]}(	{X=[0,100], Y=[3000,4000]}(	{X=[100,200], Y=[3000,4000]}(	{X=[200,300], Y=[3000,4000]}(	{X=[0,100], Y=[4000,5000]}(	{X=[100,200], Y=[4000,5000]}(	{X=[200,300], Y=[4000,5000]}Using modular arithmetic, SPEEDES is able to assign global IDs to each grid cell using the dimensions that are distributed in a space. For normal HLA dimensions, this is computed as:	NbinsDim = (MaxValueDim – MinValueDim) / TopLevelResDim	(14)	Where BinIdDim = 0, 1, 2, …, NbinsDimThen totals used for generating unique IDs can be computed as,	TotalBinDim+1 = TotalBinDim (  NbinsDim	(15)	Where TotalBin0 = 1Each cell is assigned a unique Id using the following expression:	UniqueIdGrid = (Dim BinIdDim ( TotalBinDim	(16)Individual dimension IDs can be obtained from a Unique Id by reversing the computation using modular arithmetic.	BinIdDim = mod(int(UniqueId / TotalBinDim), NbinsDim)	(16)Using unique IDs for grid cells, SPEEDES can distribute the routing space using a scatter decomposition strategy. This decomposition strategy can be further combined with THEATER dimensions described in equation (13). ENUM and CATEGORY dimensions easily integrate with this approach by binning on their integer values. SPEEDES therefore has a mechanism to distribute routing spaces based on any combination of the four types of dimensions.Appendix C: Extended Routing Spaces and Dimension Types REF _Ref454193542 \h Figure 20 provides an conceptual UML diagram showing the relationship between different kinds of routing spaces and their associated space dimensions. A Universe provides support for multiple routing spaces that come in three flavors. The declaration Management space is the simplest kind. It has only one dimension, an enumerated list of class names. All other spaces conceptually inherit from the DM space in that they all require class names when defining subscription and publication regions.In the next level of abstraction, general Routing Spaces allow for any combination or number of ENUM, DIMENSION, and CATEGORY dimensions. ENUM dimensions allow applications to explicitly enumerate their interests by name. DIMENSION is a normal HLA routing space dimension. It is represented as a double-precision floating-point number bounded by minimum and maximum values. The CATEGORY dimension simply provides a set of integers [0, 1, 2, …, NumCategories] that can be used by applications to provide their own filter mapping.Geographical Routing Spaces provide THEATER dimensions to normal routing spaces. THEATER dimensions allow users to specify three-dimensional regions in spherical coordinates that are bounded by latitude, longitude, and altitude. Multiple theaters can be used to express interests in different geographical locations around the world. It is possible for theaters to overlap in physical space or to distinguish themselves by altitude.Figure  SEQ Figure \* ARABIC 20: A conceptual UML diagram depicting the relationship between SPEEDES extended routing spaces and their dimension types. This conceptual diagram does not describe the actual software implementation.Defining Interest SpacesInterest spaces are documented in a file named Spaces.par. This file is read by SPEEDES to establish one or more routing spaces. An example of this file is provided below.// Spaces.par input fileInterestSpaces {	reference SPACE WWII	reference SPACE WWIICom}WWII {	reference THEATER Europe	reference THEATER Pacific	reference DIMENSION CrossSection	reference ENUMTYPE ShipTypes	reference ENUMTYPE Alliances	reference CATEGORIES WWIICategories}Europe {	LatLng {		Latitude {			float Lo 20			float Hi 50		}		Longitude {			float Lo -170			float Hi -130		}		float Resolution[0] 521.2		float Resolution[1] 154.3		float Resolution[2] 35.8		float Resolution[3] 3.7		logical Distribute T	}	Altitude {		float Lo 0.0		float Hi 10.0		float Resolution[0] 3.5		logical Distribute T	}}Pacific {	LatLng {		Latitude {			float Lo 10			float Hi 50		}		Longitude {			float Lo 125			float Hi -150		}		float Resolution[0] 936.5		float Resolution[1] 180.4		float Resolution[2] 25.3		float Resolution[3] 2.6		logical Distribute T	}	Altitude {		logical Distribute F		float Resolution[0] 1.3	}}CrossSection {	float Lo 1.0	float Hi 10.0	float Resolution[0] 2.0	logical Distribute F}ShipTypes {	enum BattleShip	enum AirCraftCarrier	enum PtBoat	logical Distribute F}Alliances {	enum Allies	enum Axis	logical Distribute T}WWIICategories {	int Ncategories 1000	logical Distribute T}WWIICom {	reference DIMENSION Frequency	ENUMTYPE CommTypes {		enum Radio		enum Voice		logical Distribute T	}}Frequency {	float Lo 2.0	float Hi 3.0	float Resolution[0] 1.06	logical Distribute T}In this example, two interest spaces are defined for a World War II scenario. The WWII space is composed of two THEATER dimensions, one normal DIMENSION, two ENUMTYPE dimensions, and one CATEGORY dimension. The two THEATER dimensions represent the European and Pacific theaters. Within each THEATER dimension, a LatLng area is specified that defines minimum and maximum bounding values for latitude and longitude, followed by a set of resolutions in kilometers used by the HiGrids for obtaining multiple levels of resolution. Each THEATER dimension also defines an Altitude range with its own set of resolutions. The Distribute flag indicates whether or not to distribute the space between multiple processing nodes using the top-level resolution for that dimension.The WWII space contains other dimensions that provide additional filtering to geographical regions. The WWIICom space does not contain any THEATER dimensions. It is a traditional HLA routing space without any special consideration for geographical regions.Routing Space ComponentsEntities dynamically register interests in routing spaces using SPEEDES components to encapsulate and automate the process. Components use a “plug-in” style interface to support dynamic entity reconfiguration capabilities. The routing space components manage all of the DDM distribution list information and coordinate changes to regions within a routing space. In the case of range-based DDM, theater space components also automate the region management process. The four kinds of routing space components are shown in  REF _Ref454353541 Figure 21.Figure  SEQ Figure \* ARABIC 21: Routing space components. Application entities create the routing space components for the routing spaces they wish to participate in. Separate publication and subscription components allow entities to independently publish, subscribe, both, or neither to routing space regions. The routing space components are “plugged-in” to the S_SpSimObj base class through a simple API supported by SPEEDES. The components then coordinate region management with the HiGrids and form distribution lists for circulating object proxies. REF _Ref455030759 \h Figure 22 shows a more detailed diagram of routing space components and their associated data-structures.Figure  SEQ Figure \* ARABIC 22: Routing space components for publishingThe SpDDMSpace class contains the SpSpace class that provides the standard grid mapping and lookup services for all of the routing spaces. These lookup services are common to all publishing and subscribing operations.The SpPublishSpace class provides a common infrastructure for publishing within a space. The SpPublishRoutingSpace class provides common methods for publishing any combination of DIMENSION, ENUM, or CATEGORY regions. The SpPublishTheaterSpace class provides the additional methods for handling range-based filtering in routing spaces that have one or more THEATER dimensions. It also initiates the P_PublishGrid process [28] that automatically coordinates region updates based on the position and maximum velocity of the entity.The SpPublishSpace class manages two important data structures. First, it maintains a list of grids that its current regions overlap. Second, it maintains a list of publication regions that have been defined by the entity.As publication regions change, the SpPublishSpace class determines which new grids are required and which old ones are no longer needed. It also keeps track of which grids are completely enclosed by a region.Each grid in the SpPublishSpace class manages a list of subscriber handles. The list of subscribers in each grid is combined into a single list of subscribers for the space. This single list is further combined with other space components to form a list of subscribers for all spaces. That list is again reduced into a list of destination nodes that have at least one subscriber. The destination list is used for multicasting updates as attributes are modified.Like SpPublishSpace, the SpSubscribeSpace class also maintains a list of grids and regions. Unlike publishing, the list of subscriber grids does not require any other bookkeeping. The SpSubscribeSpace class additionally manages a list of pointers to object proxies that have been discovered as a result of overlapping regions within the space. This allows entities to know which routing spaces caused an entity to be discovered.Like the SpPublishTheaterSpace component, the SpSubscribeTheaterSpace coordinates THEATER dimensions and initiates the P_SubscribeGrid process that automatically coordinates region updates based on the position, maximum sensor range, and maximum velocity of the entity.Sequence Diagrams for Publishing and Subscribing REF _Ref455048814 \h Figure 23 depicts a UML sequence diagram showing the steps involved in publishing regions for range-based filtering. These steps are described below.The publishing entity creates its SpPublishTheaterSpace component and initializes any of its non-range-based DIMENSION, ENUM, or CATEGORY regions. The component is then plugged into the S_SpHLA entity.The P_SpPublishSpace process is scheduled to coordinate the updating of publication regions for the entity.The P_SpPublishSpace process determines its range-based region using the current position of the publisher, its maximum velocity, and the lookahead value, L.The SpPublishTheater component determines which grids overlap the region and the mapping of those grids to nodes. The list of grids is returned back to the process.The list of grids is compared to the current list of grids. The set of new, old, and non-fully-enclosed grids is determined.The process adds the new set of grids to its local state.An Unpublish grids event is scheduled for every old grid that is no longer in the publisher’s region. This event removes the publisher from the grid.A Publish grid event is scheduled for every new grid and every non-enclosed grid in the publisher’s region. This event adds the publisher into the hierarchical grid data structure.The Hierarchical grid data structure in the S_SpGrid object determines overlaps with subscribers and returns the list of new subscribers to the publisher. The publisher does not process the information yet. It is collected at this point and will be processed in the next step when all of the information from the grids has arrived.After the P_SpPublishSpace process schedules all of its Publish grids events (see step 8), it waits until the messages in step 9 return before continuing. Once this happens, the process continues. It boils down the received subscriber information to form the new distribution list.The deliver proxy event is scheduled, which will facilitate the delivery of the publisher’s proxy to all new subscribers.The proxy is sent to the S_SpLocalProxyMgr on each node that does not currently have the proxy, but does have a subscriber. If the proxy is already on a node that has a new subscriber, then the subscriber’s object handle is added to the SpLocalProxyMgr’s local distribution list for the publisher’s proxy.The pointer to the proxy is passed to each new local subscriber.After the P_SpPublishSpace process finishes step 12, it then begins to work on removing its proxy from old subscribers that no longer require its proxy.The old grids from the last region update are removed and the list of old subscribers is determined.An event is scheduled for the publisher to Undeliver the proxy from old subscribers.An event is scheduled for each of the appropriate S_SpLocalProxyMgrs to remove subscribers from the local distribution list for the publisher’s proxy.An event is scheduled to remove the proxy’s pointer from old subscribers that no longer require the proxy.The process waits until the next time (t - L) to update its region (see equation 1). Then, repeat steps (3-19).Figure  SEQ Figure \* ARABIC 23: UML sequence diagram for publishing.Figure  SEQ Figure \* ARABIC 24: UML sequence diagram for subscribing. REF _Ref458516304 \h  \* MERGEFORMAT Figure 24 depicts a UML sequence diagram showing the steps involved in subscribing to regions for range-based filtering. The steps are described below.The subscribing entity creates its SpSubscribeTheaterSpace component and initializes any of its non-range-based DIMENSION, ENUM, or CATEGORY regions. The component is then plugged into the S_SpHLA entity.The P_SpSubscribeSpace process is scheduled to coordinate the updating of subscription regions for the entity.The P_SpSubscribeSpace process determines its range-based region using the maximum range of the sensor, the current position of the entity, its maximum velocity, and the lookahead value, L.The SpSubscribeTheater component determines which grids overlap the region and the mapping of those grids to nodes. The list of grids is returned back to the process.The list of grids is compared to the current list of grids. The set of new, old, and non-fully-enclosed grids is determined.The process adds the new set of grids to its local state.An Unsubscribe grids event is scheduled for every old grid that is no longer in the subscriber’s region. This event removes the subscriber from the grid.The grid schedules a Remove subscriber event for every publisher that no longer overlaps the subscriber’s region.Publishers that receive the Remove subscriber event remove the subscriber from their grid. If there are no remaining grids in the publisher that have the subscriber, then the publisher schedules a Remove proxy event to the S_SpLocalProxyMgr to remove the proxy from the subscriber. If the SpLocalProxyMgr no longer needs to deliver the proxy to any other local subscribers, then it removes the proxy altogether from the node.An Undeliver proxy pointer event is scheduled to remove the proxy from the subscriber.At the same time the unsubscribe event sequence is initiated (see steps 7-10), a Subscribe grid event is scheduled for every new grid and every non-enclosed grid in the subscriber’s region. This event adds the subscriber into the hierarchical grid data structure.The Hierarchical grid data structure in the S_SpGrid object determines overlaps with publishers and forwards the subscriber’s object handle to new publishers.The proxy is sent to the S_SpLocalProxyMgr on the subscriber’s node if the proxy has not already been delivered to that node. Otherwise, the Add proxy message is scheduled to simply add the subscriber’s object handle to the SpLocalProxyMgr’s distribution list for that proxy.The pointer to the proxy is passed to the subscriber.For every non-enclosed grid in step 11, any publishers that used to overlap the region but no longer overlap must also remove the subscriber.Same as step 9.Same as step 10.The process waits until time t + L to safely coordinate cleanup operations.The process removes the old set of unsubscribed grids.The process waits until the next time (t - L) to update its region once again (i.e., repeat steps 3-20).Appendix D: Geographical RegionsGeographical regions are treated differently from other dimensions because latitude and longitude are fundamentally coupled. One could imagine just using normal DIMENSIONs to specify bounds on latitude, longitude, and altitude. However, problems would arise in distributing the routing space when modeling global regions. Grid cells around the poles would be overly narrow, while grid cells around the equator would be excessively wide.Figure  SEQ Figure \* ARABIC 25: Decomposing a globe using latitude/longitude lines for grids results in overly narrow grid-cells near the poles and excessively wide grid-cells near the equator. For range-based filtering, this will cause excessive grid-region overlaps near the poles, while reducing the grid resolution near the equator.A better way to distribute geographical spaces is to form approximately equal area grid cells. As an example of this, imagine a global theater. Define Res as the desired resolution of a grid cell. Then, one can first decompose the globe into latitude bands as follows.	NLatBands = Nearest Integer ( REarth / Res)	(3)	LatBand = 0, 1, 2, …, NLatBands - 1From this, latitude bands can be formed around the globe with distance resolution approximately equal to Res (see  REF _Ref454330237 Figure 26).Figure  SEQ Figure \* ARABIC 26: Decomposing the globe into roughly equal area latitude bands. Figure (a) shows an example where eight latitude bands are constructed.  Notice that the polar arc-length of each latitude band is equal. Figure (b) shows that the radius of a given latitude band (i.e., the distance from the North Pole axis to the middle of the band) is Ri = REarth sin(), where i is the LatBand.The angular resolution for latitude bands can be computed as,	 = / NLatBands	(4)Then, LatBand is computed using the LatBand as follows:	LatBand =  ( (LatBand + 1/2)	(5)Note that by adding ½ to the LatBand, the middle of the band is used instead of the edges. This adds a little more accuracy to the longitudinal decomposition. The radius of each latitude band is computed treating the band as if it were a circle centered about the polar axis. The radius is computed as,	RLatBand = REarth sin(LatBand)	(6)Now, each latitude band is decomposed into cells (see  REF _Ref454352468 Figure 27) using the circumference of the band and the desired resolution. First, the number of cells in each latitude band is computed as,	NumCellsLatBand = Nearest Integer (RLatBand / Res)	(7)	LonLatBand = 0, 1, 2, …, NumCellsLatBand - 1Figure  SEQ Figure \* ARABIC 27: Decomposing a latitude band into cells. This figure looks down from the North Pole at one of the latitude bands and shows how it is decomposed into roughly equal area cells. The x-axis lines up with zero longitude.Finally, the angular decomposition of the band is computed as,	LatBand = 2/ NumCellsLatBand	(8)It is easy to perform grid lookups given latitude and longitude points. First, the latitude band is determined as,	LatBand = int((/ 2 - Latitude) / )	(9)Then, the longitude cell Id is computed as,	LonIdLatBand = int(Longitude / LatBand)	(10)A global Id is assigned to each cell using cumulative totals for each latitude band.	TotalCellsLatBand+1 = TotalCellsLatBand + NumCellsLatBand	(11)	Where TotalCells0 = 0Then,	GlobalCellLatBand,LonId = TotalCellsLatBand + LonIdLatBand	(12)Cells can then be distributed with a scatter decomposition strategy to computer nodes using the GlobalCellId and modular arithmetic.	NodeGlobalCellId = mod(GlobalCellId, Nnodes)	(13)Once the grids have been assigned unique IDs and distributed to processors, a grid lookup algorithm must be provided to complete the geographical grid decomposition algorithm. Applications provide position and range inputs to the grid lookup algorithm, which then returns back the set of overlapping grids.The grid lookup algorithm first computes the angular cone from the center of the Earth to the sphere of interest (see  REF _Ref454426374 Figure 28). The lookup function then computes low and high values for , which then analyzes which latitude bands overlap the region of interest. A list of longitude cells is then determined for each relevant latitude band using spherical geometry.Figure  SEQ Figure \* ARABIC 28: Angular wedges are used to determine latitude/longitude grid overlaps. The position and radius of a region is passed into the grid lookup algorithm.AcknowledgementsThe High-Performance Computing Modernization Office (HPCMO) sponsored this work through the Common HPC Software Support Initiative (CHSSI) project. The Space and Naval Warfare (SPAWAR) center and the Naval Research Laboratory (NRL) under the CHSSI Force Modeling & Simulation Computational Technical Area provided overall management.The SPEEDES software development team gives special thanks to Wargame 2000 and the Joint National Test Facility for co-sponsoring the Declaration Management work that led to the eventual development of SPEEDES DDM. The SPEEDES team also gives special thanks to DMSO for their overall guidance and technical support.Members of the technical staff at Metron helped make this work possible: Bill Stevens, Amber Roy, Jim Brutocao, Ron Van Iwaarden, Mitch Peckham, Guy Berliner, Kurt Stadsklev, Scott Shupe, Jim Kilgore, Jeff Jones, and Jeff Monroe. Also contributing to this effort was Azzedine Boukerche from the University of Texas.SPEEDES is a government-owned software system, managed by Metron Inc., and licensed by NASA. It currently supports several large DoD simulation projects including Wargame 2000, JSIMS, Parallel NSS, and EADTB. The government has recently formed The PDES User's Group to coordinate further SPEEDES development. Current membership includes (1) the JNTF, (2) SPAWAR, and (3) the JSIMS Enterprise. Metron provides technical support for the PDES User’s Group during configuration board meetings.BiographyDr. Jeffrey S. Steinman, Senior Analyst with Metron Inc., received his Ph.D. in High-Energy Physics from UCLA in 1988. From 1988-1995, Dr. Steinman worked at the Jet Propulsion Laboratory where he developed the SPEEDES operating system. This work resulted in more than 30 publications in the area of high-performance simulation and several patent awards. He is currently providing technical support for several large-scale DoD projects including Wargame 2000, JSIMS, JWARS, NSS, and EADTB. Dr. Steinman was the principle developer of the NSS HLA-Integration framework and was the lead integrator of NSS with the HLA Run-Time Infrastructure for the JTFp (Joint Training Federation Prototype) project. He is also a regular participant in the HLA technical exchanges for Time Management and Data Distribution Management.Tuan Tran, Senior Software Analyst with Metron Inc., received his B.S. and M.S. degrees in Computer Science from George Mason University in 1989 and 1996 respectively. Mr. Tran has been involved in a number of DoD projects at Metron including: the Acoustic Warfare Basic Decision Support System, the NodeStar submarine tracker, the Navy’s Underwater Surveillance System, and the Advanced Power Projection Planning & Execution system. Since 1998, Mr. Tran has worked on the SPEEDES project where he developed the software for Declaration Management and Data Distribution Management services.Jacob Burckhardt, Software Analyst with Metron Inc., received his Bachelor of Science degree in Electrical Engineering and Computer Science from the University of California at Berkeley in 1995. Mr. Burckhardt spent his first year at Metron developing software for the Naval Simulation System. In 1996, Mr. Burckhardt joined the SPEEDES team at Metron where his primary technical responsibilities are in the areas of software development and quality control. Mr. Burckhardt is responsible for coordinating integration, testing, and software release preparation for the SPEEDES project. Mr. Burckhardt was the principle developer of the HiGrid decomposition and grid lookup algorithms that are fundamental to the SPEEDES DDM services.James S. Brutocao, Senior Software Analyst with Metron, Inc., received his Bachelor of Science degree in Physics from Harvey Mudd College in 1990. From 1991 to 1997, Mr. Brutocao worked at the Jet Propulsion Laboratory where he provided VLSI hardware and software support for the Cassini mission to Saturn. In 1997, he joined the SPEEDES project at Metron, and is the principle developer of the process-based and component-based aspects of the SMF used to implement SPEEDES DDM services. Mr. Brutocao is the principle developer of the Parallel IMPORT language and compiler. He also provides PDES consultation to Wargame 2000 and JSIMS.References[1]	Bachinsky S., Tarbox G., Mellon L., Fujimoto R. 1998, “RTI 2.0 Architecture.” 1998 Spring Simulation Interoperability Workshop, No. 98S-SIW-150.[2]	Cohen D., Kemkes A. 1998, “User Level Measurement of DDM Scenarios.” 1998 Spring Simulation Interoperability Workshop, No. 98S-SIW-072.[3]	Dahmann J., Kuhl F., Weatherly R. 1998, “Standards for Simulation: As Simple As Possible, But Not Simpler – The High Level Architecture For Simulation.” Simulation, Vol. 71, No. 6, Dec. 1998, Pages 378-387.[4]	Dahmann J., Lutz R. 1998, “Persistent Federations.” 1998 Spring Simulation Interoperability Workshop, No. 98S-SIW-059.[5]	Defense Modeling and Simulation Office (DMSO) website, http://hla.dmso.mil.[6]	Defense Modeling and Simulation Office, “HLA Interface Specification,” Version 1.3, www.dmso.mil/projects/hla.[7]	Fujimoto R. 1990, “Parallel Discrete Event Simulation.” Communications of the ACM, Vol. 33, No. 10, Pages 30-53.[8]	Fujimoto R., Tacic I. 1997, “Synchronized Data Distribution in Distributed Simulations,” 1997 Spring Simulation Interoperability Workshop, No. 97S-SIW-045. [9]	Fujimoto R. 1998, “Time Management in The High Level Architecture.” Simulation, Vol. 71, No. 6, Dec. 1998, Pages 388-400.[10]	Jefferson D. 1985, “Virtual Time.” ACM Transactions on Programming Languages and Systems, Vol. 7, No. 3, pages 404-425.[11]	Kanarick C. 1991, “A Technical Overview and History of the SIMNET Project.” In Proceedings of the 1991 Advances in Parallel And Distributed Simulation Conference, Pages 104-111.[12]	Loral Systems Company 1992, “Strawman Distributed Interactive Simulation Architecture Description Document.” Prepared for Program manager – Training Devices Naval Training Systems Center, Orlando, FL, ADST/WDL/TR-92003010, Vols. 1 and 2.[13]	Morse K., Steinman J. 1997, “Data Distribution Management in the HLA: Multidimensional Regions and Physically Correct Filtering.” Spring Simulation Interoperability Workshop, No. 97S-SIW-052.[14]	Muller P. 1997, “Instant UML.” Wrox Press Ltd. 30 Lincoln Road, Olton, Birmingham, B27 6PA.[15]	Reynolds P., 1994, “Disorientation.” ELECSIM 94.[16]	SPEEDES website,  HYPERLINK http://www.ca.metsci.com/speedes/ www.ca.metsci.com/speedes/.[17]	Steinman J. 1993, “Incremental State Saving in SPEEDES Using C++.” In Proceedings of the 1993 Winter Simulation Conference, Pages 687-696.[18]	Steinman J., Wieland F. 1994, “Parallel Proximity Detection and the Distribution List Algorithm.” In Proceedings of the 1994 Parallel And Distributed Simulation Conference (PADS’94), Pages 3-11.[19]	Steinman J., et. al. 1995, “Global Virtual Time and Distributed Synchronization.” In Proceedings of the 1995 Parallel And Distributed Simulation Conference, Pages 139-148.[20]	Steinman J. 1997, “The NSS HLA-Integration Framework.” 1997 Spring Simulation Interoperability Workshop, No. 97S-SIW-071.[21]	Steinman J. 1998, “Scalable Distributed Military Simulations Using the SPEEDES Object-Oriented Simulation Framework.” In Proceedings of the Object Oriented Simulation Conference (OOS'98), Pages 3-23.[22]	Steinman J. 1998, “Time Managed Object Proxies in SPEEDES.” In Proceedings of the Object Oriented Simulation Conference (OOS'98), Pages 59-65.[23]	Van Hook D., Calvin J. 1998, “Data Distribution Management in RTI 1.1.” 1998 Spring Simulation Interoperability Workshop, No. 98S-SIW-206.[24]	Weatherly R., Wilson A., Griffin S. 1993, “ALSP – Theory, Experience, and Future Directions.” In Proceedings of the 1993 Winter Simulation Conf4erence, Pages 1068-1072.[25]	West D. 1988, “Optimizing Time Warp: Lazy Rollback and Lazy Re-evaluation.” Master’s Thesis, University of Calgary, January 1988.[26]	West D., Panesar K., 1996, “Automatic Incremental State Saving.” In Proceedings of the 1996 Parallel And Distributed Simulation Conference (PADS’96), Pages 78-85.[27]	West D., Ng H. 1998, “Event Distribution and State Sharing in the Thema Parallel Discrete Event Simulation Modeling Framework.” In Proceedings of the Object Oriented Simulation Conference (OOS'98), Pages 24-29.[28]	Whitehurst A., Brutocao, J. 1998, “Parallel Execution of Process-Based Simulation Models.” In Proceedings of the Object Oriented Simulation Conference (OOS'98), Pages 115-120.[29]	Yu L., Steinman J., Blank G., 1998, “Adapting Your Simulation For HLA.” Simulation, Vol. 71, No. 6, Dec. 1998, Pages 410-420.	Detectable entities (e.g. targets) are sometimes called publishers.	Sensing entities are sometimes called subscribers.	In the late 1980s, the SIMNET project, primarily used to support interactive tank simulations, offered a real-time simulation infrastructure that was used by the Army for training.	In the early 1990s, the Distributed Interactive Simulation (DIS) protocol standardized message sets for real-time training simulations. The DIS community standardized more than 100 of these message sets, called Protocol Data Units, or PDUs. At the same time, the Aggregate-Level Simulation Protocol (ALSP) was developed to allow existing logical-time simulations to interoperate using conservative time management protocols. ALSP supported the training confederation project using a variation of the Chandy-Misra conservative time management strategy.	Since UDP/IP is a best-effort transport protocol, periodic updates were required to make up for potentially lost packets. This strategy also simplified the object discovery process since “late joiners” in the distributed simulation would quickly obtain information about all of the objects through these periodic broadcasts.	Maturing DDM technology in a standard way was one of the primary factors motivating DIS simulations to make the transition to HLA.	The Concurrent Theater Level Simulation (CTLS) was the primary application for TWOS.	The Breathing Time Warp (BTW) algorithm in SPEEDES incorporates flow control for (1) optimistic computing, (2) message sending risk, and (3) network congestion.	This work was coordinated by the Joint National Test Facility (JNTF), the Naval Research Laboratory (NRL), Los Alamos National Laboratory (LANL), the Jet Propulsion Laboratory (JPL), and the California Institute of Technology.	NASA patented the “Distribution List Algorithm” for parallel proximity detection in 1994.	For example, a simulation consisting of a few thousand moving entities with sensors ran 40 times faster on a 64-processor Intel Paragon. In another experiment, 250,000 entities were modeled in real-time on a 29-processor Convex Exemplar with near-perfect speedup.	One experiment using the Time Warp algorithm without flow control produced a rollback explosion with roughly 2,000 objects executing on 32 processors. The explosion occurred when objects migrated towards each other increasing their mutual coupling.	Examples are the commercial RTI version 2.0 and several web-based RTIs that have been developed by industry and academia.	A routing space is defined by a generic set of bounded dimensions. Each dimension is given a minimum and maximum value. Publication and subscription regions are defined within a routing space to represent the publication and subscription interests of entities. When a publication region overlaps a subscription region, data flows from the publisher to the subscriber.	The SPEEDES strategy mirrored HLA, but within a common parallel and distributed simulation engine to further promote interoperability with the rest of the DoD simulation community. The success of the SMF allowed several DoD simulation projects to embrace optimistic time management on parallel supercomputers. DoD simulation projects using the SMF include Wargame 2000 (WG2K), the Joint Simulation System (JSIMS), and the Extended Air Defense Test Bed (EADTB).	Currently, time management is not required by the HLA specification for Declaration Management, Data Distribution Management, and Ownership Management.	The object proxy framework provides the Object Management services. Reflection of updated attributes is automated by SPEEDES.	Ownership Management is planned for the future through a two-way object proxy mechanism that allows entities to modify unprotected attributes of discovered proxies.	Object proxies represent the published state of an entity, which in general, is a subset of the entity’s actual state. An object proxy wraps an entity’s published state with an API providing access to its data members.	There are two types of messages in SPEEDES that promote interoperability between distributed entities. First, object proxy messages distribute information about the states of entities to other entities. Second, interaction messages allow entities to schedule events between themselves in an interoperable manner. Interactions provide the means for entities to invoke operations on other entities. Parameters that are passed in an interaction message are similar in concept to arguments that are passed in one-way remote function calls on an object. While DDM generally applies for both types of messages, this paper addresses only object proxy messages.	Publishers may work with more than one routing space. Within a given routing space, publication regions may overlap multiple HiGrids. Therefore, it is necessary for publishers to maintain a distribution list of subscribers for each HiGrid in each routing space. A higher-level distribution list merges all of the HiGrid distribution lists into a single distribution list, eliminating redundant references to subscribers. Changes to this list affect the object discovery and removal process. This distribution list is further reduced into a final list that only keeps track of the processing nodes that require its proxy. When updates are made to attributes, packed messages are only sent once to each node on this list, even when multiple entities on a node require the updates.	One could imagine the memory scalability problems that would arise if pointers to proxies were not used. For example, if every entity subscribed to every other entity, global memory consumption would go as O(NumEntities2) instead of as O(NumEntities x NumNodes).	If NLPM is too small, proxy information from different unrelated publishers will cause unnecessary rollbacks because they are managed by the same LPM. On the other hand, if NLPM is too large, memory is wasted. Typical values of NLPM range between 100-1,000.	SPEEDES automatically assigns a globally unique Id to each simulation object during its creation.	For those unfamiliar with class diagrams in the Unified Modeling Language (UML), a lightly shaded (or white) triangle indicates class inheritance. In this paper, black diamonds are used to indicate fixed object containment (e.g., objects that are defined as data members or fixed-length arrays within a class). A lightly shaded (or white) diamond indicates dynamic object containment (e.g., zero or more objects in a container class with dynamic cardinality indicated by an asterisk *).	In HLA terminology, simulationists also refer to publication regions in routing spaces as update regions.	For example, suppose there are three dimensions in a routing space, R = {0 ( x < 100, 50 ( y < 80, 20 ( z < 80}. If the bin sizes for the three dimensions are 10, 5, and 20 respectively, then there will be 10 x 6 x 3 = 180 cells in the routing space that can be distributed to different nodes.	In this case, lookahead (L) is defined as the delay between the time a new publication or subscription region is defined and the time that an object is discovered. Lookahead is not required in SPEEDES, but it can help increase processing concurrency, which reduces rollbacks.	Filtering efficiency can be defined as the fraction of discovered objects that are actually within the sensor’s true range. In a statistical sense, this can be computed as the ratio of areas: Efficiency = (Rsensor2) / [(Rsensor + R)2 + R2].	Computers tested were Origin 2000, Convex Exemplar, Sun, Dec Alpha, and networked PCs.	Operating systems supported are IRIX, HPUX, Solaris, Linux, and Windows NT.	Compilers supported are the native SGI, Sun, and HP C++ compilers, Visual Age C++, Kai, and gcc.	The 128 processor Origin 2000 at NRL was used for this experiment. This machine provides up to 128 gigabytes of memory for applications.	The SPEEDES shared memory library outperformed MPI by nearly a factor of three in isolated communications benchmarks tests.	Publishers and subscribers randomly chose one of the ENUM values. The ENUM dimension was distributed to provide better scalability.	The DIMENSION dimension was not distributed.	This is regarded as a very good filter. It is important to remember that the goal of DDM is to filter data. It is acceptable to receive unwanted data. However it is not acceptable when wanted data is not received.	If the total memory increased by a factor of three (instead of by 10%) when tripling the number of nodes, the memory usage would not scale.	Specifically, each entity maintains a distribution list of other entities that require its proxy.	In HLA terminology, receiving the proxy of an entity is called “discovering” the entity. Object discovery and object removal is dynamically coordinated over time as distribution lists change.	In HLA terminology, this is called updating and reflecting attributes. SPEEDES coordinates the reflection of attributes automatically through its object proxy mechanisms.	In the terminology of Parallel Discrete-Event Simulation (PDES), SpSimObjs can be thought of as Logical Processes (LPs).	SPEEDES currently supports a wide variety of dynamic attributes including step functions, polynomials, splines, and complex exponential functions. SPEEDES also provides a library of motion types including great circles, rhumline, circular orbits, loiter motion, extrapolated motion, polynomials, and splines. SPEEDES further provides coordinate transformations between Earth Central Inertial (x,y,z), Earth Central Rotating (x,y,z), and Earth Coordinates (latitude, longitude, altitude). All dynamic attributes provide first and second derivatives.	Proxies are sent to new nodes as either Declaration Management or Data Distribution Management services modify the entity’s distribution list.	Multiple S_ProxyMan objects are created on each processor to avoid inflow bottlenecks that could cause excessive rollbacks (a reasonable number might be 100). Using modular arithmetic, the Id of the sending entity determines which S_ProxyMan maintains the proxy.	Region overlap computations involve determining which publication regions overlap which subscription regions. It is never required to determine overlapping publication regions or overlapping subscription regions.	In this example, the Z-Subtree potentially has 14 terminating Z nodes. Working backwards, the Y-Subtree potentially has 9 terminating Y nodes, each also having a Z-Subtree. Therefore, there are 140 potential nodes in the Y-Subtree. The X-Subtree potentially has 6 terminating X nodes, each having both a Y-Subtree and a Z-Subtree. Therefore, there are 930 potential nodes in an X-Subtree. Finally, the root node uses itself and each of the three subtrees. Therefore, there are a total of 1085 possible nodes in the HiGrid for the example given. This potentially large number of nodes makes it mandatory to dynamically create the subtrees only when needed.	Inserting the region of a publisher or subscriber into a HiGrid simply adds the entity’s object handle to appropriate publication or subscription lists in the nodes that overlap the specified region at the specified resolution.	For example, an underwater theater used for modeling submarines might have an altitude range of -1km to 0km. A space theater might have an altitude range of 50 km to 1000 km.	Processes in SPEEDES are reentrant events that can pass time through timeouts or semaphore-based interrupts. Simulations developed with processes are sometimes described as using the Process Model.	Entities can define multiple regions within a routing space.	If a grid was completely enclosed in the old region during the last region update and is again completely enclosed by the new region in the current region update, then the grid does not need to be updated because nothing has changed. However, if a grid is only partially covered by either the old or the new region, then an update is required.	In contrast to IP-multicasting, SPEEDES supports destination-based multicasting. Destinations are represented as a bit-field of nodes. All messages are delivered reliably.	Initially, the current list of grids for publishing is empty. That will not be the case in later iterations.	Initially, the current list of grids for subscription is empty. That will not be the case in later iterations.	It is easy to extend this to localized latitude and longitude regions.	This equation assumes that longitude has been normalized to range from 0 to 2 instead of from – to +.	An interesting bug was caught in an early implementation of the grid lookup algorithm. It was incorrect to use the projected range along the circumference of each latitude band for determining longitude cells. This is because the actual range between two points is a straight line, not an arc along a spherical path. The fix was to use spherical geometry to correctly determine which latitude cells in each longitude band are within the angular cone.PAGE  16