The use of Basic Object Models (BOM) in the Virtual Backlot (VB) DevelopmentChris StapletonKeith GarfieldInstitute for Simulation and Training3028 Research ParkwayOrlando, FL  32836407-882-1300, 407-882-1342cstaplet@ist.ucf.edu, kgarfiel@ist.ucf.eduCharles E. HughesJ. Michael MoshellUniversity of Central FloridaOrlando, FL  32826407-823-2000, 407-823-2000ceh@cs.ucf.edu, mmoshell@ist.ucf.eduPat GarrityPaul DumanoureSTRICOMResearch ParkwayOrlando, FL 32826407-555-1111, 407-555-1111Keywords:Basic Object Model (BOM), Mixed Reality, ReuseABSTRACT: This paper examines the use of Base Object Models (BOM) as implemented in the Institute for Simulation and Training’s (IST) Virtual Backlot (VB) project.  A BOM is a SISO approved form of Reference FOM designed to enable reuse of HLA components.  SISO’s BOM Study Group has proposed general BOM design guidelines and federate development methodologies by which reuse capabilities may be designed into an HLA compliant simulation.  The VB project is a natural testbed for the BOM paradigm.  The VB project applies reuse methods that have roots in the traditional entertainment industry in which physical entities are repeatedly reused.  For example, the same physical backlot street may appear in many movies with a minimum of modification, giving the illusion of different locations (New York, Paris, Rome, etc).  It is a goal for the VB that virtual entities be repeatedly reused.  These models are specifically designed for a Mixed Reality environment in which a user encounters a physical environment enhanced by the addition of virtual entities.The current VB project is directed towards rapid population of a physical MOUT facility with virtual entities.  Ideally, the VB will provide a menu of readily re-configurable virtual entities for MOUT scenario designers to work with.  This paper will discuss aspects of the BOM philosophy that were able to be implemented in the VB as well as areas where the VB diverges from the BOM philosophy.  Lessons learned and suggestions for future use are reported.1. Introduction“All media as extensions of ourselves serve to provide new transforming vision and awareness.”--Marshall McLuhanSimulation is the newest of all media forms. The future of innovation of the simulation industry is not much different than any other media market in history.   There is a need for both the creation AND adoption of standards that are able to extend the distribution and application of assets across many applications.  The creation of standards are to consider not only technological capabilities, but also existing standards and practices of exiting industries they serve.  The military is a vast resource for the state-of-the-art of technological standards and through organizations such as SISO have provided a solid base for the creation of standards. However, the success of those standards depend as much on the adoption of standards.  The entertainment field has historically been a critical leader in the mainstream adoption of new media standards, which due to the size of consumer markets, drive the future of emerging technology.  This can be seen  just with the increase penetration and capabilities of consumer grade PCs for military simulation applications.Based on this future of simulation as media, the Institute for Simulation and STRICOM have embarked on several research initiatives to bring these important elements together to help look at how we can help provide insight to cross-industry adoption of new technology and standards.  Starting with the grant headed by Paul Dumanoir, The Enhancement of MOUT Training with Mixed Reality and Theme Park Technology, we have looked at the latest Augmented Reality technology from Canon Inc. (the Coastar), the most challenging training environment for virtual reality (Military Operations in Urban Terrain) and the most advance techniques and practices in experienced-based entertainment (theme parks and computer games).  With experts from each sector, we have created an environment where we are building multi-modal simulated assets to be integrated into the reality of live training exercises.  Since most of our “world” was real, we focused on the use, reuse and The remainder of this paper is organized as follows.  A brief overview and discussion of BOM’s is presented in section 2.  Section 3 describes three diverse areas that share a common need for BOM prototypes and a Virtual Backlot capability. Section 4 describes UCF’s VB project in detail.  Section 5 discusses some aspects of incorporating physical effects in mixed reality environment using MacroStimulators.  The paper is summarized in section 6.2. Base Object Model (BOM) BackgroundThe BOM concept extends SISO’s interest in exploring methods that support and promote the reuse of simulation components, while encouraging agile, rapid and efficient development and maintenance of FOMs.  The present disadvantage of HLA is that FOM development remains intricate and time consuming.  In addition, the exclusive use of one or two FOMs severely limits flexibility, capacity, and capability.  This hampers the HLA goal of model reuse and ease of maintenance.  The BOM concept allowed for a more flexible component-based development of assets.  The basic premise was the design and development of interoperable environments should begin with the exploration of reusing available simulation object model components.  Essentially, an individual aspect of a larger model is factored out into a BOM; new models are developed by reusing and composing these aspects.The VB project is a natural testbed for the BOM paradigm.  The VB project applies reuse methods that have roots in the traditional entertainment industry in which physical entities are repeatedly reused.  In this sense, a VB project has goals that parallel the BOM philosophy.BOMs can be used to support a wide variety of interoperability environments as Movie backlots support infinite production of movies for a hundred years.BOMs represent a “A single aspect of simulation  interplay that can be used as a building block”  as did backlots work with a network of prop houses, sound stages and locations.BOM building blocks may very well have more impact on the simulation industry economically and creatively than they do technically as did the movie industry in how it is still the major driver of entertainment and storytelling even though the technology is over a hundred years old.BOMs can be seen as the key to transferring the use of interoperability standards to other industry applications because it provides the seed of true innovation in the way that inventions are adopted by diverse applications and applied in unintended uses.Innovation is defined not by advancement in technology, but in how successful is that technology is in getting into the hands of diverse users and applications.  Innovation is gauged by how it changes our lives, not by the novelty of new technological capabilities.  A true sign of Innovation is when the use of technology incorporated into diverse applications for unintended uses.  3. Related Applications and Motivation for the Virtual BacklotThis section presents examples of three diverse areas of virtual and mixed reality simulations, showing how each of them require the same underlying characteristics, and thus provides motivation for the development of BOMs and of the VB. 3.1 Virtual ForestVirtual worlds tend to have sparse or unrealistic vegetation. The primary reason for this is the demand that dense realistic vegetation places on the computational and rendering resources of workstations. For the past two years, STRICOM has funded a UCF project to develop novel techniques for constructing and displaying virtual trees and other vegetation in virtual worlds. [1] These techniques, now available in software prototypes, produce an arbitrary number of individually unique trees and plants from a small number of leaf-and-branch texture maps. The software system constructs and manages levels-of-detail (LOD) in an efficient manner based on the mutual visibility of objects, rather than the traditional range-to-target concept. This effort also produced tools for evolving a forest based on a realistic biological model. Finally, preliminary studies were carried out on the effects of reduced details on the ability of subjects to perform tasks within the resulting virtual environment. [2] The prototype for this project resulted in models that produced a virtual rain forest. It is notable that these same models driving the rain forest imagery were reused to create a virtual pine forest and again reused to create a virtual kelp forest in an underwater environment.  UCF has proposed extending the research to produce an integrated software system that allows the creation of complex, visually correct graphical models of vegetation (trees, shrubs and grasses); the production of evolving, biologically correct forests populated by this vegetation; the traversal of these forests in virtual and mixed reality environments; and the evaluation of the resulting system for effectiveness in task-oriented training.  The challenging problems include the automatic production of varying level-of-detail models especially in light of a forest’s growth and succession; the management of these LOD models so that a user does not perceive loss of detail; the development of controlled experiments to assess the efficacy of the system; and the integration of all these components into a seamless, single development and deployment environment.This approach to creating virtual vegetation is in accordance with the BOM philosophy.  The models behind the imagery represent a single aspect of the environment (biological vegetative growth patterns) and then carry that aspect into a wide variety of virtual environments (tropical, deciduous, and under water).  A fully developed application of this technique would effectively comprise a vegetation VB.3.2 Mixed Reality TrainingConsider the problem of deploying virtual enhancements in an environment where MR based training is to be done.  The scenario planners would need to map the real world terrain and overlay the virtual elements in some fashion.  Constructing useful databases for the virtual elements of a mixed reality training exercise is a large effort. In a purely virtual environment the database modeler’s job can be made easier by considering that for skill-building forms of training (as opposed to mission rehearsal), precise correspondences to a real-world site (so-called “geospecific data”) is not really required. Thus, database builders often use convenient simplifications such as generic trees and buildings, or plausible approximations to terrain elevation and plant cover.However, when building databases for Mixed Reality systems, this kind of short-cut cannot be taken. The enormous effort required to accurately measure the user’s position and orientation in the augmented reality environment is wasted unless the virtual features are precisely constructed and registered in the environment with equal fidelity. What is needed is a technique whereby virtual objects can be added to a real-world training environment in a convenient fashion, with the required accuracy.  The scenario designers would need to draw upon a collection of existing virtual elements, and modify them to meet the needs of the specific exercise at hand. In other words, the scenario designers need access to a VB and need the tools to modify the existing elements to allow reuse.3.3 Mixed Reality and the MOUT EnvironmentThe United States Army is embarking on a revolutionary individual soldier and small team system of systems called the Objective Force Warrior (OFW).  The goal of the OFW program is to field a combat force that is capable of dominance across the spectrum of operations and environments from complex urban terrain to open and rolling terrain.  The OFW will demand higher soldier performance at all levels, placing new demands on training systems.  Exploitation of Embedded Training (ET) and on-demand simulation capabilities anywhere, anytime, are needed to support the OFW.  Of particular interest, is the ability of the dismounted soldier to interact with simulated MOUT environments within which they can train and rehearse real world missions. The need exists for a capability to rapidly interface to a mixed reality simulation system that virtual objects of interest can be added to a real world training environment.The reuse of component based objects is an ideal concept for a Mixed Reality MOUT (MR MOUT) application.  The existing highly instrumented physical facility serves as a FOM. Effective use of MR MOUTrequires a mechanism to collect, manage and integrate existing assets into a dynamic live simulation training environment.  Once more, the entertainment industry model of a backlot provides the core practices that can be adopted to provide a rapid development and deployment of training and mission rehearsal scenarios.  The BOM concepts combined with the historical backlot would encourage adoption of techniques to be developed within the VB research.4. Virtual BacklotIn the system being developed by UCF and IST, scenario planners would be able to walk through the location, carrying hand-held devices such as a wireless-enhanced personal digital assistant (PDA). By simply selecting items from a menu while standing at a particular doorway, ambush site or the location of a desired piece of "virtual rubble", a planner would automatically build up a database of virtual objects that would appear as soon as the Mixed Reality system is turned on. This database would also be observable and dynamically modifiable on map-based interfaces, running on PCs back at command centers.  In fact there are two distinct phases to the problem. First there is a kind of planning process, in which the training designers need to "block out" the overall scenario. Objects need to be associated with their locations, and a map of the overall scene developed. In a second “fine tuning” pass, the objects’' final positions and orientations need to be set up for optimal performance in the actual training scenario.The GeoPresence project at UCF has produced technology that addresses the first problem - the creation of a “planning map” that is populated with the objects to be used in the scenario. [1] In this project we have developed an interface to allow content creators to associate information with a physical location, either through a map interface or by using a wireless device to “drop”media while at the actual site. Users can then access this information through the same means (map interface or wireless device.) Our protocols allow simultaneous use by multiple creators and accessors, each using the most appropriate interface for their needs. Thus, mission planners may be observing placements in a MOUT at the same time that these placements are occurring on site. The GeoPresence interface is intended to be so simple that any user can, without prior instruction, create, place and retrieve rich media associated with specific geographical locations. In fact, it has already been used by English professors to build a walk through experience at Leu Gardens in Orlando, Florida, recording and playing media that depict the history and beauty of the gardens. It has been used by citizens of Eatonville, Florida to chronicle the history and culture of their community, the oldest incorporated African American governed town in America. It is now being used by digital media students to build databases intended to display geospecific announcements on the Lynx Lymno buses in downtown Orlando. (See [2] for a discussion of user interface issues in geospatial systems.)As alluded to above, persons who interact with GeoPresence software may be in the optimum environment for the task component they are working on. It is inconvenient to take a laptop or CRT into the field to look at a dynamic map; it is also a work intensive to accurately survey an actual MOUT or other real world facility to make and maintain maps. The GeoPresence allows a "quick sketch" ability to go into the field and send back detailed information to update a basic floor-plan map.After  the models that are to be inserted are produced (their production is not the subject of this research) there are two problems to be solved in building useful MR databases. First general layout of the objects must be established as described above in the GeoPresence section, and then the virtual objects must be accurately positioned and oriented for interaction. This establishes the need for two tasks in this project.The Layout Task will involve building a version of GeoPresence that is specialized for Mixed Reality database objects. This software will take as input, data from a fielded GeoPresence system (including PDA and base station), as well as a basic Mixed Reality database representing the intended MOUT facility. It will emit an updated MR database with appropriate objects (including concealment such as plants and rubble, as well as pop-up friend/foe targets) located in the appropriate sites within the MOUT model. Of particular interest to us is the use of data developed in the Virtual Forest Project. [3] The Fine Tuning Task consists of developing special application-level software that runs on top of the Canon Mixed Reality software. Models in the MR system can be dynamically manipulated by users of the system (e.g., the dolphins in the Contact Water SIGGRAPH exhibit.) When the training developer puts on the MR equipment and runs the Fine Tuning software, he can adjust the positions and orientations of the objects which were initially located during the Layout Task using GeoPresence software. During the layout phase, the planner is not encumbered with any head mounted equipment; he is simply using a GPS-equipped PDA. The resulting layout can be quickly examined and improved using a map-view display – or, in fact, the original planning can be done with the map view and checked with the PDA. GeoPresence's central concept is this duality between convenient map display and immediate in-the-field access to the placement data, mediated by a secure wireless link.5. Mixed Reality and Macro Stimulators In any mixed reality setting decisions need be made as to whether each effect is produced by virtual or physical means.  A MacroStimulator (MS) is a device that physically produces effects.  These effects include, but are not limited to, light, sound, and mechanical motion (referred to as “show-action” effects).  Typically, immersion into a mixed-reality situation will be performed using a mixture of virtual and physical methods.  The virtual methods currently focus on the visual and auditory senses.  The movement of real world objects is by default not virtual and so it is natural to assume a MS system performs these show-action sequences.  But the MS may also be used to augment the virtual imagery (which is in turn augmenting real imagery).  For example, an explosion may be composed of a virtual image of an object disintegrating while a MS equipped with speakers and a smoke machine provides the corresponding “boom” and residual smoke.  Likewise a vehicle may be composed of a virtual image superimposed on a mobile robotic MS emitting engine noise.The BOM philosophy supports the HLA goals of efficient model development via re-use and interoperability via well defined and documented interfaces. Specifically, the BOM philosophy advocates developing new models and scenarios by composing BOM’s that each represent a specific facet or aspect of the overall model.  The proposed MS design also follows this philosophy.  The physical design of the MS builds upon light, sound, and show-action systems used in the theater and entertainment industry.  This industry has demonstrated the need to build MS systems to support live entertainment.  Each MS configuration is used for a short duration of time (the run of a show or event) and then dis-assembled, only to be re-assembled in another configuration for the next show.  To meet this need, Commercial Off The Shelf (COTS) theatrical hardware will be used that allows unique systems to be built readily by composing individual parts.  These individual physical parts correspond to BOM’s.The physical events performed by the MS are driven by supporting software.  There is no direct correlation between the software currently used in the entertainment industry and the BOM model.  As part of the VB project, we are developing software models that can interact with the scripting software driving the events in the scenario.  The software packages will correspond with physical packages and, when combined with supporting documentation, meet the BOM philosophy.  The end result is that the physical MS and the supporting software fall within the BOM guidelines and support the Virtual Back Lot goals of rapidly composing new scenarios and situations by reusing standardized hard and soft components.6. Summary SISO have provided a solid base for the creation of standards. However, the success of those standards depend on the adoption of standards.  This paper has presented a philosophy by which BOMs are used in the creation of model standards designed to support high levels of model reuse across a range of environments and applications.  A survey of applications was presented by way of motivation for creation of both the models and a method by which they may be used.  Finally, a description of UCF’s and IST’s efforts to create a Virtual Backlot was presented that outlines one approach towards adoption of the standards.7. References[1]  J. Burnett, C. E. Hughes, J. M> Moshell, and C, Stapleton.  “GeoPresence: Interacting with Geospatial Stories,” Demonstration presented at Computer Human Interface 2002.  Minneapolis/St. Paul, 20-25 April, 2002.[2] W. Cartwright, et al., 2000.  “User Interface Issues for Spatial Information Visualization,” Cartography and Geographic Scince, Vol. 28(1).[3] C.E. Hughes, J.M. Moshell, V. Sims, and Q. Yu.  “Dynamic Computation of Levels of Detail in Complex Outdoor Environments,” Visual 2000: 3rd International Conference on Visual Computing, Mexico City, 18-22 September 2000.[4] SISO-REF-005-2001, BOM Study Group Final Report, 2001.Author BiographiesPAUL DUMANOIR is a principal investigator within the Tech Base Division, Engineering Directorate, U.S. Army Simulation, Training and Instrumentation Command (STRICOM), Orlando, FL. He currently leads the Cross-Domain Technologies (CDT) enterprise conducting research in the area of dismounted soldier embedded training and simulation. Mr. Dumanoir is the program manager for the Embedded Training for Dismounted Soldier (ETDS) Science & Technology Objective (STO).  Prior to his involvement with individual warfighter simulations, he worked as software and systems engineer on various M&S programs.  His current interests include Computer Generated Forces (CGFs), Human-in-the-Loop (HITL) networked simulators, virtual and augmented reality, and embedded training applications.  He earned his B.S. in Electrical Engineering from the University of South Alabama in 1987 and his M.S. in Computer Systems from the University of Central Florida in 1991.PAT GARRITY is a principal investigator at U.S. Army Simulation, Training and Instrumentation Command (STRICOM), engineering directorate, tech base division. He currently works in the Cross-Domain Technologies (CDT) enterprise area conducting R&D in the area of dismounted soldier embedded training and simulation.  Prior to his involvement with tech base division at STRICOM, Mr. Garrity worked as the Project Director for the Advanced Concepts Research Tools (ACRT) program in PM STI at STRICOM.  His current interests include Human-in-the-Loop (HITL) networked simulators, virtual and augmented reality, and embedded training applications.  He earned his B.S. in Computer Engineering from the University of South Florida in 1985 and his M.S. in Simulation Systems from the University of Central Florida in 1994.KEITH GARFIELD is a Research Assistant at IST.  Mr. Garfield has 16 years of experience in the aerospace community with the Boeing Corporation.  He earned his B.S. in Aeronautical Engineering from Embry-Riddle Aeronautical University in 1983, and is pursuing a Ph.D. in Computer Science at UCF.  His research interests include formal language specifications for use in parallel computation and natural user interfaces with virtual environments.CHARLES E. HUGHES is Professor of Computer Science in the School of Electrical Engineering and Computer Science at the University of Central Florida (UCF), Orlando, Florida.  He received the B.A. degree in Mathematics from Northeastern University in 1966, and the M.S. and Ph.D. degrees in Computer Science from Penn State University in 1968 and 1970, respectively. He served on the faculties of Computer Science at Penn State University and the University of Tennessee prior to joining UCF in 1980. His research interests are in distributed interactive simulation, distributed object-oriented programming, and models of distributed computation. He is a member of the ACM, the IEEE and the IEEE Computer Society.J. MICHAEL MOSHELL is Professor of Computer Science and Director of the CREAT Digital Media Program at the University of Central Florida (UCF), Orlando, Florida.  He received the B.S. degree in Physics from Georgia Tech in 1968 and the Ph.D. degrees in Computer Science from Ohio State University in 1975. He served on the faculty of Computer Science at the University of Tennessee prior to joining UCF in 1984. He founded and directed the Visual Systems Laboratory of UCF's Institute for Simulation and Training from 1990 through 1994. His research interests are in the use of simulation and virtual environments for learning and problem-solving. He is a member of the ACM, the IEEE and the IEEE Computer Society.CHRISTOPHER STAPLETON is the Director of Entertainment Research at the Institute for Simulation and Training.   He has over twenty years of international experience in the entertainment field including film, Broadway theater and theme park attractions for Universal Studios, Nickelodeon, Disney and Paramount Parks.  He earned his Masters of Fine Arts from New York University in design for theater and film. He is pursuing a PhD at the University of Central Florida in Modeling and Simulation.  He founded the Media Convergence Laboratory within IST whose mission is to develop the mainstream applications for simulation for education, entertainment and training.  Mr. Stapleton is also a faculty member for the UCF Digital Media Program and the Film Department.InteractionIF BOMFOMAttrib22Attrib21Object2Attrib32Attrib31Meta-dataObject3Attrib42Attrib41Object4Attrib12Attrib11Meta-dataObject1Interaction