Improving Simulation Analysis through Interfaces to Command Control Communications Computer and Intelligence (C4I) Systems and Use of Intelligent AgentsRanjeev MittuNaval Research Laboratory4555 Overlook AvenueWashington, DC 20375(202) 404-8716Fax: (202) 767-1122 HYPERLINK "mailto:mittu@ait.nrl.navy.mil" mittu@ait.nrl.navy.milMyriam Abramson, Ph.D.ITT Industries, Inc.2560 Huntington AvenueAlexandria, VA 22303 (202) 404-7342Fax: (202) 767-1122 HYPERLINK "mailto:mittu@ait.nrl.navy.mil" abramson@ait.nrl.navy.milJoshua WaltersITT Industries, Inc2560 Huntington Avenue Alexandria, VA 22303(703) 767-1879Fax: (703) 767-1122 HYPERLINK "mailto:walters@ait.nrl.navy.mil" walters@ait.nrl.navy.mil Keywords:  Multi-Agent Systems, Intelligent Agents, Simulation, C4I, Plan MonitoringABSTRACT:  As the complexity of modern warfare increases, managing and interpreting operational data will continue to be one of the greatest challenges to commanders and their staffs. The wealth of data collected and distributed via C4I systems during warfighting operations is staggering. The ability to effectively identify trends in such data, and make predictions on battlefield outcomes in order to affect planning is essential for mission success. Future commanders will be forced to rely upon new information technologies to support them in making decisions. Simulations have been used by analysis and planning staffs for years during exercises and operations. Typically, combat simulations are used most heavily during the planning stages of an operation, prior to execution. However, simulation is increasingly being used during operations to perform course of action analysis (COAA) and forecast future conditions on the battlefield. Recent efforts by the Defense Modeling and Simulation Office (DMSO) to improve the interoperability of C4I systems with such simulations have provided a powerful means for rapid initialization and analysis during exercises, and has made simulations more responsive and useable during the execution portion of an exercise. Real-time interfaces between the Global Command and Control System (GCCS), and the Integrated Theater Engagement Model (ITEM) have provided command staffs with the capability to perform faster, more complete, COAA. Intelligent agents offer another technology to help commanders manage information on the battlefield. The Defense Advanced Research Projects Agency (DARPA) has sponsored the development of the Control of Agent-Based Systems (CoABS) Grid. The Grid is middleware that enables the integration of heterogeneous agent-based systems, object-based applications and legacy systems. The CoABS grid was used to develop the Critical Mission Data over Run-Time Infrastructure (RTI) (CMDR) that allows dynamic discovery, integration and sharing of High Level Architecture (HLA) compliant simulation objects with legacy C4I systems and grid-aware software agents. The bridging of the CoABS grid and the HLA using CMDR makes it possible to leverage the power of agent technology with the ability to tap into multiple C4I sources and simulation systems simultaneously. This synergy could lead to profound benefits in situation assessment and plan-execution monitoring using agents.	This paper will describe each of these technology components that has enabled the development of a flexible, open environment for the integration of plan-monitoring software agents.  We will describe the integration of these components and preliminary results from our experiments conducted over the past year.  Next, we will discuss the vision to use web services to support the integration of C4I systems and simulations within the Global Information Grid (GIG).  In support of this vision, we will describe planned activities for integrating the Joint Warfare Simulation (JWARS), GCCS-M and intelligent agents using web services.  We will conclude with areas for future investigation with specific attention to the intelligent agent technology.  .1.  IntroductionAgent-aided information retrieval and decision support has attracted the attention of the agent research community for several years. The concept of large ensembles of semi-autonomous, intelligent agents working together is emerging as an important model for building the next generation of sophisticated software applications. This model is especially appropriate for effectively exploiting the increasing availability of diverse, heterogeneous, and distributed on-line information sources, and as a framework for building large, complex, and robust distributed information processing systems. The development of enabling infrastructure for mobile computing and interoperability among programs residing at distant sites, and new generations of distributed operating systems has made the construction of systems based on this model much easier. Over the past six years, the Defense Advanced Research Projects Agency (DARPA) has sponsored the development of the Control of Agent-Based Systems (CoABS) Grid [13].  The Grid is a middleware that enables the integration of heterogeneous agent-based systems, object-based applications and legacy systems.  The CoABS grid was integrated with Critical Mission Data over Run-Time Infrastructure (RTI) (CMDR) system to allow dynamic discovery, integration and sharing of High Level Architecture (HLA) compliant simulation objects with legacy C4I systems and grid-aware software agents.  The development of the CMDR has paralleled in an analogous fashion, the C4I-to-Simulation program sponsored by Defense Modeling and Simulation Office (DMSO).  This program has featured the use of the HLA RTI to pass data between systems such as the Global Command and Control System (GCCS) and the Integrated Theater Engagement Model (ITEM).  The bridging of technologies between the CoABS grid and the HLA using CMDR has provided a wonderful opportunity to leverage the power of agent technology with the ability to tap into multiple C4I sources and simulation systems simultaneously.  This has lead to profound benefits in execution monitoring using software agents.  This paper will begin by describing the developments within the DMSO C4I-to-Simulation interoperability program in Section 2. We will describe the enabling technology that permits C4I systems and simulations to exchange data via the HLA RTI. In Section 3, we will describe the technology development within the CoABS program, allowing agents to seamlessly interoperate with each other and exchange data via the CoABS grid.  In Section 4, we describe the CMDR system, allowing seamless connectivity between HLA based simulations and CoABS Agents.  Next, in Section 5, we will discuss our technical approach, design and experimentation which demonstrates the power of integrating CoABS agents with simulation/execution information available via the HLA RTI, bridged through CMDR. The purpose of this integration was to support agent-based analysis of C4I and simulation systems. We will conclude, in Section 6, with a discussion of future research areas to support the vision of the Global Information Grid (GIG), particularly the integration JWARS, GCCS-M and intelligent agents through web services.  C4I-to-Simulation via HLA RTIWork involving various instances of HLA-based C4I-Simulation applications has been detailed in numerous papers [3].  In 1998, DMSO sponsored an effort to utilize the HLA for passing data from the Joint Theater Level Simulation (JTLS) into the Global Command and Control System (GCCS) [2].  This application led to the development of a similar capability in which the Naval Simulation System (NSS) could stimulate GCCS, also using the HLA RTI [2].  While these applications successfully demonstrated the ability of the RTI to be used to pass information between simulations and C4I systems in much the same way that it was designed to be used between simulations, this capability was never used as part of an operational exercise.The demonstrated utility of stimulating GCCS from JTLS and NSS using the RTI led to further applications involving the Army's Eagle simulation.  In recent years, the TRADOC Analysis Center (TRAC) has sponsored development of an effort to use the RTI to pass information between Eagle and many of its C2 systems including All Source Analysis System (ASAS), Maneuver Control System (MCS), and Combat Service Support Control System (CSSCS). Details of this implementation can be found in [6].  During 2001, the GCCS-NSS [9] capability was revived, as part of an effort to perform rapid initialization of NSS during the Global 01 exercise.  Previous uses of NSS as a COAA tool in Global 00 were limited because of the need to manually input data, read off of C4I devices such as GCCS.  The initialization scheme required modifications to both the GCCS RTI Interface (known as the "GCCS Ambassador‚Äù) and the NSS RTI interface to allow data flow from GCCS to NSS. Details of this implementation are documented in [4]During 2002, the GCCS-NSS initialization scheme was extended for use with the Integrated Theater Engagement Model (ITEM), an analysis application used primarily by US Pacific Command (PACOM) and United States Forces Korea (USFK).  A similar scheme for initialization was developed, which relies upon data present in the GCCS Track Database Manager (TDBM) to be sent via the RTI to ITEM so that the initial state of the simulation is synchronized with GCCS as the starting point for running an analysis [8].  This capability was successfully demonstrated in Reception Staging and Onward Integration (RSOI) 02 and Ulchi Focus Lens (UFL) 02, and has been used during FY03 by Korea Battle Simulation Center (KBSC).  Details of the implementation can be found in [7]During 2003, DMSO has further extended the work done with NSS and ITEM to the Joint Warfare System (JWARS). An initial capability that synchronizes data from the GCCS Common Operational Picture (COP) with the current JWARS scenario has been demonstrated in 2003.  This capability will help to address one of the major JWARS requirements to promote its use in Combatant Commands for in-theater analysis.The Control of Agent Based System Program.The CoABS program was a DARPA effort aimed at developing techniques to safely control, coordinate, and manage large systems of autonomous software agents. The CoABS program was investigating the use of agent technology to improve military command, control, communication, and intelligence gathering. The military environment is dynamic, with quickly changing operations, moving hardware and software that are continually connecting and disconnecting, and bursty bandwidth availability. Inflexible stove-piped legacy systems that were never meant to be integrated are, nevertheless, of vital importance to military planning and operations. Multiple hardware and software platforms as well as data interfaces and standards further complicate the picture. In addition, military personnel are overwhelmed by the increased data availability from the modern battlefield and suffer from information overload. A goal of CoABS was to enhance the dynamic connection and operation of military planning, command, execution, and combat support systems to quickly respond to the changing operational picture. Software agents were developed to work side-by-side with human military planners and operators to ease the burden of their daily tasks. The CoABS Grid, developed under the DARPA‚Äôs CoABS program, arguably provides the most successful and widely used infrastructure to date for the large-scale integration of heterogeneous agent frameworks with object-based applications and legacy systems. Based on Sun‚Äôs Jini services, it includes a method-based application-programming interface to register and advertise capabilities, discovers services based on those capabilities, and provides the necessary communication between services. Systems and components on the Grid can be added and upgraded without reconfiguration of the network. Failed or unavailable components are automatically purged from the registry and discovery of similar services and functionality is pursued.The Grid supports a wide variety of applications, from simple monitoring and information retrieval to complex, dynamic domains such as military command and control. Using the Grid, agents and wrapped legacy systems can (1) describe their needs, capabilities and interfaces to other agents and legacy systems; (2) find and work with other agent components and legacy systems to accomplish complex tasks in flexible teams; (3) interact with humans and other agents to accept tasking and present results, and (4) adapt to changes in the application domain, the task at hand, or the computing environment. The Grid does this by providing access to shared policies and ontologies (mechanisms for describing agents‚Äô capabilities and needs), and services that support interoperability among agents and legacy systems with simple or rich level of semantics ‚Äì all distributed across a network infrastructure.Although most agent frameworks provide some of the interoperability and other services that the Grid provides, each framework typically supports specialized constructs, communication, and control mechanisms. This specialization is desirable because particular systems can use mechanisms appropriate to the problem domain/task to be solved. The Grid is not intended to replace current agent frameworks but rather to augment their capabilities with services supporting trans-architecture teams. As shown in Figure 1, the Grid provides helper utility classes that are local to an agent and hide the complexity of Jini. These classes automatically find any Look-up Services (LUS) in both the local area network and user-designated distant machines. The Grid supports agent and service discovery based on Jini entries and arbitrary predicates as well as by service type. The Grid also provides event notification when agents register, deregister, or change their advertised attributes. Current Mission Data via the RTI (CMDR)The CMDR is a tool for developing HLA compliant applications that significantly reduces development time.  CMDR is currently being used in support of a number of DARPA and DMSO sponsored initiatives. The software is a Java library designed to enable developers to quickly federate with HLA compliant simulation systems.   CMDR provides a general framework for interacting with the RTI.   Reusability of applications with new federations is enhanced when the applications are built using CMDR due to an independence from low-level RTI structures and data formats.  4.1 CMDR ArchitectureThe architecture of CMDR (Figure 2) allows developers to rapidly develop core HLA compliant applications.  The software acts as middleware between the application code and the RTI.  This allows the middleware functionality to be implemented once, and can then be reused by each application through library calls.   The RTI libraries and the API‚Äôs provided in the HLA specification are the under-pinning of the CMDR software [5].  Some of an application's primary responsibilities that are implemented in CMDR are: Maintaining a database or internal representation of remotely simulated objects and their current states. The RTI does not maintain a database of objects that can be queried for current attribute values by an application. It is simply the communications mechanism through which messages describing object creations, removals and attribute updates are exchanged among federates. By having this function in CMDR, an application can just query its CMDR to obtain the current state of each remote object, ignoring the details of which attributes have been updated when.  Managing the transmission of attribute updates for locally simulated objects. The RTI does not keep track of the current state of the locally simulated objects either, so it can know when attribute values are out of date and thus need to be communicated to other federates.Converting between raw data formats and actual objects.  The RTI transmits object attributes and interactions parameters as arrays of raw data.  An important feature of CMDR is the ability to automatically translate raw data into objects and back again for many data types.  This greatly reduces the amount of work necessary for examining and using the data in an application. CMDR maintains a representation of the remote objects, adding new objects in response to the discoverObjectInstance RTI service, removing them in response to the removeObjectInstance RTI service, and updating components of their current state in response to attribute updates delivered by the reflectAttributeValues RTI service (attribute updates typically contain values for only a subset of an object's attributes, rather than its entire state.)  Sometimes attributes are updated to their same value, for instance, the heartbeat that indicates an object still exists appears as a complete update of the attributes.  One of CMDR‚Äôs features to improve an application‚Äôs performance is the option to filter out updates that do not actually change the value.  This can reduce unnecessary updates to the screen or other data models.  5.  Integration between HLA RTI Federation, CMDR and CoABS Grid Agent FederationThe GCCS/ITEM federation was integrated with intelligent agents via the CMDR ‚Äúbridge‚Äù between the RTI and CoABS grid.  The purpose was to conduct agent-based plan monitoring to support the analysis of both real and simulated information to detect deviations, and provide alerts to GCCS when those deviations were of significant consequence.  The system architecture is shown in Figure 3.  Further details of system components can be found in [5].The GCCS was used to publish ‚Äúreal world‚Äù track data while ITEM was used to publish simulated tracks.  The CMDR subscribed to both the ‚Äúreal world‚Äù and the simulated track data published in the HLA federation, and provided the data as it became available via the CoABS Grid to the agents. As the agents analyze the data, alerts (or retraction of alerts) are generated according to pre-defined thresholds.  Alert ‚Äúretractions‚Äù are generated when movements result in a track moving back within a distance that is below the threshold distances.We will now describe the sequence of steps that will demonstrate the user‚Äôs interaction with the various system components within the architecture.Step 1:  Develop a pair of scenarios in ITEMSeveral pairs of combat scenarios were developed using ITEM. These scenarios were developed by a Subject Matter Expert (SME) supporting U.S. Forces Korea (USFK). Three separate scenarios were used, the largest one consisting of 490 hostile and friendly ground units and 1054 hostile and friendly ships.  Step 2:  Using a given scenario pair generated in step 1, modify the first of the pair such that some of the tracks deviate from those in the second scenario. In our experiments, we needed to artificially create an execution of the plan and also a simulation of the plan.  In our example, it was somewhat arbitrary which scenario we labeled as the ‚Äúreal-world plan‚Äù and which one we labeled the ‚Äúsimulated plan‚Äù. In either instance, ITEM was used to generate both.  One scenario from the pair was used in the simulation of the plan and the other was a variant of the plan where friendly and hostile land units and naval platforms behaviors differ from the plan.  A thirteen hour segment was simulated and captured for each of the scenario files. Step 3:  Having created a representation of the real world plan as well as simulated plan, GCCS published the initial state of the battlefield, while ITEM subscribed to this initial state from GCCS. The purpose was to support a ‚Äúcorrelation‚Äù step that would be done in the real world to get both systems ‚Äúon the same page‚Äù  The GCCS HLA Ambassador was used to publish tracks from the TDBM representing the real-world plan execution onto the RTI.  To support our experiments, the Ambassador was modified to be able to publish all tracks and their data to the RTI without the user having to rubber-band and select a group of tracks.  This allowed the user to be somewhat ‚Äúhands-off‚Äù, and guaranteed that the monitoring agents would receive all of the necessary tracks to monitor.The archived scenario was set to stop at a timestamp shortly after all of the tracks appear on the screen.  This constitutes the initial state of each track.  Once published, ITEM (through a subscription mechanism), received those tracks.  This synchronization step provided us with a one-to-one mapping between ‚Äúreal-world‚Äù and ‚Äúsimulated‚Äù tracks.  ITEM was modified to ‚Äúreflect‚Äù the same Local Track Number (LTN) -   an internal ‚Äúhouse-keeping‚Äù variable used by GCCS, as a primary key. This primary key was necessary for the agents to be able to match ‚Äúreal-world‚Äù and ‚Äúsimulated‚Äù tracks.The use of the LTN was sufficient for our initial experimentation, however it was soon discovered that the LTN was not a good choice for primary key.  The LTN is a number assigned to a track when GCCS loads the information from the TDBM or acquires the track from real world input (ITEM does a similar assignment for its objects).  This LTN is unique for the duration of the time that GCCS displays that track.  It does not exhibit persistence in the TDBM database, and therefore is not consistent throughout multiple runs. It was soon concluded that a more permanent key would be needed in the future.  Another difficulty arises when the tracks are not correlated properly in ITEM.  If the GCCS track is not correlated to an ITEM object, an ambiguous track will show up on the GCCS screen.  When a track is ambiguous, it is not correlated with an object in ITEM.  This means that the GCCS track and the ITEM object have different LTN‚Äôs.  As a result, the agents will not be able to match real and simulated track data, and monitoring process will fail.Step 4: Next we published all ITEM objects onto the RTI (the objects contained the force composition information).  ITEM published the entire scenario to the RTI at this point in time, including track objects and health status objects.  ITEM was enhanced to support the publishing of objects describing force combat worth or ‚Äúhealth‚Äù in terms of ‚Äúmass‚Äù.  For example, the relative value of each entity participating in the simulation is compared to the strength of an M1A1 tank (whose value was set to one).  For example, a soldier with a Rocket Propelled Grenade (RPG) might have a mass of 0.1, signifying that ten human/RPG pairs would equal the combat strength of the M1A1.  The objects published by ITEM contained unit strength (mass) information using these baseline values.   The ability to analyze a force‚Äôs combat worth prior to monitoring the actual execution of the plan may prove to be useful for initial plan refinement.  In the system diagram, the mass monitoring agent was responsible for detecting deviations in combat worth based on thresholds defined in the original plan (located within ITEM).  The concept of Mass Monitoring is applied to Units; a parallel measurement is used to evaluate Platforms, and it consists of comparing probabilities of survival thresholds. Future work could involve the use of Unit Order of Battle information combined with Mass Monitoring as a means of monitoring force composition and battle readiness. The following constraints were used to trigger alerts:(Current mass)/(initial mass) < Surrender threshold(Probability of survival)  < Survival thresholdStep 5: Once ITEM is finished and the agents have analyzed mass/survival of units and platforms, playback of GCCS is resumed.Several types of track deviation agents were developed to monitor track movements. The GCCS Ambassador and ITEM published track information, representing the real world execution and simulated execution, respectively, to the RTI.   These were fed through the RTI via CMDR to the agents registered on the CoABS grid.  Several modifications were made to CMDR to support our experimentation, including the ability to translate data into XML as well as improvements to the record/playback feature.Several types of track deviation agents were developed, including extrapolated deviations, interpolated deviations and positional deviations agents.  These monitoring agents were built using the Java Expert System Shell(JESS), a rule-based inference engine [1].  Furthermore, two additional agents were developed, the C4IController agent and the UserInterface agent.  Step 6:  The agents generate alerts, which are then displayed on the GCCS COP for the warfighter (Figure 4).In section 5.1, we describe the C4I controller agent and UserInterface agent.    Section 5.2 will describe all of the JESS rule-based track deviation agents.  These rule-based track deviation agents are responsible for providing the alerts on the GCCS COP as indicated in step 6 above.5.1 Supporting AgentsTwo kinds of supporting agents were developed.  These were the C4IController agent and UserInterface agent.  These agents facilitate the setup of various kinds of track deviation agents.  The UserInterface agent interface is shown in Figure 5.  Through this interface, the user is able to spawn mass monitoring or track monitoring agents.  The user may enter whether units or platforms are to be monitored for each type of agent. Furthermore, the interface provides a mechanism to specify threshold values, that, when exceeded, would warrant alerts (which are also captured in the display as well as sent to GCCS for display within its COP).  This agent was developed using the Java Swing GUI and provided the ability to ‚Äúspawn‚Äù the track deviation monitoring agents.Within our implementation, multiple track monitoring agents can be created to monitor the same types of deviations for different tracks.  For example, several extrapolation deviation agents can be created that monitor different tracks.  The monitoring agents, once created, register tracks of interest with the C4IController agent, which then routes track data to them as this information comes in from CMDR.   5.2 Rule-Based Track Monitoring AgentsThe JESS [1] was chosen as the core engine providing the agent reasoning capability.   JESS is a Java-based rule engine and scripting environment.  Originally based on the CLIPS expert system shell, it has grown into its own distinct paradigm.  There were a number of reasons for choosing JESS, the primary one being that Jess' implementation of clisp-like rules in Java facilitates the integration of intelligent, rule-based agents.  It contains a Java API for accessing the shell, and the functionality of the shell itself can be expanded through Java.  JESS uses the Rete algorithm, a powerful mechanism for triggering rules in the knowledge base efficiently, and can theoretically scale up as the number of rules remains similar to each other (i.e., the antecedent of the rules are similar). The overall monitoring task of the progression of the plan, as established by the simulation, would be decomposed into subtasks by the plan-understanding agents (future work) and delegated to the monitoring agents. The metaphor is to place a camera into each room of a building to detect small changes in situation that might influence the overall state of the entire building. Rules can be specified to automatically detect when certain conditions are present and trigger alerts instead of continuously monitoring the simulation. Each agent registering to the Grid has its own rule-based engine. Its knowledge base is initialized with simulated tracks and its deviation rules select the relevant temporal tracks to be compared against. Further enhancements will consist of incorporating background knowledge to instantiate possible explanations to those deviations and provide guidance on appropriatethresholds of deviation. The time-step interval in the simulation indicates the temporally relevant tracks. Two different methods to calculate deviations are used to make up for the unsynchronized nature of real-time events with planned events. The extrapolation method projects a position in the future when the course and speed is known and compares that projection against a simulated event. The interpolation method does a linear interpolation between two temporally consecutive simulated events to estimate the current position. That estimation is then compared with the real-time event. The procedure for the distance calculation between those events is described in Figure 6. This distance serves as the decisive factor for triggering alerts when deviations occur above threshold (and retracting alerts when below threshold). ÔÅîhe latitude and longitude, given in decimal degrees, are converted to radians and the earth radius (6378 kms) is added to the altitude. The angle Œ± between 2 points, p1 and p2, is computed first: Œ± = arcos((sin a1 * sin a2) + cos(b1 ‚Äì b2) * cos a1 * cos a2)where a1 is the latitude of p1, a2 is the latitude of p2, b1 is the longitude of p1 and b2 is the longitude of p2. The distance is then computed using the cosine law: ((r + c1)2 + (r + c2)2 ‚Äì 2(r + c1)(r + c2) cos Œ±where r is the earth radius and c1 and c2 the respective altitudes of p1 and p2. 6.  Conclusion and Future DirectionsSeveral scenarios were developed for experimentation.  The largest scenario, representing a hostile incursion, consisted of 490 hostile and friendly ground units and 1054 hostile and friendly ships.  It ran to completion in 2.5 hours (13 hours of simulated time).  It was theorized that with newer versions of software and hardware (we used GCCS 3.x), the time could be drastically reduced.  In spite of this we were able to successfully experiment with the largest scenario, and performance (throughput) was not degraded significantly.The future vision is to integrate simulations such as ITEM and the Joint Warfare System (JWARS) with C4I systems and agent technology using open standards (Figure 7).  The motivation is that within the Global Information Grid (GIG), these ‚Äúservices‚Äù will be accessible via open standards in a plug-and-play environment.  We plan to investigate the use of industry standards such as web services to support the vision of the GIG.  There are many technical challenges in fully realizing the vision of the GIG, particularly the utility of web-services in supporting agent-based communication.  Furthermore, there are other research challenges that need to be addressed solely with regard to the agent development.  We have concentrated strictly on the plan monitoring agents.  However, there is significant research to be done in the area of plan understanding.  In our experimentation, we have placed the burden on the user to select tracks of interest, for which thresholds need to be set, that should be monitored.  Through plan understanding, we would like to be able to identify critical events and relationships, thereby permitting an intelligent agent to monitor the necessary plan in terms of those critical events.  We are examining Natural Language Processing (NLP) techniques coupled with sublanguage ontologies to extract these events from Operational Orders (OPORDs).  Another promising area is the use of the Battle Management Language (BML) to support the necessary plan decomposition process.  In the domain of agent ontologies, several areas emerge for possible investigation, including the ability of the agents to federate with agents that are ontologically dissimilar [12].  The value to the GIG lies in the fact that agents will be heterogeneous, with different ontological foundations.  One might imagine a need for agent communication between communities of interest (COI) in the GIG, thereby, necessitating the need to bridge between their respective agent ontology.Another area for investigation is in agent teamwork [10, 11].  Within the GIG, one can envision agents that cooperate to form task-based teams to accomplish their objectives on behalf of their users.  For example, teams of software agents with different goals, that coordinate with each other to carry out the decomposition process as well as monitor various aspects of a plan.  By understanding the techniques used to develop agent teams (agent composition), we may also gain insight into composing web- services within the GIG. The goal of this technology development is to support the warfighter with a sophisticated capability for real-time planning and monitoring efforts.  In Fiscal Year 2002, the DMSO began a relationship with the J3 Plans Division at United States Forces Korea (USFK) during the ITEM-GCCS effort.  In Fiscal Year 2003, the development team is continuing to coordinate with USFK personnel to produce a capability that will ultimately support their planning operations.  In fact, the scenario chosen for our integration was developed by USFK personnel and they have also expressed interest in employing this technology at Ulchi Focus Lens (UFL) 04.  7.  References[1] Friedman-Hill E. Jess in Action, Rule-Based Systems in Java. Manning, 2003. [2] Furness et al.  ‚ÄúReal-time Initialization of Planning and Analysis Simulations based on C4ISR System Data‚Äù.  In Proceedings of the 2002 Command and Control Research and Technology Symposium (CCRTS), Monterey, CA 11-13 June 2002.[3]   Layman, G., Daly, J., Furness, Z., Womble, J., "C4I-Simulation Interoperability Using the HLA and DII COE", in proceedings of the 2001 Command Control Research and Technology Symposium (CCTRS), Annapolis, MD, 19-21 June 2001.[4]  Lutz, R., Salisbury, M., Bidwell, G., ‚ÄúA Demonstration of C2I System-to-Simulation Interoperability:  The NSS/GCCS-M Federation‚Äù, in proceedings of the 1999 Fall Simulation Interoperability Workshop (SIW), Orlando, Florida, 12-17 September 1999.[5] Mittu, R., Furness, Z., Emami, R., ‚ÄúAgent-mediated Interface Between C4I and Simulation‚Äù. Proceedings of 2003 Spring Interoperability Workshop (SIW).  Orlando, FL, 2003.[6] Nielsen, J., Salisbury, M., ‚ÄúChallenges in Developing the JTLS-GCCS-NC3A Federation‚Äù, in proceedings of the 1998 Fall Simulation Interoperability Workshop (SIW), Orlando, Florida, 14-18 September 1998.[7]  Ogren, J., ‚ÄúCommand and Staff Training and the Practical use of the HLA‚Äù, in proceedings of the 2000 Fall Simulation Interoperability Workshop (SIW), Orlando, Florida, 17-22 September 2000.[8]  Prochnow, D., Harrington, J., Walter, D., Womble, J., ‚ÄúAutomate Initialization of an Analysis Simulation With GCCS Track Data‚Äù, in proceedings of the 2002 Fall Simulation Interoperability Workshop (SIW), Orlando, Florida, 9-13 September 2002.[9] Prochnow, D., King, R., Harrington, J., Regala, B., Sonnenshein, J., Daly, J., Womble, J., ‚ÄúThe NSS-GCCS Federation:  Course of Action Analysis (COAA) Using a C4I-Simulation Interface‚Äù, in proceedings of the 2001 Fall Simulation Interoperability Workshop (SIW), Orlando, Florida, 10-14 September 2001.[10]  Rao A. S. and Georgeff M. P. ‚ÄúBDI agents: from Theory to Practice‚Äù. In Proceedings of the First Intl. Conference on Multiagent Systems, San Francisco, 1995.[11]	Tambe, M.  ‚ÄúAgent Architectures for Flexible, Practical Teamwork‚Äù.  In proceedings of the  National Conference on Artificial Intelligence, 1997.[12] Tamma et al, ‚ÄúAn Ontology for Automated Negotiation‚Äù. In Proceedings of the International Workshop on Ontologies in Agent Systems.  AAMAS 02 Conferece, Bologna, Italy [13] Web:  HYPERLINK "http://coabs.globalinfotek.com" http://coabs.globalinfotek.com8.  AcknowledgementsThe authors would like to thank the Defense Modeling and Simulation Office (DMSO) for providing the research funds for this effort.Author BiographiesRANJEEV MITTU is the head of the Intelligent Computing and Decision Support Section at NRL   He has over fifteen years experience in designing and developing decision support systems for the naval community.  He earned an MS degree in EE from The Johns Hopkins University in Baltimore, MD in 1995.DR. MYRIAM ABRAMSON is a computer scientist at ITT Industries, Inc.  Dr. Abramson has over ten years experience in diverse areas of Artificial Intelligence and is now working in leveraging those techniques to build intelligent agents. She obtained her Ph.D. in Information Technology at George Mason University, Fairfax, VA, in 2003. JOSHUA WALTERS is a Computer Engineer at ITT Industries, Inc.  Mr. Walters has four years experience in web based technologies, computer security, and software engineering.  He obtained his B.S. degree from Virginia Polytechnic and State University. PAGE  PAGE  9Figure 5:  UserInterface AgentAgent Type:  Mass MonitorFigure 4:  GCCS COP with AlertsUnits being monitoredFigure 6:  Distance CalculationAgent Name: MassFigure 1: Grid ArchitectureFigure 2:  CMDR ArchitectureFigure 3:  System ArchitecturePlatforms being monitoredAgent Messages & Alerts (also sent to GCCS)Agent Type: Track Deviation MonitorAgent Name: DeviationFigure 7:  C4I-to-Simulation GIG VisionAlerts on GCCS-M COP display