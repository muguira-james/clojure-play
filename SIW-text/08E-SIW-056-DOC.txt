Lessons Learned about Developing Acceptable Acceptability CriteriaS.Y. HarmonZetetixP.O. Box 705, Myrtle Creek, OR  97457(541) 863-4639 HYPERLINK "mailto:harmon@zetetix.com" harmon@zetetix.comSimone YoungbloodJohns Hopkins University Applied Physics Laboratory11100 Johns Hopkins Way, Laurel, MD  20723(240) 228-7958 HYPERLINK "mailto:Simone.Youngblood@jhuapl.edu" Simone.Youngblood@jhuapl.eduKeywords:acceptability criteria, validation, accreditationABSTRACT:  Acceptability criteria establish the measures against which to judge the appropriateness of a simulation for an intended use.  The quality of any products from VV&A processes depend directly upon the quality of the acceptability criteria they use.  Much is written on the properties of good acceptability criteria but little advice exists in this literature for how to develop them.  This paper describes one effort to develop the acceptability criteria for a major simulation development program and the lessons learned from that effort.  The process that this effort employed involved decomposing the information in the available requirements documents (i.e., a Capability Development Document, a Performance Specification and a requirements model) into individual requirements statements, determining from those statements the required functional and system capabilities, identifying any requirements that define specific performance metrics, integrating that information into observable acceptability thresholds and checking the consistency of those thresholds with the other acceptability criteria and the original requirements statements.  The resulting acceptability criteria were then circulated amongst a group of subject matter experts to verify the necessity and completeness of their characterization of the users’ needs.  This process produced high-resolution, traceable and defensible acceptability criteria that could supply detailed insight into the simulation’s capabilities in the context of a user’s needs.  However, many outside factors can influence the acceptability criteria process including program office preferences, testing limitations, developer concerns and user representative concerns.  All of these factors must be mediated to produce acceptability criteria that are acceptable to a majority of these influential players.1.  IntroductionAcceptability criteria, also called accreditation criteria, are a set of standards that a particular model, simulation, or federation must meet to be accredited for a specific purpose [1].  Acceptability criteria specify the functionality and the quality of that functionality that a simulation needs to adequately serve an intended use.  They play a critical part in accrediting or judging a simulation is acceptable for an intended use.  They can also guide a simulation developer in building or modifying a simulation to suit a particular intended use or a class of intended uses.  Thus, acceptability criteria can have an enormous impact upon determining the use of a simulation.  The implications of this impact significantly complicate the process of developing acceptable acceptability criteria.Several authors have considered the nature and development of acceptability criteria [2-10].  However, considering their importance to modeling and simulation, these resources are miniscule compared to other related topics such as simulation requirements.  Youngblood and Senko [7] and Balci [8] have articulated some important properties of acceptability criteria:They must be related to documented requirements [7].They should be should be quantitative when practical but may be supplemented by qualitative values provided by the user and subject matter experts [7].They should reflect the overall needs of the intended use [8].They should actively support the assessment of confidence for relevant simulation responses for the intended application [10].These descriptions of the properties of “good” acceptability criteria are valuable but they do not guarantee that criteria that have these properties may be acceptable.  This lack of acceptance will doom any acceptability criteria, any benefit that they could bring to a simulation program and the effort invested in creating them.  This paper summarizes the lessons learned from three years of recent experience in developing and applying acceptability criteria in real simulation programs.2.  Lessons LearnedThis section describes nine lessons learned in formulating, defending and using acceptability criteria for various real simulation programs.2.1  Acceptability criteria include far more than just the representational criteria.People working on the validation and verification (V&V) of simulations can easily lapse into thinking that acceptability criteria primarily describe the required representational functionality (i.e., the functionality that represents some aspects of the simuland) of those simulations.  However, simulations are systems and systems have many requirements beyond only those related to representational functionality.Therefore, the acceptability criteria of a simulation system fall into two broad categories: representational criteria and system criteria.  The representational criteria distinguish a simulation from any other software system.  They define either what the system should represent (i.e., in terms of the simuland’s state or behavior) or how well it should represent those things (i.e., error characteristics).  System criteria describe all of the other conditions that the simulation must meet to adequately serve a user base including interoperability constraints, reliability-availability-maintainability (RAM) thresholds, execution environment characteristics (e.g., must run on a PC with the Windows operating system), human-system interface (HSI) characteristics, embedded training features and security capabilities.In the past, the authors have suggested using the term “validation criteria” for the representational criteria but this suggestion implies that a system is valid if it meets all of the representational criteria.  That perspective is both incorrect and simulation-centric since a simulation system must be validated against all of its acceptability criteria, not just the representational criteria.2.2  Not all representational criteria need be quantitative to be useful.Several authors have suggested that validation should be as quantitative as possible [7, 11-14] thus implying that quantitative validation is better than validation that is not quantitative.  Achieving quantitative validation requires that acceptability criteria also be quantitative descriptions of what is needed to adequately serve an intended use [7].  We assert that not all acceptability criteria need to be quantitative to be useful.  Quantitative criteria require the existence of meaningful metric spaces and not all representational requirements exist in metric spaces.The authors have found that representational acceptability criteria fall into two broad types: those that describe the necessary completeness of a simulation’s functionality and those that describe the necessary correctness of a simulation’s representations.  The completeness criteria define the required functional inventory.  For simulations, the required representational inventory, part of the required functional inventory, specifies what things the simulation needs to represent in terms of the entities, their properties and the dependencies between the entity properties that determine entity behavior.  The specification of a required representational inventory generally takes the form of an enumeration or list.  These specifications cannot be meaningfully quantified and can only be tested through observation.  The specifications of needed correctness (e.g., maximum acceptable simulation error) must be quantitative and can only be tested through actual measurement.  Not all parts of the required representational inventory need to have correctness criteria associated with them.  For example, the notion of error only applies to properties that can take metric values (i.e., values that can be meaningfully arranged in an order and that the distance between two adjacent values can be meaningfully defined).  Thus, quantitative representational acceptability criteria cannot be formulated for those entity properties that take enumerated values.  It is sometimes possible to force naturally qualitative criteria to be quantitative (e.g., using nonparametric techniques) but this only works well in some cases and those cases generally require deep understanding of the implications and limitations of any underlying mappings used.  It is far better to have good qualitative criteria than poor quantitative criteria.  In the context of the Validation Process Maturity Model (VPMM) [15], the choice of using quantitative acceptability criteria resolves to choosing between applying a validation process with a maturity level of 2 (i.e., one that only requires specification of the required representational inventory) or one with a higher validation process maturity level (i.e., those that quantify error and uncertainty).  Quantitative acceptability criteria are not always possible or needed and that can be good since creating meaningful quantitative acceptability criteria can be challenging.2.3  Needing to define quantitative acceptability criteria greatly increases the effort required to develop them.User needs statements that clearly specify maximum acceptable error thresholds create the need for quantitative acceptability criteria.  At first blush, creating an acceptability criterion that specifies needed error seems simple; just say something like “The simulation should represent the value of some property to within some percentage of the correct value.”  Another approach would specify the error as a value in the units of the parent property.  This approach eliminates the need to state “of the correct value” but implies it since error is commonly defined as the difference between a measured or computed value and the true value [16].This specification of needed correctness is incomplete because it does not specify an acceptable source for the “correct” or “true” value.  All measures of error require specification of the referent.  This is the referent or standard against which to measure error.  Thus, every specification of error also needs to contain a specification of the referent that defines the true value.  Referents can come in many forms: validated theory, validated simulations, subject matter expert knowledge and empirical observations.  Some consider empirical observations the “gold standard” of simulation referents [11, 17].  Empirical referents are measurements and all measurements suffer from errors and other uncertainties.  Thus, when using an empirical referent as part of a correctness criterion, the error threshold (i.e., the maximum acceptable error) should be much greater than the error associated with the referent if possible.  Referents are not only required as part of correctness criteria but their errors place limits on the values that those criteria specify.  Specifying errors with values that are less than the errors in the referent knowledge creates inherently unrealizable criteria.The errors of all simulations vary significantly and can only be meaningfully limited over specific ranges of input variables.  Therefore, any specification of a threshold for a maximum acceptable error can only be realistically met within a specific domain of input values and that domain should be specified along with the threshold.In general, all knowledge of any sort of error is uncertain.  More correctly, the notion of error is one component of uncertainty.  This suggests that the core problem is not only related to specifying error but rather to specifying the uncertainty that is acceptable to a user.  In their simplest form, confidence intervals (i.e., defined by an error threshold, an input domain and a confidence value) provide one mechanism to specify uncertainty thresholds.  In this case, the confidence value describes the probability that any error measurement will be less than or equal to the error threshold over the specified input value domain.  Expanding the scope of correctness criteria to include uncertainty extends the generality of the concept of correctness but adds another quantitative but difficult to measure factor.  In addition, choosing this extended interpretation requires uniformly characterizing the uncertainty associated with the referent information as well.So, at a minimum, a realistic specification of an acceptability criterion for simulation correctness needs to define the threshold of acceptable error, the domain of input values over which it should hold and the referent against which it will be measured.  If the scope of the correctness criterion encompasses uncertainty then a minimum acceptable confidence value needs to be specified as well.All correctness criteria must be meaningful to the users.  Specifying correctness criteria without this connection to the intended use and users creates criteria that cannot be defended in the numerous reviews to which the acceptability criteria will be subject.The concepts underlying quantitative acceptability criteria can be very complex and, for some phenomena, poorly understood.  This means that the accreditation agent, the source of acceptability criteria, may need either to have deep understanding of the phenomena that a simulation represents and the state of the science dedicated to exploring those phenomena or to hire one or more credible subject matter experts with that knowledge.Testing simulation behavior to assess correctness criteria is no easier than specifying those criteria.  Most simulations have very large input variable spaces so any observations of that behavior can only be made over very limited parts of those accessible spaces.  Sampling techniques from the design of computer experiments can be applied to this problem [18-23] but the rational application of these techniques requires knowledge of their statistical underpinnings.  This adds further burden to the V&V personnel.  Choosing not to apply these technique in favor of more subjective sampling choices leads to unreliable and indefensible measurements of simulation errors and throws considerable justifiable doubt on any determinations of simulation acceptability based upon those measurements.The authors do not intend these observations to discourage anyone from formulating and applying quantitative acceptability criteria.  Rather, we want people to create meaningful and, therefore, useful quantitative acceptability criteria and that takes considerable effort.  Specification of quantitative acceptability criteria depends upon the quantitative specification of user needs (e.g., in terms of maximum acceptable errors).  Reference [24] provides one model for creating this specification.  Without such a specification, it is better to use SMEs to check the correctness of a simulation than to create artificial and, possibly, incomplete quantitative error thresholds that cannot be defended.2.4  Abstraction simplifies acceptability criteria development and complicates their evaluation.In formulating acceptability criteria, the accreditation agent is completely dependent upon the resolution of the user needs information (e.g., requirements documents, user surveys, requirements Delphi output).  If that information is very abstract, therefore, very low-resolution, then the acceptability criteria will likely also be very low-resolution.  Most importantly, the accreditation agent should not succumb to the temptation to invent detail where none exists in the users needs information.  That will put any acceptance recommendations based upon those criteria on indefensible ground.  Also, criteria set at a higher resolution than necessary will increase the cost of developing and validating the simulation, possibly significantly.  On the other hand, criteria set at a lower resolution than necessary increase the risk that the simulation results may not adequately serve the intended use or be credible to the users.  In short, it is not possible to create high-resolution acceptability criteria from low-resolution user needs information without supplemental information from a credible source (e.g., an SME with credibility to the users).Acceptability criteria consist of a model of the users’ needs for a simulation and like all models they abstract the real requirements to some level.  Low-resolution criteria model user needs at a high level of abstraction and high-resolution criteria model user needs at a low level of abstraction.  Following the terminology proposed in the fidelity conceptual model [25], high-resolution acceptability criteria can represent the users’ needs to a very high level of detail (e.g., the simulation should represent the interactions between every gear and valve in an automobile’s automatic transmission) and low-resolution criteria represent those user needs to a much lower level of detail (e.g., the simulation should represent an automobile traveling over the ground at a given speed).High-resolution criteria provide the best insight into a simulation’s capabilities and the best basis for evaluating a simulation’s validity for an intended use but require the most resources to produce.  Acceptability criteria at higher levels of abstraction need more expert opinion to be interpreted and evaluated.  Thus, low-resolution acceptability criteria will require SME interpretation, a process that introduces considerable subjectivity into the validity judgments and, ultimately, the acceptance recommendations but require the least resources to produce (but not necessarily to evaluate).  Uncharacterized subjectivity degrades the acceptance recommendations with unknown errors and confidence and decreases the reproducibility of any judgments associated with the criteria.  High-resolution criteria can reduce the subjectivity introduced into the validity judgments and acceptance recommendations by reducing the subjectivity in the process of evaluating the simulation against the acceptability criteria.  Rather than making the situation better, overly general or highly abstract acceptability criteria can lead to a situation where a simulation may not be accredited because SMEs interpreting the acceptability criteria cannot agree.Other factors that can affect the level of abstraction of acceptability criteria include the available funding (e.g., more detail costs more money), available information (e.g., more user needs/requirements or referent information makes more detail possible) and politics (e.g., tolerable detail varies from customer to customer).  Some people feel that fewer acceptability criteria are better thereby requiring higher levels of abstraction.  However, few acceptability criteria cannot possibly provide much insight into the capabilities of complex simulations for equally complex intended uses.  This means that the simulation can either be accredited or not because it is not possible to define meaningful limitations of use thus enabling accreditation with limitations on use.2.5  Good acceptability criteria specify the properties of the required functionality.As mentioned earlier, the accreditation agent must avoid the temptation to build design information into the acceptability criteria with the good intent of creating high-resolution criteria.  Overly written acceptability criteria can actually, and perhaps inadvertently, limit the developer’s design choices.  It is better to have lower resolution acceptability criteria than to constrain the simulation’s design space.  In some cases, the requirements documentation, the source of many acceptability criteria, may have specified particular design approaches.  These are requirements and they should be reflected in the acceptability criteria.  However, the accreditation agent must walk a fine line between accurately and completely representing the stated requirements and adding inappropriate detail to the acceptability criteria.  As discussed above, high-resolution acceptability criteria are good but those that unnecessarily limit design options are bad and will be acceptable to no one.2.6  Good systems engineering makes acceptable acceptability criteria possible.Systems engineers can strongly impact the development of acceptability criteria.  Generally, systems engineers are responsible for interpreting the requirements documents for a simulation or a system that encompasses one or more simulations and producing a coherent performance specification for that system.  Well written performance specifications can provide the detail upon which to base good high-resolution acceptability criteria and that has good traceability back to the original user needs information.  Systems engineers also perform various checks on the performance specification to ensure its consistency both internally and with the user needs.  These attributes all strengthen the acceptability criteria derived from the performance specification.  However, in order for the acceptability criteria based upon the performance specification to be acceptable and credible, the performance specification must be well regarded by everyone involved.  Performance specifications that are not regarded as credible will make them a bad source of acceptability criteria.  A strong and defensible performance specification can be invaluable to the accreditation effort.  A weak performance specification cannot provide that support.  This situation transfers the burden on interpreting the original user needs information from the systems engineering staff to the accreditation agent.  It also transfers the burden of defending the acceptability criteria to the accreditation agent and increases the value of maintaining the traceability between the user needs information and the acceptability criteria.2.7  Acceptability criteria can determine the scope and type of acceptance testing done.One of the biggest sources of both criticism and advocacy of acceptability criteria is the testing and evaluation (T&E) team because the acceptability criteria can directly affect the acceptance testing.  The T&E team will directly evaluate the quality of the acceptability criteria.  They can accept them and use them as the foundation of their test plans or completely reject them and replace them with their own critical test parameters.  Highly abstract acceptability criteria are difficult to objectively test but high-resolution acceptability criteria can impose a very large testing burden.  In the best of situations, any un-testable acceptability criteria will simply be ignored and, at worst, ridiculed thereby casting doubt upon the competence of the accreditation agent and the value of the accreditation effort.Regardless of whether they are qualitative or quantitative, all acceptability criteria must be observable or testable.  This means that someone must be able to observe the behavior of the simulation and be able to objectively determine whether that behavior meets an acceptability criterion.  Further, another independent person should be able to observe the same behavior and make the same assessment with the same answer (i.e., reproducibility).  As discussed earlier, measurable acceptability criteria apply only to simulation properties that can take values in metric spaces and are also observable.  Not all acceptability criteria need be measurable.  Measurability applies only to simulation properties that take numeric values.  When formulating acceptability criteria, the accreditation agent should also identify the means by which to evaluate acceptability criteria to ensure their observability.The interactions between testing and acceptability criteria are even subtler.  The simulation is likely only going to be tested where the acceptability criteria specifications apply.  No testing will be done where no acceptability criteria exist and no test results imply no knowledge of the simulation’s actual capabilities.  This increases the uncertainty in using those capabilities.  The users cannot know whether the simulation is providing sufficiently accurate results in areas where there is no knowledge of the simulation’s capabilities.  This can lead to type II decision errors when using the simulation’s results in decision-making.  On the other hand, users that are unsure of the simulation’s actual capabilities may make a Type I error because they do not trust the simulation’s results for some reason.2.8  Defensible acceptance recommendations are not possible without acceptable acceptability criteria.A simulation can be accredited or accepted for an intended use even though it does not meet all of the acceptability criteria.  In this case, those acceptability criteria that the simulation does not meet contribute to defining the limitations of use in the acceptance recommendations (i.e., the conditions of recommended use.)  In this way, acceptability criteria should not be viewed as black or white where a simulation can be accredited or accepted only if it meets all of the acceptability criteria and cannot be accredited or accepted if it does not.  Many people hold this mistaken understanding of the role of acceptability criteria in accreditation or acceptance.  On the other hand, unconditionally meeting all of the acceptability criteria means that the simulation should be accepted to support the intended use without limitations.Ideally, the acceptability criteria are developed after the accreditation plan is written but before the V&V plan is written.  Sometimes, practical considerations (like when the accreditation agent is hired) prevent this ideal situation.  Under any circumstances, acceptability criteria must exist before the acceptance recommendations are formulated.  Otherwise, they have no purpose and the danger exists that the acceptance recommendations and acceptability criteria will be inconsistent.2.9  Everybody must accept the acceptability criteria.The acceptability criteria representing an intended use need to be acceptable to all of the players in a simulation development, modification or application program (e.g., program management, systems engineers, users, testers, V&V and accreditation agents).  Program management may ask to have the developers included in this list because acceptability criteria may (and probably should) impact their development effort.Good acceptability criteria enable a synergistic relationship between the accreditation agent, V&V agent and T&E team.  Everyone involved may not realize this but, sooner or later, they will and then they will want input into the acceptability criteria.  Getting management buy-in is critical to preserving acceptable acceptability criteria.  The accreditation agent is responsible to building the consensus for the acceptability criteria and that process should begin as soon as a draft of the acceptability criteria report is available for review.Therefore, the acceptability criteria report should exist as a separate document because of the enormous implications that the acceptability criteria hold.  This report is also used by many other parts of the simulation project team.  Therefore, the accreditation agent should build the acceptability criteria into a separate report that can be widely circulated to all of the interested parties.  They should not be embedded in the accreditation plan (or worse the V&V plan) because no one will look at them until the end when the acceptance recommendations are being defended, a terrible time to find that the acceptability criteria require revision.The accreditation report will reference the acceptability criteria report as an appendix as it portrays the evidence that the accreditation agent uses to build the accreditation recommendations.  The accreditation support package will also include the acceptability criteria report.3.  SummaryAcceptability criteria are part of the foundation upon which to build acceptance recommendations.  They should capture the detail of the simulation capabilities required to adequately serve an intended use.  Acceptability criteria may come from many different sources but regardless of the nature of their source they must be clearly traceable to the user needs to be credible.  Acceptability criteria need not be quantitative but they must always be observable or testable.  Good acceptability criteria capture the users needs but do not constrain the developer’s design space.  Good acceptability criteria can benefit greatly from good systems engineering products.  Acceptable acceptability criteria can only be formulated with buy-in from the sponsor, program management, T&E team, V&V agent, systems engineers, users and the developer, in short, everyone involved in a simulation program.  Lack of acceptable acceptability criteria will significantly complicate the accreditation agent’s job and endanger the value of the acceptance recommendations.4.  References[1]	U.S. Department of Defense, DoD Modeling and Simulation (M&S) Verification, Validation, and Accreditation (VV&A), Department of Defense Instruction Number 5000.61, 13 May 2003.[2]	T.I. Oren, “Concepts and Criteria to Assess Acceptability of Simulation Studies: A Frame of Reference,” Communications of the ACM, 24 (4), April 1981, pp180-189.[3]	R.E. Schellenberger, “Criteria for Assessing Model Validity for Managerial Purposes, Decision Science, 5 (4), October 1974, pp644-653.[4]	T.J. Teorey, “Validation Criteria for Computer System Simulations,” Proc. 3rd Symp. on Simulation of Computer Systems, Boulder, CO, 12-14 August 1975, USA, pp161-173.[5]	N.E. Ozdemirel, “Measuring the User Acceptance of Generic Manufacturing Simulation Models by Review of Modeling Assumptions,” Proc. 23rd Conf. on Winter Simulation, Phoenix, AZ, USA, 8-11 December 1991, pp419-427.[6]	F. Liu, M. Yang & Z. Wang, “Study on Simulation Credibility Metrics,” Proc. 37th Conf. on Winter Simulation, Orlando, FL, December 2005, pp2554-2560.[7]	S. Youngblood & R. Senko, “Acceptability Criteria: How to Define Measures and Criteria for Accrediting Simulations, Proc. Fall 2002 Simulation Interoperability Workshop, Orlando, FL, 8-13 September 2002, np.[8]	O. Balci, “How To Assess The Acceptability And Credibility Of Simulation Results,” Proc. 21st Conf. on Winter Simulation, 4-6 December 1989, pp62-71. [no][9]	O. Balci, “A Methodology for Certification of Modeling and Simulation Applications,” ACM Trans. on Modeling and Computer Simulation, 11 (4), October 2001, pp352-377.[10]	O. Balci & W.F. Ormsby, “Well-Defined Intended Uses: An Explicit Requirement for Accreditation of Modeling and Simulation Applications,” Proc. 32nd Conf. on Winter Simulation, Orlando, FL, 10-13 December 2000, np.[11]	R.W. Logan & C.K. Nitta, “Verification and Validation: Process and Levels Leading to Qualitative or Quantitative Validation Statements,” Trans. of the Society of Automotive Engineers, 113 (5), 2004, pp804-816.[12]	M. Bambach, G. Hirt & J. Ames, “Quantitative Validation of FEM Simulations for Incremental Sheet Forming Using Optical Deformation Measurement,” Advanced Materials Research, 5-6, 2005, pp509-516.[13]	M.H. Butterfield, “A Method of Quantitative Validation Based on Model Distortion,” Proc. IEE Colloq. on Model Validation for Control System Design and Simulation,” London, UK, 16 January 1989, pp3/1-3/13.[14]	D.K. Pace, “Simulation Validation Qualitative Assessment Process Improvements,” Proc. SCS Summer Computer Simulation Conf., 2003, pp690-695.[15]	S.Y. Harmon & Simone M. Youngblood, “A Proposed Model for Simulation Validation Process Maturity,” Jour. of Defense Modeling and Simulation, 2 (4), October 2005, pp179-190.[16]	Standards Coordinating Committee 10, Terms and Definitions, The IEEE Standard Dictionary of Electrical and Electronics Terms, 6th edition, IEEE Std 100-1996, Institute of Electrical and Electronics Engineers, Inc., New York, NY, 8 April 1997.[17]	W.L. Oberkampf & T.G. Trucano, Verification and Validation in Computational Fluid Dynamics, Report No. SAND2002-0529, Sandia National Laboratories, Albuquerque, NM, March 2002.[18]	T.J. Santner, B.J. Williams & W.I. Notz, The Design and Analysis of Computer Experiments, Springer-Verlag, New York, NY, 2003.[19]	K-T. Fang, R. Li & A. Sudjianto, Design and Modeling for Computer Experiments, Chapman & Hall/CRC, Boca Raton, FL, 2006.[20]	W.D. Kelton & R.R. Barton, “Experimental Design for Simulation,” Proc. 2003 Winter Simulation Conf., 2003, pp59-65.[21]	J.P.C. Kleijnen, “Validation of Models: Statistical Techniques and Data Availability,” Proc. 1999 Winter Simulation Conf., 1999, pp647-654.[22]	E. Saliby, “Descriptive Sampling: An Improvement over Latin Hypercube Sampling,” Proc. 1997 Winter Simulation Conf., 1997, pp230-233.[23]	S.M. Sanchez, “Work Smarter, Not Harder: Guidelines for Designing Simulation Experiments,” Proc. 2007 Winter Simulation Conf., 2007, pp84-94.[24]	S.Y. Harmon, “Proposed Meta-Data for Describing Simulation Representational Capabilities,” Paper No. 06S-SIW-056, Proc. Spring 2006 Simulation Interoperability Workshop, Huntsville, AL, 2-7 April 2006, np.[25]	D.C. Gross et al., “Report from the Fidelity Implementation Study Group,” Paper # S99-SIW-167, Proc. Spring 1999 Simulation Interoperability Workshop, Orlando, FL, 14-19 March 1999, np.5.  AcknowledgementsThe authors would like to thank the U.S. DoD Modeling and Simulation Coordination Office for its support for conducting this research and preparing this paper.Author BiographiesSCOTT HARMON is president of Zetetix, a small business specializing in modeling complex information systems.SIMONE YOUNGBLOOD is a member of the Principal Professional Staff at the Johns Hopkins University Applied Physics Laboratory (JHU/APL).  For the past ten years, Ms. Youngblood has served as the DoD VV&A focal point at the Defense Modeling and Simulation Office's VV&A Technical Director and is currently providing VV&A technical expertise to the Modeling and Simulation Coordination Office. Leveraging an extensive background in simulation development, modification and application, Ms. Youngblood has been active in the VV&A community for the past fifteen years.  She has a Master of Science in Computer Science from The Johns Hopkins University and a Bachelor of Arts in Mathematics and a Bachelor of Science in Computer Science, both from Fitchburg State College.