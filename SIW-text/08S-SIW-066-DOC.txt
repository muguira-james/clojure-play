Decomposing the VV&A Processes to Support Their TailoringS.Y. HarmonZetetixP.O. Box 705, Myrtle Creek, OR  97457(541) 863-4639 HYPERLINK "mailto:harmon@zetetix.com" harmon@zetetix.comSimone YoungbloodJohns Hopkins University Applied Physics Laboratory11100 Johns Hopkins Way, Laurel, MD  20723(240) 228-7958 HYPERLINK "mailto:Simone.Youngblood@jhuapl.edu" Simone.Youngblood@jhuapl.eduKeywords:validation, verification, accreditation, tailoringABSTRACT: Using risk as an effective tailoring mechanism for verification, validation and accreditation (VV&A) processes depends on understanding the factors that drive the risk as well as the VV&A tasks and techniques that can and should be implemented to address the use risks.  This paper describes one perspective of the activities and tasks that constitute the VV&A processes for simulations upon which to found risk-based VV&A tailoring.  This perspective describes a VV&A process model that builds from the experience gained in developing the Validation Process Maturity Model (VPMM) and IEEE Std 1516.4-2007, IEEE Recommended Practice for Verification, Validation and Accreditation of a Federation.  This model generalizes the guidance provided in IEEE Std 1516.4-2007 to apply to simulations beyond just federations, and extends that guidance to consider the broad range of possible sources of validation evidence.  It also establishes the basis from which to extend the applicability of the VPMM to a more practical extent of circumstances.  This model defines the accreditation process in terms of the activities for planning the accreditation effort, characterizing the simulation's use, constructing the validation referent and developing the acceptance recommendations.  It defines the V&V process in terms of the activities for planning the V&V effort, applying historical information, validating the simulation conceptual model, leveraging the developer's verification products, performing supplemental verification, validating the simulation results and integrating the V&V results into a coherent set of validation evidence.  This model of the VV&A processes defines a superset of the tasks that may then be tailored to best suit the limits of acceptable use risk and optimize the resources available for VV&A.  This paper describes the components of the high level VV&A activities and the products that they produce.1.  IntroductionOne approach to tailoring verification, validation and accreditation (VV&A) processes begins with a model that defines the spectrum of activities and tasks that could be done when performing VV&A then selecting and assembling the components of that model to best match the risk and resource constraints of the simulation effort while still adhering to the policies, standards and guidelines that may be relevant to the intended use [1].  This paper presents such a model of VV&A processes that practitioners can tailor to meet the needs and constraints of their particular situations.  This model can help the VV&A community realize the goal of tailorability that several sources have cited [1-3].Four assumptions underlie the construction of this VV&A process model:This general model represents the breadth of the VV&A process from user requirements to the acceptance decision for an intended use.Accreditation refers to the process for arriving at a decision to accept or accredit a simulation for an intended use.The primary purpose for performing verification and validation (V&V) is to collect the evidence needed to support an acceptance decision for an intended use.The simulation users or their representatives are responsible for ensuring the completeness and correctness of any statements of user needs or simulation requirements.The model in this paper integrates several process models described in the VV&A literature with recent experiences in performing VV&A to support actual simulation programs.  The result is a superset of the possible VV&A activities and tasks and the context in which those components can be assembled into a working VV&A process.  This model is described in two parts: an accreditation process model that encompasses the V&V process as a part and the V&V process model that produces the validation evidence needed by the accreditation process.2.  Accreditation Process ModelThe accreditation process takes the accreditation needs, user needs and referent source information as input and produces the acceptance recommendations.  Figure 1 illustrates the top level of this proposed accreditation process model.This model defines the accreditation process in terms of five activities:Plan the accreditation effortCharacterize the simulation’s useConstruct the validation referentCollect the validation evidenceDevelop the acceptance recommendationsThe sections below describe each of these activities.2.1  Plan the Accreditation EffortSeveral authors have included accreditation planning as a critical part of their VV&A process models [1, 4-8].  Accreditation planning consists of six tasks:Assess the accreditation needsDevelop the accreditation approachBuild the accreditation scheduleEstimate the accreditation costsPrepare the accreditation planExecute and evolve the accreditation planIn assessing the accreditation needs, the accreditation agent works with the program management to define the scope of the accreditation effort.  This knowledge enables the accreditation agent to develop the accreditation approach, schedule the accreditation tasks, estimate the costs of performing those tasks and capture this information in the accreditation plan.  The accreditation agent executes and evolves the accreditation plan throughout the accreditation effort.  This includes monitoring the accreditation and V&V efforts and adjusting the plans to better reflect any new information gained while executing the accreditation or V&V processes.The task of developing the accreditation approach may include defining and prioritizing the application requirements and defining the needed level of objectivity as described in Reference [5].As is true throughout this paper, the order of planning tasks given above does not necessarily imply either their order of execution or that there is no iteration involved within this activity or between the activities in the accreditation process.  In some cases, the accreditation agent can only formulate a rough plan that is then revised throughout its execution.2.2  Characterize the Simulation’s UseCharacterizing the simulation’s use includes several tasks that have been traditionally associated with the accreditation process [1, 4, 5, 9]:Collect the user needsVerify the requirements/objectivesDocument the intended useAssess the use risksDefine the conditions of expected useDevelop the acceptability criteriaSpecify the accreditation information requirementsThe first task collects and integrates the available documented requirements information (e.g., operational requirements document, capabilities development description) as well as elicits needs from the users or their representatives in order to develop as complete a picture of the user needs as possible.  Several authors have suggested verifying the consistency of any user requirements or objectives as well [1, 4, 9].  However, this process purposely avoids the notion of validating the users’ needs or requirements as suggested in Reference [10] because the authors feel that this intrudes sharply into the users’ domain.  As assumed above, the users or their representatives need to ensure the completeness and correctness of their requirements.  The VV&A team can only check the requirements for consistency if desired.  The consistency checking process may uncover problems in the requirements statements but the users or their representatives should resolve those problems.  Similarly, the VV&A team cannot tailor the application requirements as suggested in Reference [5].Knowledge of the user needs enables documenting the intended use, usually a brief statement of how the users will apply the simulation results.  This task commonly requires considerable iteration with the users or their representatives and, possibly, other interested parties (e.g., developer).  Iteration is also needed in assessing the use risks.  This task includes identifying the risk areas and defining the impact levels as described in Reference [5].  The resolution of the use risk assessment depends upon the scope of the accreditation effort and the amount of information available at the time.  In some cases, the risk assessment may only involve prioritizing the user needs.  In other cases, the use risks can be explicitly described subjectively or quantitatively through such techniques as fault tree analysis [11].  The accreditation agent should understand the risks of using a simulation as well as the simulation’s capabilities and limitations [12].  Failure to do this will impact the subsequent accreditation and V&V activities and tasks [13].The acceptability criteria specify the thresholds of functionality and error that a simulation needs to meet or exceed in order to be unconditionally accredited for an intended use [13].  Acceptability criteria may be derived entirely from user needs documentation (i.e., user requirements) [13], through an iterative review and interview process [12] or some combination of those techniques.  If sufficient information is available, the acceptability criteria may be prioritized according to risk [12].  The assessment of tolerable use risks can contribute to this prioritization.  Reference [13] discusses the derivation and properties of acceptability criteria in detail.  The conditions of expected use complement the acceptability criteria by defining the properties of the inputs that the user expects to provide to the simulation and the outputs that they desire the simulation to produce.  These conditions define the bounds of V&V testing [2], the domain of applicability of the acceptance recommendations [3, 14] and, thus, of the intended use [2, 3].  Like the acceptability criteria, the conditions of expected use need to be derived from the user needs documentation and interactions with the users or their representatives.Finally, the accreditation agent assembles the assessment of use risks, conditions of expected use and acceptability criteria into the accreditation information requirements that, together with the accreditation plan, guide the V&V effort.  In many cases, the accreditation agent may need to iterate between this activity and accreditation planning.2.3  Construct the Validation ReferentThe validation referent is the best available knowledge about the things being simulated [15, 16] and establishes the standard against which to measure simulation error.  Constructing the validation referent involves three tasks:Identify practical referent information sourcesCollect the applicable referent informationAssemble the validation referentThe sources for validation referents exist in many forms, ranging from subjective and qualitative descriptions to objective and quantitative descriptions [4]:Data from controlled experiments describing the functionality and performance of a system or phenomenon under well-known conditionsEmpirical data from observations of the behavior of a system or phenomenon under conditions ranging from unknown to well-characterizedExperience, knowledge and intuition of subject matter expertsMathematical models of the behavior of a system or phenomenon that have been validated against experimental or empirical dataOther simulations that have established credibility with the users for their particular intended usesCombinations of the types described aboveThe accreditation agent collects the information from these referent sources and assembles that information into a single consistent validation referent for the intended use.  It is important to understand the uncertainties associated with any referent because those uncertainties add to those associated with the validation evidence.  In addition, validation referents need to be credible to a simulation’s users since they will be impacted by the simulation’s deviation from the behavior of the thing being simulated.2.4  Collect the Validation EvidenceThe accreditation process and the acceptance recommendations both depend upon the evidence that the V&V process produces [2, 5].  The collection of this evidence involves eight V&V activities:Plan the V&V effortApply relevant historical informationVerify and validate the simulation conceptual modelPerform supplementary verification on the development productsLeverage the verification productsVerify and validate the data and knowledge setsValidate the simulation resultsIntegrate the validation evidenceThe V&V process model, presented in the following section, details these activities.2.5  Develop the Acceptance RecommendationsThe final step in the accreditation process develops the recommendations for the acceptance of the simulation [1, 4, 5, 7-9, 12, 17].  These are called the accreditation recommendations when a designated authority makes an official decision to accept a simulation for an intended use [4, 8, 9, 12].  Preparing these recommendations involves four tasks:Analyze the validation evidenceDevelop the acceptance recommendationsPrepare the accreditation report and accreditation support packageSupport the official decision to accreditMost authors who address the accreditation process realize that the accreditation agent should analyze the evidence produced by the V&V process to determine its implications for the intended use [4, 5-9, 17, 18].  Acceptance recommendations fall into five broad classes [2, 4, 5, 12]:Accept the simulation for the intended use without limitationsAccept the simulation with limitations on the useDefer the recommendations until further validation evidence is availableDefer the recommendations until the simulation is further modifiedReject the simulation for the intended use.In the many cases where the simulation is recommended for the intended use with limitations, the accreditation agent needs to identify the specific situations that the users should avoid to ensure that the simulation results will be sufficiently complete and correct for the intended use.  Regardless of the recommendations, the accreditation agent should assemble the accreditation plan, V&V plan, V&V report, accreditation report and acceptance recommendations into an archival package [1, 6, 12, 18].3.  V&V Process ModelFrom the assumptions above, the primary purpose for the V&V process is to collect the evidence upon which to base the acceptance recommendations.  Figure 2 illustrates the proposed model for the V&V process.  This figure aggregates the activities for performing supplementary verification and leveraging the verification products into a single box for verifying the development products.  However, these two activities are discussed in separate sections below.3.1  Plan the V&V EffortMost sources on VV&A processes addressed V&V planning [2, 4, 5-8, 12, 17].  The tasks for planning the V&V effort mirror those for accreditation planning with one exception:Develop the V&V approachBuild the V&V effort scheduleEstimate the V&V costsPrepare the V&V planExecute and evolve the V&V planUnlike the accreditation process, the V&V process does not need a task for collecting the V&V needs since the accreditation agent supplies those as the accreditation information requirements.  As with the accreditation process, the V&V agent executes and evolves the V&V plan throughout the V&V effort.  This includes monitoring the V&V effort and adjusting the V&V plan to better reflect any new information gained while executing the V&V processes.3.2  Apply Relevant Historical InformationIn those situations where a legacy simulation is applied directly to a new intended use or modified for an intended use, considerable historical information may exist upon which to base acceptance recommendations.  In fact, the entire V&V effort may focus upon the collection and analysis of historical information.  Applying the relevant historical information includes six tasks:Collect and analyze the V&V and testing historyCollect and analyze prior use historyCollect and analyze developer accounts of simulation capabilities and limitationsIdentify factors that may constrain use from the historical informationIntegrate the historical evidenceIdentify effective output sampling areas from the integrated historical evidenceApplying any historical information begins by determining how closely the prior intended uses match the current one.  In effect, this decides what part of the history is relevant to the current problem.  Historical information can come from prior V&V and testing activities, the records of prior uses, and the developer’s accounts of the simulation’s capabilities and limitations.  The V&V agent should then analyze the relevant historical record to identify the factors that may constrain use.  Then, a coherent picture of the simulation’s capabilities and limitations that the historical evidence depicts can be assembled.  If results validation will be performed, the V&V agent should use the historical evidence to identify possible output sampling areas to improve its efficiency.Only the availability of relevant historical information can reliably substitute for results validation since it will include the results of past V&V efforts.  This is only possible when the prior uses either individually or in combination cover the current intended use.3.3  Verify and Validate the Simulation Conceptual ModelMost of the surveyed sources recommended verifying and validating the simulation conceptual model [1, 2, 4, 6-10, 12, 14, 16, 18-20].  Pace has described the structure and content of a prototypical simulation conceptual model [21-23].  Eight tasks are associated with verifying and validating the conceptual model:Characterize conceptual model coverageCheck the internal consistency of the conceptual modelInfer the intended representational capabilities from the conceptual modelEvaluate the conceptual model validityVerify and validate the available scenariosIdentify factors that may constrain use from the conceptual modelIdentify effective output sampling areas from the conceptual modelIntegrate the conceptual model validation evidenceAs with all models, the simulation conceptual model abstracts the functionality of the simulation.  As a result, the V&V agent should begin by characterizing its coverage of the intended use and infer the simulation’s capabilities from the information that the conceptual model contains.  In between these tasks, the conceptual model verification involves checking it for internal consistency problems.  Depending upon the detail of the conceptual model, these tasks can vary from trivial to complex and time consuming.  After that, the V&V agent can evaluate the conceptual model against the acceptability criteria to determine its validity.  If available, the V&V agent should also verify and, if needed, validate the use scenarios [8].  The remainder of the tasks parallels those in applying the relevant historical information.As discussed earlier, the level of detail of conceptual models can vary wildly.  If the conceptual model includes the mathematical or theoretical model in addition to a high level description of the functionality of the simulation, it can be used to assess the simulation’s correctness against the validation referent.3.4  Perform Supplemental VerificationThis is the first of two development product verification activities.  How much the V&V agent contributes to development product verification depends strongly upon the amount and quality of verification that the developer performs (or has performed for legacy simulations).  The nature and degree of the verification that can be performed depends largely upon the development products available [24].  While most sources recommended verifying the simulation design products [1, 2, 4, 7, 9, 10, 12, 18] and the implementation products or executable model [1, 2, 4, 8-10, 12, 14, 18, 20], the developer may perform some fraction of the verification needed to support a simulation’s validation.  The V&V agent should leverage as much of the developer’s verification as possible and only perform what supplemental verification is needed to increase the confidence in the validation evidence to the desired degree.  Supplemental verification includes eight tasks:Collect the developer’s verification productsDetermine the scope of supplemental verification neededCheck for computational anomaliesAnalyze the development productsTest the development productsVerify the development products for standards complianceVerify the interoperability and compatibility of the development productsVerify the development products against the conceptual modelIn the tasks given above, the development products cover both the simulation design products (e.g., architectural design [1, 6, 7, 24], formal model [20], detailed design specification [1, 14, 20, 24]) and its implementation products (e.g., simulation code [2], interfaces [8]).  Design and implementation verification can serve many purposes but the V&V agent performs supplemental verification primarily to bolster the validation evidence.  As mentioned, the V&V agent begins the supplemental verification by collecting the developer’s verification products and determining how much additional verification is needed to achieve the desired confidence.The next three tasks address the types of verification that the V&V agent could perform including checking for computational anomalies (e.g., stability, convergence, representational errors), analysis (e.g., control sequence, data flow) and testing (e.g., static, dynamic).  Often standards play an important role in simulation development and the adherence to some standards could significantly affect validity.  Thus, the V&V agent may need to assure the compliance to those standards [6-8, 24] if the developer has not already done so to the desired degree.  Similarly, the V&V agent may need to verify the interoperability and compatibility of the implementation components both internally and externally (e.g., with other simulations in a federation) [7, 8, 24].  These supplemental activities produce evidence to assure that the simulation executable model is functioning correctly and could, therefore, perform validly.  After having gained confidence that the simulation’s execution is free from error, the V&V agent needs to verify that the development products faithfully reproduce the functionality described in the simulation conceptual model.  This task links verification firmly to the simulation’s validity (limited by the conceptual model’s level of abstraction) but depends upon the traceability information that exists.  The detail of this traceability information between the development products and the requirements through the conceptual model will ultimately determine what the V&V agent can infer about simulation validity from the verification products.The list of tasks given above does not include all of the verification tasks that some authors have suggested.  One source recommended validating the detailed design [6] and another recommended validating the executable model [23].  These steps were excluded because of their difficulty and limited value to the accreditation process.  Another source included the software quality and documentation assessments with the verification efforts [18].  Although this is a reasonable idea, these tasks are not simulation-specific and should be applied to all software regardless of its type.3.5  Leverage the Verification ProductsPrior to this activity, the V&V agent has collected the developer’s verification products, assessed their coverage to determine what supplemental verification is needed and performed that supplemental verification.  In this activity, the V&V agent applies the collected verification information to develop evidence on the simulation’s validity, add to the information to support the acceptance recommendations and provide guidance for output sampling.  This includes the following six tasks:Characterize the collective verification coverageInfer the representational capabilities from the verification productsEvaluate development product validity from the verification productsIdentify effective output sampling areas from the verification productsIdentify factors that may constrain use from the verification productsIntegrate the verification evidenceIn the first task, the V&V agent maps the verification information onto the simulation’s representational space to determine what of the required functional inventory that information addresses, what parts of that inventory are not covered and what parts that evidence from other sources (e.g., conceptual model validation) also considers.  With the verification coverage, the V&V agent can then ascertain the simulation’s representational capabilities and deduce the simulation’s validity from the development product verification by comparing those capabilities against the acceptability criteria.  The outcomes from these tasks create a foundation from which to develop effective output sampling guidance and identify factors that may constrain use.  Finally, the V&V agent needs to compile the evidence produced both by analysis of the developer’s verification activities and the supplemental verification into a coherent picture of the simulation’s validity painted from the verification products.As mentioned earlier, development product verification can improve the confidence in the entire body of validation evidence.  At this level, the V&V agent actually quantifies the impact on confidence that the verification evidence carries in its contribution.  Estimates of the verification coverage of the simulation’s completeness and correctness can also be used to estimate the probability that any other independent verification activities will produce the same conclusions.  Correlated verification results that produce consistent conclusions about the simulation’s capabilities improve the confidence in the evidence describing those capabilities.  Likewise, uncorrelated results weaken the confidence in that evidence and may require further exploration to resolve any disagreements.  In some cases, further verification can provide the information to resolve these problems and once again improve confidence in the combined evidence.3.6  Verify & Validate the Data & Knowledge SetsVV&A requires analysis of both models and data [14].  The activities heretofore have concentrated upon the simulation model.  This activity focuses upon the data that is input to that model or upon which that model sits to derive its output (e.g., capabilities description files).  Many of the sources surveyed emphasized the importance of verifying and validating the data that the simulation uses [1, 2, 12, 14].  The authors of this paper distinguish data and knowledge even though some could argue that knowledge is simply a type of data.  Knowledge differs from data when it employs a specific knowledge representation (e.g., production rules, semantic networks, neural networks).  Intelligent systems and human behavior representations commonly employ knowledge bases just as simulations of simpler physical phenomena employ databases.  The V&V of knowledge differs from that for data because it can leverage the vast resources of techniques, tools and guidance from knowledge-based system verification, validation, evaluation and testing [25].  Verifying and validating the data and knowledge sets includes the following eight tasks:Identify the data and knowledge sources and their pedigreesFind authoritative sources for data and knowledge with noneVerify the internal consistency of data and knowledgeVerify all data transformationsValidate data and knowledge sets where neededIdentify effective output sampling areas from the data and knowledge setsIdentify factors that may constrain use from data and knowledge V&VIntegrate the data & knowledge V&V evidenceData and knowledge set V&V should be performed because of the independence of those sets from the simulation and because the simulation relies upon those sets to produce its output.  Invalid data or knowledge will lead to invalid simulation results.  A data set essentially represents another form of model, a model that will affect the validity of the simulation’s results, and therefore needs to be validated.The tasks for data and knowledge V&V begin by identifying their sources and the pedigrees of those sources.  This takes into account the fact that in many cases the organizations responsible for producing the data for a simulation differ from the organizations that either develop or use the simulations.  The credibility of those data-producing organizations often weighs heavily in determining the credibility of the data itself.  Sometimes, data and knowledge comes to a simulation program without any obvious pedigrees.  In those cases, the V&V agent needs to trace the history of the data (through configuration management documentation if available) to identify the sources and describe the authority of those sources.  In some instances, the V&V agent may need to recommend using other data with pedigrees that are credible to the users.After establishing the pedigrees of the data and knowledge, the V&V agent should verify the internal consistency of the data sets, verify any transformations used to make the data accessible and meaningful to the simulation (e.g., units transformations, coordinate transformations) then validate the completeness and correctness of the data sets against the acceptability criteria.  The V&V agent can then use the data validation information to identify output sampling guidance and factors that may constrain simulation use.  The V&V agent then assembles the products from the data and knowledge V&V into a consistent package of evidence.3.7  Validate Simulation ResultsSimulation results validation is almost universally included in VV&A process models [1, 4, 6-10, 12, 14, 16-18, 20].  In this activity the simulation produces output through the execution of test scenarios, the V&V agent interprets the simulation’s representational capabilities from that output and then compares those capabilities against the acceptability criteria to determine validity.  Results validation involves seven tasks:Plan for results validationLeverage developer test results for validationCollect simulation output for validationVerify the simulation outputInfer the simulation’s validity from its outputIdentify factors that may constrain use from results validationIntegrate the results validation evidenceOn the surface, results validation appears straightforward.  That would be true if complete testing of the simulation was possible but, most times, complete testing is impractical and, many times, infeasible [3].  Therefore, results validation can only sample from the simulation’s behavior space and the V&V agent needs to infer validity from that limited sample.  The first task, planning for results validation, addresses the deliberate choice of where and when to sample.  This can be done through design of experiments techniques [26] among others.  The previous V&V activities should produce guidance for sampling simulation output.  The conditions of expected use can further constrain simulation testing by defining the boundaries of use.  This guidance can make a computationally intractable output-sampling problem tractable.  The V&V agent should also use the developer’s test results to improve the efficiency of output sampling.Some authors suggest that the V&V agent should verify the output that will be used for results validation [1, 17, 20].  This task can identify problems with the output before expending the resources needed for results validation.  Finally, the V&V agent needs to infer the simulation’s validity from its output, identify factors that may constrain use and integrate the results validation products into a coherent evidence package.3.8  Integrate the Validation EvidenceThe preceding V&V activities produce the evidence that this activity integrates into a lucid description of the simulation’s validity to support the acceptance recommendations.  This activity involves the four tasks:Infer simulation validity from collective V&V resultsForm the validation conclusionsPrepare the V&V reportSupport any archival of the V&V productsThe V&V agent should examine the evidence produced by the preceding activities, form a consistent picture of the simulation’s validity and estimate the confidence in that determination if needed and possible.  The V&V agent can also assemble the factors that may constrain use into a single set of use constraints then combine those constraints with the assessments of the simulation’s completeness and correctness for the intended use.  Then, the V&V agent assembles the results of this integration with the evidence produced by the prior V&V activities into the V&V report [1, 5, 6, 12, 17, 18].  The information in the V&V report should present the evidence upon which the accreditation agent can base the acceptance recommendations.  This activity may include any post-execution follow-up and archival performed [1, 9].4.  Summary & ConclusionsThis paper presents a comprehensive model of the VV&A processes and describes the activities and tasks that could be performed to meet a simulation program’s accreditation needs.  However, the authors do not intend VV&A practitioners to apply this model in totality or universally.  This model only supplies the cloth from which to cut the VV&A processes needed to suit the demands and resource availability of specific programs.  The authors also intend VV&A practitioners to use their knowledge of the risks that their simulation users can and cannot accept in their judgments of which VV&A activities and tasks to perform and at which maturity level to perform them.Research is ongoing to develop a comprehensive model of VV&A process maturity to accompany and complement this VV&A process model.  As Balci has observed “No rigid ‘cookbook’ simulation VV&A process can fit all situations all the time.” [3]  The process model presented in this paper is aimed at helping VV&A practitioners tailor their processes to best fit their situations.5.  References[1]	IEEE Computer Society, IEEE Recommended Practice for Verification, Validation and Accreditation of a Federation – An Overlay to the High Level Architecture Federation Development and Execution Process, IEEE Std 1516.4-2007, Institute for Electrical and Electronic Engineers, New York, NY, 20 December 2007.[2]	Australian Defence Simulation Office, Simulation Verification, Validation and Accreditation Guide, Department of Defence, Canberra, Australia, 2005.[3]	O. Balci, “Verification, Validation, and Accreditation,” Proc. 1998 Winter Simulation Conf., Washington, DC, 13-16 December 1998, pp41-48.[4]	Defense Modeling and Simulation Office, Verification, Validation and Accreditation (VV&A) Recommended Practices Guide (RPG), Build 3.0, Department of Defense, Alexandria, VA, September 2006.[5]	Synthetic Environment Coordination Office, Modelling and Simulation Verification, Validation and Accreditation (VV&A) Guidebook, Ver. 0.0, Canadian Department of National Defence, Ottawa, Canada, May 2003, (at <http://www.drdc-rddc.gc.ca/seco/documents/VVA_Guidebook_DND_SECO_May_2003_e.html>)[6]	E.H. Page, B.S. Canova & J.A. Tufarolo, “A Case Study of Verification, Validation and Accreditation for Advanced Distributed Simulation, ACM Trans. on Modeling and Computer Simulation, 7 (3), July 1997, pp393-424.[7]	O. Topcu, Review of Verification and Validation Methods in Simulation, TM 2003-055, Defence R&D Atlantic, Canada, April 2003.[8]	J. Graffagnini, S. Youngblood & R. Lewis, “An Overview of the Verification, Validation, and Accreditation (VV&A) Process for the HLA FEDEP,” Proc. 1999 Summer Computer Simulation Conf., Chicago, IL, 11-15 July 1999, pp421-428.[9]	R.O. Lewis & V.T. Dobe, “Verification, Validation and Accreditation (VV&A) Process Overlay for the FEDEP,” Paper 03S-SIW-085, Proc. 2003 Spring Simulation Interoperability Workshop, Kissimmee, FL, 30 March – 4 April 2003, np.[10]	D. Caughlin, “An Integrated Approach to Verification, Validation, and Accreditation of Models and Simulations,” Proc. 2000 Winter Simulation Conf., Orlando, FL, 10-13 December 2000, pp872-881.[11]	C. Mugridge, Verification, Validation and Accreditation of Models and Simulations Used for Test and Evaluation - A Risk/Benefit Based Approach, Defence Evaluation and Research Agency, Ministry of Defence, United Kingdom, March 1999.[12]	Navy Modeling and Simulation Office, Modeling and Simulation Verification, Validation and Accreditation Implementation Handbook, Volume I, VV&A Framework, Department of the Navy, Washington, DC, 30 March 2004.[13]	S. Youngblood & R. Senko, “Acceptability Criteria: How to Define Measures and Criteria for Accrediting Simulations,” Paper No. 02F-SIW-091, Proc. 2002 Fall Simulation Interoperability Workshop, Orlando, FL, 8-13 September 2002, np.[14]	R.G. Sargent, “Verification and Validation of Simulation Models,” Proc. 2007 Winter Simulation Conf., Washington, DC, 9-12 December 2007, p124-137.[15]	Defense Modeling and Simulation Office, DoD Modeling and Simulation (M&S) Glossary, U.S. Department of Defense, Alexandria, VA, nd.[16]	D. Girardot & R. Jacquart, “A Proposed Evolution of Validation Definition,” Proc. Foundations of VV&A, Tempe, AZ, October 2004, np.[17]	F. Liu, M. Yang & Z. Wang, “Study on Simulation Credibility Metrics,” Proc. 2005 Winter Simulation Conf., Orlando, FL, 4-7 December 2005, pp2554-2560.[18]	R. Stroud, Modeling and Simulation (M&S) Verification, Validation and Accreditation (VV&A), Teledyne Brown Engineering, 29 May 2005.[19]	S.R. Goerger, “Validating Human Behavioral Models for Combat Simulations Using Techniques for Evaluation of Human Performance,” 2003 Summer Computer Simulation Conf., Montreal, Canada, 20-24 July 2003, pp737-747.[20]	D. Brade, “Enhancing Modeling and Simulation Accreditation by Structuring Verification and Validation Results,” Proc. 2000 Winter Simulation Conf., Orlando, FL, 10-13 December 2000, pp840-848.[21]	D.K. Pace, “Conceptual Model Descriptions,” Proc. 1999 Summer Computer Simulation Conference, Chicago, IL, 11-15 July 1999, np.[22]	D.K. Pace, “Development and Documentation of a Simulation Conceptual Model,” 1999 Fall Simulation Interoperability Workshop, Orlando, FL, 12-17 September 1999, np.[23]	D.K. Pace, “Simulation Conceptual Model Development,” Proc. 2000 Spring Simulation Interoperability Workshop, Orlando, FL, 26-31 March 2000, np.[24]	R.S. Sandmeyer et al., “Comparison of VV&A for A2ATD Experiment One to DMSO Nine-Step Process,” Paper No. 13-95-039, Proc. 13th Distributed Interactive Simulation Workshop, Orlando, FL, 18-22 September1995, np.[25]	S.Y. Harmon, “Validation of Human Behavior Representations,” Proc. Paper 99S-SIW-048, 1999 Spring Simulation Interoperability Workshop, Orlando, FL, 14-19 March 1999, np.[26]	K. Hinkelmann & O. Kempthorne, Design and Analysis of Experiments, Vols 1 &2, John Wiley & Sons, Inc., Hoboken, NJ, 2005.6.  AcknowledgmentsThe authors would like to thank the U.S. DoD Modeling and Simulation Coordination Office for its support for conducting this research and preparing this paper.Author BiographiesSCOTT HARMON is president of Zetetix, a small business specializing in modeling complex information systems.  Mr. Harmon has been developing rigorous techniques for the validation of simulation federations and human behavior representations.SIMONE YOUNGBLOOD is a member of the Principal Professional Staff at the Johns Hopkins University Applied Physics Laboratory (JHU/APL).  For the past ten years, Ms. Youngblood has served as the DoD VV&A focal point at the Defense Modeling and Simulation Office's VV&A Technical Director and is currently providing VV&A technical expertise to the Modeling and Simulation Coordination Office. Leveraging an extensive background in simulation development, modification and application, Ms. Youngblood has been active in the VV&A community for the past fifteen years.  She has a Master of Science in Computer Science from The Johns Hopkins University and a Bachelor of Arts in Mathematics and a Bachelor of Science in Computer Science, both from Fitchburg State College.