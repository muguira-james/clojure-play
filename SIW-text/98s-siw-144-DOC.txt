INTRODUCTIONAs the ability of the modeling and simulation community has matured, the synthetic environment (SE) is being expanded to support more than just training.  One example of such an effort is the on-going program to examine the ability of the SE to support the Bradley A3 testing and evaluation.  As demonstrated previously in the Advanced Armor Technical Demonstration (A2ATD), the Abrams tank was evaluated in the SE and this work is now being applied to the Bradley A3.  In particular, the Bradley Synthetic Environment Live Fire (SELF) program, jointly supervised by the U.S. Army Operational Test and Evaluation Command (OPTEC) and U.S. Army Simulation, Training, and Instrumentation Command (STRICOM), will be conduct a series of tests in parallel with the Bradley A3 live tests to provide the test community with data to evaluate the test support capabilities of the SE. The SELF program mission is to examine how the SE could support Live Fire Testing (LFT).  Since the Bradley A3 must address crew proficiency, which is the ability of the vehicle under test (VUT) to acquire a target, generate a fire control solution, project its ordnance onto the target, and destroy the target, the SE could ideally the SE could be ideally suited for assisting in the evaluation of the first two objectives since the digitized traffic is the primary source for internal communication.  The purpose of the SELF program is to prepare for and conduct a series of tests in the SE of the updated Bradley’s ability to acquire and target an enemy vehicle.  The results of these SE tests, to be performed in conjunction with the live fire exercise in the live environment (LE), will be provided to OPTEC for evaluation.  OPTEC will review the Test Data Reports provided by STRICOM at each test milestone (i.e. LUT-1, LUT-2, and IOT&E) and assess the validity and contribution of the SE test execution.The SE can be used to aid in LFT by practicing the live fire test scenario, pre-testing the live fire exercise, and executing the LFT to collect and analyze data.  With this additional support to LFT, an enhanced testing process with more robust data can be performed and obtained.  The primary objectives of this effort are to develop and document a test plan for the SE to support LFT for LUT-1, as well as provide an infrastructure and supporting documentation.  The test procedures developed were executed to collect data comparable to the Bradley A3 LUT-1 LFT data.Due to many advances in the SE, an effort to effectively augment LFT and simultaneously create a SE to mirror the Live Fire Test environment is being investigated.  SELF testing for the A3 is sponsored by Office of the Secretary of Defense (OSD) and Live Fire Testing and Evaluation (LFT&E).  This effort investigated opportunities to LFT issues in the SE.  Although it is realized that the SE will never replace Live Fire testing, the SE provide additional data, insight into future test events, and an infrastructure in which to introduce variables without interfering with the developing system schedule.  The SELF effort explores SE approaches to testing lethality issues by utilizing the Bradley A3 development to illustrate the opportunities.CURRENT SYNTHETIC ENVIRONMENT SUPPORT OF THE BRADLEY A3 LIVE FIRE TESTINGThe Bradley Fighting Vehicle System (BFVS) A3 System Evaluation Plan (SEP) was reviewed and MOPs related to LFT were selected.  Measures of Performance (MOPs) amenable to testing in the SE were selected from those MOPs related to LFT.  The resultant MOPs were then filtered for MOPs planned to be tested during LUT-1. Test Areas Relevance to LFTThe first investigation of the A3 MOPs assessed the degree to which each of the 407 MOPs concerned LFT.  A three-level scale was used; the scale is described in Table 2.1 - 1.  Those MOPs that fell into either category 2 or 1 were selected for further evaluation.Table 2.1 –1  LFT MOP CategoriesValueCategory0Unrelated1Supporting LFT but not directly related2Directly related to LFTTest Areas Supportability in SEThe MOPs resulting from the first filter were then classified by their supportability in the SE.  Those MOPs whose LFT relevance score was greater than 0 (i.e., 1 or 2) and whose supportability score was greater than 1 (i.e., 2, 3 or 4) were selected to be addressed.  The five-level scale used is depicted in Table 2.2 - 1.Table 2.2 -1  LFT Supportability CategoriesValueCategoryDescription1Not SupportableIndicates that the SE could not support the MOP at all.2Low SupportabilityIndicates that the MOP could be studied to a limited extent in the SE but none of the issues could be resolved.3Moderate SupportabilityIndicates that some, but not all, of the issues could be resolved.4Conditionally SupportableIndicates that the MOP could be addressed in the SE if certain infrastructure enhancements could be implemented.5High SupportabilityIndicates that the issue could be fully supported.LUT-1 Synthetic environment live fire measures of performanceThe results of the MOP analysis included thirty-two (32) MOPs that were highly supportable and seventy two near term supportable MOPs, totaling one hundred four (104) SELF MOPs. These MOPs were then separated.  The Data Source Matrix provided as an appendix to the Bradley SEP was consulted.  This matrix lists the 407 MOPs and provides insight as to whether each MOP is tested under each of the test events. Thirty (30) SELF MOPs could be tested in the SE in support of LUT 1.   LUT-1 TEST PREPARATIONMuch planning and coordination took place prior to SELF LUT-1 testing.  The BFVS A3 Event Design Plan (EDP) for LUT-1 was reviewed in an effort to understand the scope of the activity.  This document contains a synopsis of LUT-1 test plans as well as the list of target MOPs and data elements for which the data would be collected.  An EDP was not developed for the SELF tests because of time constraints, although two documents were developed in preparation for SELF LUT-1: a Pattern of Analysis (POA) and a Detailed Test Plan (DTP).  The POA lists the target MOPs and data elements for SELF LUT-1 testing.  The DTP discusses the SELF LUT-1 test events and expected results.  Both documents were provided to OPTEC for review.  In addition to developing this documentation, many details were coordinated with OPTEC, TEXCOM, and TSM Bradley. Because this was the first SELF test event, details regarding data elements and data management were discussed until the day prior to testing.  STRICOM had to coordinate soldier support with TSM Bradley and TEXCOM because the A3 trained crews were assigned to LUT-1 training and testing.  Besides actual test preparation, STRICOM and U.S. Army Materiel Systems Analysis Activity (AMSAA), co-developed the Verification and Validation (V&V) Plan for this phase of the SELF infrastructure.  SYNTHTETIC ENVIRONMENT INFRASTRUCTURE TO SUPPORT SELF LUT 1 TESTINGIntegration of the SELF components and software began a week prior to SELF LUT-1 testing. The Bradley Advanced Training System (BATS) was used as the VUT. The BATS was developed as a prototype trainer to train the crews scheduled to support LUT-1 of the BFVS A3 system.   The SELF effort utilized the BATS and integrated flat target representations similar to the panel targets used at the range.  These targets were incorporated into the BATS visual system along with the AMSSA ballistic models which provided a higher fidelity fly-out simulation and error modeling into the BATS.  Bradley Advanced Training System (BATS)The BATS was selected to function as the VUT since its crew station and software is representative of the Bradley A3.  BATS is a complete gunnery training system that consists of a high fidelity gunner and commander's weapon station, and an Instructor Operator Station to control gunnery exercises and provide After Action Reporting.  The BATS employs a visual and computational system to provide the required images to develop and sustain gunnery skills for the BFVS A3.   The computational system also utilizes tactical software that closely replicates actual fire-control functions and capabilities of the BFVS A3.  The BATS provided the BFVS A3 physical and functional fidelity. The purpose of the simulator was to represent the A3 and be the VUT.  Because the simulator used actual vehicle hardware and software, hence accurately representing the A3, it will enable an easier to link into other simulations as opposed to physically cabling a moving vehicle while still represent the actual vehicle.To support the SELF LUT-1, the BATS included simulation of the vehicle dynamics, weapons, fire control and sensors, survivability, communication, and navigation of the BFVS A3.  Figure 4.1 - 1 illustrates the functionality of the BATS Simulator.  The BATS design goal was be to minimize required unique simulator/trainer software and hardware by using actual BFVS A3 components (hardware and software). Figure 3.1 - 1 BATS Crew Station Functional DiagramTargetsLUT-1 Live Fire exercises were run at Hastings Range in Fort Benning, Georgia.  The panel targets on Hastings Range vary between BTR frontal view, BTR flank view, BMP frontal view, and BMP flank view, the flat panel targets used for SELF LUT-1 testing were synthetically modeled after the actual plywood thermal targets that are used at Hastings Range.  These models were developed using the Night Vision Lab (NVL) Paint the Night (PTN) texture files as a guideline. Figure 3.2 - 1 depicts the flank view of a BMP panel target. EMBED MSPhotoEd.3 \sFigure 3.2 - 1  Flank BMP Panel TargetFigure 3.2 - 2 depicts the frontal view of a BMP panel target. Figure 3.2 - 2  Front BMP Panel TargetSTRICOM intends to explore using NVL PTN for 2nd generation FLIR representation for future tests. Error ModelingAMSAA algorithms were implemented into the BATS software although the A3 Ballistic Software Solution (BSS) provides for cant error correction and environmental parameters. In addition, three different round dispersion tables were used throughout testing.  These tables generated a random dispersion of shots based on the first shot.  Analysis ToolsThe Verification, Validation, and Accreditation Test Tool (VVATT) was used prior to SELF testing to collect data during the target acquisition verification and validation (V&V) tests.  The VVATT was developed for A2ATD and allowed the user to generate reports necessary to assess the simulators target generation capabilities.  The Simulation Analyzer (Simulyzer) was used to collect Distributed Interactive Simulation (DIS) Protocol Data Units (PDUs) on the network.  These PDUs provided information such as shot, hit and kill statistics, intervisibility over time, an engagement timeline, and vehicle information.  Simulyzer displayed the data being collected in graphical displays during the V&V and SELF LUT-1 exercises.  A software module was added to Simulyzer that parses the logged data file and populates a database used for data reduction and analysis.VERIFICATION AND VALIDATIONPrior to the test, AMSAA led a verification and validation (V&V) test of the SELF infrastructure hardware and software integration.  The V&V focused on three areas for SELF LUT-1:  form, fit, and function; target acquisition; and delivery accuracy of the 25-millimeter gun.  This section decribes the testing done in each of these areas as well as the findings of each test.FORM, FIT, AND FUNCTIONThe test of form, fit, and function consisted of two parts: use of the simulator by soldiers (accompanied by interviews with the soldiers), and testing of the simulator by test personnel.  The issues that were addressed can be divided into three categories: crew station layout, sensor views, and sound system.Test DescriptionAfter each use of the simulator by soldiers, test personnel conducted crew interviews to gather data on the operation and ease of use of the simulator controls, the quality of the sensor views, and the appropriateness of the sound system.  The questionnaires used to guide the interviews are included in Appendix A of this document.  A total of six soldiers were interviewed:  two subject-matter experts (SME) who helped to conduct crew training on the system, and four crewmen who had completed training on the system and participated in the SELF LUT-1 test.Evaluation of form and fit relied primarily on documentation of the contractor’s development process, which used the system design to guide the layout of the BATS.  Test personnel examined the BATS simulator, test controls, and switches for functionality; measurements for form and fit were only taken if the soldier interviews revealed a potential problem.Testing of the operation and ease of use of the simulator controls had the following goals:Crew Station (Layout).  Verify that the controls, displays, instruments, placards, and hand grips are in locations in accordance with the specification.Controls and Switches (Operation).  Verify that the correct inputs and outputs are provided to and from the switch.  Verify that the control or switch provides the correct state change.  Verify that the state changes occur in the time required after switch or control activation.Controls and Switches (Location).  Verify that the location of the control or switch is in accordance with the System Requirements Document.Controls and Switches (Type).  Verify that the control or switch represents the required type (such as push-button or toggle).Controls and Switches (Simultaneous Operation).  Verify that simultaneous actuation of multiple controls or switches does not affect the operation of the simulator and that the required hierarchy is followed.  (Limited verification was done in this area, demonstrating only functions that have a likelihood of simultaneous actuation.)Instruments (Operation).  Verify that the instruments included in the simulation in support of crew operation provide the functions described in the specification.  Verify that the instruments are provided the correct inputs from the simulator.For the evaluation of the sensor views, test personnel fired 25-millimeter ammunition from the BATS at short, medium, and long ranges; observed other entities firing weapons; observed the motion of other entities; and observed the motion of the BATS simulator.Testing of the sensor views had the following goals:Munition Impact.  Verify that the munition impact signature appears, for both hits and misses, for each sensor view;Detonation.  Verify that the detonations of other weapons appear, for each sensor view;Motion of Other Entities.  Verify that the motion of other entities is appropriately smooth, for each sensor view;Motion of Simulator.  Verify that the motion of the simulator (both as the vehicle moves and as the turret slews) is appropriately smooth, for each sensor view;Changing Field of View.  Verify that the simulator changes Field Of View (FOV) at the same rate as the actual system.For form, fit, and function, the sensor views were evaluated to ensure that munition firing and motion are represented realistically.  A more detailed evaluation of the sensor views was conducted during the image generation and target acquisition evaluation.For evaluation of the sound system, test personnel listened at each situation where sound should be transmitted to verify that a sound is provided.Limitations and AssumptionsForm and fit were not comprehensively assessed by taking measurements and comparing to the system specifications.  As a result, all of the differences between the BATS and the system may not have been noted; however, by interviewing the soldiers after each use of the simulator, those differences detectable during use of the simulator are likely to have been captured.Generally, one purpose of the sensor view test is to judge the ability of the computer image generator (CIG) to continue to present smooth images during an intense battle.  Since the scenarios were limited to two targets at a time, that aspect was not addressed.The test of the sound system was highly subjective.Observations and EvaluationFor the capabilities addressed in the SELF LUT-1 test, the BATS simulator generally looks, feels, and functions like the actual system.  The sensor views presented by the BATS are smooth and appropriate, and the sounds produced by the BATS are appropriate.  The following differences were noted:Sensitivity of Handgrips.  The handgrips on the BATS for both the commander’s and gunner’s station are more sensitive than those on the system.  Five of the six soldiers noted this problem, and it was confirmed by the engineering staff; however, it could not be corrected prior to the SELF LUT-1 test.  This problem could have an impact on the test results, possibly decreasing accuracy or increasing engagement times.Slew Rate for Target Hand-off.  The slew rate of the turret during target hand-off on the BATS is faster than that of the system.  (While the gunner is engaging a target, the commander can acquire the next target.  When the first engagement is complete, the commander can designate the new target, which will slew the turret to that new target.)  Four of the six soldiers noted this problem, and it was confirmed by the engineering staff; however, it could not be corrected prior to the SELF LUT-1 test.  This problem could have an impact on the test results, possibly decreasing engagement times.Mount for Data-Entry Tool.  Two of the soldiers noted that the commander’s data-entry tool in the BATS appeared to have a different mount than in the system, and it could not be stowed.  This problem is unlikely to impact the test.Ghosts of Deleted Entities.  In order to simulate the behavior of the target boards, dead targets would disappear from view.  (Actual target boards will fall down when hit.  Ordinarily, simulated vehicles that are hit and killed will remain on the battlefield burning.  In order to mimic the target boards, the targets that were hit disappeared.)  Occasionally, one of the dead targets appeared in the sensor view for a split second.  Since targets will not normally disappear, and since this problem is unlikely to have an impact on the test, the problem was not corrected prior to the test.TARGET ACQUISITIONThe Bradley sensor view is displayed to the gunner and commander through a CIG.  The display must allow the soldier to detect, recognize, and identify targets in a manner consistent with system characteristics.  This implies that the CIG must portray targets consistent with their infrared and visual signatures.  Thus, the evaluation of the target acquisition representation of the Bradley simulator must include an evaluation of the Bradley CIG.The Bradley employs the following sensors:Direct-View Optics (DVO);Improved Bradley Acquisition System (IBAS), equipped with a Foward-Looking Infrared (FLIR) sensor and a Day Television (TV) Camera;Commander’s Independent Viewer (CIV),  also equipped with FLIR and Day TV;Vision blocks (periscopes).Test DescriptionThe test of target acquisition consisted of three phases:  use of the simulator by soliders (accompanied by interviews with the soldiers), testing of the simulator by test personnel, and a stationary target acquisition test.After each use of the simulator by soldiers, test personnel conducted crew interviews to gather data on the target acquisition capability of BATS.  The questionnaires used to guide the interviews are included in Appendix A of this document.  A total of six soldiers were interviewed.Test personnel conducted target acquisition testing to check the following:Verify that the proper sequence of control actions must be performed before target acquisition is allowed.Verify that a second target may be acquired while one target is being tracked.Verify that the proper symbology is displayed at the appropriate times during target acquisition.Two soldiers participated in a stationary target acquisition test.  The soldiers were presented with a series of stationary targets on National Training Center terrain.  The six targets presented were a Blue tank (M1A2), a Red tank (T72), a Blue armored fighting vehicle (M2A3), a Red armored fighting vehicle (BMP-2), a Blue wheeled vehicle (HMMWV), and a Red wheeled vehicle (BTR-60).  The ranges were short, medium, and long; these ranges are representative of the ranges used during the SELF LUT-1 test.  Each soldier observed a total of 54 targets during a test, plus four null targets (scenes where no target was presented).  The test was conducted with each soldier using the IBAS FLIR in wide FOV, the IBAS FLIR in narrow FOV, the IBAS Day TV in wide FOV, and the IBAS Day TV in narrow FOV.Limitations and AssumptionsThe SELF LUT-1 test was executed using target boards rather than vehicles (which is consistent with the live test), but the target acquisition test was conducted using vehicles.  Target boards were available for only two of the targets.  Using a mixed set of vehicles and target boards would likely have given the soldiers an additional cue for those two vehicles, so vehicles were used for all six targets.The live test was conducted on Fort Benning.  An area with similar features on the NTC terrain database was chosen for the SELF LUT-1 test; this area was also used for the stationary target acquisition test.  Generally, locations would be chosen at random throughout the terrain database.  However, for this test, the BATS was stationary and located in the same location it would be during the SELF LUT-1 test.  The background was varied by rotating the turret in different directions.Due to the very limited sample size of two soldiers, no statistical analysis has been performed on the results, and the results should not be used to draw any conclusions about the capability of the BATS in general.  However, since the soldiers who participated in the test also participated in the SELF LUT-1 test, the data here provide an indication of their capability during the test.Observations and EvaluationFor the ranges used in the SELF LUT-1 test, The BATS provided the soldiers with a capability to detect and identify targets adequate for conducting the test.  This is a summary of the problems discovered during soldier interviews and testing:The FLIR image presented by the BATS is sharper than that of the system.  The problem was noted during soldier interviews and confirmed by the stationary target acquisition test.  The effect of this problem is that the BATS has a better ability to identify targets using the FLIR in wide FOV at the ranges of interest.  The ability of the BATS to detect in either FOV and the ability of the BATS to identify in the narrow FOV matched that of the system.  Because standard practice is to detect in wide FOV and identify in narrow, this problem was unlikely to have any impact on the SELF LUT-1 test.The Day TV image presented by the BATS might not be as sharp as that of the system.  The results of the stationary target acquisition test indicated a potential problem with the ability to identify targets in the wide FOV.  However, the ability of the BATS to detect in either FOV and the ability of the BATS to identify in the narrow FOV matched that of the system.  Since the wide FOV is used primarily for detection, this potential problem was unlikely to have any impact on the SELF LUT-1 test.The vision blocks on the BATS may present a clearer image than those on system.  No further investigation was conducted on the vision blocks.  This problem had no impact on the SELF LUT-1 test because the vision blocks were not used.DELIVERY ACCURACYPrior to the SELF LUT-1 test, the standard Army algorithm for delivery accuracy was implemented in the BATS.  (This is the same algorithm used in the Close Combat Tactical Trainer.)Test DescriptionTest personnel fired several rounds at targets to verify that the simulator interfaces with the Ballistic Software Solution (BSS) properly, and test personnel conducted testing to check the following:Verify that fratricide is possible by engaging a friendly target.Verify that the simulator cannot engage targets successfully beyond a maximum range.Verify that the simulator cannot engage targets successfully within a minimum range.Verify that the simulator will miss a target when it is aimed away from that target.Verify that the simulator cannot fire at a rate faster than the BFVS A3 system.Limitations and AssumptionsAlthough the methodology for a moving firer was implemented in the BATS, it was not tested.  (This methodology adds an additional dispersion when the firer is moving.)No testing of the TOW was conducted.Observations and EvaluationThe BATS has the standard algorithm for delivery accuracy correctly implemented.  No problems were noted.SELF LUT 1 TEST EXECUTIONSELF LUT-1 testing for the Bradley A3 was completed during the week of 17 Nov 97.  The SELF LUT-1 tests replicated the Live Fire exercises planned for the LE A3 LUT-1.  Test data was obtained using the BATS.  The BATS was selected as the VUT because it is being developed in parallel to the BFVS A3.  Testing was conducted at Fort Benning, Georgia, where both the simulator and the Bradley A3 crews were located prior to LUT-1. The gunnery exercises planned for the Bradley A3 LUT-1 were carried out in the BATS simulator two weeks prior to LE LUT-1 tests.  Data was collected to satisfy 30 MOPs that focused on the ability of the Bradley A3 to perform Target Acquisition and Delivery Accuracy.  The SELF LUT-1 explored the Bradley’s lethality,  not its vulnerability.The End-to-End Data Run was conducted on Monday, 17 November 1997, and Record Tests were conducted from 18-20 November 1997. A Bradley A3 trained commander and gunner (TEXCOM LUT-1 Crew #5) ran through a series of scenarios that required target detection and engagement. These day and night engagements were developed to generate the data necessary to satisfy MOPs concerning target acquisition, target tracking, and delivery accuracy of rounds.  The crew's actions were recorded by the BATS Instructor Operator Station (IOS) and analyzed as DIS PDUs by the Simulyzer. These scenarios allowed the test team to evaluate how well the vehicle under test meets the requirements stated in the BFVS A3 MOPs. Visiting test personnel also noted that this system captured all the relevant data that is not always possible in the field. It also provided a controlled environment to assess the system under test.  The SELF data was then provided to the Live Fire testers in support of the A3 system LUT-1 in December 1997 and will be referenced when OPTEC generates a SELF system assessment in March 1998.SELF LUT 1 TEST DATA ANALYSISThe SELF LUT-1 data analysis was not complete at the publication of this paper.  This information will be contained in further details in the SELF LUT-1 Test Data Report.  OPTEC will also generate a SELF system analysis with a comparison of this data to the LUT-1 data.  SELF LUT 2 preparation and plannningPrior to SELF LUT-2, other enhancements will be made to the existing SELF infrastructure.  These modifications are currently being investigated in an effort to better coordinate with OPTEC and TEXCOM prior to LUT-2.  In addition to enhancing the BATS infrastructure to support SELF LUT-2 testing, several options are being examined.  STRICOM intends to incorporate portions of other LFT&E funded modeling and simulation programs, such as Virtual Target Program and Hit and Miss Assessment.  The virtual target program can provide high fidelity target geometry models that can be configured to customized requirements and provide traceability from hardware to low and high fidelity models.  The virtual targets provide increased realism to the synthetic environment by providing high fidelity texture maps for use by lower fidelity models.  Current virtual target models include the T-72M, the BTR-70, the Bradley M3A1, and the MQM-107D.  The hit and miss program can provide more realistic visuals impact effects.  This will  provide higher fidelity visual representation to the soldier, thus providing a more realistic environment in which to engage targets.  Another area of interest is more in depth assessment of the target damage areas.  Also, it may become necessary to incorporate more realistic ballistic data, thus requiring classified data. Finally, the lessons learned from the SELF LUT-1 test have provided STRICOM with the insight necessary to better prepare for and execute LUT-2 testingAbout the AuthorsAngela M. Alban is a Systems Engineer at TASC, in Orlando, Florida.  She is the program manager for the SELF program.  Angela has been working in the modeling and simulation industry since receiving her Bachelor of Science degree in Mathematics and Computer Science from Emory University in Atlanta, Georgia.  Angela is currently pursuing a Master of Science degree in Computer Engineering from the University of Central Florida in Orlando, Florida.Alesya Paschal is a Systems  Engineer  at  U.S. Army Simulation, Training, and Instrumentation  Command (STRICOM) in Orlando, Florida.   Ms. Paschal received a Master of Science degree in Industrial Engineering  from  Texas  A&M  University and a Bachelor of Science degree in Industrial Engineering from University of Tennessee.  Ms. Paschal’s research interests include the simulation of C4I systems and Simulation Based Acquisition. Michael J. McCarthy has worked as an Operations Research Analyst at the US Army Materiel Systems Analysis Activity for the past ten years, since receiving a Bachelor of Science degree in Applied Mathematics from Carnegie Mellon University.  His experience includes leading one of the Anti-Armor Advanced Technology Demonstration experiments; participating in each of the Rapid Force Projection Initiative experiments; and working with constructive combat models such as Vector-in-Commander and the AMSAA Division-Level Wargame.Louis Santone is employed by American Management Systems, Inc, (AMS) based in Fairfax, Virginia on a subcontract to VRC Corporation of Alexandria, Virginia, supporting Operational Evaluation Command (OEC).  After receiving a Bachelor of Science degree in Education from Millersville University, he began his career at Aberdeen Proving Ground (APG) performing aerodynamic analyses for the Ballistic Research Laboratory. He then worked at Frankford Arsenal at Philadelphia, Pennsylvania and received a Master of Science degree in Mathematics from Temple University.  After ten years at Frankford Arsenal, where he was a weapons systems analyst for systems ranging from small arms to recoilless rifles to anti-ballistic missile defense system, he joined the staff of the Strategy and Tactics Analysis Group (now Concepts Analysis Agency).  Mr. Santone left military agencies for civilian agencies by joining the National Bureau of Standards. He retired from the Department of Commerce and joined AMS where he has worked in support of OEC and its predecessor OTEA for the past 14 years.SYNTHETIC ENVIRONMENT SUPPORT OF LIVE FIRE (SELF) TESTINGFOR THE BRADLEY FIGHTING VEHICLE (BFV) A3Ms. Angela Maria AlbanTASC, 12443 Research Parkway, Suite 202, Orlando, FL 32826(407) 275-8755 X249, Fax: (407) 275-7399, amalban@tasc.comMs. Alesya PaschalU.S. Army Simulation, Training, and Instrumentation Command (STRICOM), Orlando, FL 32826 (407) 384-3867, DSN 970-3867, Fax (407) 384-3830, alesya_paschal@stricom.army.mil Mr. Michael McCarthyUS Army Materiel Systems Analysis Activity (AMSAA),392 Hopkins Rd, Aberdeen Proving Grounds, MD 21005-5071278-6612, DSN 298-6612, Fax (410) 278-6585, mmccarth@arl.milMr. Louis SantoneU.S. Army Operational Test and Evaluation Command (OPTEC), 4501 Ford Avenue, Alexandria, VA 22302-1458(703) 681-, DSN 761, Lou_Santone@mail.amsinc.comKeywords: Bradley A3, Bradley Fighting Vehicle System (BFVS), Live Fire Testing (LFT), Synthetic Environment (SE), Synthetic Environment Live Fire (SELF), Limited User Test 1 (LUT-1), Limited User Test 2 (LUT-2), Bradley Advanced Training System(BATS)ABSTRACTThis paper provides a discussion on the design, execution, and analysis of the Synthetic Environment Live Fire (SELF) support of the Bradley A3 program.  The SELF infrastructure included various simulations to mirror the Bradley M2/M3A3 Live Fire Test environment.  SELF Limited User Test (LUT) 1 testing was executed in November 1997 to effectively augment Live Fire testing by providing additional live fire test data, insight into future live fire test events, and an infrastructure that could introduce variables without interfering with the developing system schedule.   This effort specifically focuses on Bradley testing while also exploring how a SE can assist Live Fire Testing and testing in general.Using the M2/M3A3 System Evaluation Plan (SEP), the Bradley M2/M3A3 Measures of Performance (MOPs) were reviewed to determine the live fire MOPs then, of these live fire MOPS, where the SE could assist live fire issues.  Based upon the MOPs analysis, a SELF test architecture was designed comprised of current simulation technology.  The SE for LUT 1 utilized the Bradley Advanced Training Simulator (BATS) as the test vehicle.  This trainer was then integrated with the Synthetic Theater Of War-Architecture (STOW-A) synthetic environment (principally Modular Semi-Automated Forces (ModSAF) and simulation support services), flat model targets based on Night Vision Labs Paint the Night for imagery, and AMSAA's ballistic models for higher fidelity fly-out simulation as well as error modeling.  The resulting SE replicated the Bradley Fighting Vehicle System (BFVS) lethality performance.The SELF LUT 1 tests were executed in this SE and the data was analyzed by evaluating how well the vehicle under test met the stated requirement of the Measures of Performance (MOP).  This analysis resulted in similar conclusions as the actual Live Fire Test and revealed Live Fire Test and Synthetic Environment infrastructure improvements areas.This paper primarily discusses the results of SELF LUT-1 testing.  The paper will outline the steps required to execute Bradley SELF LUT-1 testing and perform the required test data analysis as well as discuss SE areas that can be used to augment Live Fire Testing (LFT) and SE improvements needed for future LFT support.