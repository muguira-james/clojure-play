A Survey of the Theory and Practice of Simulation InteroperabilityRobert FranceschiniTom ClarkeBrian GoldiezAllison GriffinAndre HuthmannGuy SchiavoneBrad SchrickerInstitute for Simulation and Training3280 Progress DriveOrlando, FL  32826-0544{rfrances, tclarke, bgoldiez, agriffin, ahuthman, gschiavo, bschrick}@ist.ucf.eduKeywords:Distributed Simulation, InteroperabilityABSTRACT:  While international forums such as SIW have been established to address simulation interoperability, the fundamental issues of interoperability have not been settled.  Attempts to define simulation interoperability have generally followed one of two strategies:  top down or bottom up. A top down strategy attempts to define simulation interoperability abstractly based on mathematical principles.  The current state of such approaches is that they tend to be theoretical in nature and not practically useful (although the hope is that they might be sufficiently extended to be practically useful in the future).  A bottom up strategy attempts to define simulation interoperability concretely by specifying in detail all of the data values, timings, interactions, etc., that must be exchanged between simulations and hoping that the resulting simulations are interoperable.  These approaches are meant to provide practical results, but it is still unproven if any of them provide insight into whether the resulting systems are truly interoperable.  This paper is an initial survey of the literature of simulation interoperability.  The discussion is organized around the top down and bottom up divisions discussed above.  The paper provides a single source that summarizes what has been explored in defining simulation interoperability.IntroductionA large literature on simulation interoperability has been developed over the last decade, as researchers and practitioners have explored seamless connections of heterogeneous simulators.  Given the tremendous interest in this topic (around 300 papers annually are published in forums such as SIW), one would reasonably expect that fundamental issues of the nature of interoperability have been settled.  However, this is not the case.  There is not even a commonly accepted precise definition of simulation interoperability.Difficulties with Understanding Simulation InteroperabilityWebster’s Dictionary [1] defines interoperability as the “ability of a system (as a weapons system) to use the parts or equipment of another system.”  Several difficulties arise when attempting to formally understand simulation interoperability.  Useful definitions of simulation interoperability must encompass many intuitive notions about simulations.  First, what is it that is being made “interoperable”?  On the surface, the answer to this question seems obvious enough.  In the distributed simulation community, we are usually interested in taking two different simulations A and B, connecting them together in some manner to form a new simulation system C, and ensuring that C produces meaningful results.  A close look at this, however, reveals the complexity of the problem.  What are simulations A and B?  Are they single “simulators” that each control a single simulated entity?  Are they simulation systems (such as computer generated forces) that each control many simulated entities?  Are they simulations of portions of simulated entities (such as one simulation of the cognitive processes of a human and another simulation of the physical representation of the same human)?  Are they individual modules within an overarching simulation infrastructure (for example, a model of a tank and a model of an airplane in the same computer generated forces system)?  Are they themselves distributed simulations?  For that matter, the term “interoperability” is often applied to cases in which A is a simulation and B is another kind of device (such as a ruggedized command and control workstation that is actually used in the field). Second, what is the purpose of declaring two simulations to be interoperable?  This depends on the intended use of the simulation.  Simulations that are not interoperable for one use might be interoperable for another use.  For example, suppose A and B from the previous discussion are two tank simulators whose movement models are compatible but whose weapons models are not.  If the combination C is used only for movement training, then A and B can be considered interoperable; however, if C is used for weapons training, then A and B are not interoperable.  While widely applicable use cases may be identified, different uses generally impose different interoperability requirements.  It seems unlikely that a definition of simulation interoperability will emerge that applies to all potential uses of simulations without explicitly addressing these uses.Third, can interoperability be measured quantitatively?  Intuitively, there are probably degrees of interoperability, and simulations that are “almost” interoperable might still be useful (for example, the tank simulators from the previous paragraph).  A quantitative measurement for interoperability would be extremely useful both for evaluating systems and for attempting to rectify interoperability inconsistencies.  However, it is not at all clear what complete set of factors should be measured to cover interoperability.Attempts to Define Simulation InteroperabilityAttempts at defining simulation interoperability generally follow one of two strategies:  top down or bottom up.  A top down strategy attempts to define simulation interoperability abstractly based on mathematical principles.  The current state of such approaches is that they tend to be theoretical in nature and not practically useful (although the hope is that they might be sufficiently extended to be practically useful in the future).  A bottom up strategy attempts to define simulation interoperability concretely by specifying in detail all of the data values, timings, interactions, etc., that must be exchanged between simulations and hoping that the end result is interoperable.  These approaches are meant to provide practical results, but it is still unproven if any of them provide insight into whether the resulting systems are truly interoperable.Paper StructureThis paper collects a representative sample of the simulation interoperability literature and organizes it into top down and bottom up categories.  This organization presents a succinct and contemporary view of simulation interoperability research.  This paper also explores the topic of interoperability as it has been applied to areas outside of simulation.  This is intended to highlight good ideas that might be useful in the context of simulation interoperability.The paper is organized as follows.  Section 2 discusses top down approaches.  Section 3 surveys bottom up approaches, and breaks these into categories corresponding to different simulation components.  Section 4 introduces interoperability ideas in other areas.  Section 5 concludes the paper by identifying potential areas of further work.Top Down Approaches to Simulation InteroperabilityWhile fewer in number, top down approaches provide a more fundamental approach to trying to solve the issue of interoperability in its entirety.  According to Butler [2][3], one approach is to break interoperability down into three types:  weak, strict, and strong.  Weak interoperability involves simply establishing proper communications between simulators.  Strict interoperability involves measuring the amount of interoperability in a mathematical way.  Strong interoperability involves guaranteeing that interoperation is correct in every possible aspect of interaction.  Butler further proposes three layers of interoperability:  synthetic, logical, and physical.  The synthetic layer is the top layer and describes the synthetic environment at a high level.  The logical layer is the middle layer and describes entities and their interactions.  The physical layer is the lowest layer and describes specific realizations of the higher layers in terms of implementation technology.  [2][3]A similar approach breaks interoperability into categories.  However, in this case the categories suggested by Rush and Whitely [4] describe different types of interoperability instead of different levels.  Those different types are sensing interoperability, direct interactive interoperability, indirect interactive interoperability, associative interoperability, and communications interoperability.  Sensing interoperability is defined as the ability of one entity to observe other entities in real-time.  Direct interactive interoperability is the ability of one entity to react in real-time to physical interactions with other entities, collisions, and movement over terrain, for example.  Indirect interactive interoperability is the ability of an entity to react in real-time to indirect physical interaction with other entities, such as getting hit by shellfire.  Associative interoperability is the ability of an entity to be associated with another entity such that it can be represented in real-time as being physically connected to another entity.  An example of this would be a tank being towed or a missile being carried on an aircraft.  Communications interoperability is the ability of one entity to have real-time communications with other entities.  [4]Cortes and Schiavone [5] suggest that the level of interoperability between simulators can be estimated by defining a set of “interoperability parameters.”  These parameters are separated into Run-Time-Database, Computer Image Generator, Display, and Human Factors.  Every parameter is explained by its importance to interoperability, but there is no means for testing each of them.  Cortes and Schiavone propose that two identical simulators can be used to test and estimate the influence of the mentioned parameters to interoperability.  [5]Miller [6] proposes yet another way to break down interoperability.  Miller lists a set of requirements that must be met to ensure that interoperability occurs between all simulations.  Miller does not state whether this list is a subset of requirements.  These requirements are:Common definitions of simulation eventsStandardizing simulation protocolsStandardizing communication protocolsStandardizing bandwidth requirementsProcessing loads on existing simulationsMiller further proposes that if any of these requirements are not met, then interfaces must be created to allow these requirements to be met.  Two examples of this situation involve dead reckoning algorithms and terrain representation.  First, it is very common for the dead-reckoning algorithms in different simulators to be different.  In those cases, an interface must maintain tables of the current states of all simulation entities.  Whenever enough of a discrepancy occurs, the interface must update the state so all other entities can extrapolate correctly.  Second, it is common for different simulators to have different protocols responsible for the terrain and visual effects.  A common result of this can be vehicles seemingly submerged in the ground or floating above the ground.  Again, in order to correct this difference in simulation protocols, an interface would have to be designed to allow smoother consistency.  [6]Harmon [7] develops a top down decomposition of the interoperability problem in a mathematical way.  Harmon describes axioms and theorems comprising an interoperability formalism.  The main advantage is that this top down approach could eventually provide definitive guidance on developing interoperable simulations.  The main disadvantages are that the definitions and theorems do not seem to be carefully considered, and the formalism is tedious.  [7]Another approach has been to attempt to demonstrate interoperability and, based on observations, attempt to define it.  According to Kwan and Garingo [8], an example of that is the Modeling and Simulation Resource Repository (MSRR), which is being implemented as a distributed system of repositories of models, simulations, object models, algorithms, instance databases, data sets, data standardization, and administration products, tools, and utilities linked by the Internet.  Object-oriented technology is used so that MSRR objects are a collection of interfaces that describe a standard set of external behaviors together with their encapsulated data.  [8]Myjak, et al. [9] examine interoperability from the point of view of allowing multiple different High Level Architecture Run Time Infrastructures (HLA RTI) to be interoperable.  (As a bit of background, there are HLA specifications for the kinds of services that an RTI must provide, but there is not a concrete specification for the RTI itself.  This leads to the possibility—which has actually been realized—of many different RTI implementations that are not able to work together within the same exercise).  Importantly, Myjak, et al. step back from HLA and explore the meaning of interoperability in general before examining the HLA RTI in particular.They propose the following interoperability definition:  “there is a functional equivalence provided by interchangeable components within a system or process in order to allow its components to be able to work together with no prior agreement over an agreed upon data communications path.”  They further identify a layered interoperability architecture involving application (the ability to achieve identical simulation results no matter which federate owns entities), model (the “protocol” for communicating state and behavior information between federates), service (sharing RTI state information, i.e., the states of the RTI “services”, between RTIs), and communication (specifics of how messages are transmitted between RTIs).They continue by exploring hypothetical use cases of interoperating RTIs and describing the different levels outlined above in each use case.  While they focus on HLA and the RTI, there are some high-level ideas that may be useful in the definition of interoperability and the layers proposed. [9]Smyth [10] discusses a framework for interoperability.  He casts interoperability in terms of completeness, consistency, and coherency.  Completeness implies that all of the effects and agents of the simulation are useful.  Consistency results from models successfully being complete or providing the desired level of interactions.  Coherence is the combined effects of completeness and consistency when all of the simulation elements are integrated.  He goes on to recommend that the simulation community can learn much from other communities that have developed interoperability standards.  [10]Boys and Darlington [11] propose one theoretical problem with interoperability.  They suggest that interoperability and validity are at odds.  That is, interacting two valid simulators does not necessarily imply that the combined simulation will also be valid, due to unexpected interactions and unknown intended use of the interacting simulations.  The precise reason is that a simulator can be a valid representation of a system for a given purpose, but invalid for other purposes.  [11]Time management is an interesting interoperability issue as well.  Time is discussed heavily by Herbst, et al. [12].  Specifically, they explain, in a modest level of detail, the hierarchical structure of control provided by the RTI in time management.  [12]Sutton [13] has suggested another top down strategy for defining interoperability using an engineering approach.  He proposes a notion of interoperability as a probability function based on, “…The successful interoperation of all subscribers in a network under specified conditions for a given mission time.”  Sutton also describes intrinsic components of interoperability as physical (networks, nodes, …), technical (rules, interfaces, …), functional (elements, tasks, …), and mental (perception, behavior, …).  These levels are listed as concentric circles of activity with physical being the inner level.  Progression to outer levels involves increasing levels of abstraction.  He concludes that a single definition for interoperability must be established.  [13]An additional notion to consider is Federation Object Model (FOM) interoperability as part of HLA interoperability.  According to Harless [14], one issue of concern when dealing with a FOM is the limitation in applicability and the need to track FOM updates.  This limitation could present problems to interoperability paradigms tied too closely to something like HLA.  However, having a general approach causes interoperability problems because of interpretation and implementation variations.  It is suggested that a mapping engine be developed to transform information from one FOM to a form understandable by another FOM.  This mapping requires creation of an API and an approach for configuring a FOM.  [14]In short, the top down approaches attack the problem of defining interoperability from a theoretical point of view.  They attempt to form generalities about simulators.  At this time, these approaches offer little practical use other than as backbones for further research ideas.Bottom Up Approaches to Simulation InteroperabilityJust as important as the top down approaches are the bottom up approaches.  These attempt to solve sub-problems that can be composed into the overall interoperability issue.  This section begins by examining two interoperability issues that cut across the entire simulation design:  miscorrelation and resolution.  Following those are discussions of interoperability issues in several fundamental simulation components:  time, space, computing systems, human-computer interfaces, and aggregation and disaggregation.MiscorrelationA questionnaire conducted by the Institute for Simulation and Training [15] attempted to establish a clear picture of what the simulation industry thinks are the most important interoperability issues.  In this questionnaire, data was considered in four layers: source data, runtime data, image data, and perception/behavior.  In the end, research strategies for interoperability were divided into three components: 1) Developing means for measuring miscorrelation at each layer; 2) Understanding causal relationships; and 3) Controlling causes of miscorrelation.  It is no coincidence that miscorrelation is mentioned twice.  Miscorrelation is an enormous issue in any simulation and, especially, in simulation interoperability.  From a bottom up approach, it is extremely important to address the issue of miscorrelation between the different simulators. [15]ResolutionHoffman [16] points out that another important interoperability issue is resolution.  Specifically, that is, image resolution and communication resolution.  For example, consider connecting two dissimilar simulators for the purpose of having two players engage each other.  The simulators may be different because of a host of reasons.  In an exercise such as this, the goal is to determine whether the players have equal skills.  If there are differences in visual resolution, some sites in the simulators will appear the same, while others will not.  In this case, one player may see a more realistic view than the other.  When one player sees the real environment and the other does not, then there is an advantage.  [16]While that is an issue of resolution that deals with image representation, not all issues of resolution involve that aspect.  Another deals with simple communication between the simulators.  Most simulators, when initially developed, were not intended to be part of any larger simulation.  As a result, they often have their own, unique communication protocols.  Because of this, communication mechanisms have to be established in order to allow the group of simulators to be able to communicate the proper information to each other.  Humphrey [17] suggested a database for entities and their physical behavior/capabilities to ensure correct entity description in the different simulators and proper communications between them.  [17]Other individuals, such as Woodard, et al. [18] have suggested methods for estimating interoperability based on the following resolution factors:The fidelity of individual simulationsThe difference in simulation fidelitiesThe accuracy of the simulationsThe realism of the simulationsThe problem with this is that it does not provide concrete objective and subjective testing methods for the estimation of interoperability.  [18]TimeFrom the time of early rockets to satellites to the Global Positioning System, there is a strong history of needing to synchronize time.  Time is essential to almost all simulations.  It becomes doubly important when dealing with interoperability because the different simulators must be able to keep a standard time between them.  Many arguments have been made about how to deal with synchronizing time, including the recent argument by Paterson [19] that DIS should encode time using binary coded decimal to avoid computer cycles required to do time conversions.  [19]To that end, many have tried to define requirements that must be met in order to allow the synchronization of time between two or more simulators.  For example, it has been suggested by Ferguson et al. [20] that minimum levels of time and spatial coherency must be achieved for a given exercise and that simulations must congregate about a time-space area with degree of interoperability dependent upon the area.  Warfighter-in-the-loop experiments determine the shape of the interoperability area.  A correlation construct introduces the concept of absolute and relative correlation, the latter being the tendency to cluster.   Absolute correlation makes reference to entity characteristics against a known reference.  Correlation metrics under consideration include local terrain, intervisibility metric, and target detection metric, all with respect to time.  [20]SpaceAccording to Woodard [21], another important issue concerning simulation interoperability is spatial representation.  If space is not defined and represented the same way on dissimilar simulators then one can gain an advantage over the others.  When connecting two or more simulators, they must all have a consistent terrain representation.  Otherwise, one or more of the simulators has an advantage.  A tremendous amount of work has been done to ensure that terrains are represented the same in all simulators of a system.  To that end, it has been proposed that three actions must be accomplished to ensure an interoperable environment between two simulators:Establish a common source data base format and contentResolve coordinate transformationsDevelop terrain mobility modelsOne possible solution to these would be an environment server.  The environment server contains all physical models used by the simulation participants.  The environment server also provides services such as mobility, collision, and intervisibility computation.  Interoperability is achieved because the computations are centralized and therefore consistent. [21]Many believe that an environment server is absolutely necessary in order to establish interoperability.  According to Kamsickas [22], for simulated entities to participate effectively and realistically in the same exercise, they must have access to the same simulated environment information.  One of the major problems with a distributed environment is that there is no assurance of complete correlation among the distributed environments.  This results in unfair interactions between entities and redundancy of effort.  An environment server would maintain a correlated and common view of the ground truth for all entities, eliminating anomalies.  Visual system interoperability is one of the largest contributors to the “fair fight” issues between simulators.  A common environment would help level the playing field.  [22]Others, such as Ferguson et al., [20] propose another method dealing with space and terrains that can be employed to allow interoperability.  For instance, one idea is a new database standard that characterizes the entire electronic battlefield, including cartography, platforms, munitions, electromagnetic data, environmental effects, and much more.  With respect to visual systems, algorithms to manage and control scene loading can have a significant effect on correlation.  It is also suggested that correlation can be tracked from source data through the image generation pipeline.  [20]Goldiez [23] provides a useful first step in assessing the contribution of terrain to interoperability considerations.  Goldiez suggests the classification of components of a simulation system into four areas:  entity physical model, entity behavior model, space, and time.  He focuses on the space component and in particular, terrain databases (terrain databases representing surfaces are a common space representation in ground-based military simulations).He notes the definition of interoperability as provided in the DoD M&S Master Plan:  “the ability of a model or simulation to provide services to, and accept services from, other models and simulations, and to use the services so exchanged to enable them to operate effectively together”.  He further notes that this definition does not provide technical insight into how to achieve interoperability and in fact is quite broad when viewed from a technical perspective.  It is suggested that interoperability could be described in a layered fashion, with layers of compliance (the simulation relates to an authority), compatibility (the quantitative similarity between two simulations), and interoperability (ability of entities to interact acceptably).  It is also mentioned that one can examine the information communicated between simulations to determine whether they are interoperable.He explores the question of how much terrain correlation is needed to achieve interoperability between simulations.  The general strategy used was to start with a terrain database T, a simulation scenario S, and a computer generated forces (CGF) system that runs S.  Several trials of S are run on the CGF system using T.  Then T is modified in a controlled way (essentially degrading T) and the trials are repeated.  This process is continued until an observer of the scenario recognizes deviations from acceptable behavior by the CGF entities; these deviations are attributed to poor spatial correlation caused by the degradation of T.  Goldiez used the Range 400 terrain for experiments and found that resolutions of 2m and 3m were acceptable, but starting around 5m or 10m resolutions the CGF behaviors degraded.  This behavior degradation was correlated with a critical value measurement made using the ZCAP tool, and it was found that the behavior degradation could be related to having a critical value of 1 or more.  Goldiez concludes with the presentation of a methodology to be used to assess interoperability of simulators based on terrain measurements.  [23]Purdy [24] describes the issue of non-reciprocal visibility, which is best understood as a subproblem of simulation interoperability for a particular class of simulations.  Consider two entities A and B, owned by two different simulators S1 and S2, respectively.  Non-reciprocal visibility occurs when the line of sight between A and B as computed by S1 does not match the value computed by S2.  Note that here line of sight refers to the lowest level geometry calculations, not any higher level processing involving focus of attention, etc.  He proposes a DIS PDU to handle the problem.  The PDU potentially contains two kinds of information: a list of entities that cannot be seen by another entity controlled by the sending simulation (as computed by the sending simulation) and information about the structure of the terrain in the sender’s database.  The PDU’s role is to provide enough information to other simulations on the network so that consistent visibility results can be computed.This approach seems to have some limitations.  First, line of sight calculations are performed very often; it seems problematic that a simulation could afford to wait until it receives this PDU from other simulations before proceeding with its own calculations.  Second, there is no discussion of how multiple simulations should coordinate their use of the PDU (for example, what if S1 and S2 both send incompatible PDUs simultaneously?)  Third, he seems to be focused on “simulators” such as the SIMNET M1 tank simulator (the key characteristic being that there is one entity per simulator); what happens to this proposed approach when a CGF system with potentially hundreds of entities per simulator is used?  It appears that the size of this PDU coupled with the frequency of sending these PDUs will be prohibitive for network traffic.  [24]Hardis and Sureshchandran [25] address terrain correlation.  In particular, they document the process of constructing terrain databases for computer image generators.  Different vendors follow this general process although the details of creating a database for a particular image generator are slightly different.  They exploit the general process by pointing out that terrain correlation testing can be effective during particular phases of the general process.  Generally, they suggest that early application of correlation testing is most effective in ameliorating errors, due to dependencies incorporated in subsequent stages of terrain database generation.  While they focus on terrain databases for image generators, the approach probably would apply to terrain databases for computer generated forces as well. [25]The measurement of terrain database correlation is a key component in the overall assessment of interoperability.  In visual systems, the traditional method of evaluating terrain databases correlation involves a side-by-side viewing of a series of fly-overs or drive-throughs of the terrain data, sometimes without particular regard to sampling sufficiency.  In this type of approach the characterization of correlation is categorical and subjective.  Quantitative correlation testing offers several potential advantages over the traditional approach, including a greater degree of automation, and the obtainment of objective, repeatable results.  Schiavone et al., [26] provide a comprehensive review of quantitative test procedures in their review, and several examples are reviewed below.One correlation test procedure proposed by Schiavone, et al., [27] is designed to yield a measure of the culture correlation and line-of-sight correlation, with the assertion that such measures constitute a necessary subcomponent of the overall measure of interoperability between two terrain databases.  Input is in the form of statistical samples of the runtime databases.  These provide a small set of global measures to assess interoperability.  Diagnostics tests have also been designed to return information that will be useful in the mitigation of errors.  This information is in the form of deterministic representation of the runtime database.  The sampling strategy is a one-to-one mapping.  That is, each type of feature should map to only one texture or a unique set of textures.  [27]Schiavone, et al. [28] presented another method based on cross-correlation for assessing the degree of match between two terrain data sets.  The method uses the normalized cross-correlation between two sets of gridded data.  If the data are triangulated, as is typical of a simulation terrain database, then the summation operations needed for the cross-correlation calculation are performed on a sampled version of the triangulated data.  This method was used on two versions of the Ft. Hunter-Liggett terrain, one originating from the Battlefield Distributed Simulation-Developmental (BDS), the other from the SIMNET version.  The correlation technique found a shift between the two terrain databases of 1140 meters E-W and 260 meters N-S in the high-detail inset region of the BDSD data.  Much of this shift could be accounted for by a confusion between the NADS27 datum and the WGS84 datum.  [28]In a 1992 paper, McCarter [29] examines issues of correlation requirements with respect to simulation application type and terrain database elements.  He classifies visual simulation applications for computer image generators by mission type such as “ground”. “rotary-wing”, “fixed-wing”, or “ship”, and suggests required levels of correlation for different classes of terrain data for each mission type.  He resolves level of correlation into three classes (low, medium or high), and assessment of correlation is accomplished through side-by-side visualization.Yet another method to test terrain correlation was suggested by Clarke, et al. [30]  In this case, the testing was done between HLA and DIS simulations.  A SAFOR-like automated entity randomly samples the terrain database and collects significant data to estimate the terrain correlation.  The main difference, in concept if not tools, is the use of a non-intrusive means to test terrain correlation.  [30]Computing SystemsSome groups have attempted to define interoperability with respect to the computer systems that the simulators are running on.  According to Tufarolo, et al., [31] one practical example of needing to get around different computing systems involves attempting to interoperate the US DoD Joint Simulation System (JSIMS) with the Japanese Defense (JDA) Japan Defense Simulation System (JDSS).  Technical issues for interoperating these two include:Agreement on interchange techniquesAlignment of models and algorithmsSharing of models and algorithmsForce resolution and detailData representationIn this case, these issues must be resolved in order to achieve interoperability.  [31]In order to try to cope with the differences in computing systems that exist for different simulators, it is important to understand what is affected by these differences.  Knight [32] points out that a comprehensive network that allows two simulators to interoperate must consider all of the functional areas represented by the simulations being connected.  They include visual, com./nav., tactical, instructional, and more.  Additionally, the real measure of interoperability for human-in-the-loop simulators will be maintaining crew workload at levels commensurate with the operational world.  In order to maintain workload, users need to impose maximum fidelity differentials among simulators.  These fidelity differentials will vary based on intended use of the networks that connect the simulators.  [32]Human-Computer InterfacesOne important idea to realize when considering interoperability, according to Cortes and Schiavone [5], is that the perceptions generated by the human-computer interface of a simulator can affect interoperability.  Interoperability must be defined, in any sense, in terms of perceptions.  Since the human-computer interface greatly affects humans’ perceptions in a simulator, it can have great effect on determining whether a group of simulators is interoperable.  This idea has been taken farther by suggesting that an estimation of the interoperability of a group of simulators be defined using a set of “Interoperability Parameters.”  These parameters include human factors and display, among others.  Both of these play a role in the human-computer interface, which can thus play a big role in interoperability. [5]Furthermore, the human-computer interface can be sub-categorized into at least two other parts.  Those categories are Images/Graphics and Command, Control Communications, Computers and Intelligence (C4I).Images/GraphicsOften, the same visual databases input into different image generators render different visuals.  The differences in images can be determined by the comparison of their visual cues as important to the simulation.  Therefore, it is crucial to define what is important with regards to an image.  Sureshchandran [33] points out several problems that can occur when rendering images from two dissimilar image generators.  The different parts of the database may be translated, scaled and rotated in three dimensions.  Or, the viewpoint parameters also might be different.  In some research in sensor transformation, the coefficients of the polynomials are estimated from a set of corresponding “landmark” points.  The “landmark” points are selected on features then compared to other images.  This method attempts to solve the problems involved when features are not represented identically on two or more different simulators in a system.  [33]An example of this, according to Butler [2], is the “Architecture Design-Focus System” (ADFS).  This system was composed of the Aviation Testbed (AVTB), the Crew Station Research and Development Facility (CSRDF), and a proposed site of upgraded flight simulators.  There are significant differences in these three simulators.  A major problem is the visual feedback received by the trainees due to the different image generator and display systems used in the simulators.  Some of the differences are update rate, diurnal effects (day to night), visual rendering range, and moving models.  Specifically, advantages with regards to line of sight manifest.  This is due largely to a miscorrelation between the databases.  [2] Moskal, et al. [35], describe an experimental methodology for measuring interoperability.  The procedures essentially involve testing human ability to perform certain tasks based solely on a visual stimulus provided by a simulator’s image generator.  The experiments are set up using a “fully interoperable” simulation, and then degrading the performance of the visual system of that simulation in a controlled way.  The main importance of this paper is the discussions of interoperability offered near the beginning.They quote two other papers ([34] and [18]) for an interoperability definition:  “the ability of two or more systems to perform a coordinated task with the expressed intent of achieving a common goal.”  They propose that one can view interoperability as “a measure of consistency between internal representations of the common, simulated environment produced by networked simulators.”  Accordingly, this consistency can be measured by examining visual parameters such as correlation of terrain, scene density, and resolution.  Furthermore, they point out that an important component of interoperability is human perception and performance (i.e., how the system is to be used).  They suggest a definition for “sufficient interoperability”: agreement between the desired training outcome and the simulated outcome.  This definition obviously presupposes a particular use of a simulation (training) and it leaves open the question of what “agreement” means (how much deviation can be tolerated and still have sufficient interoperability?)  [35]C4IC4I is an excellent example of attempts at simulation interoperability.  In one experiment conducted by McKenzie [36], C4I is interfaced with the Joint Simulation System (JSIMS).  C4I is categorized into four sets:Comprehensive – the sum totalSimulation—interoperableSimulation—averseC4I uniqueThe JSIMS to C4I interface is being implemented by the External Systems Interface (ESI) Working Integrated Project Team (WIPT).  Federation Object Models (FOMs) are being used and the result will be HLA compliant.  The JSIMS Core Interface for External Systems (JCIES) provides a set of common services necessary to support C4I, Simulations, Simulators and Live Ranges.   Federation Objects are exchanged via a Mission Space Object (MSO) Avatar.  The MSO Avatar provides JSIMS simulation behavior as a service to external systems that cannot directly support JSIMS requirements.  Remaining issues such as a routing space for MSO Avatar services are being investigated.  [36]Carr [37] describes interoperability of C4I systems with simulation.  In this context, interoperability is taken to mean that the C4I system does something meaningful in the context of the simulation, and that communications flow between the C4I system and the simulation.  Carr does not explore (or even offer) a definition of interoperability; rather he jumps right into the explanation of how C4I systems are made “interoperable” with simulation.  This is a good example of the type of discussions that appear in the interoperability literature.  [37]Aggregation and DisaggregationAccording to Karr and Root [38], one need that can complicate interoperability immensely is the need for aggregation and disaggregation.  When multi-resolution simulators are used the complexity of making them interoperable increases dramatically.  Specifically, one problem in particular can make things very difficult.  It is more complicated to make the disaggregate-to-aggregate transition.  A judgement is required as to when to collect the individual indirect fire events and aggregate them into a message perceptible at the aggregate or constructive level.  One solution was to use foreknowledge provided by the operator of when the individual firing events need to be aggregated.  Another difficulty is determining the intended aggregate target on the basis of indirect firing impacts.  [38]Another example of problems with interoperability when dealing with aggregation and disaggregation follows, as pointed out by Bundy, et al. [39]  The system was an interface between the Air Force War SIM (AWSIM) and ModSAF. The approach was to divide the simulation arena geographically so that when an entity crossed into the ModSAF window, it was disaggregated into simulated aircraft that were controlled by ModSAF.   Specific issues discussed are problems engendered by the disaggregation/aggregation of entities, the problem of maintaining a coherent view of the battlefield from both virtual and constructive simulations, the problem of providing a consistent mechanism for command and control for both virtual and constructive simulations, and the problem of managing the combined constructive and virtual simulation.Following a presentation of the attributes of aircraft in ModSAF versus flights in AWSIM, three methods of rectification are discussed:Adding a new modeling or activity to a simulationAdding new data elements or representations to the simulationCreating new information required at the agg/reagg interfaceAll three approaches were used to rectify virtual aircraft controlled by ModSAF with construction flights controlled by AWSIM.  [39]Interoperability in Domains other than SimulationOne field in which interoperability has received recent study is multimedia [40].  In this field, interoperability is an important issue of open distributed processing.  Interoperability is described as follows: “an implementation of an open distributed processing platform developed by one manufacturer should be able to interwork with a different platform implemented by a potentially different manufacturer” [40].  The overall definition of open distributed processing as the capability to interact with services from anywhere in the distributed environment without concern for the underlying environment is a key idea.  Three main systems have been studied in the multimedia context:  Reference Model of Open Distributed Processing (RM-ODP), Common Object Request Broker Architecture (CORBA), and Distributed Computing Environment (DCE).RM-ODPThe aim of RM-ODP is to “enable the development of standards that allow the benefits of distribution of information processing services to be realized in an environment of heterogeneous IT resources and multiple organizational domains” [41].  So RM-ODP is no specific standard but gives the framework to enable specific standards to evolve. The RM-ODP standard consist of four parts: an overview, basic modeling concepts for distributed systems, the architecture which constrains the basic model by introducing concepts which a conformant RM-ODP system should possess and finally architectural semantics which provide a formalization of the concepts behind RM-ODP.Objects in RM-ODP can be accessed by multiple interfaces. The objects were defined in terms of templates. The important point of the modeling concept is that it makes no assumptions about its applications.  Another part of the RM-ODP approach to a standard is the concept of viewpoints to cover the wide scope and inherent complexity of the domain of open distributed processing. There are five viewpoints: enterprise, information, computational, engineering and technology. Through these viewpoints the system should be specified but without losing the consistency between the viewpoints.  RM-ODP does not define a conformance standard (interoperability), but gives the framework for definition of 'conformance points'. At these points the conformance is tested by giving a stimulus and monitoring the resultant behavior of the system.  RM-ODP is a meta-standard defining a framework to enable the emergence of more specific object-oriented technologies for open distributed processing.CORBAThe particular goal of CORBA is to provide a mechanism by which objects make requests and receive responses.  The architecture of CORBA consists of four components: The Object Request Broker (ORB), the Object Service, the Common Facilities, and the Application Objects. The ORB is the logical heart of the architecture and provide the means for objects to interact in a heterogeneous environment. The Object Services are a set of objects offering basic services to the platform, the Common Facilities are objects offering service at a higher level. The interfaces of the objects in CORBA are described in terms of an Interface Definition language (IDL). IDL closely resembles C++, but with added features to support distribution.A concept of Bridges allows CORBA to interact if the systems/domains run with different protocols. But this bridge approach does not fulfill the portability requirement of conformance.  CORBA is a specific object-oriented technology providing interoperability of objects in a heterogeneous environment by identifying an architecture with associated interfaces and IDL language.  CORBA does not provide an explicit support in the areas of quality of service (QoS) management, real-time synchronization or multiparty communication.DCEDCE is a client-server architecture achieving integration between a range of key services. But it is not object oriented it does not fulfill the requirements for multimedia.RM-ODP could be a good framework for developing a standard in open distributed processing of simulations. But the development of the requirement list and a common understanding of “conformance” for simulations seems to be the biggest problem. Perhaps a combination of the approach for multimedia via RM-ODP and a “conformance” defining application like CCTT is a way to a common understanding of interoperability for simulations? [40]Summary and Future Research DirectionsResearch is necessary to address critical issues in interoperability.  The authors intend to extract prominent features from the papers cited herein and create a process for quantitatively determining simulator interoperability.  Currently, key concepts to be studied include a categorization of intended use of interoperable simulators, an optimal method for simulator decomposition for pairwise comparison of interoperability characteristics (e.g., terrain data base comparison), and integrated simulator and system issues when simulators interact.ReferencesWWWebster Dictionary: “Interoperability”, Merriam-Webster, Inc., retrieved from the World Wide Web January 5th, 2000: http://www.m-w.com/dictionary.htm.B. Butler: “Enhancements to the Distributed Interactive Simulation Architecture for Training Simulator Interoperability” Interservice/Industry Training Systems and Education Conference, pp: 195-208, 1993.B. Butler: “The Layered Architecture Model for Distributed Interactive Simulation:  Developing the Layers for Simulation System Interoperability”, 10th DIS Workshop on Standards for the Interoperability of Distributed Simulations, Vol. II, Position Papers, pp: 173 – 187, 1994B. Rush and D. Whiteley: “Architectural Framework for Distributed Interactive Simulation Systems,” Proceedings of the 10th Workshop on Standards for the Interoperability of Distributed Simulations, pp: 663-686, 1994.A. Cortes, & G. Schiavone: “A Practical Parametric Approach to Resolving Interoperability” 10th DIS Workshop on Standards for the Interoperability of Distributed Simulations, Vol. II, Position Papers, pp. 83 – 87, 1994D.C. Miller: “Interoperability Issues for Distributed Interactive Simulation” Proceedings of the 1992 Summer Computer Simulation Conference, pp. 1015-1018, 1992S. Harmon: “Dependencies of Fidelity upon Simulation Interoperability” 1998 Fall Simulation Interoperability Workshop, Vol. 2, pp. 470 – 477, 1998S. Kwan: “Object Templates to Facilitate Interoperability between Repositories” 1999 Fall Simulation Interoperability Workshop, Vol. 1, pp. 334 – 338, 1999M. Myjak, D. Clark, & T. Lake: “RTI Interoperability Study Group Final Report” 1999 Fall Simulation Interoperability Workshop, Vol. 1, pp. 1 – 28, 1999C. Smythe: “Application Integration Frameworks for Interoperable Defense Simulations” The Fifth Workshop on Standards for the Interoperability of Defense Simulations, Summary Report, Appendix A, pp. 139 – 146.R.M. Boys: “Shades of Grey in Interoperability”, 1998 Spring Simulation Interoperability Workshop, Workshop Papers, Vol. 1, pp. 276 – 281, 1998J.C. Herbst, J H. Kirkland, S.H. Grigsby, & H. Heckathorn: “An Architecture to Support Highly Interactive, Large N, Distributed Simulations” 13th DIS Workshop on Standards for the Interoperability of Distributed Simulations, Vol. 1, Position Papers, pp. 507 – 519.P.W. Sutton: “Interoperability: A New Paradigm” 1999 Spring Simulation Interoperability Workshop, Vol. 1, pp. 118-125, 1999W.N. Harless.  “Consideration for Inclusion of the Gateway in the Long Term HLA Interoperability Tools Suite” 1999 Fall Simulation Interoperability Workshop, Vol. 1, pp. 194 – 202, 1999IST: “Correlation Questionnaire”, Interservice/Industry Training Systems and Education Conference, 1993.L.L. Hoffman: “The Concept of Fair Fight on Mismatched Simulator Environments,” Proceedings of the Southeastern Simulation Conference, pp. 234-236, 1995.Humphrey: “Defining Critical Parameters for Interoperability” 10th DIS Workshop on Standards for the Interoperability of Distributed Simulations, Vol. II, Position Papers, pp. 245 – 247, 1994.P.S. Woodard, Bennett, & R. Matusof: “Measuring Fidelity Differential in Simulator Networks” Proceedings of the 14th Interservice/Industry Training Systems and Education Conference, pp: 712-712, 1992.D.J. Paterson: “Synchrony and Time Representation for Distributed Simulations” 12th DIS Workshop on Standards for the Interoperability of Distributed Simulations, Vol. 1, Position Papers, pp. 325 – 328.R.L. Ferguson & B.F. Goldiez: “Interoperability of Visual Simulation Systems” Proceedings of the IMAGE VI Conference, pp. 517-527, 1992.P.S. Woodard: “Requirements for Interoperable Environments” Proceedings of the IMAGE VII Conference, pp. 173-177, 1994Kamsickes: “Distributed Simulation: Does Simulation Interoperability Need an Environment Server?” Interservice/Industry Training Systems and Education Conference, pp: 235-244, 1993.B.F. Goldiez: “Techniques for Interoperability between Terrain Data Bases” Interservice/Industry Training Systems and Education Conference.E. M. Purdy: “A PDU Solution to the Intervisibility Problem in Distributed Interactive Simulations Due to Miscorrelated Terrain” 12th DIS Workshop on Standards for the Interoperability of Distributed Simulations, Vol. 1, pp. 101 – 104.K.C. Hardis & S. Sureshchandran: “Terrain Database Correlation Testing within Database Generation Systems for DIS” 13th Workshop on Standards for Interoperability of Distributed Simulations, pp. 367-380, September 1995.G. Schiavone, S. Sureshchandran, & K. Hardis: “A Multifaceted Approach to Interoperability Issues in Training with Distributed Interactive Simulation using Dissimilar Simulators” ACM Transactions on Modeling and Simulation (TOMACS), Vol. 7, No. 3, pp. 332-367, July 1997.G. Schiavone, J. Chen, & Kim: “Culture Feature Correlation Testing for Terrain Databases in Distributed Interactive Simulation” 13th DIS Workshop on Standards for the Interoperability of Distributed Simulations, Vol. 1, Position Papers, pp. 289 – 294.G. Schiavone, K.C. Hardis, & S. Appadwedula: “Detecting Datum Shifts and Coordinate Transform Errors in DIS Terrain” 13th DIS Workshop on Standards for the Interoperability of Distributed Simulations, Vol. 1, Position Papers, pp. 271 – 279.S.M. McCarter: “An Approach to Designing Interoperable Visual Data Bases for Networked Environments Utilizing Computer Images Generators of Varying Fidelities” Proceedings of Interservice/Industry Training Systems and Education Conference ’92, pp. 725-731, November 1992.T.L. Clarke, G. Prasad, & R. Long: “Non-Intrusive HLA Interoperability Testing” 1997 Fall Simulation Interoperability Workshop, Vol. 1, pp. 203-209, 1998.J.A. Tufarolo, L. Surprise, & M. Raker: “International Interoperability for Simulation Based Training” 1997 Fall Simulation Interoperability Workshop, Vol. 2, pp. 863-868, 1997.S. Knight: “Issues Affecting the Networking of Existing and Multi-Fidelity Simulations” Proceedings of the 2nd Workshop on the Interoperability of Defense Simulations, pp. 1-8, 1990. S. Sureschandran: “Qualitative and Quantitative Comparison of Images from Dissimilar Image Generators in Visual Simulation” 12th DIS Workshop on Standards for the Interoperability of Distributed Simulations, Vol. 1, Position Papers, pp. 59 – 61.C. Fullmer, P. Woodard, & R. Matusof: “Interoperability: The Key to Successful Team Training and Rehearsal.” Proceedings of the 12th Interservice/Industry Training Systems Conference, 1990.Moskal, Johnson, G. Schiavone, & A. Cortes: “A Parametric Approach to Examining Interoperability (IOP) Issues: Initial Experiments” 11th DIS Workshop on Standards for the Interoperability of Distributed Simulations, Vol. 1, Position Papers, pp. 553 – 558, September 1994.F.D. McKenzie: “Joint Simulation System Approach to C4I System Interoperability”, 1998 Fall Simulation Interoperability Workshop, Vol. 2, pp. 700 – 707, 1998. F.H. Carr: “Achieving Interoperability by Stimulating C4I Systems and allowing their Control of the Simulated Battlefield” 1997 Spring Simulation Interoperability Workshop, Vol. 2, pp. 951-957, 1997.C.R. Karr & E. Root: “Integrating Aggregate and Vehicle Level Simulations” Proceedings of the 4th Conference on Computer Generated Forces and Behavioral Representation, pp. 425-435, 1994.G.N. Bundy, D.W. Seidel, B.C. King, & C.D. Burke: “An Experiment in Simulation Interoperability” Proceedings of the 1996 Winter Simulation Conference, pp. 959-966, 1996.G. Blair & J.-B. Stefani: “Open Distributed Processing and Multimedia”, Addison Wesley, 1998. International Organization for Standardization.  “RM-ODP”.  Standard ISO/IEC 10746, retrieved January 5, 2000 from the World Wide Web:  HYPERLINK "http://www.iso.ch" http://www.iso.ch.Author BiographiesROBERT FRANCESCHINI is a Senior Research Computer Scientist at the Institute for Simulation and Training and a Visiting Assistant Professor in the School of Electrical Engineering and Computer Science at the University of Central Florida.  He has performed research in distributed simulation, computer generated forces, multi-resolution simulation, data compression, and graph theory.  He has over 30 published papers in those areas.  Dr. Franceschini received a B.S. in Computer Science from the University of Central Florida in 1992 and a Ph.D. in Computer Science from UCF in 1999.TOM CLARKE is Principal Mathematician at the Institute for Simulation and Training, University of Central Florida.  He has been working in the field of simulation for 11 years and is currently the PI on Infrared Targets for Testing and Training.  He also has over ten years experience in the field of underwater acoustics and marine geology.  Dr. Clarke has a B.S. in Mathematical Sciences from Florida International University in 1973, a M.S. in Applied Mathematics from the University of Virginia in 1975 and a Ph.D. in Applied Mathematics from the Unviersity of Miami in 1983.BRIAN GOLDIEZ is the Deputy Director and Research Manager at the Institute for Simulation and Training, University of Central Florida.  He has been active in interoperability research for 10 years and presently leads a research program whose goal is to move forward in addressing interoperability issues in simulators.  He has published and lectured extensively in areas of testing, systems integration, interoperability, and computer graphics.  Mr. Goldiez received a BS in Aerospace Engineering from the University of Kansas in 1973 and an MS in Computer Engineering from the University of Central Florida in 1979.ALLISON GRIFFIN is an Associate in Simulation at the Institute for Simulation and Training, University of Central Florida.  She has been working in the field of simulation for 4 years and is currently the project lead on the Support to the Simulation Interoperability Standards Organization.  In the past, Allison has also participated in the HLABDS-D project.  She also has over ten years experience in the field of nuclear power generation and the manufacture of nuclear fuel rods.  Ms. Griffin has a B.S. in Mechanical Engineering from the University of Alabama at Birmingham in 1983 and a M.S. in Simulation Systems from the University of Central Florida in 1996.ANDRE HUTHMANN, CPT (GER) has been a member of the armed forces since 1986. Study of Mechanical Engineering at the university of the Armed Forces in Hamburg Germany. Finished as academically trained engineer 1993. After that 2 years as platoon commander and three years company commander. Three years teacher at the NCO Academy I in Muenster, Germany. Study of 'Interactive Simulation and Training Systems' (MS) and 'Simulation Modeling and Analysis Option' (MS) since August 1999.GUY SCHIAVONE is a Visual Systems Scientist at the Institute for Simulation and Training who has over 5 years of research experience working in the areas of computer modeling, numerical analysis and electromagnetic scattering.  He also has over 5 years experience working as a research engineer and technician for General Motors.  He has several published papers and conference presentations in the areas of electromagnetics, environmental databases, and Distributed Interactive Simulation.  Dr. Schiavone received his Ph.D. in Engineering Science and B.E. in Electrical Engineering.BRAD SCHRICKER is an Assistant Research Computer Scientist at the Institute for Simulation and Training at the University of Central Florida.  He has been involved with most of the development associated with Phase III of the Combat Trauma Patient Simulator project.  Mr. Schricker's research interests include simulation interoperability, HLA, and virtual reality.  Mr. Schricker received a B.S. in Computer Science from Florida State University in 1998.