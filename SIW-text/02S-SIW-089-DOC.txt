Single Integrated Air Picture, System Engineering Task Force Analysis:   People, Tools, and ProcessesCDR Joseph N. Giaquinto, USNMr. Jon BartoSingle Integrated Air PictureSystem Engineering Task Force1931 Jefferson Davis HwyArlington, VA  22246703-602-6441 HYPERLINK "mailto:giaquintojn@navsea.navy.mil" giaquintojn@navsea.navy.mil HYPERLINK "mailto:jbarto@mitre.org" jbarto@mitre.orgKeywords:SIAP, Attributes, Metrics,Common Reference ScenarioABSTRACT:  The Single Integrated Air Picture (SIAP) System Engineering (SE) Task Force (TF) was directed to implement a "disciplined system engineering process" to achieve a SIAP that satisfies warfighter needs.  This mandate is predicated on development of an infrastructure, which includes standardized policies, procedures, and processes, to support rigorous engineering analysis.  Three important elements of rigorous engineering analyses include the capabilities to:  (1) evaluate warfighting capability to understand differences between "as-is" and required performance, (2) prescribe improvements to meet requirements, and (3) predict warfighting performance based on those improvements.  The "discipline" in the analysis is accomplished through clear and consistent standards.  These standards include the use of common methods, tools, architectures, operational contexts, and metrics to perform meaningful, repeatable, and operationally representative analysis. SIAP SE efforts and initial results will build a disciplined analytic infrastructure while prescribing near-term "Block" improvements to existing integrated air defense systems.  The Task Force has developed an Integrated Assessment Plan (IAP) defining a collaborative team of service, agency and CINC subject matter experts who are federating existing tools to support engineering decision-making.  The overall vision for the analysis process outlined in the IAP is to attempt to integrate a variety of potential data sources including live venues such as ASCIET/JCIET; hardware/operator-in-the-loop events such as JDEP, and VWC; and 'digital' models and simulations to support parametric assessments of the SIAP allocated baseline and the subsequent impact on warfighting performance.BackgroundOver a number of years, the Department of Defense has accumulated substantial evidence, which indicates that significant warfighting capability shortfalls exist in our nation’s Integrated Air Defense System (IADS). Post-action reports from military operations, training exercises, and joint evaluations such as those conducted by the All Service Combat Identification Evaluation Team (ASCIET), now called the Joint Service Combat Identification Evaluation Team (JCIET), point to specific “Air Picture” quality issues that must be corrected. These issues are often characterized as known/repeat integration and interoperability anomalies, which result in degraded operational effectiveness.  These anomalies manifest themselves in tactical information and displays at command and control (C2) nodes, which often present inaccurate, confusing, or inoperable representations of the air picture to operators.  The manifestations include:  track dualing, track number changes and swapping, reporting responsibility (R2) conflicts, Identification Friend or Foe (IFF)/Selective Identification Feature (SIF) conflicts, track identification conflicts/swapping and excessive net loading issues.It is widely accepted that these problems are the result of family of system (FoS) integration issues and that elimination of these issues will lead to a Single Integrated Air Picture (SIAP) and increased warfighting performance. The Theater Air and Missile Defense (TAMD) Capstone Requirements Document (CRD) states “The SIAP (the air track portion of the Common Tactical Picture (CTP)) consists of common, continual, and unambiguous tracks of airborne objects of interest in the surveillance area.  SIAP is derived from real-time and near real-time data and consists of correlated air object tracks and associated information.   The SIAP uses fused real-time and near real-time data, scaleable and filterable, to support situational awareness, battle management, and target engagements” reference [].Numerous engineering analyses have attempted to more clearly define IADS warfighting shortfalls, and recommend engineering solutions to improve performance. Most of the Department’s efforts to date have proven ineffective in correcting systemic, doctrinal, and process issues contributing to IADS shortfalls.  These shortfalls result in an increased risk of fratricide, reduced capability of being able to operate our systems to their full design capability and difficulty in addressing emerging threats.  Because of the prevalence and continued persistence of these shortfalls, the Joint Requirements Oversight Council (JROC) established the Single Integrated Air Picture System Engineering Task Force [SIAP SE TF] for leading a joint disciplined system engineering effort to improve IADS warfighting capability.  The SIAP SE TF is composed of a small core group of technical professionals from the various Services.  This group leads the efforts of a much larger ‘virtual’ task force composed of Service Subject Matter Experts (SMEs).  Together, in a disciplined, collaborative systems engineering process, the core and virtual staffs identify problems within the FoS that prevent formation of a SIAP, and develop proposed solutions to those problems.  Those proposed solutions are then analyzed for their effectiveness, and the results presented to the Joint Requirements Oversight Council (JROC) by the SIAP SE with implementation recommendations.Purpose of SIAP SETFBuilding a disciplined system engineering process within a complex system of system (SoS) environment is a daunting task.  Integrated development normally starts from the ground up.  System reliability traditionally is achieved through rigid architecture definitions and interface standards combined with an uncompromising adherence to technical requirements.  Unfortunately, the SIAP SE effort must build reliability into an IADS comprising a mix of legacy and new development systems.  This imposes the significant challenge of building infrastructure and discipline after the fact.   Dealing with this challenge invariably leads to “interfaced” vice “integrated” systems.The SIAP SE Task Force Charter mandated the implementation of a “disciplined system engineering process” to “achieve a SIAP that satisfies the warfighter needs” reference [].  The foundation for implementing a “disciplined system engineering process” includes establishing standardized policies, procedures, and processes, which support the capability to perform rigorous engineering analysis. Three important elements of rigorous engineering analyses include the capabilities to:  (1) evaluate present capability to understand differences between “as is” and the required capability, (2) prescribe improvements to meet requirements, and (3) predict performance based on those improvements.  The “discipline” in the analysis is accomplished through clear and consistent standards.  These standards include the use of common methods, tools, architectures, operational context, and metrics to perform meaningful, repeatable, and operationally representative analysis.The SIAP SE analytical goals are to institutionalize clear and deterministic assessment measures and assessment processes based on joint requirements.  Joint requirements also define an integrated system architecture.  It is the system and technical views of this architecture that support engineering.  These standards support the use of collaborative teams, common tools, and institutionalized processes across test and evaluation venues.While the evaluation of the improvement of the quality of the SIAP due to any specific engineering change is a relatively straightforward engineering analysis exercise, the determination of the incremental improvement in warfighting capabilities is more complex.  To address this more complex issue, which needs to be fully understood before definitive implementation recommendations can be made to the JROC, the SIAP SE must work within the Joint Air and Missile Defense (JTAMD) process to flow system performance analysis into a broader operational analysis framework to define value-added in terms of operational benefits. These operational benefits are the measures that will be most important to the JROC.Initial Focus on the Joint Data NetworkOne of the key requirements for creation of a SIAP is information sharing.  The Joint Data Network (JDN) is a communications network that provides interoperability between US military Services’ systems including allied programs/systems. The JDN enables information systems operating at the coordination and execution levels to communicate.  The JDN facilitates Joint Service battle managers’ comprehension of the tactical situation (called situational awareness (SA)) and provides the means to exercise C2 and engagements beyond the range of organic sensors.  JDN carries near-real-time tracks, friendly position and unit status information, engagement status, coordination data and force orders.  The primary “picture” components of the JDN are those that contribute to a shared SA, the near real-time tracks generated by active and passive sensor systems, position and status of friendly platforms, and engagement status for friendly blue force assets against the rest of the tracks.  The primary means for distributing SIAP information is by Tactical Data Links (TDLs).  Tactical data links provide technology-based implementation to satisfy Command, Control, Communication, and Information (C4I) exchange requirements.  C4I is the framework for situational awareness, decision-making, and mission execution throughout the battlespace.  Efficient execution of Information Exchange Requirements (IERs) throughout the joint battlespace is key to evolving C4I toward the ultimate goal of seamless information exchange.  The primary component of this infrastructure is the C4I TDL comprised of data elements/messages and physical media. No single TDL supports every C4I system or is able to operate in all battlefield environments reference [].  The Joint Tactical Data Link Management Plan has cognizance over 18 separate TDLs.  These 18 TDLs include the J-Series Family of TDLs i.e., Link 16, Link 22, and Variable Message Format (VMF) C4I TDLs.  CINCs, Services and Agencies developing C4I TDL systems must comply with DoDD 4630.5, Interoperability and Supportability of Information Technology (IT) and National Security Systems (NSS), as amplified by the 18 October 1994 ASD(C3I) Memorandum, designated  LINK 16 as the DoD primary data link, reference [3]. Therefore, proper implementation of Link 16 is critical to the development of a SIAP.  Unfortunately, while Link 16 is a highly flexible and capable information distribution system, its proper integration into a ‘host’ platform is complex and costly.  Poor or improper integration of Link 16 functionality in systems is a significant contributor to FoS joint interoperability problems. The SIAP SE efforts focuses on recommending specific changes in the systems that support the JDN, while ensuring the fixes are on a path to achieving an “objective” SIAP capability.  Incremental SIAP upgrades will evolve from near-term JDN fixes that improve current air picture functionality and build to fielding a SIAP capability that meets the joint requirements.  Implementation of material JDN fixes is expected to require modification to affected host-system computer programs and equipment.  System computer programs and equipment must be modified to implement the improvements, and then the systems must be tested and certified following implementation of any changes.  This process becomes quite complex given that many of the systems have existing upgrade/update plans and schedules, and they may be impacted by overall host platform requirements such as shipbuilding schedules or Operational Flight Program upgrades.The most cost-effective approach to implementing near-term JDN fixes is to consolidate them into logical block upgrades to minimize the number of times that host computer programs must be modified and tested.  Because “The backbone of the JDN is Link 16 transmitted via JTIDS and MIDS terminals.” reference [ NOTEREF _Ref507334 \h  \* MERGEFORMAT 1], the SIAP SE Task Force is focusing on TDL fixes, with an emphasis on Link 16.Block Definition and Link 16Historically, Link 16 SIAP related issues fall into two major categories, those dealing with host platform integration, and those dealing optimization of information distribution.  Because it is a multi-functional information distribution system, Link 16 (the international version is, in fact, the Multi-functional Information Distribution System (MIDS)) integration is complex, touching multiple areas of host system operation.  Sharing of information with other platforms brings in a new level of integration complexity.  This complexity was not fully appreciated by system designers when many of the initial FoS platform integration efforts were undertaken.  Interoperability philosophies such as “Network Centric Warfare” cannot be achieved without a realization that the FoS must be integrated into Link 16 rather than individually integrating Link 16 functionality into platforms. In addition to these complex integration issues, the DoD policy requiring migration of platforms toward Link 16, while promoting standardization and interoperability, is projected to lead to levels of Link 16 equipped platforms in future operational theaters that will stress and potentially overload the capacity of the system.  The state of potential overload, and anticipated subsequent IADS degradation, will be upon us more quickly, if the system is not adequately integrated into the FoS.The factors discussed and historical experience lead to the two main categories of Link 16 fixes and enhancements that currently populate the prospective SIAP analysis list, the host integration issues, and the migration toward the most efficient methods of Link 16 utilization by all platforms in the FoS.Analysis ApproachThe goals of SIAP analysis start with the functional decomposition of the SIAP.  That is to say, the SIAP SE objectives are to define what functions are to be performed and how well FoS platforms must perform those functions to meet an acceptable level of IADS performance.  Such functionality includes but is not limited to system capabilities to adequately perform: Tracking, track correlation, combat identification, sensor registration, geodetic registration, interface unit registration, and host system synchronization and latency reduction.	The SIAP SE block improvement process addresses these and many other issues.  The SIAP SE approach is to parametrically assess the contributions of SIAP functions, on a system-by-system basis, to IADS performance.  This analysis must be accomplished through a mix of analysis teams, tools, and processes.The SIAP SE’s success can only be achieved through highly motivated collaborative teams adhering to well-understood and disciplined processes.  It was only through engineering rigor and dedicated teaming that our nation’s greatest technological successes including efforts such as: Polaris, Apollo, the Manhattan Project, and nuclear propulsion industry achieved their success.  Most experts agree that loss of engineering discipline contribute to technical failures.  “When technical systems fail, however, outside investigators consistently find an engineering world characterized by ambiguity, disagreement, deviation from design specifications and operating standards, and ad hoc rule making” reference [].The SIAP SE is challenged with developing both a disciplined system engineering process while providing recommendations for SIAP-related improvements.  This is akin to building the bridge as we try to traverse the abyss.  What is particularly daunting in both these efforts is the cultural and infrastructural improvements required of both tasks.  No standardized process exists today to support joint collaborative system engineering.  Our efforts add order to the chaos.  Our analytic approach standardizes collaborative teams, tools, and processes to support focused, repeatable, and rigorous analysis to effect change.  People--The SIAP Analysis Team (SAT)The SIAP Analysis Team (SAT) is a SIAP SE led joint analysis team composed of CINC/Service/Agency  (C/S/A) analysis experts.  The mission of the SAT is to provide cross-service IADS analysis to support SIAP-related system engineering decision-making.  This mission includes:  planning, data collection support, evaluative, root-cause analysis, and after-actions required to system engineer improvements to the IADS.  The SAT does not replace existing service and agency analysis groups.  Rather, it sets standards for comparing results from SIAP-related live exercises/critical experiments, hardware-in-the-loop (HWIL), operator-in-the-loop (OITL), and other modeling and simulation (M&S) events to support the collaborative system engineering decision-making process.  Products of the SAT will include planning reports and schedules; documented root-cause analyses of IADS deficiencies, lessons learned, force capabilities and limitations, analysis results and engineering recommendations.Tools—Venues, Common Reference Scenarios and MetricsThe most significant tools used by the SAT include: test and evaluation venues i.e., Models and Simulations (M&S), Operator-in-the-Loop (OITL) events, HardWare-in-the-Loop (HWIL), Live events, operational contexts called Common Reference Scenarios (CRSs) and metrics.   VenuesStandardized evaluation venues form the basis for repeatable analysis.  The Department invests significant resources in test and evaluation tools annually.  To optimize this investment, FoS analysis must utilize various assessment venues in conjunction with live exercise evaluations to reduce development risk.  M&S, HWIL, and OITL venues support evaluation of specific components of the FoS from system specific to integration performance perspectives.  The objectives of our efforts are to leverage existing tools to the maximum extent practical.Many M&S/HWIL/OITL venues exist today. These resources are used by the Services and Joint organizations to provide an analytical basis for design, development and evaluation of TAMD systems.  System specific and joint integrated tools provide a broad range of analysis capabilities at various measurement levels.  By federating models and analytic constructs to support parametric measurements at the system level, variations in system functional performance can be traced to force level capability improvements.  Ultimately, integrated performance can be evaluated in live evaluations and exercises such as JCIET, Roving Sands, and OPEVAL.No one tool can measure the interoperability of the IADS.  In fact, “measuring interoperability” is a carelessly used term.  Interoperability in and of itself cannot be measured.  Interoperability is the product of warfighting capability.  Interoperability can only be traced to quantifiable measures, which provide an indication of how well the FoS is supporting the warfighter. Many past efforts had sought to provide insights into the quality of the SIAP (some long before the term ‘SIAP’ was even introduced into the DoD lexicon) in one form or another.  A review of those venues showed that each of the many had its own mix of strengths and weaknesses when it came to evaluation of the SIAP.  It was this diversity of venues and assessment ‘tools’ that led to the development of the SIAP Integrated Assessment Plan (IAP).  The IAP provides an overarching strategy for integration of the results from the many different potential SIAP assessment venues.  The overarching goal of the IAP is to leverage the strengths of various venues to provide an assessment of proposed SIAP improvements that are better than that which could be achieved with any specific individual approach.Unfortunately, of the many potential assessment venues available, there has never been an attempt to coordinate or synergize across them in the past.  The result is that, while many were characterizing SIAP related performance in similar ways, none are common enough to equitably compare results from one venue with another, or to build a consistent cumulative story of SIAP performance based on their multiple contributions.  To equitably relate results of various venues with one another, it is necessary to standardize metrics, scenarios, and analysis methodologies to provide as much consistency in the resulting assessments as possible.  The SIAP SE lead an effort with C/S/A representatives to develop a standardized set of metrics, for use across all of proposed assessment venues.  In addition, to ensure that assessments (where feasible) are conducted in the same or similar operational contexts, the SIAP SE is leading the development of several Common Reference Scenarios (CRSs).  The level of standardization introduced across the many potential assessment venues facilitates utilization of all available data collected by many different DoD activities, and allows the SIAP SE to leverage the considerable efforts of many joint DoD activities for minimal extra effort and expense.The following paragraphs briefly describe some of the primary venues, which will be utilized for SIAP analysis, citing some of the major strengths and weaknesses.Live eventsFrom an initial assessment fidelity perspective, live exercises provide the best legacy system functionality representations, because actual system hardware and software are used.  We would also expect that live exercises provide that best characterization of warfighting performance of the FoS.  However, limited availability of assets may preclude flying an exercise with desired platform configurations and force mix.  Consequently, since many platforms have multiple baselines, and may be in various stages of upgrade, the configurations available may not be adequately representative of the FoS ensemble our operators will bring to the next conflict. In addition, live exercises generally are expensive, require long lead times to set up, are subject to many uncontrolled variables, and so not support multiple repetitions of a particular test to assess the effects of “before and after” changes.Additional limitations of live exercises include networking restrictions on Link 16 operation, unrealistic network threat loading, and an unrealistic volume of participation of blue forces when compared to actual wartime employment.  On the plus side, live exercises can be used to baseline real system performance and validate M&S results.  Future events support analysis of performance deltas after changes to the systems are implemented.  Each system can record data for post-event forensic analysis.  The amount and type of data can vary from system to system as well as event to event.  While the many factors influence the amount and quality of the data available (e.g. equipment failures, weather, number and type of platforms participating in the events), empirical analysis can provide representative information from real system hardware, operated by real warfighters, in realistic engagements and therefore represents very credible exhibitions of SIAP performance.  So-called data-driven modeling tools such as those used by the Center for Naval Analyses e.g., Operational Data Driven for Correlation Algorithm Performance Evaluation (ODDSCAPE) and Naval Surface Warfare Center, Corona Division’s Performance Evaluation Tool (PET) support evaluation of live exercises.Hardware-in-the-Loop (HWIL) venuesHWIL venues such as the Joint Distributed Engineering Plant (JDEP) and the Joint National Integration Command’s (JNIC) Missile Defense System Exerciser (MDSE) attempt to retain the fidelity of real hardware and software-in-the-loop by using real systems in a controlled environment.  This permits better control and repetition of experiments, use of specific desired hardware and software configurations, and the ability to do more reliable “before and after” comparisons.  FoS analysis is somewhat limited with most of these types of venues due to the limited numbers of platforms participating in the exercise.  In addition, venues such as JDEP are extremely limited in their capability to replicate sensor, link, and environmental issues.  Operator-in-the-Loop (OITL) venuesThe OITL venues such as those conducted at the Virtual Warfare Center (VWC) and at the JNIC War game 2000 (WG2K) facility, are cost-effective ways to evaluate integrated system environments with respect to the man-machine interface and human response to various levels of SIAP.  These venues provide a very important ingredient to FoS evaluation e.g., operator interaction and decision-making effects.  This effect is a significant element of evaluating the military utility of system level improvements.  The VWC has been successfully utilized by JTAMDO in assessing warfighting effectiveness for various levels of selected SIAP attribute performance and War game 2000 provided similar results for TMD missions. However, OITL events like HWIL events generally are limited in their capability to fully replicate family of systems functionality.  While HWIL events permit better control and repetition of experiments, by using specific desired hardware and software configurations, and the ability to do more reliable “before and after” comparisons, OITL  repeatability is somewhat difficult to achieve, especially when attempting to attain statistically significant numbers of runs.  Digital modeling and simulationThe best way to achieve robust parametric evaluations of system-of-system performance with the potential to dynamically evaluate force-on-force engagement effects is through digital M&S.  The primary strength of available digital simulations are the capability to permit scale-up of limited engagement vignettes to force-on-force and campaign level analysis. This scale-up can be conducted in a controlled, repeatable environment, with the ability to execute multiple runs with many individual parameter variations representing many different potential systems engineering fixes or improvements.  However, while digital M&S is strong in numbers of systems realism and repeatability, it is limited in the precision with which individual system performance can be replicated.  The greatest loss in moving from HWIL, OITL and Live venues to the M&S domain is in representation of system functionality.  Digital M&S must emulate or replicate specific system representations (SSRs).  The level of fidelity of those SSRs directly contribute to the credibility of the M&S analysis.   Primary/secondary functionsIn any modeling environment, the required fidelity of the model’s specific system representations depends on the specific purpose of the analysis.  For SIAP analysis purposes, system functions fall into two broad categories.Primary system functions are those that need to be modeled at relatively high fidelity because they are to be varied in the course of the analysis to show the effects of different implementations.  For example, conducting a before/after assessment of different implementations; (e.g., modeling of current legacy sensor alignment process or its result when a new sensor alignment process is to be evaluated and compared with the current function).  Secondary functions are those whose inputs are required for conduct of a particular analysis, but which need not be varied, so that only a constant representative implementation is needed (e.g., a generic tracker to generate representative inputs to a track file, when a new correlation algorithm is being evaluated).Network/Link fidelityWhen non-network issues are being investigated (e.g., sensor alignment, geodetic registration, system correlation), high network fidelity is not required.  In such a case, the network is a secondary function and a model with a generic low or medium fidelity data link representation may be adequate; e.g. assumption of perfect communications.  However, when investigating particular SIAP degrading effects associated with network overload, for example, the network and the data link equipment that create the network are primary systems, and high network fidelity is required to provide an accurate representation of the network effects on SIAP.Common Reference Scenarios (CRS)Perhaps the most important part of establishing a disciplined analytical environment is the establishing a baseline for that analysis.  As noted above, the SIAP SE has a number of evaluation tools each with varying strengths and weaknesses to draw upon for assessing IADS performance and making recommendations to the JROC based on those assessments.  To level the playing field among those venues, a common baseline or frame of reference must exist.  This frame of reference must bear some resemblance to the operational environment.  In almost all product development efforts comparative analysis must be conducted from a common point of departure representing the user environment.  When it is cost prohibitive (as is often the case) to test product development efforts under exact user conditions, scenario-based design is used.Scenario-based design is recognized as a significant tool to support the system engineering process. “Scenarios afford multiple views of an interaction, diverse kinds of and amounts of detailing, helping developers manage the many consequences entailed by any given design” reference []. In addition, “Scenarios promote work-oriented communication among stakeholders, helping to make design activities more accessible to the great variety of expertise that can contribute to design…”  reference[ NOTEREF _Ref511168 \h  \* MERGEFORMAT 5].  In this context, the establishment of scenarios can help users define specific objectives and mission requirements.The SIAP SE is extending the concept of scenario-based design by supporting the contextual representation of the SIAP operational concept.  A representative operational context supports evaluation of IADS capabilities in settings representative of the real-world.  In addition, the operational context provides a foundation for the development of the Operational View of the TAMD Integrated Architecture.The SIAP SE disciplined process is based on standardization of key tools for “apples-to-apples” comparisons.  In addition, to support the work-oriented communication cited above, common reference frames support joint evaluation of the IADS in both the simulated and live environments.  One of these standardized tools is the establishment of a set of Common Reference Scenarios.Three Common Reference Scenarios (CRS) representing potential regional conflicts have been defined based on the Defense Planning Guidance process.  The CRSs are composed of digitally scripted red and blue force dynamic interactions in operationally significant time-phased campaigns.  Engineering vignettes, extracted from the CRSs provide stressing limited scope platform engagements and excursions, which can be applied to the M&S, HWIL, OITL, and live exercises to support repeatable SoS assessments.  Within this framework, evaluations of SIAP-system enhancements and the resulting impact on warfighter capabilities from the system/unit through the campaign level may be quantified.  CRSs are being jointly endorsed through the JTAMD process.MetricsA critical part of system engineering the SIAP is identifying a standardized quantification of performance. Metrics are used to objectively evaluate candidate approaches to meet JROC-validated Capstone requirements.  Additionally, they allow us to understand how we are progressing toward the end-state.  The operational requirements must be translated (in a traceable way) into lower-level technical requirements that can be used by the disciplined system engineering process, and to objectively assess progress in achieving the required SIAP capability.  However, as indicated, one of the SIAP SE’s jobs is to help evolve the definition of SIAP.  This will be natural fallout of the system engineering efforts required to create a SIAP that most efficiently and effectively meet warfighting requirements.The benefits of the SIAP can only be captured through a broad range of warfighting Measures of Effectiveness (MOEs), mission level attributes, and system level Measures of Performance (MOPs).  Quantifiable and testable MOEs, attributes, and MOPs, are the linchpin to the SIAP system engineering efforts.   MOEs and MOPs must support various analyses including sensitivity assessments, technical trade-offs studies, modeling and simulation, experimentation, land-based test and evaluation, interoperability certification (such as that provided by Joint Interoperability Test Command (JITC)), and evaluation in live environments (such as JCIET).  Several efforts have been undertaken to develop a quantifiable set of SIAP-related measures.  Such measures provide answers to three fundamental questions:What do we have today? (Evaluative measures)What is required? (Predictive measures)How do we get what we need? (Prescriptive measures)Quantifying answers to these questions provides an analysis roadmap for system improvement.  Ultimately these types of measurements must be evaluated at various levels of aggregation i.e., MOPs at the system/platform level, Attributes at mission/effectiveness, theater, and force level and MOEs at the force-on-force/Campaign level. These levels determine a hierarchy of quantifiable characteristics as shown in  REF _Ref770767 \h  \* MERGEFORMAT Figure 1.  The flow-down of quantifiable measures from MOEs at the force level to system level MOPs provides the capability to determine how systemic problems and improvements affect warfighting capability.To build a common lexicon, and make progress toward achieving the SIAP, it is critical that the processes and products that result from the various measures and attributes efforts converge to a standardized approved set.  At a minimum, a standard set of definitions and derivations of SIAP attributes must be used across services and joint organizations.  These attributes provide a common reference to measure a SIAP. In addition, the appropriate MOEs and MOPs must be identified and used by testers, analyzers and evaluators such that common criteria may be used to evaluate, predict, and prescribe performance.Figure  SEQ Figure \* ARABIC 1 MOP/Attribute/MOE MappingSystem Engineering Level (MOPs)At the system engineering level, each potential change will have a number of specific engineering level MOPs that can be defined to characterize the relative performance of a specific improvement.     REF _Ref769943 \h  \* MERGEFORMAT Table 1 lists representative MOPs. Time difference between system internal time at central track stores and JTIDS terminalLatency of messages due to buffering, prioritization, staleness, and time slot allocationTranslational and rotational error quantitiesPercent of time units correctly report track quality with respect to MIL-STD- 6016Table  SEQ Table \* ARABIC 1 Representative IADS MOPsSIAP Attributes Under the leadership of the SIAP SE, Service/Agency SMEs have developed a rigorously defined set of SIAP performance attributes.  The effort included specific procedures for implementing and using the attributes.  SIAP attributes define and characterize IADS performance in terms that are directly related to the TAMD and CID CRD KPPs.  While the CRD KPPs are oriented toward theater-wide performance values, the SIAP attributes also include methods for characterizing IADS performance at the individual unit level.   REF _Ref511968 \h  \* MERGEFORMAT Table 2 lists the set of SIAP attributes.  Completeness:  The air picture is complete when all objects are detected, tracked and reported.Clarity:  The air picture is clear when it does not include ambiguous or spurious tracks.Continuity:  The air picture is continuous when the tracks are long lived and stable.Kinematic Accuracy:  The air picture is kinematically accurate when the position and velocity of a track agrees with the position and velocity of the associated target.ID Completeness:  The ID is complete when all tracked objects are labeled in a state other than “unknown”.ID Accuracy:  The ID is accurate when all tracked objects are labeled correctly.ID Clarity:  The ID is ambiguous when a tracked object has two or more conflicting ID states.Commonality:  The air picture is common when the tracks held by each participant have the same track number, position, and ID.Table  SEQ Table \* ARABIC 2 SIAP AttributesMilitary Utility (MOEs)The SIAP SE TF is, by design and charter, an engineering organization, but it is also chartered with making recommendations to the JROC with respect to what engineering changes should be implemented across a broad range of Service platforms.  Since such recommendations should not be made based on SIAP performance improvement alone, there is a need for the SIAP SE TF to gain some insight into the more operationally oriented MOEs as well.  However, ‘final’ MOE/military utility analysis is the purview of other operationally oriented organizations.  Therefore, the SIAP SE TF must coordinate its analysis and recommendations with the appropriate warfighting benefits assessment organizations within the DoD.  The primary interface for this coordination is JTAMDO.  Example MOEs are depicted in  REF _Ref534965912 \h  \* MERGEFORMAT Table 3 . Distance target penetrated blue air spaceNumber of blue losses to red due to air picture or CIDNumber of blue losses to blue due to blue being misidentified as redNumber of blue defended assets lost; blue casualtiesTable  SEQ Table \* ARABIC 3 Representative IADS MOEsThe SIAP SE is working closely with the Director of JTAMDO in defining an IADS analysis framework for evaluating the military utility impact system level improvement recommendations.   REF _Ref770807 \h  \* MERGEFORMAT Figure 2 depicts the roles and responsibilities of the two organizations.  Fundamentally, the SIAP SE is chartered with the responsibility of evaluating IADS functional performance and associated system improvements to meet SIAP attribute metrics requirements.  JAMDO is responsible for evaluating the warfighting benefit of that attribute level of performance.  This relationship requires careful coordination.  While OITL venues such as VWC and War game 2000 fall under the leadership purview of JTAMDO, other venues span the gamut of IADS analysis.  To support, the federation of tools required to link system improvements to warfighting benefit, it is clear that the SIAP SE must work closely with JTAMDO to define the appropriate linkages between pure warfighting assessment venues and other venues/tools .  This relationship is supported by SIAP SE sensitivity analysis of the defined MOEs to variations in the SIAP attributes in order to focus the usually more time and cost intensive operational venues on operational evaluation of engineering changes that appear to have the greatest impact on the warfighting metrics of interest. EMBED PowerPoint.Show.8  Figure  SEQ Figure \* ARABIC 2 JTAMDO-SIAP analysis relationship Processes-Integrated Assessment PlanGiven the strengths and weaknesses of the multiple assessment venues available, and the desire to leverage as much ongoing and past work as possible, the SIAP SE, in conjunction with C/S/A SME’s have developed an Integrated Assessment Plan (IAP).  Successful implementation of the IAP concepts requires a high degree of collaboration in standardizing the teams, tools, and processes defined above.  Standardization in analysis methodology, metrics,  and scenarios facilitates a more meaningful and repeatable comparison and integration of results of the many different venues.For example, venues in which specific system representation fidelity is high (e.g., live-flys, JDEP) can be used to determine the sensitivity of the SIAP to specific changes in the system functions represented.  Likewise, venues with high network fidelity can be used to determine the sensitivity of the SIAP to specific network and data link function changes.  Lower level, less expensive tools to set up and run can be used to scope, at a gross level, which changes have the most pronounced effect on the SIAP, so that other more time consuming and expensive venues can focus their attention on those potential high payoff areas.Real world exercises such as JCIET and Roving Sands, and certain HWIL evaluations e.g., JDEP will be used to help calibrate and validate the outputs of digital models.  Ultimately, these venues should be used to validate the performance of IADS improvements.  Tools such as the Center for Naval Analyses ODDSCAPE perturbation model support the calibration of digital models using live exercise data.  ODDSCAPE provides the capability to show the potential effects of system computer program changes in live events.  High fidelity models such as Extended Air Defense Test Bed (EADTB) will used to help validate the outputs of lower fidelity models, and establish a repeatable environment for parametrically assessing SoS improvements.Past Study ResultsA valuable source of data is past studies.  Over the last few years, there have been many assessments of SIAP capability in many different venues.  While these did not have the benefit of calculation of the standard metrics (which did not exist at the time), many calculated similar metrics, which can reasonably be expected to show the same trends as new data using the standard calculations.  In this way, past, present, and future analyses can all contribute to a growing body of knowledge which, if consistent, can increase overall confidence in trends shown, and, if not consistent, help uncover shortcomings in various study and analysis techniques which may need improvement.Establish a Standardized Performance BaselineTo establish the value of a particular proposed engineering level change, or block of changes, on the SIAP, assessment tools and venues must reveal the current level of SIAP performance.  Then the improvement(s) must be introduced and the change in the particular measured parameter(s) or metric(s) must be measured.  With sufficient control over the experimental variables, the delta in performance of the introduced engineering improvement(s) may be determined.  This type of before (i.e., before an improvement is made) and after (i.e., after an improvement is made) comparative analysis is essential to the SIAP analysis.At the highest level, the SIAP performance baseline used to determine when the SIAP is ‘good enough’ will be determined by joint warfighting requirements as evaluated by JTAMDO.  By adding the recommended engineering changes of each SIAP block improvement initiative to the previous block changes, a cumulative record of progress may be mapped-out, with the deltas shown over the previous block performance. An increasing absolute value of the metrics over time should culminate in a set of values with the last block upgrades that meet all of the operational requirements.  Block 1 Issues a depicted in  REF _Ref927325 \h Figure 3.  EMBED PowerPoint.Show.8  Figure  SEQ Figure \* ARABIC 3 SIAP SE Candidate Block 1 issuesIntegration of Multiple VenuesNo one venue can completely evaluate end-to-end system to military utility performance.  As noted above, each potential assessment venue has its own strengths and weaknesses.  Therefore, the IAP lays out a strategy that seeks to use the strengths of each venue to compensate for the weaknesses of others.  The result of this strategy should be a set of SIAP assessment results, which is better than the individual sum of the parts.With initial focus on Link 16, SIAP analysis requires venues, which are strong in both host integration representations, and venues that are strong in network representations.  Of course, in both cases the use of real systems and real networks with real wartime loading and play-out in the same venue would be the ideal assessment environment.  Unfortunately, simultaneous high fidelity representation of systems and networks within a single assessment environment are not currently available.  The SIAP SE is working to federate high fidelity system representation tools such as EADTB with high fidelity network representation tools such as the Modeling, Analysis and Simulation Center’s Air Defense Simulation (ADSIM) IADS network tool.  The federation of such venues will provide a credible high fidelity IADS simulation environment strong in both host platform implementation and JDN operations.The SIAP SE is leveraging existing test and evaluation assets to provide credible, and complete system engineering analysis to support recommendations to the JROC.  Evaluation of mission area performance supports iterative utilization of the previously described analysis venues.   By linking various venues, an end-to-end analysis effort will support forensic investigation of SoS integration deficiencies, prescribe improvements to systems, and predict warfighting performance based on those improvements.The end state, however, is to ensure that the tools support development of quality and reliably integrated systems, which perform flawlessly during open-air exercises.  Live events generally support two purposes, training and quality assurance.  In addition, operators and engineers compete for hardware and time during exercises.  This competition limits the quality of both endeavors.  It would be naïve to think that no measure of open-air validation of integrated SoS capability is required prior to deploying systems.  However, the goal should be to be surprised when interoperability fails, rather than when systems do work together.  In addition, by increasing the capability to credibly evaluate land-based integrated system performance, a large measure of quality assurance testing is moved out of the live environment back into the shore-based test facilities.Development of Analysis InfrastructureInitial Block 0 analytical capabilities were limited to a few tools that had been previously developed and used in various SIAP type analyses in the past.  Many of those tools have extensive experience and capabilities related to IADS performance evaluation, but none have much in the way of formal validation, verification, and accreditation (VV&A).  This remains a significant issue for the SIAP SE.  While extensive VV&A must be accomplished to conduct credible SoS analysis, the cost and schedule associated with this effort must be carefully monitored .  The result must be a process, which supports the appropriate level of VV&A to gain reasonable joint consensus on results.SummaryThe SIAP SE was chartered to implement a disciplined system engineering process for the purpose of evaluating IADS shortfalls, and recommending improvements to joint warfighting effectiveness.  To accomplish the task at hand, the SIAP SE must establish a standardized analytical infrastructure composed of teams, tools, and processes.  This infrastructure is supported by the Services and Joint Agencies and leverages the significant DoD investment in evaluation capabilities.  The SIAP SE will use this infrastructure to support a Block improvements process aimed at incrementally building a SIAP.The coordination of the various players who support the teams, venues and processes is a daunting task.  To add order to the chaos, the SIAP SE has developed an Integrated Assessment Plan providing an overarching philosophy for all SIAP analysis.   The IAP leverages and integrates the results of many past and ongoing assessment efforts.  It establishes standardized metrics, scenarios, and analysis methodologies to use across venues for the purpose of comparing and contrasting results and standardizing analysis. It also provides for development and upgrade of tools and venues over time, including VV&A of all tools used in SIAP analysis.  The SIAP assessment approach is designed to leverage and synergize results of multiple analytical methodologies, with the aim of providing the highest quality recommendations to the JROC within the time and budgetary constraints imposed.References[] Theater Air and Missile Defense, Capstone Requirements Document,  March 1, 2001 (classified) [] Charter for Single Integrated Air Picture System Engineering Task Force, as forwarded by OUSD (AT&L) memorandum Oct 26 2000.[] Joint Tactical Data Link Management Plan, DoD Manual, June 2000 [] Vaughn, Diane The Challenger Launch Decision-Risky Technology, Culture, and Deviations at NASA” The University of Chicago Press, Chicago 1997.[] Carroll, John M. “Five Reasons for Scenario-Based Design” Proceedings of the 32nd Hawaii International Conference on System Sciences – 1999  Los Alamitos, CA:  IEEE computer society Press 1999.Author BiographiesJOSEPH N. GIAQUINTO is a Navy Engineering Duty Officer serving as the SIAP SETF Analysis Branch Head.  He is responsible for leading the development of the analysis infrastructure and assessment activities for all SIAP system engineering efforts.  CDR Giaquinto has served in a variety of operational and technical assignments. He has served a significant portion of his career in strategic missile development and fielding. He earned his Bachelor of Science degree in Aerospace Engineering from the U.S. Naval Academy in 1980.  CDR Giaquinto also earned a Masters Degree in Electrical Engineering from the Naval Postgraduate School (1989) and an MBA from the University of Phoenix (2001). JON BARTO is a lead communications engineer in D440 (Architecture and Interoperability), the MITRE Corporation.  Mr. Barto is currently serving the Air Force and the SIAP SETF.  He joined MITRE in 1989 after a 25 year career as a Navy Fighter Pilot and as an aeronautical engineering duty officer.  Mr. Barto's primary duties at MITRE involve LINK 16 engineering, working for the LINK 16 Joint Program Office and Air Force Tactical Data Link Integration Office.  Mr. Barto earned a Bachelor of Science Degree in Naval Architecture from the U.S. Naval Academy in 1969 and earned a Master of Science Degree in Aerospace Physics from the Naval Postgraduate School in 1973.