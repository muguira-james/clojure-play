Using HLA/RTI in an Engineering-Level Federation to Support Reconfigurable SimulationDavid W. FlorekMichael BaldwinMichael CarusoAustin StoudenmireScience Applications International Corporation4001 N. Fairfax Drive, Suite 800Arlington, VA  22203703-704-1767david.w.florek@cpmx.saic.comMaximo LorenzoU.S. Army CECOM Night Vision and Electronic Sensors DirectorateFt. Belvoir, VA  22060lorenzo@nvl.army.milMichael John MuussU.S. Army Research LaboratoryAPG, MD  21005-5068410-278-6678mike@arl.milABSTRACT:  A justification is made for a new design philosophy for visual simulation systems, emphasizing modularity, modeling at the appropriate level of detail, designing for future capabilities, and allowing arbitrarily large virtual environments.  A virtual hardware backplane concept is introduced to support the design philosophy, and the use of HLA/RTI to implement the virtual backplane is discussed.  An application under development is discussed in some detail, and other applications are proposed.  Issues addressed during system development are presented, along with successful solutions.1.  IntroductionThe authors believe that the design of a next-generation visual simulation system in support of sensor simulation should be predicated on four ideals: 1) design for future capabilities and implement for today’s, 2) emphasize modularity to simplify re-use and reconfigurability, 3) model at the level required to support the needs of the simulation, and 4) allow for arbitrarily large virtual environments.  We have designed a framework that supports these ideals, and have implemented it using the High Level Architecture/Run Time Infrastructure (HLA/RTI).  We have assembled a visual simulation system supporting remote control and remote display of a virtual sensor, found good solutions for issues that arose during the implementation, and demonstrated the utility of HLA/RTI as the basis for modular and reconfigurable engineering-level distributed simulation.2.  BackgroundThe U. S. Army’s Communication and Electronics Command (CECOM) Night Vision and Electronic Sensors Directorate (NVESD) has been developing an unparalleled visual simulation capability over the past six years.  The need for this capability arose from our mission to simulate various sensors, and such simulations’ dependence on high fidelity real-time input of simulated thermal energy from a synthetic thermal virtual environment.  At the start of this endeavor, existing visual simulation applications were of insufficient fidelity for our needs, and as our requirements have evolved, continue to be insufficient to this day.Requirements for sensor simulation involve supporting dismounted soldiers, scout vehicles and armored personnel carriers, and low-flying air support.  Geometric fidelity, particularly accurate small-scale terrain features and trees, is critical to simulating the environment to the detailed extent required for sensors.  Most visual simulation applications have evolved from flight simulators and are still best suited for moderate- to high-altitude operations.  These systems approach realism by applying detailed imagery as texture on minimal geometry.  While the environment looks believable from an aerial viewpoint, the illusion is broken if the viewpoint is moved to the ground; what looked like hills, trees, and buildings are now blurry green and white splotches on a very flat piece of ground.Most visual simulation systems are designed as single-purpose, monolithic applications.  Adjusting their behavior in any but non-trivial ways requires writing additional code and recompiling the entire system.  Run-time configurability tends to be limited if not impossible.  Since such systems tend to be designed to solve a specific problem, or small set of problems, they also tend to address one area of expertise well at the expense of other areas.  Even if adding source code to the system is an option, it is the authors’ experience that inserting new expertise is a painstaking process, which involves not only adding new code, but also finding and removing portions of the existing system which conflict with the intended new functionality, while ensuring that no other desired functionality is lost.  Also, these monolithic visual simulation applications are designed around and implemented on single image-generation computer architectures (IGs), using single visual simulation application programming interfaces (APIs), making it extremely difficult, if not virtually impossible, to port the system to another architecture, let alone to a heterogeneous computing environment.Efforts with our current visual simulation system, named Paint The Night [1], have led us to a set of ideals for designing and implementing large-scale simulations, which are outlined in this paper and drive our development for the next-generation simulation in support of sensors.3.  VisionOur sensor simulation update addresses four primary ideals.Design for the future, implement for todayDesign for flexibility (Modularity)Model at the appropriate level (Include what’s needed when it’s needed)Allow for arbitrary database sizeFirst, as hardware performance is increasing at an exponential rate, it becomes increasingly important that the design of systems not be limited to today’s capabilities, or by today’s costs.  Within two years, a more powerful hardware architecture will be available at a lower cost.  As a goal, it must be a relatively straightforward matter to extend the detailed design and re-implement the relevant portions of a well-designed system to take advantage of a newer, more powerful, and less expensive platform’s capabilities.Second, the visual simulation system must address all of the desired functionality of each application, and therefore be somehow divided into smaller pieces in order to address each specific set of requirements.  One answer, although necessarily vague, is to divide up the system by conceptual functionality, so that each piece has a specific task to perform, yet relies minimally on communication with other pieces.  This modular design mentality allows optional pieces to be removed when not needed or added later if required to provide additional functionality (see Figure 3.1).  Equally as important, it allows each module to be designed to do what it does best, without sacrificing anything for the needs of other modules.  In particular, probably the most important part of a visual simulation system is the image generator (IG) itself.  Its module should ideally be involved in nothing except the generation of images.  All other aspects of simulation (i.e., vehicle movement or thermal modeling) should be performed by other modules, and only those results needed by the IG for rendering the scene should be communicated to it.Third, for a number of visual simulation applications, the terrain can be considered to be a set of elevations over a flat plane.  While this is a convenient mathematical construct that looks sufficiently realistic for small environments, it quickly breaks down as the geographic scale of the environment increases: the actual curvature of the earth comes into play.  While the difference may seem trivial for most current applications, both the horizontal placement of a given point and its actual elevation are affected, and simulations which require a curved-earth model (e.g., intercontinental ballistic missiles or high-altitude reconnaissance aircraft) do not correctly correlate to simulations which assume a flat-earth model.  For a given visual simulation system, the model chosen should be the simplest model which meets the requirements of all potential interacting systems.  In fact, if any one of the interacting systems is using a curved-earth model, all the other systems need to do so as well.While it is relatively straightforward to determine the location of the sun (the primary light source in visual simulation applications) based on time of day, time of year, and geographic location, many other celestial bodies are pertinent to the night vision problem.  The moon’s position relative to the earth can be correctly determined without reference to any additional bodies, however the amount of the surface lit by the sun and the orientation of that portion as viewed from the earth require the position of the sun relative to the moon.In addition, several major planets are clearly visible in the night sky, even to the naked eye, as are notable constellations and the general position and orientation of the Milky Way as a whole.  If we assume that distant stars do not move significantly relative to the sun, the entire visible solar system can be conveniently modeled relative to the sun, and then transformed into earth-based coordinates, and if necessary, local geographic environment coordinates.  It is the authors’ opinion that a realistic night vision simulation should provide these visual cues, and that computing anything more than merely the positions of the sun and moon is needlessly complicated if undertaken from the earth’s point of view; thus a heliocentric model is the most expedient for the nighttime sky.Fourth, a visual simulation system should not have an arbitrary a priori upper limit set on the size of the virtual environment database it can handle.  There is increasing demand for larger terrain areas, and a requirement for much finer triangulation (geometric fidelity) in support of high-resolution sensors.  It is not unreasonable to consider a geographical region on the order of two degrees latitude by two degrees longitude, with terrain geometric detail on the order of ten meters, along with approximately the correct number of individual trees, detailed vehicle models, and believable human figures.  Current state-of-the-art visual simulation systems are unable to handle even the terrain aspect of such an environment.  At any one time, only a small fraction of the environment is actually visible, and considering the decrease in resolvable detail over distance, an even smaller fraction of the detailed environment is visible.  A visual simulation system should be designed to determine on-the-fly what portions of the environment are visible and at what required level-of-detail, and manage resources so that as little unneeded data as possible is actually consuming runtime resources (physical and virtual memory and processing power).  The authors are currently working on a Database Pager to accomplish this ideal.  The Database Pager is the topic of a forthcoming paper.4.  SolutionEarly on in our design process, we adopted the concept of a virtual backplane and virtual hardware modules, any number of which could be plugged into the backplane at any position, in much the same way extra hardware cards might be added onto the shared bus of a computer.  The virtual hardware modules correspond exactly to the functional modules described above, and the virtual backplane is the method that the modules use to communicate with each other. The virtual backplane concept readily lends itself to reconfigurable simulation; at any particular time, the visual simulation system consists of the functionality provided by whichever modules happen to be plugged into the backplane.For the coarse implementation of the virtual backplane concept, we chose to have modules exist as distinct and completely independent programs, communicating among themselves in a common way.  This allows modules to be tested in isolation, and to be added to or removed from a system simply by deciding whether or not to run them.  Also, in a networked environment, each module can be run on the most optimal platform; decisions can be made as to how many modules should simultaneously exist on a given platform, which platform will meet the needs of a module or set of modules at the lowest price, or how complex a module can become before it can no longer run on a given platform.  The basic design premise for modules is: determine what basic self-contained functions are performed in the simulation, and have one module for each such function, so that to the extent reasonable, each module does one and only one thing.Choosing a communication method for the virtual backplane involved a number of considerations.  First, it should support a networked computing environment, so that a simulation system could become almost arbitrarily large rather than being limited to the resources a single computer could provide.  Second, the computing environment need not be homogeneous: a module or small set of modules that need to be replicated for a given application should be able to run on smaller and less expensive machines, while a module requiring enormous resources might run on High Performance Computing (HPC) resources at a remote location, rather than be limited by the immediately available hardware.  Third, a given functionality should be able to be implemented at a number of fidelities, and it should be possible to choose the module that provides the fidelity desired for a particular application.  We initially considered three potential communication methods for the virtual backplane:HLA/RTIJoint Modeling and Simulation System (JMASS)A home-grown solutionWe selected HLA/RTI because it addressed several initial concerns.  First, HLA provided sufficient capability and infrastructure, and fit well with our primary ideals.  Given that the Department of Defense has mandated that simulation systems be HLA-compliant in order to interact with other simulations, the authors agreed that using HLA/RTI throughout as our simulation-internal communication method was the most efficient implementation and would ensure interoperability with other simulations.  Finally, the defense industry as a whole appears to be backing HLA, so that as it evolves, improvements should require minimal additional effort towards existing modules, and support should be easy to find.  Initial success with the HLA/RTI effort precluded any need to further investigate JMASS or a homegrown solution.5.  Issues AddressedAs the authors have progressed through the design of a couple of initial prototype systems and a first useful visual simulation system, we have uncovered and addressed a number of interesting issues.One of the earliest issues we discussed was where to use HLA Objects vs. HLA Interactions.  As we started determining very coarsely what modules would exist in an enormous night vision simulation system, it became apparent that modules would likely be event-driven and that most communications across the virtual backplane would be point-to-point, generated by one module and of interest to only one other module.  In this scenario, each module would proceed along, doing whatever it was doing, until it received a message from another module, telling it to do adjust its behavior or providing it a new value or set of values to inject into its process.  For example, a hardware-mapping module would detect when the user pushed a button on a panel, and send a “zoom in” event message to a sensor-controller module; the sensor-controller module would update its state from “wide field-of-view” to “narrow field-of-view” and send one “set field of view to x degrees horizontal by y degrees vertical” message to the IG module and a “current field of view is: NARROW” message to a heads-up-display module, which would update the annotation on the simulated heads-up display.At first it seemed most logical to implement all communications as interactions, since every message appeared to be related to some kind of event.  Then we considered the problem of late-joining federates determining the state of the visual simulation system, and looked into having everything that could be thought of as state implemented as objects, whose ownership would be transferred to the module wishing to update a value.  The problem in this case was: what if the module causing an update to a value doesn’t care what the actual value is?  (For example, in the case above, the hardware-mapper knows about a “zoom in” button, but doesn’t need to know how many zoom levels the sensor-controller supports, especially since in another application, the same hardware-mapper might be updating a different sensor-controller.)  We determined that an interaction as simple as “sensor: zoom in one setting” was an efficient way for the hardware-mapper to pass information to the sensor-controller.Finally, we settled on the idea that all simulation state information would exist in and be communicated as objects and attributes, and all other communications (such as instructions, like the zoom-in example above) would be issued as interactions.  To minimize redundancy of objects and their attributes, and interactions and their parameters, we decided that if one module wished to explicitly set another module’s value, it would acquire ownership of the appropriate object or attribute, change the value, and return ownership; otherwise, if it wished to provide an implicit instruction (such as “move vehicle forward n meters” without knowing or caring about the vehicle’s current location and heading), it would issue an interaction.The authors have begun investigating the exchange of bulk data in an HLA/RTI system.  In particular, we have a geographically distributed application, where the bulk of the simulation system will be hosted locally, while the control interface and display will be at a remote site (see NVESD’s submission on using HLA/RTI for remote visual simulation [2]).  Our initial approach was to transfer frames of real-time video as HLA entities (objects or interactions).  We then tried moving the video frames via TCP/IP sockets, with some improvement.  Most recently, we bypassed TCP/IP and transferred the video data as ATM AAL5 frames over the DREN (Defense Research and Engineering Network), using a direct ATM-level API for the ATM adapter card.  We were able to utilize directly almost all of the available bandwidth on an OC-3 (155 Mbit/s) ATM link.  We are continuing to investigate ways to use HLA/RTI for synchronizing such bulk data exchange between federates.6.  Implementation & ApplicationsOur application currently under development involves remotely controlled and displayed sensor simulation.  It consists of a hardware input device (a BGSystems FlyBox: a three-axis joystick, two levers, and several buttons), a real-time image processing platform (a Datacube), a remote display, a copy of the HLA-compliant Joint Semi-Automated Forces (JSAF), and six HLA/RTI federates (see Figure 6.1).  The FlyBox and the first federate, a flybox-mapper, will reside at the remote location (Ft. Knox, Kentucky), as will the display.  Four other federates make up the visual simulation system: a JSAF-interface, an environment-controller, a sensor-controller, and an IG.  The last federate, a sensor-simulator, runs on the Datacube.  Each federate currently runs on the DMSO RTI version 1.3 release 5 [3].The flybox-mapper federate handles the serial interface to the FlyBox hardware, and generates interactions for the environment-controller and the sensor-controller.  In this way, the FlyBox can be used to modify the environment, the sensor position, and sensor controls.  The flybox-mapper resides on the remote computer to which the FlyBox is attached.The environment-controller federate receives time-of-day and atmospheric-attenuation update interactions from the remote flybox-mapper, and modifies the position and intensity of the IG’s light source object (simulating delayed solar loading on the environment) and the density of the IG’s fog object.  The sensor-controller federate receives location and orientation update interactions from the remote flybox-mapper, and modifies the location and orientation of the IG’s viewpoint.  It also receives sensor-parameter (field-of-view, electronic zoom, level, gain, polarity) update interactions from the flybox-mapper, and modifies the field-of-view of the IG’s viewpoint and the electronic zoom, level setting, gain setting, and polarity (white-hot or black-hot) of the sensor-simulator.  The jsaf-interface federate receives HLA messages concerning the position of vehicles from JSAF, and updates a list of vehicle objects with their locations and orientations.  Since it was undesirable to have the jsaf-interface federate belong to two simultaneous federations, our complete Federation Object Model (FOM) is the union of our internal engineering-level FOM and the JSAF FOM.  The IG has its light source, fog, and viewpoint parameters updated by the environment-controller and the sensor-controller.  It subscribes to all available vehicle objects, which are owned by the jsaf-interface.  It places geometric targets in the environment, places the light source and viewpoint relative to the scene, and renders the thermal scene according to the light intensity, fog density, and field-of-view.  The IG hardware platform transfers the thermal image digitally via a dedicated video connection to the Datacube hardware.The sensor-simulator federate resides on the Windows NT-based Datacube.  Its sensor parameters are updated by the sensor-controller federate, and it directs the image-processing hardware to use those parameters to generate a visual output image from the thermal input image.  A separate non-HLA process transfers the image data across the ATM network to the same remote computer to which the FlyBox is attached.  A final non-HLA process on the remote computer receives the image data and displays it in a window, to complete the loop.Further development will allow multiples of each kind of federate, along with individual vehicle-controller federates, so that a group of scattered remote users, each requiring only a simple input device like the FlyBox and a relatively inexpensive computer, may simultaneously interact in the same virtual environment, comparing the relative merits of various sensors.  Also, any of the federates might be implemented at whatever fidelity is required for a given situation.  The environment-controller can be made considerably more sophisticated, and even be split up into physics simulations of solar loading, atmospheric attenuation, smokes and obscurants, et cetera.  Individual vehicle-controllers can be made as simple (such as a primitive “stealth” viewer) or as complicated (including a full dynamics simulation of the vehicle drive train and suspension) as desired or as needed for a particular application.  Even the IG can range from a personal computer with a 3D graphics card up to the real-time physics-based ray tracing solution currently being pursued by the authors, utilizing HPC resources [4,5].Another application of such a system is help train and evaluate Assisted Target Recognition (ATR) algorithms, which would be included in weapons systems to aid in locating and identifying potential targets.  A modular simulation built on this framework could even be integrated into a vehicle or other system to provide an embedded training solution.7.  ConclusionsThe authors have found HLA/RTI to be ideal as an implementation of our virtual backplane concept for a flexible, modular, and reconfigurable visual simulation system, each component of which can be implemented at various and freely-interchangeable levels of fidelity.  The overall responsiveness of the current application, in terms of end-to-end latency, has proved quite satisfactory.  The direct mapping of the virtual hardware module concept to HLA/RTI federates makes new system configurations amenable to parallel development efforts: once any necessary additions to the FOM (to support entirely new federates) are agreed upon, the design, implementation, and stand-alone test of new federates can proceed simultaneously.  We believe we have demonstrated the value of HLA/RTI as the basis for engineering-level simulation.8.  References[1] M. Lorenzo, J. Ratches, Y. Lu, J. Cha, R. Deaso, E. Stoudenmire, B. Nystrom: “Advancements in ‘Paint the Night’ Real-Time Synthetic IR Scene Simulation”, 2nd NATO-IRIS Joint Symposium, London, England, June 1996.[2] A. Stoudenmire, et. al.: “The Remotely Locating of Simulator Display and Controls through High Speed Video Transfer.”  Submission to Fall 1999 SIW.[3] Defense Modeling and Simulation Office: “HLA Interface Specification Version 1.3”, IEEE P1516.1 Draft 1, 2 April 1998.[4] M. J. Muuss: “Towards Real-Time Ray-Tracing of Combinatorial Solid Geometric Models”, BRL-CAD Symposium ’95, Aberdeen Proving Ground, MD, 5-9 June 1995. [5] M. J. Muuss, M. Lorenzo: “PST: A Distributed Real-Time Architecture for Physics-based Simulation and Hyper-Spectral Scene Generation”, Multi-Spectral Scene Generation Workshop, Redstone Technical Test Center, 27 April 1999.Author BiographiesDAVE FLOREK is a Senior Computer Graphics Engineer with SAIC, working at the Night Vision and Electronic Sensors Directorate.  He has been a key member of the Paint the Night development team for over three years, bringing expertise in 3D computer modeling, rendering, and animation to the challenges of high-fidelity real-time visual simulation.MIKE BALDWIN is a software engineer at SAIC working at the Night Vision and Electronic Sensor Directorate.  He has worked in distributed networked simulation for four years and is currently involved in real time visual simulation, high speed video transfer, 3D terrain generation, and HLA federate development.MIKE CARUSO is currently a Software Engineer employed by SAIC working in the fields of real-time graphics, geometry reduction and fine tuning, thermal sensor simulation, integration of HLA in network distributed applications, and object oriented design.EUGENE STOUDENMIRE is a software engineer at SAIC at Fort Belvoir VA supporting simulation projects at Night Vision Electronic Sensors Directorate at Ft Belvoir VA.MAX LORENZO is branch chief of the Virtual Simulation/Prototype Branch at CECOM's Night Vision Electronic Sensor Directorate at Fort Belvoir VA.MIKE MUUSS is a Senior Scientist at the U.S. Army Research Laboratory at Aberdeen Proving Grounds, MD.  He is currently leading efforts in real-time ray-tracing, and high-resolution physics-based multi-spectral synthetic image generation. EMBED PowerPoint.Slide.8   EMBED PowerPoint.Slide.8  