The Dynamic Adaptive Threat Environment Architecture for Computer-Generated ActorsMartin R. Stytz, Ph.D.Sheila B. Banks, Ph.D.Air Force Research LaboratoryCalculated InsightWright-Patterson AFB, OH  45431Orlando, FL  32828 HYPERLINK "mailto:mstytz@att.net" mstytz@att.net,  HYPERLINK mailto:mstytz@acm.org mstytz@acm.org HYPERLINK mailto:sbanks@calculated-insight.com sbanks@calculated-insight.comKeywords:software architectue, component software, gauges, software frameworks, computer-generated actors, containerization, actor migration, rapid prototyping, exploratory prototyping, migration assistantAbstract: We describe the architecture of the Dynamic Adaptive Threat Environment (DATE).  DATE supports the development and deployment of different types of computer-generated actors (CGAs) in large-scale Distributed Interactive Simulation (DIS) and High Level Architecture (HLA) based simulation environments. To address critical issues in software architecture and rapid prototyping, the architecture exploits the technical advantages provided by object-oriented techniques, component software, software frameworks, gauges, migration assistants, and containerization to achieve composability, flexibility, re-usability, and generality.1.	IntroductionIn this paper we describe the architecture of the Dynamic Adaptive Threat Environment (DATE) software.  DATE supports the development and deployment of different types of computer-generated actors (CGAs) for large-scale Distributed Interactive Simulation (DIS) and High Level Architecture (HLA) based simulations. The architecture exploits the technical advantages provided by object-orientation, component software, software frameworks, gauges, migration assistants, containerization, and rapid prototyping to achieve its goals of composability, flexibility, re-usability, and generality for CGAs.  The DATE object-oriented architecture supports the Rapid Evolutionary and Exploratory Prototyping (REEP) rapid prototyping software development approach,31 thereby helping to minimize costs and refine requirements.  The DATE architecture consists of highly modular components and two frameworks where interdependencies are well-defined, documented, and minimized.  The DATE architecture and design is an open architecture and and supports an open design.  Some component and framework software will be open source.  DATE is a completely composable system and permits components and objects to be independently developed and then integrated into DATE without disturbing or distressing existing DATE software.DATE supports assembly of a distributed threat environment composed of CGAs operating as surface threats, air threats and jamming systems within a distributed virtual environment (DVE).  CGAs in DATE must mimic the observable behaviors of their real-world counterparts; therefore they must perceive the environment, maintain a model of the environment and tactical situation, plan actions, react to situations, monitor their own action(s), and communicate with other actors (both human and computer-controlled).  However, the technologies needed for the capabilities that are required are still being developed and advanced; hence, the system architecture must support experimentation, prototyping, and continuing development.  Nevertheless, the prime concern for the software architecture must be addressing the run-time challenges for a CGA that arise when computing human-like behaviors and reactions to a complex dynamic environment at a human-scale rate of time.  Most important of all, the CGA’s behavior must be realistic and accurate enough so that it triggers tactically and behaviorally correct responses by other humans and computer-controlled actors.  As a result of the complexity of computer-generated actor applications and the uncertainty of the specific requirements, we designed DATE to be able to exploit evolutionary software development34 and REEP31,36. In this paper we discuss the architecture for DATE that we have developed.  In the next section we present a background on related work.  In Section Three we describe the architecture and its constituent elements and their functionality.  Section Four contains a summary and suggestions for future work.  2.	BackgroundIn this section we present the background information that we relied upon when formulating the DATE architecture.  The next subsection contains a set of definitions of terms used in the architecture.  Subsection Two contains a brief review of previous related work.  Subsection Three contains a description of the foundational architectural technologies used in DATE.  Subsection Four contains a discussion of some CGA considerations that had an impact upon the architecture.2.1	DefinitionsThroughout the remainder of this paper, we will be referring to the terms architecture and system architecture.  The term architecture refers to the components, organization, and communication used within an application.  The architecture for a system documents the set of decisions that have been made concerning the organization of a software system, the structural elements in the system, the interfaces between the elements, and the behavior of the elements and the system.  The architecture for a system describes how the structural and the behavioral elements are composed, and in essence describes all of the components of a system and how they cooperate to achieve the functionalities desired for the system.  An architecture is much more than a simple enumeration of system capabilities; an architecture consists of a specification of the capabilities and the pieces of software that are needed to achieve these capabilities.Within the DATE architecture XE "architecture" , the following terms are defined33.  An entity is a computer model in use within DATE that can change its state.  An entity can be a computer model of a manned aircraft, a manned armored vehicle, an unmanned combat air vehicle, the weather, solar activity, a command and control network, a computer network, or any other actual or theoretical thing in the real world.  An actor is an entity that has intelligence (either computer-based or human-based). An information stream is a logical path through the architecture from an information source to a designated information sink.  A container XE "container"  is a permanent, unvarying software object that consists of a data structure plus software methods.  Containers are used in information streams.  Every container holds data that is exported from an object or a component within DATE.  Containers are structured into pallets and slots.  A pallet XE "pallet"  is a major category of information about entities within a container.  For example, in a military simulation, there would be pallets defined for Red entities (for entities that belong to enemy forces), Blue entities (for entities that belong to friendly forces), Green entities (for entities that belong to neutral forces), and Yellow entities (for entities that belong to unknown forces).  Pallets within a container can be nested hierarchically.  The data for an individual entity XE "entity"  is assigned to a pallet within a container according to the entity’s type and within a pallet each entity has its own slot XE "slot" .  An entity has only one slot in a container.  All of the information for an entity needed by any recipient on a given information stream is contained within its slot in the container.  An incoming data stream is data destined for an entity.  An outgoing data stream is data that originated at a local entity and is headed for the network environment.  Incoming and outgoing are global views.  An inbound container XE "inbound container"  is a container that is carrying data into the Common Object DataBase (CODB) or entity that the CODB/entity must read.  An outbound container XE "outbound container"  is a container that is carrying data away from the CODB or an entity and the CODB/entity must write to the container.  Inbound/outbound are information centric views of container operation.  A gauge is software that converts data collected by a software probe into a measure that is meaningful for a particular system for the purpose of performance tuning, information assurance, functional validation, compatibility, or assessment of operational correctness. Gauge outputs are written in XML.  A software probe is software that interacts with an operating system, operational application, or subset of an application to collect data for a gauge.  Software probe output data are written in XML.2.2	Previous WorkThe run-time challenges for a CGA lie in computing human-like behaviors and reactions to a complex, dynamic environment at a human-scale rate of time. A large body of work has been developed that discusses these and other issues that must be addressed when assembling a CGA.  Architectural aspects of CGAs have been addressed often and a wide variety of approaches have  been reported 1, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30.  The architectural approaches range from modular software libraries and interacting processes to closely coupled objects, data-flow architectures, and clusters of agents.  We will discuss a few approaches in the following paragraphs.The system described by Braudaway7 uses a single blackboard for all of the CGAs hosted on a single computer. The blackboard has three components: the blackboard data structure, knowledge sources, and a controller.  Each knowledge source is a self-contained specialist intelligent agent.  Each agent can be implemented as a rule-based program, procedural program, a logic program, or a functional program.  The blackboard database holds all of the information, hypotheses, partial solutions, and the problem solving state.  The controller determines the next action to be performed based upon actions nominated by the agents.Bokma and Slade6 describe the Consensus Method, which also takes an agent and blackboard-based approach to implementing CGAs.  Their architecture consists of knowledge sources, local databases, local controllers, and communication channels at each host computer.ITEMS, (Interactive Tactical Environment Management System) as described by Siksik30, was designed to simulate Army company-level activities in a battlespace.  In ITEMS, actors are defined using libraries that specify sensors, weapons, communications, dynamics, and knowledge.  ITEMS uses rule-based expert systems and frames to perform decision making.  Within ITEMS, knowledge is encapsulated in frames, with each frame responsible for representing facts about a specific type of knowledge.  The ITEMS expert system has three separate knowledge representations: 1) a database, 2) a rules database, and 3) a common database.  The database holds all of the information required to represent a scenario and it holds the rules database.  The rules database holds the knowledge needed to support decision-making for tasks such as motion, navigation, communication, tactics, and mission execution.  The common database holds transient knowledge concerning the state of the DVE.Calder10 discusses a software architecture for a command forces (CFOR) command entity (CE) system that can simulate company and above level commanders.  The main software components of their system are the Situation Awareness class, the Terrain class, the Planner class, and the Event Processor class.  Knowledge is organized into frames, which they call events.  Events are managed by the Event Processor.  The Situation Awareness class assembles information concerning mission objectives, location and strength of known enemy and friendly formations, the location of known obstacles, and remaining mission time.  The Terrain class computes mobility corridors, battle positions, and routes.  The Planner class generates, evaluates, and selects courses of action using the knowledge contained in Constraint Sets.  The Mission Tracker class monitors the progress of the unit in executing the mission. Calder9 discusses the ModSAF command and control system.  ModSAF is composed of a set of extensible software modules that were functionally defined using object-oriented techniques.  The system exploits four software engineering techniques: 1) layering, 2) object-based programming, 3) strict object interface specification, and 4) data-driven execution.Reece and Kelly27 describe a software architecture for individual human combatants in a DIS DVE.  The architecture consists of four levels: 1) physical, 2) feedback control, 3) action selection, and 4) problem solving.  The physical level describes the physical interaction and perception of the combatant and contains their fatigue model.  The control level provides feedback control to the activities/actions in the physical level that require feedback.  In the action selection level knowledge is used to select actions to be performed.  The problem solving level performs planning.Butler8 describes the Joint Simulation System (JSIMS) software architecture.  The JSIMS reference architecture has three tiers: the virtual tier, the model tier, and the physical tier.  The virtual tier contains the description of the synthetic environment, performs rendering, and handles networking tasks.  The model tier organizes the model.  The physical tier describes the physical resources, such as computers, networks, facilities, and software, that support the operation of the simulation environment.  JSIMS uses a core software subsystem to support the architecture and to support composition; composition efforts are based upon attaching changed parts to the stable core.  The layers of the architecture are the basic unit of decomposition.2.3	Foundational Architectural TechnologiesA prime goal for the software development community has been enabling the assembly of complex software systems from simpler software components.  The reused software components are bound together by the unique software needed to achieve the performance desired by the objective system.  Software component technology is becoming a key element of achieving this vision and in enabling software reuse coupled with cost-effective maintenance of legacy software systems.  A software component15 is a nontrivial, nearly independent, and replaceable part of a system that fulfills a clear function in a well-defined architecture or is a unit of composition with contractually specified interfaces and explicit context dependencies.  A component can also be defined as a physical packaging of executable software with a well-defined and published interface.  In general, a component is an amalgam of objects and encompasses more complexity than is found in a single object.  A component is a physical manifestation of an object (or set of objects) with a well-defined interface and a set of implementations for the interface.  The emphasis in component software is on well-defined interfaces that are separate from the underlying implementation.  Component design decisions are driven by a number of factors, the leading factors being those that define component granularity.  Intercomponent communication is typically expensive in terms of resources consumed, which encourages the use of large components.  However, larger components have more complex interfaces, are more affected by change, and are more challenging to reuse, which encourages the use of small components.  Additionally, as components increase in size, the flexibility of the underlying system structure decreases.  Additional design tension arises from the need to balance conflicting demands between the need for high cohesion and for low coupling between components and objects.To function, components need a reference model for the purposes of interface definitions, message passing, and data transfer.  A framework performs these functions within a component-based software architecture. A generally accepted definition for a framework is that it is a reusable part of a system that is represented by a set of abstract classes and a description of the manner in which the class instances interact.  In DATE, the framework is a software skeleton for an application that can be customized by a developer.  The frameworks that are used in DATE provide a support infrastructure, interface standards, and execution scaffolding for DATE components and objects.  A framework is more than a single object, a framework is generally composed of a number of objects knitted together to express a larger unit of functionality.  Conceptually, a framework serves as a backplane that interconnects the components and objects that form the application.  A framework provides an execution environment for implemented domain components and domain objects and provides services and facilities to support a set of primitives for a group of components.  Frameworks are an ideal conceptualization for enabling re-use and experimentation because they allow functionality to be captured at multiple levels of abstraction and enable re-use at multiple levels of encapsulation.There are several key tasks for a framework designer.  These tasks are the specification of the contents of the framework (which are the methods and variables), the specification of the data and the control flows between the components via the framework, the specification of the interface to the framework, and documentation of the procedures used to insert a new method, variable, or functionality into the framework.  The framework specification should also contain a description of the critical functions performed by the framework and of all the variables used in the framework.  One of the chief motivations for and attractions of components and frameworks are their capability for supporting the assembly of systems through composition.An architectural framework must support adoption of technical advances achieved in other projects and enable low coupling and high information hiding between components and objects with the goal of enabling composition at all levels and enabling rapid interchange of components and objects.  The framework should support experimentation and evolutionary development of functionality for all components and objects in a CGA.  In a CGA, a framework should support the operation of any type of decision mechanism or hybrid decision mechanism, be able to integrate any type of knowledge, and inherently support actor migration and data logging.Software gauges are constructs used to enable rapid assembly of systems from components.  A software gauge is a display system for data collected using a software probe.  A software probe collects information by intercepting data in transit between components/objects in a system.  Software gauges arose out of the need to be able to assemble new systems from existing software and out of the realization that current and future complex software systems can never be adequately tested in their intended environment.There are a number of objectives for software gauges.  The objectives include a reduction is software debugging time, reduction in the size (number of megabytes) of compiled software, simplification of software evolution and maintenance, a need to measure satisfaction of system configuration and performance requirements, and a need for detection of events that indicate that system errors have occurred or that the system is behaving in an unacceptable manner.  Software gauges allow a designer and implementer to rapidly answer questions such as: 1) are the versions of the software compatible, 2) what data is moving across the seams in the system, 3) where is most of the computational activity in the system, 4) is the data being correctly translated, 5) are the components/objects in the system receiving the data that they need in the proper format, 6) what components are connected and exchanging data, and 7) does the implementation of a component satisfy the design requirements.  A software gauge can help to determine if changes made to a system have introduced error and where errors are located.  Gauges can also be used to aggregate and visualize configuration and usage information.  A software gauge allows a designer or implementor to view the configuration of a system at multiple levels of abstraction and to conduct experiments with different configurations.  Software gauges can enable otherwise incompatible components to interact and also help to avoid the lengthy and complex process of developing standards for interaction between components of otherwise competing software vendors.  Gauges can be used to assess the suitability of two components for interaction before, during, and after software architecture transformations.  Gauges can provide data concerning temporal usage patterns, component activity, and differences in configuration between different versions of the same component.  Gauges help to insure architectural conformance and help to determine the functional similarity between different components.There are seven basic types of gauges:  1) event gauges, 2) component gauges, 3) connector gauges, 4) configuration gauges, 5) constraint gauges, 6) runtime gauges, and 7) dependability gauges.  An event gauge measures the performance of interaction protocols and helps to assess usage patterns.  A component gauge is designed to enable functional compatibility and prevent functional saturation.  A component gauge measures a component's semantic fit in a system using ontological information embedded in both the system and the component.  Ontology-based gauges assess functional “distance."  Connector/conformance gauges define and provide terminators for connectors, and thereby insure data and infrastructure compatibility.  Connector/conformance gauges allow a system architect to determine when it is meaningful and safe or unsafe for two or more components to interact.  A data compatibility gauge measures the amount of work required to transform data as it moves from component to component, where work is defined as the number of intermediate functional components and data converters used to connect components.  This type of gauge helps to determine if there will be difficulties in replacing or combining candidate components caused by differences in functions that they can perform or by differences in the content of the data that they manipulate.  A configuration gauge measures component interaction and usage and is used to measure quality of service between the components in a system.  A configuration gauge enables up-front prediction of the architectural suitability of systems/components being integrated to form an application.  A configuration gauge measures the information needed to perform component architectural merging, determines inconsistencies, and calculates the percentage of essential and optional functionality remaining within a system once a given type of component is integrated.  A constraint gauge is used to verify that an architecture meets design needs and component resource needs and is also used to compute bounds for runtime gauges.  A runtime gauge is used to determine if a system’s dynamic behavior satisfies expectations and requirements.  A dependability gauge is used to automatically measure and determine if a component and an overall system conforms to specified security policies.In addition to the seven gauges discussed above, other types of gauges have been suggested.  Some of these gauges are the connectivity gauge (which measures how changes were made to accomodate a new component), the functional gauge (which enables automatic system tuning for performance or other uses), the intercomponent compatibility gauge (which measures the risks/challenges associated with inserting a new component into a system), and the real-time performance gauge (which measures the real-time performance and quality of service for a system).  Gauges should be inserted at the seams/interfaces of a system.  An object-oriented analysis and the resulting documentation are crucial for locating these seams and interfaces.The Extensible Markup Language (XML) is a meta-language that permits the definition of special-purpose components of a language (syntax, data types, vocabulary, and operators) needed to achieve a required capability using a Document Type Definition (DTD).  XML also supports the definition of customized markup components.  The customized markup components, called tags, are documented within a Document Type Definition (DTD).  The DTD describes a vocabulary and syntax for use within a document to be transmitted. Several factors supported our decision to use XML within DATE for certain specific communication tasks.  First, XML supports a flexible approach to formatting.  The XML capability to define and use custom tags and the minimal requirements for the language gave us great confidence that we would be able to express any format we needed within the boundaries of the language. Second, XML is widely used and is standardized and the basic components of the language are stable and well understood; as a result, the language supports re-use and extension of the formats we developed.  Finally, XML is precise; it has a well-defined set of rules for describing a document and for ordering the contents of a document without imposing semantics.2.4	Additional CGA Architectural  ConsiderationsIn the development of the architecture, a prime consideration was support for CGA migration because of the difficulties migration poses, the need to bound the costs imposed on a host by an incoming CGA, and because of migration’s importance to future simulation efforts.  Migration is important because it is the chief means by which a distributed simulation environment can enable a CGA (or other actor or entity) to satisfy real-time performance requirements for a geographically dispersed set of actors that it must intercat with.  Migration, within the distributed environment that we envision, is important and possible because CGAs are persistent within the environment and because CGAs have long-lived goals (such as mission objectives, tactical plans, and doctrine) that drive and constrain their activity.  Migration requires that a computer host capture the state of the CGA and transmit that state to a recipient host where the CGA is re-instantiated in a manner that makes the migration undetectable (transparent) to other distributed simulation participants.An issue that arises as a result of actor migration is the computational burden imposed upon the recipient host by the arriving CGA.  The current common approaches for migration either statically assign the migration times and locations or permit migration to occur whenever it is required to whatever host is convenient.  We find both of these approaches to be deficient.  The first approach is too brittle for effective use in long-term simulation environments because additional hosts will be added to the system, computational load can vary widely during the course of an environment’s/federation’s execution, and the cost of hand-crafting the performance of an environment will continue to increase.  On the other hand, an approach to migration that permits a CGA to move to a host at any time without consideration for the cost imposed upon the recipient host is clearly untenable because of the potential impact upon the performance of other actors at a recipient host by the incoming CGA.  We believe that for migration to be effective and useful, the migrating CGA should be permitted to dynamically determine the host it will move to based upon the costs it would impose upon a recipient, the CGA’s performance requirements, and the performance capability and unused capacity of potential target hosts.As suggested by Jamali18, an economic model of the costs and requirements for the CGA appears to be an effective means for guiding the selection of a recipient host for a migrating CGA.  This same cost assessment would also provide a bounds for the costs imposed by a CGA and thereby permit a host to determine if it should permit a CGA to migrate to it.  To make use of this or any other cost model for migration, a protocol for the exchange of information between the CGA and a host is required.  The protocol must allow the CGA to provide information concerning its computational, memory, network bandwidth, security, and processing priority costs it will impose upon the recipient host.  The protocol should also allow the CGA to describe the granularity of the costs it will impose (how often it must be scheduled to run, the amount of incoming and outgoing information required, etc.), its expected duration at the host, and the CGA’s publication and subscription requirements (for the HLA).  Because of the complexity of the communication (or negotiation) and of the protocol and because the migration communication should be used by every CGA, we believe that these functions should be separated from the CGA and provided by the supporting software infrastructure at each host.The supporting infrastructure, or migration assistant, operates by first gathering a migrating CGA’s requirements.  With the requirements in hand, the migration assistant then begins the search for a host that can satisfy the requirements for the CGA.  The search should be guided by the priority that the migrating CGA assigns to each of its requirements, so that the highest priority item for the CGA serves as the initial discriminator in the selection process for potential hosts.  Once a host commits to accepting the CGA, the migration assistant at the current host negotiates with the potential recipient to determine if the potential host can support the CGA and if it will agree to accept the CGA.  Once a host is located, the migration assistant informs the CGA and the host of the information each needs to have in order to successfully perform the migration and also informs the RTI and simulation environment manager(s) of the migration and of the new host for the CGA.  The CGA then fills in the migration container and dispatches it to the recipient host.  Once the migration completes, the migration assistant at the recipient host informs the RTI and simulation environment manager(s) that the migration is complete, thereby permitting all affected participants in the simulation environment to update their publication and subscription information for the CGA.  Other considerations and requirements have been discussed in depth by us in a number of papers2,3,4,32.3.	The DATE ArchitectureThe DATE architecture is a data-handling architecture that exploits the technical advantages offered by object-oriented techniques, the CODB, component software15, object frameworks, gauges, information streams, migration assistants, and containerization31. The DATE architecture supports assembly of composable applications and permits components and objects to be independently developed and then integrated without disturbing or distressing existing software.Components define the major aspects of the architecture, objects are used to flesh out the specification of the components.  The degree to which component software supports composition relates to the degree to which the architecture/design supports “plug-in” of components and the degree to which components adhere to predefined constraints and conventions.  To be useful, the constraints and conventions must specify the functionality that each component offers to the application as well as the architecture/component interface properties.  These properties, constraints, and conventions are captured within an object framework.  The DATE object framework, which is specified at two levels within the architecture, provides the communication and coordination services needed to assemble applications from components and acts as the plumbing that interconnects the components.  The DATE framework provides an execution environment for implemented domain components and domain objects and provides services and facilities to support a set of semantic primitives for a group of components15.  The DATE framework also guarantees message delivery and performs transaction management.Two levels of frameworks are used in DATE.  One framework is at the highest level of the architecture (CODB and containers) and the second level framework supports the individual actors.  The highest level framework provides the information routing (information stream) and data management services required by the major system components.  The second level framework provides a set of services that all actors require (such as data filtering, sensor filtering, and data management) and de-couples the individual CGA components from each other and from the remainder of the system.  The CODB functions as the central data repository and information router between all of the DATE components and also insures that all of the information publication and subscription requirements in the applicable Federation Object Model or Simulation Object Model are met.  During operation, the CODB receives inbound information for all of the data streams that it services, determines the recipients of the data, and stores the information until requested, at which time the information is dispatched in a container.  From the perspective of the high-level framework, each actor is a component in DATE.  Each actor receives the information that it requires (both data and control) from the CODB along a dedicated information stream and sends data and control information back to the CODB on the same dedicated information stream.  Data is transported on an information stream using containers.  Each actor component is, in turn, composed of a framework consisting of a set of components and two data interfaces (the Physical State Information Interface and Sensor Interface) that perform data management within the actor component.The logical view of the DATE architecture, highlighting its major components, is presented in Figure 1.  In the architecture, the Network Interface and Network component is responsible for the transmission of information between DATE and the other computers that are instantiating the DVE.  This component encapsulates the HLA Run Rime Infrastructure (RTI).  As information arrives at a DATE host, it is forwarded from the Network Interface software to the World State Manager (WSM).  The World State Manager maintains the entire state of the DVE based upon the information it receives from the DVE.  The World State Manager takes incoming data and updates its information about all the entities in the DVE and places the information into a container for transmission to the CODB.  In addition, the WSM performs dead-reckoning  for entities in-between receipt of state updates for each entity.  Whenever the CODB requests an update from the WSM, the WSM is required to have a container with current DVE state information ready to be dispatched.  Once in the CODB, the data is routed to every resident entity via information streams.3.1	The Common Object DataBaseOnce the DVE state information reaches the CODB, the data is repackaged into outgoing containers and dispatched on an outbound information stream.  The repackaging is accomplished by methods in the CODB that perform coordinate conversion, filtering, and routing.  The data from the CODB either moves directly to an actor or the data moves to a sub-CODB, which is an information stream specialized form of the CODB.  Once information reaches a sub-CODB, the data is dispatched from there to the actors serviced by the sub-CODB.  The containers that depart the CODB or a sub-CODB along an information stream for a recipient are customized for the entity(s) and actor(s) on the stream. In addition, the migration assistant in the architecture resides in the CODB.  The migration assistant gathers the requirements for a migrating CGA from the CGA in XML.  These requirements are used to determine the candidate target hosts for the outbound CGA.  Once a target host is selected, the migration assistant transmits the required information in XML to the migration assistant at the target host.  All communication and negotiation between migration assistants is performed using XML.  Once an intended recipient host indicates its agreement to accept the migrating CGA and its ability to satisfy the CGA’s operational requirements, the migration assistant at the sending host signals the recipient host that migration will commence and signals the CGA that it can start migrating by sending the CGA an XML-formatted message in a control container. Performance considerations constrain the information that is allowed to be sent during a migration, in addition to basic state information about the CGA, the CGA is permitted to transmit only a limited portion of its knowledge base.  The software and almost all of the knowledge base required for CGA execution must reside at the target host before migration occurs.  The Figure 1:  The DATE System Architecture.migration assistant is responsible for insuring CGA software and knowledge base availability at the recipient before migration commences.The containers that depart a CODB or sub-CODB hold the DVE information required by the actors/entities on  the stream or they hold control XE "control"  information targeted at one or more actors/entities on the stream.  The CODB, and all of its sub-CODBs, is also used to store and forward state information from actors/entities hosted by a DATE instantiation to the network environment through the WSM XE "World State Manager" .  The CODB and WSM components XE "components"  work together to insure that each DATE application instantiation satisfies its data transmission requirements XE "requirements"  by consolidating the output from the actors/entities and then transmitting data to the rest of the DVE.Whenever a new entity XE "entity"  appears in the networked environment, the CODB XE "CODB"  is informed of this event by the WSM XE "World State Manager"  via a message from the WSM that is placed in a control XE "control"  container XE "container" .  When the CODB is informed of the new entity, the WSM must supply the entity state, including ID, alliance, type, class, and location, at a minimum in addition to the container that the new entity will be assigned to, its pallet XE "pallet" , and its slot XE "slot" .  Finally, the CODB determines which of its outbound container XE "outbound container" s require information about this new entity and then makes the appropriate container assignments and instantiates a new container if it is required.  When an entity is removed from the environment, the WSM informs the CODB of this event.  The CODB then destroys any containers occupied only by this entity and informs the entities served by any affected containers that the entity was removed.3.2	DATE Information StreamsIn the DATE architecture XE "architecture" , the inbound and outbound information stream XE "information stream" s organize the information transportation activities and the services provided by the highest level framework XE "framework" .  Within the architecture, all of the information (data and control XE "control" ) required by an entity or actor XE "actor"  comes to the entity or actor via its inbound information stream.  All of the information (data and control) produced by an entity or actor and destined for the network environment or for another local DATE component XE "component"  departs the entity or actor via its outbound information stream.  By using information streams, we minimize the volume of information transported from the CODB XE "CODB"  to the entities or actors in a DATE instantiation and simplify the construction and operation of the actors/entities.  Information streams also allow us to simplify the information flows and control flows within the architecture.  Information streams also serve to explicitly specify the information and control flows within the DATE architecture.  There is generally one information stream for each type of entity/actor within a DATE-architecture application operating on a given host.  There can also be one dedicated information stream used to transport “hard” real-time data from an actor or entity or component to the CODB and a separate dedicated information stream to transport “hard” real-time data from the CODB to any actors or entities or components that require it.  Containers on an information stream are double-buffered; that is, there are two containers on each information stream, one for reading and one for writing.  These two containers switch roles when the readers complete their read function.3.3	DATE ContainersThe data in the DATE containers is written XML, which insures that any component that is attached to an information stream can access the data in the stream.  Container access is straightforward.  A component uses its internal methods to access the container on the stream(s) that service it, retrieves the data in the container in XML, and then translates the data from XML to whatever internal format(s) that the component may require.  The CODB is responsible for translating the data from the format used in the external computer network (DVE) into XML and for placing the resulting data into the proper containers on the information streams that service the intended recipients of the data.  A single piece of data can be placed into more than one information stream at any given time, data recipients determine the content of their streams and the CODB is responsible for servicing the recipients and placing the required data into whichever streams require the data.  The methods portion of a container is composed of software routines that provide gauges, handle the movement of the data in the container along the information stream, and insure that the data remain uncorrupted during transmission.  The gauges allow a DATE-based application to assess its own health, assess the accuracy of its performance, assess information accuracy and assurance, insure that components integrate correctly, and provide a variety of other information concerning the operation of DATE and the accuracy of the data.  The containers also contain intelligent agents that are used to verify the accuracy of connections (at run-time and during assembly and select the gauges that should be enabled.  The output of the agents is written in XML.The main CODB XE "CODB"  has six types of container XE "inbound container" s that are inbound from the WSM XE "World State Manager" : 1) entity XE "entity" , 2) phenomenology XE "phenomenology" , 3) emissions XE "emissions" , 4) transient XE "transient" , 5) control XE "control" , and 6) migration XE "actor migration" .  The information in these six containers departs from the WSM or in the case of a control container it can also depart from a DATE object or component.  The entity container contains state information for all entities in the network environment.  The phenomenology container holds information about all phenomenology in the network environment except for sensor emissions.  The emissions container holds all sensor emission data, such as radar, infrared, sound, etc. This container holds the information about the status, operational wavelength, orientation, waveform, and power for every sensor in the DVE.  The transient container holds information about transient DVE events such as missile launchings, weapon firings, or other occurrences that are known to have a brief existence within the DVE.  The control container holds information concerning filtering XE "filter"  or other object control information, like halt, migrate, or resume.  The migration container contains information concerning the state of an entity that is either migrating to or from a DATE host.  The CODB has five types of outbound container XE "outbound container" s that carry data to the WSM: 1) entity, 2) phenomenology, 3) emissions, 4) transient and 5) migration.  The functionality of the outbound containers mirrors the functionality of the corresponding inbound containers.  The same types of containers are also used on the inbound and outbound information streams from the CODB to the actors/entities on its information streams.  Because all reader side components of a container share the same copy of the virtual environment’s state, we insure that they access a consistent description of the world.  When a reader finishes with a container, the reader switches to a newly filled container of data provided by a writer, such as the WSM, once the writer signals that the new container is ready.  Within the architecture, data logging can be performed at the CODB or selectively at any of the containers or sub-CODBs on an information stream.  Logging can be enabled via a control container or can be triggered by an intelligent agent operating within the CODB, sub-CODB, or in a container.  Data logging outputs are written in XML.3.4	DATE Actor FrameworksWithin each DATE actor framework, we identified six major components.  These six components are the Physical Representation Component, the Cognitive Representation Component, the Skills Component, the Physical State Information Interface, Sensor Interface and the Threat Knowledge DataBase (or Knowledge Base).  The Physical Representation Component (PRC) contains the description of all of the physical attributes and properties of an individual CGA and has three major sub-objects, the Dynamics, Sensor Interface, and the Sensor objects.  The PRC encapsulates one or more physical models for the operation of a dynamics unit or sensor within a single package for the CGA, and each CGA can access one or more dynamics models or sensor models.  The PRC’s Dynamics sub-component includes the information and models that define CGA-specific motion properties, performance capabilities, weapons load, damage, and physical status.  The Sensor sub-component contains the sensor model(s) used by the CGA.  The other component of the PRC is the Sensor Interface (SI).  The Sensor Interface is responsible for extracting information from incoming containers and providing each sensor model for the CGA with the information that it requires to function.  To increase the fidelity of the operation of DATE CGAs within the DVE, each CGA within DATE has its information restricted by filtering the incoming data so that the CGA operates only upon a realistic set of information.  The sensor-filtered information is then forwarded for use in the CRC. Data filtering occurs before the incoming information arrives at an actor’s decision engines.  Data filtering is performed by the Physical State Information Interface, the Sensor models, and in the Sensor Interface. The decision-making system consists of two components, the Skills Component (SC) and the Cognitive Representation Component (CRC).  The SC models the skills and ability of the simulated operator of a CGA’s vehicle.The movement of data through the DATE architecture at the actor framework level is precisely specified.  The Sensor Interface is the data warehouse and data router on the information stream to an entity/actor and its Physical Component models.  The output of the Physical Component (which is the motion and sensor model outputs) is sent to the decision-making component via the Physical State Information Interface (PSII).  The PSII stage routes the information from a Physical Component to the decision engines that require the information produced by a particular computational model.  The incoming data is used by the CRC, which consists of the Long-term Decision Engine (LTDE), Mid-term Decision Engine (MTDE), and the Critical Decision Engine (CDE), in conjunction with the information contained in the knowledge bases to perform long-range, mid-range and immediate decision-making operations.  The LTDE, MTDE, and CDE send the outputs of their computations, written in XML, to the Arbitration Engine (AE), which selects the action to be performed.  The action to be performed is modified by the actor’s skill level and combat psychology model.Each DATE CGA accesses a knowledge base that was constructed specifically for its actor type.  The architecture makes no assumptions about the human behavior model or human behavior representation that a CGA will use, that decision is left to the CGA designer.  There are two sub-components of the Knowledge Base for each actor type: the Environment Database and the Mission, Strategy, and Tactics Database.  The Environment Database contains the specification of the terrain and other static portions of the DVE.  The Mission, Strategy, and Tactics Database contains the information about a CGA’s mission, the tactics for the CGA, and the strategies to be employed by the CGA.4.	Summary and future workDATE supports the development and deployment of different types of CGAs in DIS- and HLA-based simulation environments.  The architecture exploits the technical advantages provided by object-oriented techniques, component software, software frameworks, gauges, migration assistants, and containerization to achieve composability, flexibility, re-usability, and generality.  The DATE architecture supports an open source development approach for individual actor components and objects, such as dynamics models, sensor models, knowledge bases, or reasoning systems in parallel with continued development of the main components of the architecture and ongoing refinement of its services.  As a result of the decision to open the system, the architecture, design, and implementation support focused and broad-based experimentation and development on any or all of the components of a CGA; reasoning system, knowledge base, cognitive models, and psychology models.We are currently addressing issues related to the evaluation and refinement of the architecture.  We intend to improve the two DATE framework’s capacity for enforcing behavioral specifications for interface and component operation.  Another area to be addressed is re-casting the architectural and design specifications into the Unified Modeling Language (UML) to improve the foundation for future DATE development.There are two open research issues remaining to be addressed regarding the use of CGAs for distributed simulation.  These are customization of scenarios to individual trainee proficiency and creating an explicit linkage between training requirements and desired CGA behavior.  DATE is uniquely positioned to support research in these areas.  Additional research areas for DATE are the operation of the migration assistants, the XML formats needed for the communication and negotiation between migration assistants, and the XML formats needed for intra-DATE communication and to describe migration requirements.References Adkins, M.K.  (1996) “Polling vs. Event-driven Computer Generated Forces (CGF) Architectures,” Proceedings of the 6th Conf. on Computer Generated Forces & Behavioral Rep.,  Orlando, FL, 23-25 July, pp. 313-318.Banks, S.B.; Stytz, M.R.; Hutson, L.J.; & Silver, S.M. (1998) “A Computable Combat Psychology Model for Computer Generated Forces,” The 1998 Fall Simulation Interoperability Workshop, Orlando, FL., 13-18 Sep., pp. 35-45.Banks, S.B.; Hutson, L.J.; Stytz, M.R.; & Santos, E. Jr.  (1998) “Incorporation of Multiple Skill Levels into a Domain-independent Computer Generated Force Architecture,” 7th Conf. on Computer Generated Forces & Behavioral Rep., Orlando, FL, 12 - 14 May, pp. 497-508.Banks, S.B. & Stytz, M.R. (1999) “An Approach to Enhance Human Behavior Modeling for Computer-Generated Actors,” Proceedings of the 4th International SIMTECT Conf., Melbourne, Australia, 29 Mar – 1 Apr, pp. 199-204.Becket, W. & Badler, N.I. (1993) “Integrated Behavioral Agent Architecture,” Proceedings of the 3rd Conf. on Computer Generated Forces & Behavioral Rep.,  Orlando, FL, 17-19 March, pp. 57-68.Bokma, A. & Slade, A. (1993) “Developing Large-Scale Agent-Based Systems:  An Example from Air Traffic Control,” Proceedings of the 3rd Conf. on Computer Generated Forces & Behavioral Rep.,  Orlando, FL, 17-19 Mar, pp. 21-32.Braudaway, W. (1993) “A Blackboard Approach to Computer Generated Forces,” Proceedings of the 3rd Conf. on Computer Generated Forces & Behavioral Rep.,  Orlando, FL, 17-19 Mar, pp. 11-20.Butler, B. (1998) “Simulation Composability for JSIMS,” Proceedings of the 7th Conf. on Computer Generated Forces & Behavior Rep., Orlando, FL, 12-14 May, pp. 393-406.Calder, R.B., Smith, J.E., Courtemanche, A.J., Mar, J.M.F., & Ceranowicz, A.Z.  (1993) “ModSAF Behavior Simulation and Control,” Proceedings of the 3rd Conf. on Computer-Generated Forces & Behavioral Rep., Orlando, FL, 347-356.Calder, R.B.; Carreiro, R.L.; Panagos, J.N.; Vrablik, G.R.; Wise, B.P.; Chamberlain, F.L.; & Glasson, D.P.  (1996)  “Architecture of a Command Forces Command Entity,” Proceedings of the 6th Conf. on Computer Generated Forces & Behavior Rep., Orlando, FL, 23-25 July, pp. 19-30.Calder, R.B. & Drummey, J. (1999) “Definition of a Military Intelligent Agent Architecture,” Proceedings of the 8th Conf. on Computer Generated Forces & Behavioral Rep., Orlando, FL, 11-13 May, pp. 551-562.Ceranowicz, A. (1998) “Evolutionary CGF Development,” Proceedings of the 7th Conf. on Computer Generated Forces & Behavior Rep., Orlando, FL, 12-14 May, pp. 421-431.Coradeschi, S.; Karlsson, L.; & Torne, A.  (1996) “Intelligent Agents for Aircraft Combat Simulation,” Proceedings of the 6th Conf. on Computer Generated Forces & Behavioral Rep.,  Orlando, FL, 23-25 July, pp. 93-99.Courtemanche, A.J. (1999) “Design Patterns for Computer Generated Forces,” Proceedings of the 8th Conf. on Computer Generated Forces & Behavioral Rep. Orlando, FL, 11-13 May, pp. 25-36.Digre, T. (1998)  “Business Object Component Architecture,” IEEE Software, vol. 15, no. 5, Sep./Oct., pp. 60-69.Ge, Z.; James, J.; & Nerode, A.  (1995) “A Multiple Agent Hybrid Control Architecture for Automated Forces:  Design and Software Implementation,” Proceedings of the 5th Conf. on Computer Generated Forces & Behavioral Rep.,  Orlando, FL, 9-11 May, pp. 45-52.Howard, M. & Lee, C. (1998) “Architecture of a Generic Command Entity,” Proceedings of the 7th Conf. on Computer Generated Forces & Behavior Rep., Orlando, FL, 12-14 May, pp.569-579.Jamali, N.; Thati, P.; and Agha, G.A. (1999) “An Actor-Based Architecture for Customizing and Controlling Agent Ensembles,” IEEE Intelligent Systems, March/April, pp. 38-44Jennings, N.J. (2000) “On Agent-Based Software Engineering,” Artificial Intelligence, no. 117, pp. 277-296.Jones, R.M.; Tambe, M.; Laird, J.E.; & Rosenbloom, P.S. (1993) “Intelligent Automated Agents for Flight Training Simulators,” Proceedings of the 3rd Conf. on Computer Generated Forces & Behavioral Rep., Orlando, FL, 17-19 Mar, pp. 33-42.Kuokka, D.R. (1993) “A Framework for Integrating Autonomous Agents,” Proceedings of the 3rd Conf. on Computer Generated Forces & Behavioral Rep.,  Orlando, FL, 17-19 Mar, pp. 181-190.Kwak, S.D. (1998) “Cognition Oriented Emergent Behavior Architecture,” Proceedings of the 7th Conf. on Computer Generated Forces & Behavior Rep., Orlando, FL, 12-14 May, pp. 445-454.Laird, J. E., Newell, A., & Rosenbloom, P.S. (1987) “SOAR:  An Architecture for General Intelligence,” Artificial Intelligence, vol. 33, no. 1, pp. 1-64.Laird, J.E.; Johnson, W.L.; Jones, R.M.; Koss, F.; Lehman, J.F.; Nielsen, P.E.; Rosenbloom, P.S.; Rubinoff, R.; Schwamb, K.; Tambe, M.; vanDyke, J.; vanLent, M.; & Wray, R.E. III.  (1995) “Simulated Intelligent Forces for Air:  The SOAR/IFOR Project 1995,” Proceedings of the 5th Conf. on Computer Generated Forces & Behavioral Rep.,  Orlando, FL, 9-11 May, pp. 27-36.LaVine, N.D.; Kehlet, R.; & Peters, S.D. (1999) “A Client-Server Approach to CGF Behavioral Rep.,” Proceedings of the 8th Conf. on Computer Generated Forces & Behavioral Rep., Orlando, FL, 11-13 May, pp. 411-421.Oztemel, E. & Kocabas, S.  (1996) “Design Principles for Intelligent Agents in Distributed Interactive Simulation,” Proceedings of the 1st International SIMTECT Conf., Melbourne, Australia, 25-26 Mar, pp. 103-106.Reece, D.A. & Kelly, P.  (1996) “An Architecture for Computer Generated Individual Combatants,” Proceedings of the 6th Conf. on Computer Generated Forces & Behavioral Rep.,  Orlando, FL, 23-25 July, pp. 337-344.Sale, N.; Usher, T.; Page, I.; & Wonnacott, P. (1999) “Multiple Representations in Synthetic Environments,” Proceedings of the 8th Conf. on Computer Generated Forces & Behavioral Rep., Orlando, FL, 11-13 May, pp. 349-361.Sherman, R.H.  (1994) “Using Computer Generated Forces to Support Cooperative Mission Planning,” Proceedings of the 4th Conf. on Computer Generated Forces & Behavioral Rep.,  Orlando, FL, 4-6 May, pp. 37-49.Siksik, D.N. (1993) “Intelligent Computer Generated Forces Through Expert Systems,” Proceedings of the 3rd Conf. on Computer Generated Forces & Behavioral Rep., Orlando, FL, 17-19 Mar, pp. 3-10.Stytz, M. R., Adams, T., Garcia, B., Sheasby, S. M., & Zurita, B.  (1996) “Rapid Prototyping for Distributed Virtual Environments,” IEEE Software, vol. 14, No. 5, Sep-Oct, pp. 83-92.Stytz, M. R.; Banks, S. B.; & Santos, E.  (1996) “Requirements for Intelligent Aircraft Entities in Distributed Environments,” 18th Interservice/Industry Training Systems & Education Conf., Orlando, Florida, 3 - 5 Dec., on CD-ROM.Stytz, M.R. & Banks, S.B. (1999) “The Distributed Mission Training Integrated Threat Environment System Architecture, Rules, & Design Overview,” The Spring Simulation Interoperability Workshop, Orlando, FL, 14-19 Mar, pp. 858-867.Woodward, S. (1999) “Evolutionary Project Management,” IEEE Software, vol. 32, no. 10, Oct., pp. 49-59.		PAGE  