Conceptual Model Descriptions Dale K. PaceThe Johns Hopkins University Applied Physics Laboratory11100 Johns Hopkins RoadLaurel, Maryland 20723-6099(240) 228-5650(240) 228-5910 (FAX) HYPERLINK mailto:dale.pace@jhuapl.edu dale.pace@jhuapl.eduDecember 30, 1998ABSTRACT  The variety of simulation conceptual models provide application domain context for simulation requirements, the linkage between simulation requirements and simulation specifications, and the ideas for representation of elements within a simulation.  Evaluation of a simulation’s conceptual models is the only basis for assessing simulation appropriateness for application of a situation for which it has not been tested explicitly.   All judgments of simulation capabilities to interpolate or extrapolate beyond those conditions specifically tested must be based upon evaluation of simulation conceptual models.  Conceptual models may be implementation independent or may include some aspects of implementation.  The formats employed in describing conceptual models have an impact on both conceptual validation and simulation development.  When different descriptive formats are used for the various conceptual models of a simulation, potential for introduction of error is increased with every transformation from one descriptive format to another.  This paper discusses potential impact on conceptual validation and simulation development of conceptual model descriptive format and proposes an approach to description of the conceptual models of elements within a simulation that both facilitates conceptual validation and enhances simulation development.  Conceptual model issues of special importance for distributed simulation are given particular attention in this paper.1.  Introduction  Conceptual validation should be the foundation for simulation credibility.  Validation of results from simulation testing and use can determine how well the simulation performs for specific test cases, but without validation of the concepts and algorithms of the simulation, one has no basis for judgment about how well the simulation can be expected to perform for any other conditions.  Conceptual validation can provide a basis for expectation of how well the simulation will perform under a variety of conditions and can identify limits upon appropriate applications of the simulation.  This is true for both distributed and unitary simulations.  The descriptive format employed with conceptual models has a major impact on how easily and how well conceptual validation can be performed [Pace 1998b].   This paper begins with contexts for both simulation development and simulation verification, validation, and accreditation (VV&A).  It then identifies several kinds of conceptual models and describes the several approaches to conceptual model description.  A suggestion for how to describe system representation-type conceptual models is presented.  The paper concludes with a discussion of the special impact that conceptual model descriptions can have on distributed simulations.2.  Constructs for Simulation Development and VV&AIt is useful to begin with an approach to simulation VV&A.  Figure 1 provides a standard VV&A paradigm which has been used widely in both Defense and non-Defense communities.  Figure 1 distinguishes between conceptual validation and results validation, and shows how verification relates to these.  It also indicates three kinds of data, and shows where accreditation fits in this paradigm.  Other VV&A paradigms exist, but generally those within the Defense community and much of academia can be reduced to this basic VV&A paradigm.  Some [e.g., AIAA or Roache] combine simulation V&V with scientific experimentation which results both in somewhat different paradigms and in somewhat different connotations for VV&A terms.Figure 2 takes a step back from the paradigm of Figure 1 and illustrates the source of some of the items in Figure 1.  In particular, Figure 2 identifies three referents:  1) real world Simulation Results Referent, 2) the Fidelity Referent, and 3) the Validation Referent. Simulation development starts with the “real world,” but the real world contains imagined reality as well as material reality.  Material reality is defined as the material universe (or those parts of it) which are pertinent to the application domain.  Imagined reality is a concept which has no specific counterpart in the material universe, such as a unicorn (a horse would be part of the material reality -- a unicorn is imagined reality).  Part of the concept may have a counterpart in the material universe.  Imagined reality may overlap material reality but cannot be contained completely within it (otherwise, the imagined reality would merely be part of the material reality).Imagined reality is an oxymoron – but it seems to be the best term available since a term is needed for the part of the “real world” that does not yet exist (relative to a future system) or which may never exist (relative to a concept, such as the unicorn, or relative to a theory, such as a scientific hypothesis).  Without this kind of a concept, it would be necessary to declare some kinds of simulation as beyond the realm of fidelity and validity evaluation.  However, with this kind of a concept (the inclusion of imagined reality in the real world), it is possible to perform fidelity and validity assessments on all kinds of simulations – including those with little or no data from the material reality portion of the real world.Understanding and description of reality are limited and flawed, as indicated by designation of reality observations and descriptions as imperfect in Figure 2.  The perceived material reality forms the basis for both standardized databases and descriptions as well as the referent for simulation results. The application domain is derived from perceptions and descriptions of (imagined and material) reality, i. e., the real world, and is influenced by standardized databases and descriptions related to the real world.  Full description of the application domain is the primary source for definition of the fidelity referent.  Full description of the application domain means that relevant entities, tasks, actions and activities, possible states, factors, parameters, processes, and relationships among them have been identified and specified.  In this perspective, simulation fidelity is an absolute (i. e., not tied to a particular application within the application domain).  Thus, fidelity assessment can provide an objective measure of how close the simulation comes to the perception of reality described by the application domain.It is appropriate to ask,  Why the real world referent for simulation results (of Figure 2) can not also serve as the fidelity referent?  The answer is relatively simple:a)  Simulation results will be compared with material reality data as part of fidelity evaluations.  Where data are available, that data will be the basis for evaluations of accuracy in the assessment of simulation fidelity.  However, mere collection and organization of material reality data as the Figure 2 referent for simulation results is unlikely to have data adequately parsed to fully support fidelity evaluations.  Here the fidelity referent draws upon all of the data and information available to the simulation results referent, and can also draw upon the concepts of imagined reality (such as theoretical constructs to parse and organize the data). b)  For future systems, the material reality referent for simulation results may be a null set because no information yet exists:  as shown in Figure 2, all information in the simulation results referent must come from perceived material reality.c)  Before a simulation is completed (such as when conceptual validation reviews are performed), there will be no simulation results.  There are only anticipated results (or possibly, partial results).  Anticipated results are best compared with articulation of the application domain (which as noted, contain all the information available in perceived reality).  Simulation requirements are related to intended applications within the simulation application domain.  A subset of these requirements (or derived parameters) forms the acceptability criteria for the simulation.  Acceptability criteria are specific test points for simulation assessments. Acceptability criteria are a viability compromise.  They identify the more important simulation requirements:  few enough to be assessed and only those which can be assessed or tested explicitly.  In a data-rich, ideal world without resource constraints, acceptability criteria might be identical with requirements.  Usually resource or data limitations force acceptability criteria to be only a subset of requirements.  If wisdom is exercised in specification of acceptability criteria, maximum confidence in the simulation can be generated by assessment of the acceptability criteria -- given resource and data limitations.The simulation conceptual models provide the linkage between simulation requirements and simulation specifications  Simulation conceptual models coupled with the acceptability criteria define the validation referent. Acceptability criteria specify “performance” required from the simulation while the conceptual model identifies the parsing of the application domain that is to be examined.  As noted earlier, validation is a relative concept, being application dependent – in contrast to fidelity which is an objective measurement (i. e., fidelity is not application dependent).  Results from simulation test cases and from use of the simulation are subject to comparison with the three (validation, fidelity, and simulation results) referents.It should be noted that this context for simulation development does not promote a particular modeling or simulation theory and should be compatible with the variety of such theories that have been articulated to date.  This simulation development context does not specify how one organizes a description of reality’s abstractions, which is an important consideration for theories that derive from Casti.  Nor does the context above identify how one should approach the frame of reference used with the simulation, whether the Klir-based system science approach preferred by Zeigler [1998] or some other approach.  Nor does the context identified here address the details of measurement that lie at the heart of statistical approaches to simulation theory, such as espoused by Oberkampt et al or by Morrison and McKay, nor the intertwining of simulation and basic scientific laws and phenomena in computational science, as articulated by Roache.  Basic elements needed for a theory of simulation development have been identified [Zeigler 1997], but such a theory for simulation development has not yet been fully articulated although it is expected that progress will be made in that regard [e. g., Zeigler 1999].It should also be noted that the paradigms of Figures 1 &2 are compatible with the full spectrum of simulation development paradigms:  waterfall, spiral, rapid prototyping, etc.  One just has to be careful about deciding when a particular stage in development is mature enough for fidelity or V&V assessments.3.  Conceptual ModelsConceptual model is an abused term.  It has too many meanings for clear communication.  Figure 1 uses the term conceptual model for the linkage between simulation requirements and simulation specifications.  This paper identifies several levels of abstraction related to simulation application, and explains how this connotation for conceptual model fits with them.A simulation has an intended application domain.  Some call that the simulation’s mission space.  Others call it the simulation’s problem domain or subject domain.  The first abstraction of the problem, subject, reality which the simulation is to address concerns the significant entities (along with identification of their possible states, characteristics, behavior, and performance), their relationships,  and processes.  This approach is taken by the Defense Modeling and Simulation Office (DMSO) with its Conceptual Model of the Mission Space (CMMS):  “the first abstraction of the real world . . . an authoritative knowledge source for simulation development . . . capturing the basic information about important entities involved in any mission and their key actions and interactions” [Lewis & Coe].  In this, the CMMS is a simulation implementation-independent functional description of real world processes, entities, and environment associated with a particular mission [Sheehan et al].The second level of conceptual model abstraction is the simulation concept identified in Figure 1 above.  It is the response to a set of requirements that will support a simulation application within the domain of the first level of abstraction.  In this second level of abstraction, the conceptual model is the linkage between simulation requirements and specifications.Subsequent levels of conceptual model abstractions address entities and processes within the simulation concept.  These include systems, subsystems, equipment, components, composites (which are collections of components that function together), processes, and environmental considerations.  At these levels of abstraction, the term conceptual model may be used for how a system (or one of the other items listed above) is represented within the simulation.Whether at the second level of abstraction (representing the entire simulation) or at a subsequent level of abstraction (representing only part of the simulation), a conceptual model consists of the four factors identified in Figure 1:  1) assumptions, 2) algorithms, 3) architecture (i. e., the description of relationships with other items), and 4) the availability of data pertinent to the intended application.  For the remainder of this paper, the term conceptual model will be used in this way, without too much concern about which level of abstraction is being addressed since conceptual validation processes for the second and subsequent levels of abstractions are very similar.4. Conceptual Model DescriptionsA variety of standard descriptive formats exist for elements of software development that have contributed significantly to advances in software quality.  Unfortunately, similar standard descriptive formats do not exist of key simulation elements such as conceptual models.CMMS endeavors emphasize a disciplined procedure by which the simulation developer is systematically informed about the real world and a set of information standards that simulation subject matter experts (SMEs) employ to communicate with and obtain feedback from military operations SMEs.  Keys to removing potential ambiguity between the ideas of the warfighting SMEs and the simulation development SMEs are common semantics and syntax, a common format database management system (DBMS), and data interchange formats (DIF).  While significant progress has been made in developing a CMMS Toolset to provide the keys noted above, their implementation to date, such as reported by Johnson, has shown that information beyond that likely to be in the first level abstraction (i. e., CMMS) may be required for conceptual models at the second and subsequent levels of abstraction as SMEs may be “called upon to fill in details needed by [simulation] developers” that are “not provided in doctrinal and/or authoritative sources.”Several basic descriptive formats are possible for simulation conceptual models.  This section identifies four basic descriptive formats:  1) ad hoc, 2) scientific paper, 3) design accommodation, and 4) CMMS paradigm.  These labels were chosen for use in this paper because they hint at the approach taken by the descriptive format.  The most common descriptive method for conceptual models is the ad hoc method.  This label is used because ad hoc means for the particular end or case at hand without consideration of wider applications.  No formal attention normally is given to consistency (or completeness) of conceptual model descriptions using the ad hoc method.A second descriptive method for conceptual models is the scientific paper method.  This approach employs the standard construct of a scientific paper (or report) and tends to be more complete in its identification of assumptions, more explicit in its statement of algorithms in accord with standard mathematical and technical conventions, and more rigorous in its specifications of limitations arising from the conceptual model.  However, this approach has most often been used after simulation development as the basis for preparation of user manuals and analyst guides related to the simulation.  The following section shows how the scientific paper method during simulation development when conceptual validation can be most valuable.A third descriptive method for conceptual models is the design accommodation method.  In this approach, the simulation developer employs descriptive formats which have been selected to support simulation design as the method to describe conceptual models.  Sometimes this approach is taken by a simulation developer in an attempt to minimize resources required for simulation development.  For example, if the Unified Modeling Language (UML) is being used in conjunction with Rationale Rose tools, conceptual models of systems represented in the simulation may be described by UML constructs, Rationale Rose diagrams, and assorted textual descriptions associated with such.  A problem with such an approach is that there may be no standard method for capturing assumptions and limitations inherent in the conceptual model.  Likewise, portions of the conceptual model for a particular system (such as a sensor) may be scattered throughout the simulation design – and may be very difficult for a SME responsible for a conceptual validation review to have confidence that the materials which have been gleaned from the mass of design description information describe all aspects of the conceptual model which is to be reviewed.  Some design accommodation descriptive formats do not easily capture all critical elements of a conceptual model, such as temporal dependencies and relationships among simulation elements.However, it should be noted that in those formal development environments, in which requirements, conceptual models, specifications, and design are all expressed in formats which allow mathematical proof of correctness, the design accommodation approach for description of conceptual models should fully capture all attributes and characteristics of the conceptual models and facilitate their conceptual validation.  The recent text by  Alagar and Periyasamy shows how this can be done.A fourth descriptive format for conceptual models is the CMMS paradigm.  CMMS approaches to date have emphasized DBMS structures.  They have  focused upon entities, actions, tasks, and interactions (EATI) [DoD 1997]. It should also be noted that descriptive formats used for future Defense simulation requirements, conceptual models, specifications, and designs may be dictated by later versions of the Joint Technical Architecture (JTA).  The current JTA Version 2.0 (May 1998) emphasizes the evolving IEEE P1320.1, IDEF0 Function Modeling, for activity modeling and emphasizes IDEF1X97 and UML for data modeling.5.  Suggested Descriptive Format for System Representations The scientific paper method is the descriptive method recommended for system representations, the third level of conceptual model abstraction.  This approach employs the standard construct of a scientific paper (or report) and tends to be more complete in its identification of assumptions, more explicit in its statement of algorithms in accord with standard mathematical and technical conventions, and more rigorous in its specifications of limitations arising from the conceptual model.  As such, this descriptive format facilitates conceptual validation and development of simulation user/analyst guides.  It also makes it much easier for subsequent modifications to the simulation by the simulation developer.  An outline for description of a system representation in this format is presented below.The description of a system representation can be expected to change as the concept changes through simulation development.  This makes it important to keep a close tie between the system representation description and the simulation design.System Representation Description Outlinein the Suggested Descriptive FormatSystem Representation Identification.  This item should be unique and easily comprehended by a human – a short version for easy indexing may also be used.  Both should include a date for the system representation.  For example, a human-friendly name might be “GBI System Representation for Block 10 Revision 3 dated 15 Oct 98” and the short form for indexing might be “SR-GBI-BLK10-R3-10/15/98”.Principal Simulation Developer Point(s) of Contact (POCs) for the System Representation.  This item should identify the specific individual(s) associated with the system representation so that it will be easy for a subject matter expert (SME) to know whom to contact for clarification, additional information, or discussion of the system representation.  Contact information should include phone, fax, and email for each person identified.  In some cases, it may also be appropriate to indicate the person’s area relative to the system representation.System Representation Requirements/Purpose.  This item should be a brief, but specific and detailed description of what the system representation is supposed to do.  The item should also have a section which references specific simulation requirements to which the system representation responds.  This item sets the specific perspective that should be employed when the system representation is reviewed. In many cases, it is useful to parallel the CMMS data structure in the description of a conceptual model so that there can be more direct linkage between them.  However, in some cases where there are multiple levels of resolution in the simulation (as in the federation of a HLA distributed simulation), it may be appropriate to use a data  different structure, as discussed by Davis and Bigelow in their research on multiresolution modeling.System Representation Overview.  This item provides a general description of the simulation and explains how this system representation relates to the larger simulation context.  This is where interactions and interfaces of the system representation with that larger simulation context must be described and specified. General Assumptions.  This item should identify the general assumptions that apply to the system representation.  Assumptions that are specific to a particular algorithm should be stated in association with that algorithm.  It is probably desirable to repeat assumptions when that aids clarity even if it makes the description of the system representation somewhat longer.  For example, in the system representation requirements section for a radar, it might be noted that ducting and clutter would be ignored.  In that case, a general assumption might be that free-space propagation algorithms would be used – and the assumption should be repeated when the detection algorithm is specified.  Implications of an assumption should be stated with the assumption – normally such implications are a significant part of simulation limitations.  Assumptions can address many factors.  They may relate to the nature of an algorithm (as in the example above).  They may relate to how other parts of the simulation (or federation) function.  They may relate to sources and availability of information and data.  They may relate to the significance of the fidelity of different parts of the simulation (such as display realism).Identification of Entity/Process Possible States, Tasks/Actions/Behaviors, Relationships/Interactions, Events, and Parameters/Factors.  This item is where the basic ingredients of the system representation are identified.  These ingredients are not mutually exclusive, but the totality of the system representation should be identified in the total collection of them.  For example, in the case of a missile defense interceptor, possible states include down for maintenance, standby, ready, preparation for launch, boost phase, mid-course, terminal guidance, intercept, and possibly post-intercept condition.  The tasks and actions for a missile defense interceptor would include a variety of things such as:  accept/respond to/reply to command messages via its umbilical connection to the launch, launch and flyout to the intercept region, accept/respond to/reply to command messages via a communication link, monitor and report the interceptor’s status/condition during flight operation, deploy its guidance sensor, etc.  Relationships and interactions would include such things as receive and transmit messages via its communication link with the ground system, accept and act upon sensor information from target(s), etc.  Of particular importance is identification of  dependencies and independence among actions, events, processes, etc.  The material in this item is the basis for assessment of the completeness of the scope of the representation during a conceptual validation review.Identification of Algorithms.  Algorithms define ways that an entity or process behaves and how it may interact with other things in the simulation.  This item should identify all algorithms and relate them to entities and processes.  Sources (pedigrees) of algorithms (and the data to be used in them) should be specified and the relationship of the algorithm selected to similar algorithms used elsewhere should be noted (if known).  Assumptions embedded in algorithms should be noted.  Algorithms should be expressed in standard scientific and technical notation where possible.  It is recognized that some algorithms may only be easily expressed in the jargon of a particular technical field (such as that employed with decision tables), whereas other algorithms may be capable of expression in the more traditional algebraic forms normally employed for calculus, statistics, and differential equations.  Data elements of algorithms should be specified (e. g., the power and antenna gain of a radar representation that uses the standard radar range equation).  If the precise values of data elements are not available, then the expected data sources should be identified and a postulated value for the data parameter (or range of possible values) should be presented.  This kind of algorithmic information is essential if rational judgments are to be made about potential accuracy and fidelity of system representations.Simulation Development Plans.  This item addresses plans for evolutionary development of a system representation over the lifecycle of a simulation.  For example, the initial version of a sensor representation may not take into account the aspect dependence of target signature strength.  There may be plans to more fully develop the system representation for that sensor later during the simulation’s lifecycle.  This item would identify such a plan.  It must also indicate when such expansion of the system representation would be expected to occur and what its implications might be.  For example, to have a meaningful representation of aspect-dependent target signature, it is necessary that target motion be described adequately so that such information exists (e. g., a 6-dof representation of target motion and orientation is required instead of merely a 3-dof representation of target position and motion).  The development plan should provide as much detail as available about what the conceptual model (system representation) will be for its future evolution.Summary and Synopsis.  This item is the wrap-up for the system representation.  It should clearly identify limitations of the system representation as well as summarize its expected capabilities.  This item should also note explicitly any parts of the system representation which are incomplete, and indicate when those parts can be expected to be completed.  This item is where the developer of a system representation should express caveats about the system representation that should be known and understood by those who perform conceptual validation (and by those who modify the simulation later).  The variety nature of conceptual models makes it impossible to provide a simple cookbook approach for their description that will work for all system representations.  Flexibility must be allowed in order to accommodate the variety of system representations.  The structure presented above is intended as a guide to ensure that all needed information is available in the system representation.  The objective of the descriptive format for a conceptual model (system representation) is to provide a coherent set of information that fully and correctly describes the system representation so that its capabilities, limitations, and characteristics can be readily understood by SMEs and simulation development personnel.  The goal must always be to provide adequate information for a scientifically compelling understanding of the conceptual model.  The description of the system representation must provide adequate information for logical and factual assessment of the system representation.In many simulation development paradigms, descriptions of conceptual models precede initiation of simulation design – in other paradigms, design is begun prior to completion of conceptual models for the simulation. The earlier that descriptions of system representations are available, the more valuable the feedback will be from conceptual validation reviews in preventing (or correcting) faults in simulation design and implementation.  The importance of this is illustrated by the frequency with which serious problems (faults) are found during conceptual validation reviews.  The author has been involved directly in conceptual validation reviews of several dozen simulations, whose sizes range from small (about 6 staff months of development effort) to large (more than 100 staff years of development effort).  Serious flaws were uncovered in the conceptual models of every one of these. He has second-hand awareness of conceptual validation reviews of another 30-40 simulations as well as familiarity with the literature on this subject.  Others report the same experience; Law and Kelton note: “We have never seen a structured walk-through [of the conceptual model] where all of the model assumptions were found to be correct.”   A common major simulation problem is a simulation design that only partially satisfies simulation requirements.  Use of the descriptive format suggested in this paper is an important step in reducing the risk of a simulation design that will not fully satisfy its requirements. 6.  Conceptual Validation and System Representation Descriptive FormatConceptual validation is primarily performed by review processes in which Subject Matter Experts (SMEs) examine a simulation’s system representation (conceptual models at the third level of abstraction) to address simulation capabilities.  The impact of system representation descriptive format on conceptual validation has three primary dimensions:  a)  collection of materials which describe the system representation, b) completeness and logical adequacy of those materials, and c) their perspicuity.  When the system representation (conceptual model) description is largely implementation independent, general scientific and technical notation can be employed which facilitates SME correct and complete comprehension of the system representation – an important consideration for conceptual validation review.  This is possible because simulation design notation does not have to be used for implementation independent descriptions.  Verification activities provide the assurance that a validated system representation (described in implementation independent format) is what has been designed for the simulation.Collection of the Materials Describing a System Representation.  When the descriptive format suggested in this paper is employed to describe a system representation, there will be distinctly identified materials which describe the system representation.  When a design accommodation method is employed, distinctly identified materials which describe the system representation are unlikely – which means, that at least in some cases, SMEs performing conceptual validation reviews will have to root through the total collection of simulation descriptive materials and pick out those materials which the SME believes pertain to the system representation of interest.  This has great potential to corrupt the conceptual validation review process because the review may be based upon an incomplete (and sometimes incorrect – especially when preliminary materials may be intermingled with final materials) description of the system representation.  This is less likely to be a problem for conceptual validation reviews either performed by members of the simulation development team or when the conceptual model description is organized by the simulation developer than it is to be a problem when conceptual validation reviews are part of independent verification and validation (IV&V) for the simulation and the IV&V team has to collect and organize the conceptual model description.Completeness and Logical Adequacy of System Representation Descriptive Materials.  Even when there is no question that all the materials pertinent to description of a system representation have been collected, there can be questions about the completeness and logical adequacy of the materials to support a conceptual validation review. A very important point in this regard is that the information needed to support a conceptual validation review is the very same information needed to support development of a simulation design which fully satisfies simulation requirements.  For this reason alone, the simulation developer should have adequate incentive to produce an acceptable description of the simulation’s system representations.Perspicuity of Conceptual Model Descriptions.  SMEs possess a variety of backgrounds.  Some understand software development descriptive constructs; others do not.  Thus, conceptual validation reviews of system representations described in a design accommodation format either runs the risk of misunderstanding by some SMEs -- or requires that SMEs be adequately oriented to the design description format.  Such an orientation requires additional resources and may cause some SMEs to decline the opportunity to participate in the conceptual validation review (SMEs can be cantankerous).  While neither of these problems is likely to be overwhelming, they should not be ignored.  System representation descriptions in the format suggested by this paper can be readily understood by any person with general scientific and technical literacy.Conceptual Validation Reviews.  Conceptual validation is primarily performed by review processes in which SMEs examine a system representation in order to assess the inherent capabilities of that system representation (conceptual model).  This means an examination of system representation assumptions and limitations (along with their implications for anticipated simulation applications), of the entities and processes represented by the system representation, of algorithms and embedded data (including the pedigrees of the algorithms and data as well as their relationship to the algorithms and data used elsewhere within relevant communities), of relationships among elements of the system representation and with other system representations, and of architectural implications of the simulation.  Assessments of fidelity and accuracy of representation are always critical parts of conceptual validation reviews.SME competence in the topic domain and SME potential bias (or vested interest) are always matters of concern.  Sometimes SMEs with different interests (perspectives) should be used in a review to ensure that the review is comprehensive and objective (by providing balance with the diverse orientation of the SMEs involved) [Pace 1998a].  It is always desirable to define or identify evaluation criteria that will be used in a conceptual validation review prior to initiation of the review.  Review worksheets should be developed as guidelines for conceptual validation reviews.  It is generally good practice to develop such guidelines prior to initiation of the reviews.  This helps to allay unnecessary concerns about the nature and scope of the reviews since the guidelines identify the review scope and criteria.  Their identification prior to commencement of the actual reviews allow any disputes about review scope and criteria to be resolved prior to actual reviews -- which prevents (or at least minimizes potential) confusion of possible disagreements about review conclusions with differences about intended scope and criteria for the reviews.  It is always helpful when differences about conclusions can be separated from differences about approaches and philosophy.7.  Implications for Distributed SimulationThe importance of compatibility of the different elements of a distributed simulation has been recognized by VV&A guidance provided for distributed simulation applications [IEEE], by general VV&A works [e. g., Pace 1998a or DoD 1996], and by those addressing analysis [e. g., Lucas et al] and other uses of distributed simulation [e. g., Dewar et al].  Capability for effective and efficient comparison of system representation conceptual models in the various simulations of a distributed simulation is important for determination of compatibility among those simulations for the intended application(s).  It is comparison of conceptual models that will determine whether it is appropriate to use the simulation elements together in the application, or whether those simulation elements have conflicting assumptions and incompatible algorithms, etc.  All of the comments in the preceding section about the impact of the descriptive format on conceptual validation pertain here also.  Such capability for correct assessment is the heart of meaningful determination of a distributed simulation’s appropriateness for a particular application.8.  ConclusionThis paper has addressed how system representation conceptual models should be described.  Because of space limitations, the paper has not tackled descriptive formats for other kinds of simulation conceptual models.  A context for both VV&A, with emphasis upon conceptual validation, and for simulation development, with emphasis upon fidelity and validation referents, have been presented.  Implications of conceptual model descriptions for conceptual validation and for distributed simulation have been discussed.  A method for describing system representation conceptual models has been described, which, if used, will enhance conceptual validation assessments and determination of compatibility among the elements of a distributed simulation.9.  Bibliography[AIAA 1998]     Guide for the Verification and Validation of Computational Fluid Dynamics Simulations, American Institute of Aeronautics and Astronautics (AIAA) Guide G-077-1998.  ISBN 1-56347-285-6.[Alagar & Periyasamy 1998]  Alagar, V. S. and K. Periyasamy, Specification of Software Systems, Springer-Verlag (New York), 1998.[Casti 1989]     Casti, John L., Alternative Realities – Mathematical Models of Nature and Man, John Wiley & Sons, 1989.[CMMS nd]     CMMS Project at DMSO website, http://www.dmso.mil.[Davis & Bigelow 1998]     Davis, Paul K. and James H. Bigelow, Experiments in Multiresolution Modeling (MRM), The RAND Corporation National Defense Research Institute Report MR-1004-DARPA, 1998.  ISBN 0-8330-2653-4.[Dewar et al 1995]     Dewar, J., S. Bankes, J. Hodges, T. Lucas, D. Saunders-Newton, and P. Vye, Credible Uses of the Distributed Interactive Simulation (DIS) System, RAND Corporation Report MR-607-A, 1995.[DoD 1996]     Department of Defense/Defense Modeling and Simulation Office (DMSO), Verification, Validation, and Accreditation (VV&A) Recommended Practices  Guide, November 1996. [DoD 1997]  Defense Modeling and Simulation Office (DMSO), Conceptual Model of the Mission Space (CMMS) Technical Framework, 13 February 1997.[IEEE 1997] Institute of Electrical and Electronic Engineers Standard 1278.4 for Distributed Interactive Simulation.  Recommended Practice for Distributed Interactive Simulation -- Verification, Validation, and Accreditation. [Johnson 1998]  Johnson, Thomas H., “Mission Space Model Development, Reuse and the Conceptual Models of the Mission Space Toolset,: ,” 98 Spring Simulation Interoperability Workshop Papers, March 1998, Volume 2, pp. 893-900.[Klir 1991]     Klir, George J., Facets of Systems Science, Plenum Press, 1991.[Knepell & Arango 1993]     Knepell, P.L. and D. C. Arangno, Simulation Validation, IEEE Computer Society Press Monograph, Los Alamitos, CA., 1993. [Law & Kelton 1999]  Law, A. M. and W. D. Kelton, Simulation Modeling and Analysis, 3rd Edition (McGraw-Hill), to be published late-1999.[Lewis & Coe 1997]     Lewis, Robert O. and Gary Q. Coe, “A Comparison Between the CMMS and the Conceptual Model of the Federation,” 97 Fall Simulation Interoperability Workshop Papers, September 1997, Volume 1, pp. 1-11.[Lucas et al 1997]     Lucas, Thomas et al, A Guide for Analysis Using Advanced Distributed Simulation (ADS), The RAND Corporation Report MR-879-AF, 1997. [Morrison & McKay 1998]     Morrison, John Dale and Michael D. McKay, “Toward practical Mathematical Methods for Conducting Model Design and Validation,” Proceedings of the Fall 1998 Simulation Interoperability Workshop, September 14-18, 1998, Orlando, FL. [Oberkampf et al 1998]     Oberkampf, William L., Kathleen V. Diegert, Kenneth F. Alvin, and Brian M. Rutherford, “Variability, Uncertainty, and Error in Computational Simulation,” HTD-Vol. 357-2, ASME Proceedings of the 7th. AIAA/ASME Joint Thermophysics and Heat Transfer Conference (Book No. H1137B – 1998).[Pace 1998a]     Pace, Dale K., “Verification, Validation, and Accreditation (VV&A),” Chapter 11 in Applied Modeling and Simulation, Cloud, D. J. and L. B. Rainey (eds.), Mc-Graw Hill, 1998, pp. 369-410. [Pace 1998b]     Pace, Dale K., “Impact of Simulation Description on Conceptual Validation,” Proceedings of the Fall 1998 Simulation Interoperability Workshop, September 14-18, 1998, Orlando, FL. [Roache 1999]     Roache, Patrick J., Verification and Validation in Computational Science and Engineering, Hermosa Publishers, 1998.[Sheehan et al 1998]     Sheehan, Jack, Terry Prosser, Harry Conley, George Stone, Kevin Yentz, and Janet Morrow, “Conceptual Models of the Mission Space (CMMS):  Basic Concepts, Advanced Techniques, and Pragmatic Examples,” 98 Spring Simulation Interoperability Workshop Papers, March 1998, Volume 2, pp. 744-751.[Zeigler 1997]     Zeigler, Bernard P.,”Components of a Theory of Modeling and Simulation,” Volume 9 Modeling and Simulation, Technology for the United States Navy and Marine Corps 2000-2035:  Becoming a 21st-Century Force, (NRC Naval Studies Board) National Academy Press, 1997. [Zeigler 1998]     Zeigler, Bernard P.,  “A Framework for Modeling and Simulation,” Chapter 3 in Modeling and Simulation:  An Integrated Approach to Development and Operation, Cloud D.J. and L.B. Raines (eds.), McGraw-Hill, 1998, pp. 67-103. [Zeigler et al 1999]     Zeigler, B. P., T. G. Kim, H. Praehofer, Theory of Modeling and Simulation. New York, NY, Academic Press, to be published in 1999.  It is expected that this text will elaborate upon the basic ideas contained in Zeigler’s “A Framework for Modeling and Simulation,” Chapter 3 in11 in Modeling and Simulation:  An Integrated Approach to Development and Operation, Cloud D.J. and L.B. Raines (eds.), McGraw-Hill, 1998, pp. 67-103.10.  About the AuthorDale K. Pace, a member of the Principal Professional Staff of The Johns Hopkins University Applied Physics Laboratory, is a specialist in operations research, modeling and simulation, analysis, and wargaming.  An initial member of the Simulation Interoperability Workshop (SIW) Conference Committee, Dr. Pace was an instigator of SIW’s current simulation fidelity endeavors.  He is also co-chair of the Military Operations Research Society (MORS) Simulation Validation (SIMVAL) 1999 Workshop and a member of the Defense Modeling and Simulation Office (DMSO) Verification, Validation, and Accreditation (VV&A) Technical Working Group and its VV&A Technical Support Team.  He is leading the validation part of the independent verification and validation (IV&V) team for Wargame 2000.  And he is Simulation’s Associate Editor for Validation. This paper was prepared under sponsorship of the Defense Modeling and Simulation Office (DMSO), with oversight by the DMSO Verification, Validation, and Accreditation (VV&A) Technical Director, Ms. Simone M. Youngblood.  The views of this paper are those of the author and should not be construed to represent views of DMSO or of any other organization or agency, public or private.