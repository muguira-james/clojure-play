Implementation of Verification, Validation, and Accreditation (VV&A)In a CMMI-Compliant Simulation Development Environment Robert O. LewisThe Boeing Company499 Boeing Blvd.Huntsville, AL 35824-6402256-461-5920 HYPERLINK "mailto:robert.o.lewis@boeing.com" robert.o.lewis@boeing.comKey Words: Capability Maturity Model Integration, CMM, CMMI, M&S, Modeling, SEI, Simulation, Software, Systems Engineering, Verification, Validation, V&V, VV&A1.  IntroductionThis paper addresses the implementation of the VV&A process in an SEI Capability Maturity Model Integration (CMMI)-compliant development environment.  This paper compares the CMMI-specified form of internalized Verification and Validation (V&V) to the more classical VV&A approaches advocated and in use by the Modeling and Simulation (M&S) community for some years. The comparison highlights the general lack of specific V&V guidance found in the CMMI framework and discusses how this weakness makes it difficult to implement a robust, value-added V&V effort unless the CMMI guidelines are treated as the bare minimum required effort and are then expanded significantly to meet the needs of M&S development.  Major issues center on the need for V&V to be able to generate and collect sufficient evidence during development to support the Accreditation Decision, which is related to the intended use of the M&S.  Because the CMMI has embedded many of its V&V functions into the roles formerly performed by other parts of the development team (e.g., performing quality assurance, leading peer reviews, performing product evaluations, and thinly disguising traditional testing as V&V), there is less added value than one might expect.  Therefore, the premise of this paper is that regardless of conformance to the CMMI integrated V&V processes, there is still a critical need for additional V&V to fulfill the remainder of the requirements needed by the Accreditation Authority and its team to establish the necessary quality, validity, credibility, and utility of the M&S in satisfying its intended use.  Because the CMMI is neither very thorough nor prescriptive in how to implement the required V&V effort, this paper describes how the VV&A concepts expressed in the DMSO Recommended Practices Guide (RPG) and the various Service VV&A Practices can be easily adopted and used to provide an already proven and mature solution to these V&V requirements. VV&A practitioners see this as an excellent opportunity to continue to widen their support base and to propagate best practices throughout both the simulation and system software communities. The paper also addresses how a large simulation and software developer can use a highly responsive, well-balanced VV&A methodology as an augmentation of its CMMI processes.    2.  BackgroundBefore discussing the CMMI, which only officially came on the scene in 2001, it is beneficial to look back at how and why it evolved in the first place.  For more than 20 years, there has been an strong indication that much of the answer to developing high-quality, ultra-reliable software would be found in the processes used to develop it.  This initiated work under the sponsorship of the Software Engineering Institute (SEI) at Carnegie Mellon University and funded by the Dept. of Defense to evolve comprehensive software processes.  W.S. Humphrey and W. L. Sweet, who were working at the SEI, are largely credited with defining and documenting the Software (SW)-CMM.   The goal of the SW-CMM was to rank the maturity of management and technical processes and capabilities of software developers (both contractors and government agencies) on a scale of 1 to 5, with 5 being the best.  For the last several years, Level 3 has been specified by many government agencies as the minimum acceptable level for their procurements.  In other words, if you are not operating at Level 3 or above, you cannot bid certain jobs.  Thus, there has been a great deal of emphasis among government contractors to reach Level 3 as rapidly as possible.  However, this point in the maturity of a developer should be viewed as a way station and not the final destination.  The next stop is Level 4, which requires doing both quantitative and qualitative analyses on the measures and metric data that the development teams within the organization have been collecting at Level 3.  Level 4 helps developers better understand the factors of cost, performance, productivity, and defects with much greater insight.  Level 5 is at the top and is the zone in which the developer focuses hard on improving and more completely managing the processes, introducing appropriate new technology, and performing proactive defect prevention.  Granted, this discussion is just a quick overview of the five levels, but it serves to focus on the concept of continuous process improvement, which has many benefits to the producer as well as the recipient of software developed using a SW-CMM-compliant process.  Although it does not make any claims of perfection, there are many demonstrated positive effects on the quality of the end product, including significant contributions to error detection and reduction, better documentation and change control, as well as reduced cost and schedule perturbations.  Because of the successes and demonstrated improvements made by the SW-CMM over the past 10 to 15 years, the SEI began looking at other engineering disciplines as candidates for process improvement and cloned the idea of capability maturity models (CMMs) as the primary mechanism for modeling these additional processes:P-CMM People - Capability Maturity Model SA-CMM Software Acquisition - Capability Maturity Model SE-CMM Systems Engineering - Capability Maturity Model IPD-CMM Integrated Product Development - Capability Maturity ModelElectronic Industries Alliance/Interim Standard (EIA/IS) 731, which supports Systems Engineering but is not a full-blown CMMISO 9000-2000, which includes comprehensive processes but is also not a full-blown CMM.The biggest problem with all these independent models was lack of integration, too many conflicting objectives, and too many different approaches, which threatened the very acceptance of the CMM concept.  Multiple models were proving to be very expensive and hard to understand, diverting development resources and impacting overhead rates.The SEI chartered a group to evolve a solution.  The result is the Capability Maturity Model Integration (CMMI), which embodies many good features of these older models as well as aspects of ISO and other best practices.  The CMMI thoroughly covers software and system engineering in one combined practice, and adds special processes for integrated product and process development, and supplier sourcing.  The first official release was December 2001.  To further the acceptance of the CMMI, the SEI expects to discontinue support of the present SW-CMM at the end of 2003.   3.  How do these CMMs Interact with and Affect V&V?It can be observed that software technology has evolved for about 40 years, and yet, it is still a very inexact science.  Those of us who have taught it for a number of years have long known that no two people ever write exactly the same software solution for the same problem unless it is purely mathematical or very simple.  Because software is an expression of cognitive strategies, it is personality- and intellect-driven.  Thus, because no two people think exactly alike, software reflects some differences, based on who designed and coded it, even when the requirements are the same.  Thus, what we have in Computer Science is partly art and partly science.  It is also this variation in approach and the creative nature of software developers that often makes software very hard to debug and fix if someone else wrote it.  Now let us bring the SW-CMM into the discussion.  Even when a common software development process is followed, it does not actually penetrate far enough down into the design and coding processes to remove the personality of its creator.  It is also this personality that makes some software more efficient and bug free than others.  Furthermore, all the positive attributes of quality can be there in the software (Verification), yet the end product may not meet the needs of the customer or end user.  In other words, it fails the Validation axiom that goes like this, “Did I build the right thing?”  Many examples come to mind that illustrate the concept of V&V.  The Sergeant York gun worked fine mechanically, but the safest place on the test range proved to be the target.  It simply did not do what was intended, so it failed validation.  In another example, if someone had used comprehensive V&V techniques to examine a well-known planetary probe, the mission would likely have been saved, but instead, it self-destructed on impact because of a data conversion problem.  If several rocket launches that ended in destruction had a V&V program that included hardware-in-the-loop testing, they would probably not have been lost.  The reason we can make these suppositions with some confidence is based on the long successes of the manned space program in which V&V has always been present.  For example, the Shuttle Main Engine controller software has not ever had a software problem that scrubbed a mission, because extensive V&V is always performed throughout development and checkout, despite frequent changes.  Furthermore, it has only been when known high risks were ignored that big mistakes resulted, such as with Challenger. 3.1 The SW-CMM Ignored V&VIt appears that from the beginning, the creators of the SW-CMM did not spend much time contemplating the effects or impacts of having or not having V&V.  One has only to read A Method for Assessing the Software Engineering Capability of Contractors, CMU/SEI-87-TR-23, to realize that V&V was out of the boundaries of the SW-CMM focus at that time (and ever since).  In being true to their mission, the SW-CMM creators concentrated on the development (also known as engineering) process and on the management aspects of software development.  Many of us who have written about V&V through the last 25-plus years have shared a common belief that V&V is not part of the development process, but rather that it is an ancillary process that attaches itself to the development effort in a symbiotic manner.  The added value of V&V to the program comes from many sources, but mostly from having other minds put to the task of trying to uncover high risks, deficiencies, incomplete or poorly expressed requirements, errors and defects, poor decision-making, inadequate documentation, incomplete testing, poor coding practices, etc. Incidentally, those items just listed concentrate mostly on “verification” steps.  The harder part is yet to come and deals with “validation,” which examines performance, behaviors, fidelity, accuracy, utility, mission effectiveness, and takes on the user’s perspective to determine if the product is going to meet the intended use and satisfy the customer.  The importance of the next points cannot be over-emphasized.  Quality provided by SW-CMM insistence on good processes is an “internalized” characteristic; whereas, quality provided by V&V comes both from inside influence via participation throughout development and from influence “outside” the development process in a more objective frame of reference and with a higher degree of technical analysis.  In addition, SW-CMM-based processes focus on the integrity of all software developed within the organization, using common processes that are not optimized for any specific effort.  Conversely, the V&V effort concentrates on the quality and suitability of the specific system or simulation, and is not diluted or coerced by the program’s constant struggle to maintain schedules, realign budgets, change priorities, and the other multitude of factors that originate from within the development effort.  In other words, the V&V effort is much less encumbered because it is largely looking into the development effort and is not an in-line development activity, although it can serve as or support the test activity.  In point of fact, a number of V&V efforts have been shown to be worth twice their cost in direct benefits to the projects they supported [Lewis 1992, Case Studies].  A V&V effort that is too internalized, that is, always performed by resources within the development process, tends to lose many of the benefits discussed above.  The fox that watches the hen house and the banker that does his or her own audits simply cannot maintain their necessary objectivity.  The most effective V&V effort is conducted by bringing resources from outside the project whenever possible.     Much of the difficulty of building modern complex software-intensive systems is the breakages and disconnects that occur between system requirements and software requirements, and in ensuring that the testing effort addresses the right problems and issues (validation).  These are invariably combined systems and software engineering problems, which the CMMI covers in a comprehensive manner, using a “Continuous Representation” or a “Staged Representation,” selected at the discretion of the developer.  Refer to the CMMI documentation in the References section for more information on the differences between the two.    3.2 What Makes CMMI More Supportive of V&V than Was SW-CMM?When looking in detail at the CMMI model, V&V is quite prominent in both its role and degree of importance.  It is integrated into the process much the same as in the ISO model.  Thus, the underlying admission from the framers of the new “best in practice” processes for software and systems engineering includes V&V in a very obvious way.  If they had believed that it were not necessary, they would have avoided it, but instead, they placed it in a position of high importance.  The conclusion of the author of this paper is that V&V is here to stay in any CMM-compliant processes as a significant part of the quality system.  In fact, the CMMI specifies that 1) Process and Product Quality Assurance (PPQA) ensures that processes are implemented, and 2) Verification ensures that requirements are satisfied.  Since this paper discusses V&V of models and simulations, it adds the accreditation step where needed and is referred to as VV&A. The problem with the CMMI’s interpretation of what might be called “V&V best practices” is they are a mixture of traditional V&V and integrated development practices including leading peer reviews, overlapping tasks with quality assurance, performing product evaluations, and conducting testing separately under the names of “verification” and “validation.”  This tends to be confusing to the hard-core V&V practitioner who is familiar with V&V roles in support of development, and not as a substitute for the developer’s responsibilities.  For example, CMMI V&V avoids the use of the word “testing” and substitutes first “verification” and then “validation” in its place.  The author’s adoption of the CMMI processes adds back the name “testing” where appropriate and then offers to provide V&V services to augment the testing analysis performed by the development team or independent testers.  The differences in the standard CMMI V&V process and the author’s adaptation of those processes are discussed in Tables 1 and 2. They are then captured in Figure 1, which is a spread sheet of the development life cycle that includes all the CMMI-required activities and tasks supplemented by those that the author believes are essential to make V&V a comprehensive, value-added process.  The CMMI V&V process has definite holes in its coverage, which, if implemented as-is, could result in lack of coverage in key areas such as architectural and detailed design.  Thus, in an effort to cover all contingencies, the author’s tailored model even accommodates the inclusion of an independent V&V Agent, when used.  If this is the case, the customer should balance the workload and negotiate between the scope of work assigned to the IV&V Agent and that to be routinely performed using the tailored model. The division of tasks and activities must be clearly defined in the V&V Plans produced by both V&V groups to provide complete coverage without redundancy. There are some V&V tasks that should not be assigned to the IV&V Agent such as managing peer reviews, performing/supporting unit level testing, and others that provide close development support.  Basic knowledge of the CMMI is required to fully understand the tables and figure, and their ramifications in planning a cost-effective and highly productive V&V effort.    Discussion of What Is Missing in the CMMI and How to Fix ItTable 1 is directed at providing a summary of what is missing in the CMMI’s rendering of Verification.  It focuses on the most significant deficiencies and later provides suggestions on how to fill these voids.  In some cases, it poses questions, which need to be addressed in the V&V plans that are required.  CMMI references are included.Table 1. What Is Missing in CMMI’s Rendering of VerificationCMMI Scope and PA ReferencesBut Tends to Miss the Following1. Shares responsibility for work product evaluations with PPQA; however, responsibilities are different (PPQA ensures processes are implemented, while verification ensures requirements are satisfied). PPQA Intro & SP 1.1Fails to mention that verification environment, methods, and procedures are dependent and tightly linked to processes. The two issues cannot be separated as CMMI indicates. 2. No mention of specific verification tasks until Verification of Tech Data Package at end of Detailed Design.TS SP 2.2Fails to include numerous ongoing verification tasks (beginning with requirements, then architectural design/conceptual modeling, and detailed design) that are essential to project success and to carry out downstream tasks.3. Verifies implementation of each product and component, and conducts peer reviews as required. TS SP 3.1Has nothing to base these tasks on unless those specified above are added.  This requires detailed knowledge and verification of all implementations.  4. Determines and verifies integration sequences, integration environment, and written procedures and criteria.PI SP 1.1, 1.2 &1.3Fails to differentiate between “verification” and “traditional testing”.  CMMI merges the two, which are actually quite different in perspective, and raises the need for answers to the following questions:  Does this require two test environments or just one? Is it budgeted against V&V or development?  Are they two parallel functions or one?  Concept is very ambiguous as written.  V&V Plan must delineate what will be accomplished and by whom.   5. Manages and verifies interfaces.PI SP 2.2Same issues as stated above but related to interfaces.  Is V&V environment the same or different from that of developer? 6. Confirms readiness and verifies integration and assembly of product. PI SP 3.1 & 3.2Same issues as above.  CMMI has merged verification and testing and the verification process area is non-specific.  V&V Plan must define appropriate scope and activities.  7. Package, deliver, and verify product.PI SP 3.4 Provides insufficient guidance in verification process area on how to accomplish this and which parts stay with the developer. It is the opinion of the author that one can develop a CMMI-compliant Verification Plan and still not address many of the core verification issues that should be addressed.  The missing items include:Ensuring that preliminary (architectural) design (conceptual model in M&S applications) is adequately verified. Ensuring that detailed design is adequately verified.  Without these first two tasks, the remainder of the V&V effort will be seriously handicapped.Ensuring that each product component is adequately tested.  Here is where the CMMI begins combining verification with traditional “unit testing” and from here forward in the CMMI model, it becomes increasingly difficult to separate the two functions.  Specifically, CMMI implies that without verification, there would be no unit testing.  Definition of what constitutes the V&V environment and what separates it from the developer’s test and integration environment. These could be separated or combined and shared, but CMMI needs to distinguish between them and state its opinion on how to juggle these two requirements. Having two separate comprehensive environments is very expensive and leads to much disagreement in results obtained from two different ones.  A single shared environment seems the better solution for obtaining optimal benefits from the two perspectives.  The merger of verification and traditional integration and testing tasks throughout the remainder of the development makes for considerable confusion.   This reduces the value-added aspects of V&V to a minimum by placing all the burden of testing on V&V, and fails to provide “objective” evidence for acceptance of a system or accreditation of an M&S. Using a merged role would require V&V to perform the testing and play watchdog over itself at the same time. The fox should not be paid to watch the hen house.  As such, it represents a significant shortfall in the CMMI’s understanding of the purpose, value, and effectiveness of a well structured and executed V&V effort. Table 2. What Is Missing in CMMI’s Rendering of Validation   CMMI Scope and PA ReferencesBut Tends to Miss the Following1. Validates requirements using comprehensive methods. RD SP 3.5Fails to mention how to select an appropriate method such as another M&S or prototyping, nor does it address the cost and time required to provide such validation activity.2. Establishes procedures and criteria for validation.PI SP 1.3 Fails to mention how to determine what measures should be used for validation criteria.  3. Validates interfaces (non-prescriptive).PI SP 2.2Does not mention testing performance of interfaces prior to and during integration.  In addition, at least part of this activity falls on the developer, but it is not mentioned.4. Validates components (configuration items) (non prescriptive). PI SP 3.2Fails to mention interoperability or compatibility with other systems or simulations and with the environment.  Very shallow.5. Evaluates assembled components. PI SP 3.3Lacks the ties to operations and mission requirements. Does not address effectiveness or performance in the operational environment.  Fails to delineate what part of this task falls upon the developer or independent tester and what part on validation?   6. Package and deliver product.PI SP 3.4 Does not mention re-running acceptance test suite once the product is installed at the operational site.  It is the opinion of the author that one can develop a CMMI-compliant Validation Plan and still not address many of the core validation issues that should be addressed.  These include:Mission and operational suitabilityCredibilityUser satisfactionThorough testing after delivery and installation at siteEffectiveness and Performance measurement against referents and actual systems in the case of M&SThoroughly tested interfaces and measured interface performanceKnowledge of limitations and constraintsSufficient test samples to ensure stable operations and predictable results and performance.5.  Proposed Tailoring of the CMMI ProcessTo address all the issues and weaknesses discussed in Tables 1 and 2, the author proposes a number of clarifications, expansions, and interpretations to the V&V process provided in the CMMI model.  It is rather obvious that when the CMMI was being drafted, the team of writers avoided being prescriptive in many places with the expectation that companies would take it and build a comprehensive process on top of what was there.  This paper proposes such a process.  It is designed to take the basic approach outlined in the CMMI model and build on it.  To do so requires a significant number of subtle and some not-so-subtle changes (or more correctly termed “tailoring” what is there).  The major points of improvement are noted below and are captured in a life cycle-based chart depicted in Figure 1. The reader is urged to review the proposed tailoring of the CMMI V&V approach discussed below; then study Figure 1.   Clarification of the Requirement Validation process in which CMMI alludes to a list of possible approaches.  To support M&S applications, the author has narrowed the list to two: rapid prototyping of critical requirements based on real system parameters and measured results, and use of other M&S with proven performance measurements to assist in the validation process.  Before embarking on this effort, planners should be aware that attempts to thoroughly validate requirements using either technique just mentioned is likely to prove expensive, time consuming, and difficult.  From an M&S perspective, it is often better to determine which items are most critical to the mission and intended use, and focus on validating them as much as possible within schedule and budget constraints.  In addition, in M&S applications, this activity is best performed when the conceptual model is being validated.  It should rely on whatever authoritative sources are available for the systems in question.  This early-on approach often saves considerable time and effort.  This activity is accompanied by a thorough “verification” of all the source requirements by going back to the documents that describe the real systems being modeled in the simulation or by using authoritative sources when available.  Comprehensive validation of the total set of requirements may neither be necessary nor possible under the schedule, budget, and resource constraints imposed on the V&V effort.  Definition of V&V activities for preliminary or architectural design; the CMMI model contains no such recommended activities. Our VV&A model assumes that the conceptual model, which substitutes for the architectural design phase, requires both verification and validation tasks.  Verification is performed to ensure that the conceptual model is sufficient, accurate, complete, and correctly links to its source requirements.  Validation then ensures that it embodies the user’s needs, operational and mission performance and effectiveness, and can interact with the environment in a manner that will represent the assets and systems being modeled realistically enough to satisfy its intended use.  Once the conceptual model is validated, it becomes the referent for the rest of the development effort.  Definition of verification activities for detailed design; the CMMI model contains no such recommended verification activities except to inspect and verify the Technical Data Package (TDP) prepared at the end of this phase. Verification of the TDP is very difficult to accomplish unless the V&V team has been moving along together with the development effort evaluating and verifying the work products and design representations for adequacy, completeness, and correctness. Verification has to be deeply involved in the analysis of the evolving architecture and detailed design; therefore, our tailored V&V process adds comprehensive verification activities to both phases.  Additional contributions include verification of key algorithms and input data. Distinguish verification from implementer’s unit testing. The CMMI model refers to verification as the substitute for unit testing.  Our interpretation of this requirement is to continue the traditional practice of allowing both hardware and software implementers to perform their own ‘unit” testing and have the V&V team examine the thoroughness and scope of this testing making sure it is adequate.  Verification focuses on the most critical functions first and uses whatever analytical methods are appropriate to ensure correctness of the application for its intended use.  Modern developers typically use computer-aided software and hardware development (CASE and CAD) tools and it is a simple process for the V&V team to obtain necessary copies of these tools for its own use.  Duplicating the developer’s tools provides extremely trustworthy results and eliminates most of the arguments about validity of V&V analysis results.   Distinguish V&V from intermediate and configuration item (CI) level testing.  The CMMI model refers to V&V as the substitutes for these.  Our interpretation of these requirements is to continue the traditional practice of allowing both hardware and software implementers to perform their own intermediate level integration and testing and have the V&V team examine the thoroughness and scope of this testing to make sure it is adequate.  As the final hardware/software integration occurs, the V&V team continues to conduct comprehensive analysis of each test series, collecting data, analyzing the results, reporting problems, and collecting the evidence to support the accreditation. There are several extremely important validation functions that include keeping track of all requirements in terms of how thoroughly they are tested, ensuring that the acceptance criteria is being met, and tracking the acceptability criteria used for accreditation.  The V&V team should be involved with specifying test criteria and data recording, witnessing, and even participating in the testing effort. By providing a continuous level of involvement throughout the development, the V&V team is well prepared to participate in the testing and accreditation of the M&S at any level. Extend and clarify delivery and installation verification. The CMMI model requires verification to monitor/participate in the delivery and installation of the M&S at the customer site(s). Our tailored version of the process also ensures that the appropriate test suite runs correctly once the M&S is installed and is on call to repeat any part of the V&V process whenever changes occur.  6.  Summary of V&V Functions and CharacteristicsAdditional insight may be needed to better understand the V&V activities included in the V&V processes as tailored by the author.  Verification and Validation are discussed separately below and are both CMMI Level 3 activities.  The characteristics include the following: Verification:Verification is an incremental life cycle wide process that occurs throughout the development of the work products. Verification planning requires the generation of a verification strategy and detailed plan for each type of product to be verified.  Scope is similar to V&V plans advocated by DMSO and the Service Instructions on VV&A.Peer reviews that were stand-alone in the SW-CMM are folded in under verification, which promotes a close working relationship among the various stakeholders who should participate in them.    An appropriate environment is required to support the verification activities ranging from quite simple to complex elaborate simulations, emulations, scenario generators, data reduction tools, and external interfaces.As verification is performed, reports are produced and logged, and corrective actions are documented.  Then re-verification is performed as required.The verification process must be defined in concert with an organizational policy and then planned and institutionalized.  Part of this process is to acquire the required resources, assign responsibilities, and train the personnel. The CMMI is non-prescriptive in how to structure the organization responsible for performing it. Moreover, it can be assumed from the nature of the defined work such as process and product audits, that a high degree of independence must be present.  Some of these tasks may be assigned to PPQA.     Products to be verified must be configuration managed, and relevant stakeholders must be identified and involved at the appropriate levels. The verification process is monitored and controlled, and information collected and analyzed to improve development and V&V processes and conserve project assets.  This includes objectively evaluating the adherence to the processes and reviewing the status with project and senior management.  Validation: Validation demonstrates that the as-built product actually performs its intended functions in its intended environment.It uses approaches similar to verification and often shares the same environment. The two may even run concurrently. The main difference is that verification demonstrates compliance with requirements, while validation demonstrates satisfactory suitability for use in the intended operating environment.  Thus, the orientations are quite different.  Validation strategy may include only the end product or may include appropriate levels of the components used to build the product.  This means the practitioner can start as early as the requirements phase to validate such things as customer requirements against operational needs of the end-user, although most V&V experts argue that requirements can only be validated via testing (prototyping and simulation can be used early).  The environments used for product integration and verification should be considered a collaborative effort for validation, except that validation pushes the requirements for realism and analysis to the highest degree possible. This means deliberate planning to share resources to reduce cost, improve efficiency, and increase the believability of the results.  Validation often includes sophisticated test tools, data reduction tools, simulated components and interfaces, real interfaces, possible interactions with other simulations, and unique networking configurations.  In a similar manner to verification, the validation process is defined in accordance with an organizational policy and is then planned and institutionalized.  This process includes acquiring the required resources, assigning responsibilities, and training the personnel, which can be done in collaboration with the verification training.  It is recommended that the V&V efforts be performed by a single team when practical to do so.  Validation also shares the needs for configuration management of the products and for involving the relevant stakeholders in the process.  The validation process is monitored and controlled, and information collected and analyzed to improve the processes and process assets.  This includes objectively evaluating the adherence to the processes and reviewing the status with project and senior management.When V&V is used to collect evidence for accreditation of a model or simulation (M&S), the acceptability criteria are specified as early in the development process as possible.  These criteria, together with the verification evidence gathered incrementally throughout development, are melded by the validation strategy and procedures to ensure that the test results and collected evidence address all the needs of the accreditation decision. Space limitations in this paper preclude a full comparison of the CMMI V&V processes to those found in the DMSO Recommended Practices Guide (RPG) and Service V&V and VV&A Instructions, but from those summary items listed above, it becomes immediately apparent that there is a very strong match between them. Because the CMMI does not define “how to” instructions that pertain to developing the V&V strategy or processes and procedures, it is recommended that the appropriate Service directive or guidebook or the DMSO RPG be used to flesh out the VV&A process that is described in this paper, but not fully defined in the CMMI documentation.  Another important thing to remember is that no Capability Maturity Model discussed in this paper has ever assumed the responsibility of defining a detailed procedure for any aspect of software development.  This has always been considered as “going too far,” since it would tend to work only for a very narrow band of programs and would fail in the large.  Thus, the CMMI has made it possible for V&V practitioners to import their own already-proven methodologies and practices into this environment.  This includes experienced people, tools, analysis techniques, and methodologies that provide a preconceived level of confidence, utility, and effectiveness.  It is much better to start from a known baseline and do a little tailoring of the processes than to attempt to build something new.  With the former, there is a basis of estimate from historical data; with the latter there is nothing for comparison purposes.  It must be noted that the CMMI will to levy some additional requirements on peer reviews, metric data collection, monitoring and control, reporting, and data archival, all of which are beneficial to all the stakeholders.  Thus, the conversion from a typical well-constituted and adequately funded V&V or VV&A effort to the CMMI-defined version will add a little to the level of effort and hence to the cost, but some of this, for example, peer review cost can be re-allocated from the Development budget. 7.  Tailoring the VV&A Effort to Match Risk, Complexity, and Type of ProductTraditionally, VV&A efforts have been scaled and tailored to match the inherent risk embodied in the program to which they are attached.  In fact in the cost estimating method for VV&A efforts [Lewis 2000], cost is adjusted in a direct relationship to risk.  All of the more sophisticated parametric cost estimating tools today examine many factors that contribute to risk in order to produce more realistic estimates.  Complexity is another factor that is treated much like risk to adjust the multipliers inside the cost models.  The type of product, e.g., real-time, algorithm-intensive software vs. non-real-time, data-intensive systems can have a spread of as much as 10-to-1 in cost because of the difficulty associated with their development, so the VV&A effort tends to follow the system costs in rough proportion. Other factors that affect the VV&A effort include the development tools and environment, the software language being used, and the development paradigm and approach, e.g., structured, object-oriented, etc. Thus, the VV&A effort must always be tailored to the key program factors to be most effective and efficient.  It is never a “one size fits all” approach.  It should also be remembered that because there are many advantages in sharing portions of the development and test environments with V&V that these relationships and collaborative efforts must be clearly specified in the procurement packages that accompany the development and V&V efforts.  All of these factors should be defined and discussed in the VV&A strategies and detailed plans to avoid downstream misunderstandings that concern roles and responsibilities, scope, and sharing of resources.  For example, as mentioned previously, turning over responsibility for Peer Reviews to Verification is a significant departure from traditional approaches and forces a more disciplined relationship between development and verification teams.From a process view, there is not much difference except that simulation software in general brings more baggage with it than real systems. This baggage includes such things as use or generation of a synthetic environment in which the simulation operates, sometimes having to use data from sources that have a poor pedigree, difficult interface problems, requirements to run faster than or at real time, and all sorts of issues of abstraction and fidelity.  In the world of simulation, the developer is faced with a large number of issues related to how closely the simulation has to match the real system and real environment, so validation for use becomes a much more difficult problem than with traditional systems. The validation process should always take the user’s perspective and, where possible, involve the actual user. The VV&A effort includes an accreditation step that begins with early planning by establishing the acceptability criteria and culminates at the end of the development with a decision process, again involving the user or its representative, who determines if these criteria were met sufficiently to allow the M&S to be used for the intended purpose. 8.  Conclusions and RecommendationsConclusions:  The end result of well-applied V&V effort is a much more usable M&S product that has higher quality and credibility than it would otherwise.  The attributes and benefits of a well-defined and executed V&V effort are simply not found in the current SW-CMM.  They are found in the CMMI; however, there are a number of weaknesses and concerns that need to be addressed as discussed in this paper.  The goal should be a well-tailored plan that provides a balanced, consistent V&V effort that flows evenly across the development of the product.  The CMMI V&V model is perhaps even more complicated than it needed to be since it assumed the responsibility peer reviews, which is traditionally performed and managed by the project leads.It must be remembered that the CMMI concentrates on the “what,” while the V&V described in the DMSO and Service directives and guides adds the “how” to the processes.  Thus, the premise posed at the beginning of the paper leads to the conclusion that SEI CMM-rated developers rated Level 3 or above cannot just by virtue of their processes provide the same capabilities and objectives of a well-structured V&V effort. In essence, V&V is the most reliable and predictable way of assuring high-quality products that meet the user and customer needs.  Recommendations:   The first recommendation of this paper is to maintain V&V (and VV&A) in its currently-defined role for development efforts that include SW-CMM and CMMI-rated developers, regardless of that rating. The second recommendation is to view the gains in quality derived from a CMMI-rated developer as possible leverage toward increasing the tailoring (reducing but not eliminating the effort altogether) of design and implementation verification.  The third recommendation is to use currently defined V&V and VV&A practices per Service and DMSO directives and guides together with the latest CMMI processes, tailoring them only as necessary to meet all those additional requirements discussed in this paper.  It must be remembered that the CMMI defines “what” needs to be done, whereas the established V&V processes include emphasis on “how,” as well.  Thus, these two sets of processes work together quite nicely and supply a comprehensive solution.  The fourth recommendation, therefore, is continue to use V&V as the primary mechanism of ensuring the suitability, adequacy, and credibility of important systems and simulations.  An oft-quoted way of saying this is, “when the cost of failure exceeds the cost of V&V, do the V&V!”In addition, there are two additional things to remember: 1) SW-CMM and CMMI advocate “common processes” for an organization, whereas V&V and VV&A always focus on and are highly tailored to the specific product and project. 2) A SW-CMM Level 5 rating does not signify that an organization has the expertise and domain knowledge to do an adequate job of developing a weapon system or a complex simulation.  Remember these things when planning a V&V effort, regardless of the SEI rating of the developer.  ReferencesA Method for Assessing the Software Engineering Capability of Contractors, CMU/SEI-87-TR-23, Carnegie Mellon, 1987.CMMI for Systems Engineering/Software Engineering/Integrated Product and Process Development/Supplier Sourcing, Staged Representation, Version 1.1, Carnegie Mellon, March 2002.Lewis, Robert O., Cost Estimating Tutorial and Rationale Guide, Tec-Masters, Inc., Huntsville, June 1, 2002. Lewis, Robert O., Independent Verification and Validation, John Wiley & Sons, New York, 1992.Verification, Validation, and Accreditation Recommended Practices Guide, This is an on-line publication available by accessing the DMSO website at www.dmso.mil.   About the AuthorRobert O. Lewis is a member of the technical support team that developed the VV&A Recommended Practices Guide (RPG) for the Defense Modeling and Simulation Office (DMSO), and contributed to several Service-level VV&A practices.  He also works at Boeing Huntsville as a core member of the Software Engineering Process Group (SEPG) developing SW-CMM compliant processes for the site and working on the transition to CMMI.  He has worked on numerous V&V and VV&A efforts over the past 30 years, authored a textbook on V&V published by John Wiley & Sons, and is a frequent contributor of technical papers on the subject of VV&A.  He is also an ISO-9000-2000 Lead Assessor and an SEI-certified CMM Assessor and has participated in a large number of assessments and audits.  This combined experience gives him an excellent perspective into the relationships and value of V&V in the context of CMMI-compliant processes.  PAGE  1Figure 1. Comprehensive V&V for CMMI Applications (Sheet 1 of 2)Figure 1. Comprehensive V&V for CMMI Applications (Sheet 2 of 2) EMBED Word.Picture.8  