A Proposed Model for Simulation Validation Process MaturityS. Y. HarmonZetetixP.O. Box 2640, Agoura, CA  91376(818) 991-0480harmon@zetetix.comS. M. YoungbloodDefense Modeling and Simulation Office1901 N. Beauregard Street, Alexandria, VA  22311(703) 824-3436syoungblood@dmso.milKeywords:validation, process maturity, fidelityABSTRACT:  This paper proposes a preliminary model of process maturity for simulation validation.  This model begins where users demand absolutely no validation of the models they apply.  The first layer of maturity, where validation plays any role at all, consists only of face validation of simulation results, an assessment that depends entirely upon subjective sources of requirements, referent and validity judgments.  The next layer describes a process that primarily improves the objectivity of representational requirements.  The next two layers progressively improve the objectivity of the conceptual modeling and results validation component processes, primarily by leveraging rigorous descriptions of simulation fidelity.  The final maturity layer posits a validation process that does not currently exist.  The processes at this level of maturity automatically transform informal user need statements into formal validation criteria then apply formal techniques to prove conceptual model and simulation results validity.  At this final state of maturity, only the user introduces subjectivity into the validation process, an inescapable error source for simulation validation.IntroductionThe software and systems communities have both advanced the state of their practices by formulating models of capabilities to gauge the maturity of their practitioners and their processes [1, 2].  These models build capability maturity in layers that successively lead to greater end product quality.  The success of these capability maturity models suggests that such a concept could promote the advancement of simulation validation practice as well.Others have considered this problem both for simulation in general [3] and simulation verification, validation and accreditation (VV&A) [4].  Scholten and Udink ten Cate have developed a Simulation Maturity Model (SMM) in which they show the role of validation in promoting simulation quality [3].  Conwell, Enright and Stutzman show how to use the Carnegie Mellon University (CMU) Software Engineering Institute’s (SEI’s) Software and System Acquisition Capability Maturity Models (SW-CMM and SA-CMM) to improve simulation development and acquisition [4].  Both of these approaches demonstrate the importance of validation to the simulation development and use processes but do not explicitly structure validation processes themselves.  On the other hand, Pace squarely considers this structuring by arranging validation into three progressively more complete levels [5].  These levels loosely relate possible uses to levels of validation effort.  However, these levels do not specifically structure the validation processes associated with each level and do not consider process maturity.  He does address the elements of improved validation, and indirectly validation process maturity, in later work [6] but does not structure these processes into consecutive levels.  Logan and Nitta [7] propose a series of successive validation levels that correspond to the completeness of validation rather than the quality of the validation process or its products.A structuring of simulation validation can serve several purposes:To organize validation processes so that increased effort leads to increased confidence in the validation results (implying reduced risk in using the simulation for a purpose);To more clearly show the return of increased investment in validation;To illustrate a systematic approach for moving from completely subjective validation decisions to completely objective validation decisions (by sequentially attacking the high payoff areas of validation technology); andTo identify the information needed to support different approaches to validation.With these goals in mind, a preliminary model of simulation validation process maturity was constructed.  This paper describes this model and some of its attributes.Model EvolutionThe proposed model for simulation validation process maturity grew from the ideal of a parallel to the SEI SW-CMM or the System Engineering Capability Maturity Model (SE-CMM).  These maturity models evolved from the notion of Total Quality Management (TQM) for manufacturing systems.  This notion emphasizes the need for closed looped control of manufacturing processes to achieve product quality.  In developing the CMM, the SEI modeled the software development process essentially as a manufacturing process.  They chose to improve that process by Characterizing the software development process,Measuring the quality of that process’ products, andControlling the development process to optimize product quality.These steps led to the natural structuring that has become one of the strengths of the CMM.  However, the processes associated with validating simulations differ from those associated with manufacturing products in a few important ways.  Simulation validation does not produce a shrink-wrap product.  Changes in purpose or the simulation characteristics make each validation effort unique.  Using the same simulation for the same purpose can justifiably and safely depend upon the original validation information.  The quality of validation products cannot be easily measured by the error reports produced by using it.  This fact weakens the control loop philosophy underlying the SEI CMMs.  The validation process produces information upon which decisions, and the risk in making those decisions, depend.  So, those processes look more like the systematic quest for true and complete information rather than the assembly line manufacturing of the same or similar products.  Thus, the essential measures of validation information quality are its truthfulness and completeness.  As validation information approaches absolute truth, its quality and utility approaches it maximum value to the decision maker using it.These observations of the dissimilarities between software development and simulation validation support the conclusion that a process maturity model for validation could differ substantially from that for software or systems development.   Thus, the proposed model resembles the SEI’s CMM only to the point of having layered process maturity.  Since validating simulations more resembles a search for truthful knowledge than a manufacturing process, the maturity of a validation process increases, not with increasing control of the manufacturing process as does the SEI CMMs but rather, with the increasing objectivity of the validation assessment.  This discovery of an underlying notion, like that for the SEI’s CMMs, drove the structuring of the proposed process maturity model for simulation validation.Proposed Process Maturity ModelThe proposed model, shown in Table 1, organizes validation activities into five levels of process maturity.  Each succeeding level provides more objective validation decisions.  Much of the technology required to achieve the higher levels is unproven, underdeveloped or undeveloped.  The levels defined by Table 1 do not define levels of simulation credibility but may contribute to it if the simulation users subscribe to its rationale.The levels in Table 1 define validation activities in terms of the operations performed upon the information artifacts associated with simulation development and use.  These include Validation criteria, Referents, Conceptual models, Development products, and Simulation results.  The development products include such intermediate implementation artifacts as detailed design information, software development products and implementation components.Level 0 assumes that no validation is performed and is included for completeness.  Obviously, this level represents the very weakest form of simulation validation, none.Level 1 depends entirely upon face validation because no other reliable information about requirements or the referent is available.  The credibility of this level, as well as the validation decisions it generates, depends completely upon the faces doing the validating.  Many past and current efforts to validate simulations of complex phenomena (e.g., theater level warfare) depend upon this level of validation process maturity.Level 2 begins the process of removing the dependence upon the SME.  While it still depends upon the SME to interpret requirements from user needs statements, the resulting validation criteria describing these requirements are stated in the observable terms of theObjects that the simulation should represent, Properties characterizing and distinguishing those objects, and Dependencies that couple the behavior (i.e., state changes) of those properties These representational characteristics are the first elements of simulation fidelity [8, 9].  These attributes define a simulation’s representational completeness.  This level takes the first step in achieving more objective validation by getting better requirements stated in terms of observable validation criteria.  However, the absence of validation criteria that define required representational accuracies and a referent against which to measure those accuracies prevents removing the SME from evaluating the validity of both the conceptual model and the simulation results.  The determination of validation criteria from user needs assumes that the user verifies the adequacy and correctness of the resulting criteria in this level and in every subsequent level.  This step contributes to building the user’s belief in validation decision credibility as well as ensuring the correspondence between validation criteria and actual user needs.Level 3 continues with a richer description of the validation criteria that include Domains, Ranges, and Accuracies for each dependency represented.   These attributes begin to define the quantitative elements of simulation fidelity [8, 9].  However, the need to quantitatively evaluate accuracies stimulates using a referent independent of a single SME to compensate for human’s unreliable innate ability to make accurate quantitative estimates [10].  Thus, the first step in improving referent objectivity constructs that referent from multiple independent sources, correlates the information from those sources then statistically characterizes the variance associated with the correlated result.  This referent must also define the ranges and domains over which its measures of accuracy pertain.  This, in effect, characterizes the conditions under which the referent information is useful.  The availability of validation criteria describing required accuracy and an independent referent make validating the conceptual model and simulation results with an objective party (i.e., not an SME) possible.Level 4 adds additional description to the validation criteria in terms of all of the fidelity components [8, 9].  This further improves the conceptual model validation and, in turn, the utility of the verification activities to validation.  This level also adds steps that analyze the conceptual model and the verification results to identify the parts of the simulation behavior space that should be sampled for results validation.  This step improves confidence in the results validation and strengthens the ties between conceptual model validation, verification and results validation.  The referent at this level comes from multiple disparate, but correlated, sources to improve its independence from the specific sources and to reduce and better characterize its uncertainties.In Level 5, formal derivation of a provably necessary and sufficient set of validation criteria enables automatically validating a formally stated conceptual model.  This provability comes from the formal arguments for abstracting the causal chains necessary to accomplish the simulation’s purposes.  This step, along with the results from analyzing the conceptual model and verification results, also permits automatically collecting and validating the simulation results.  This emphasis upon mathematical techniques requires a strong referent that has been formally abstracted from the available knowledge.  This degree of automation provides the greatest objectivity but assumes the correctness of the mathematics upon which that automation is based.  Further, for the resulting validation decisions to be credible, the user must also believe in the sufficiency and correctness of those mathematical techniques.  The formal arguments and mathematical techniques needed to support this level have not been sufficiently developed today.The SEI’s CMMs emphasize a progression toward product quality in its levels.  The structuring of validation effort presented in this paper adopts a similar emphasis but rather on the quality of the information produced by those validation processes.  Decision makers employing simulation in their decision processes depend upon the quality of the information from those simulations.  Simulation information quality, in turn, depends upon the quality of the validation information.  The approach underlying this model of validation process maturity relies upon two key assumptions:1.	The quality of validation information depends upon its truthfulness and completeness and improved truthfulness and completeness can only be achieved through improved objectivity; and2.	Reliably improving validation process objectivity requires understanding the fundamentals of that process.The emphasis upon objectivity and fundamentals distinguishes this approach from that suggested by Pace [6].  Pace stresses the importance of quantitativity and reliance upon “real world” referents in validation.  Both of these properties are undoubtedly important to validation but many aspects of simulation representations are not naturally quantifiable and “real world” referents can be as poorly understood as the qualitative ones described by Pace [6].  These aspects will neither necessarily improve nor ensure the truthfulness and completeness of the validation information.  Poorly understood attempts to quantify information or statistically characterize information can lead to a false sense of improvement where the information truth may have actually been degraded.Table 2 shows an estimated correspondence between the levels of validation process maturity, defined in Table 1, and the objectivity gained at each level.The information given in Table 2 is strictly a subjective comparison of the objectivity supplied by the different maturity levels.  It is provided simply to illustrate the progression toward objectivity each level contributes to each information component of the validation process.Model ImplicationsThe proposed structuring has implications for the conceptual model content, the decision risk and effort cost.As the level of validation process maturity increases, the dependence upon the conceptual model information increases.  The conceptual model represents the first development product that provides sufficient information to validate against the validation criteria.  The inner three levels systematically build the information the conceptual model contains:Level 2 – specifies the objects, properties and dependencies that the simulation will represent;Level 3 - specifies the objects, properties and dependencies as well as the ranges, domains and accuracies of those dependencies that the simulation will represent; andLevel 4 - specifies the objects, properties and dependencies as well as all of the fidelity characteristics of those dependencies that the simulation will represent.Specification completeness of the conceptual model parallels the specifications of the validation criteria because those create the standard against which the conceptual model is validated.  Increased specification of the conceptual model also makes its information more useful in tailoring verification & results validation activities.Table 3 takes a stab at informally linking the proposed levels of validation process maturity to the cost of the validation effort and the reduction of the decision risk using information from a validated simulation.Table 3 assumes that decision risk is controlled by the completeness and correctness of the knowledge available to the decision maker.  Thus, the likelihood of making a wrong decision when given correct and complete information is assumed to be zero.  Table 3 also assumes that the effort cost is determined by the amount of human effort needed to produce the required information (e.g., no costs of assembling the referent are included).  As designed, the increasing levels of validation process maturity systematically reduce decision risk by increasing information objectivity and, hopefully, truthfulness and completeness.  Interestingly, the validation effort cost peaks at Level 3.  This occurs because validation effort increases up to Level 3 then increasing automation reduces the human component of that effort in the successive levels.Assuming no costs associated with building the referent constitutes a very large assumption and potential Achilles heel of this process model.  Assembling an adequate referent can easily exceed the costs of executing the validation process, even at the highest levels of effort. Further, assembling an adequate referent may be very difficult or impossible in some cases.  The lack of an adequate referent will contribute uncertainty to the validation process and can limit the value gained from exercising the higher process levels.  However, a single referent can serve many validation efforts examining the representations of the same or suitably similar simulands.  This analysis has not examined the issues associated with simulation referents sufficiently & more work will be done in this important area.ConclusionsStructuring the validation problem does not make simulation validation any easier.  It does take a step toward clarifying the dependence of decision risk upon rationally managed validation effort.  As a result, like the SEI’s CMMs, this structuring more clearly depicts where an application requires more scrupulous validation attention.  This organization of validation process maturity is also largely independent of simulation and use specifics.  As an unexpected consequence, this arrangement illuminates the future of validation.  It suggests that developing a clearer understanding of validation fundamentals can both improve validation results, with a commensurate reduction in decision risk, and reduce validation effort cost. References[1]	M.C. Paulk, C.V. Weber, W. Curtis & M.B. Chrissis (eds.), The Capability Maturity Model: Guidelines for Improving the Software, Addison-Wesley Publishing Company, Reading, MA, 1995.[2]	R. Bate et al., A Systems Engineering Capability Maturity Model, Version 1.1, CMU/SEI-95-MM-03, Carnegie Mellon University Software Engineering Institute, November 1995.[3]	H. Scholten & A.J. Udink ten Cate, “Quality Assessment of the Simulation Modeling Process, Computers and Electronics in Agriculture, 22, 1999, pp199-208,[4]	C.L. Conwell, R. Enright & M.A. Stutzman, “Capability Maturity Models Support of Modeling and Simulation Verification, Validation, and Accreditation,” Proc. 32nd SCS Winter Simulation Conf., Orlando, FL, December 2000, pp 819-828.[5]	D.K. Pace, “An Aspect of Simulation Cost,” PHALANX, 30(1), March 1997, pp12-15.[6]	D.K. Pace, “Validation Elaboration,” Proc. 2002 SCS Summer Simulation Conf., San Diego, CA, July 2002, np.[7]	R.W. Logan & C.K. Nitta, Verification and Validation (V&V) Guidelines and Quantitative Reliability Confidence (QRC): Basis for an Investment Strategy, UCRL-2002x0266, University of California, Lawrence Livermore National Laboratory, Livermore, CA, August 2002.[8]	D.C. Gross, “Report from the Fidelity Implementation Study Group,” Paper # 99S-SIW-167, Proc. Spring 1999 Simulation Interoperability Workshop, Simulation Interoperability and Standards Organization, Orlando, FL, 1999, np.[9]	S.Y. Harmon & D.C. Gross, “On Fidelity and Related Concepts,” Appendix B, Fidelity Interim Study Group Report, Simulation Interoperability Standards Organization, Orlando, FL 1998. [10]	R. Tourangeau, L.J. Rips & K. Rasinski, The Psychology of Survey Response, Cambridge University Press, Cambridge, England, 2000.AcknowledgmentsThe Validation, Verification and Accreditation Program of the Defense Modeling and Simulation Office, managed by Ms. Simone Youngblood, generously supported this work.  The author profoundly thanks Ms. Youngblood, Mr. Dirk Brade, Mr. David Gross, Mr. Michael Metz, Dr. Dale Pace and Mr. Robert Senko for their insightful and helpful comments and suggestions.  The author is especially grateful to Dr. Pace for supplying pointers to his and others’ work on this problem.Author BiographiesSCOTT HARMON is president of Zetetix, a small business specializing in modeling complex information systems.  Mr. Harmon has been developing rigorous techniques for the validation of simulation federations and human behavior representations.SIMONE YOUNGBLOOD is a member of the Senior Professional Staff at the Johns Hopkins University Applied Physics Laboratory.  She is currently on assignment at the Defense Modeling and Simulation Office (DMSO) serving as the Technical Director for VV&A.  Ms. Youngblood currently serves as chair for the Simulation Interoperability Workshop's (SIW)VV&A Forum and co-chair for the DMSO VV&A Technical Working Group. Ms. Youngblood has supported several VV&A efforts, including the Navy's Naval Simulation System, the Joint Warfare System (JWARS), and the development of the current DoD VV&A Recommended Practices Guide.Table 1.  Proposed Levels of Simulation Validation Process Maturity.LevelValidation CriteriaReferentConceptual ModelDevelopment ProductsSimulation Results0none derivednone chosennone formulatedverified only enough to support  developmentnot validated at all1represented by SME opinionrepresented by SME opinionnone formulatedverified only enough to support  developmentvalidated by SME observing simulation results2determined from user statements in terms of objects represented, their properties & the dependencies between themrepresented by SME opinionvalidated against the validation criteria by the SMEverified against the conceptual model inventoryvalidated by SME against the validation criteria3determined from user statements in terms including property ranges, domains & accuracysampled from SME population or other multiple independent sources & correlated statisticallyvalidated by objective party from validation criteria & referentverified against the conceptual modelevaluated by objective party from validation criteria & referent4determined from user statements in terms of all fidelity componentsderived from multiple independent sources & correlated statisticallyvalidated by objective party from validation criteria & referent; analyzed to suggest results sampling spaceverified against the conceptual model; provides information to guide results samplingsampled from guidance developed from CM & verification results analysis; validated by objective party from validation criteria & referent5formally derived from user statements using causality argumentsformally derived from multiple independent sources & characterized statisticallyformally stated & validated by automatically from validation criteria & referent; analyzed to define results validation sample spaceverified against conceptual model & used to define results validation sample spaceautomatically sampled from guidance defined from CM & verification results analysis; validated by automatically from validation criteria & referentTable 2.  Levels of Validation Capability Maturity & ObjectivityInformation ArtifactLevel of Validation Capability Maturity012345Validation Criterianonevery lowlowmediumhighvery highReferentnonevery lowlowmediumhighvery highConceptual Modelnonenonelowmediumhighvery highDevelopment ProductsnonenonelowmediumhighhighSimulation Resultsnonevery lowlowmediumhighvery highTable 3.	Levels of Validation Process Maturity and Risk & CostLevelDecision RiskEffort Cost0extremely highzero1very highlow2highmedium3mediumhigh4lowmedium5very lowlow