A Never Ending StoryThe Need For Verification and Validation Throughout the Life of a TestGary J. MarchandJADS JTF (SAIC)11104 Menaul Blvd NEAlbuquerque, NM 87111505-846-1165 HYPERLINK mailto:marchand@jads.kirtland.af.mil marchand@jads.kirtland.af.milABSTRACT: The End-To-End (ETE) Test, conducted under the auspices of the Department of Defense Joint Advanced Distributed Simulation (JADS) Joint Test and Evaluation (JT&E), developed a synthetic test environment that can be used for future testing, training and doctrinal development.  This synthetic environment was used initially to conduct developmental and operational testing of the Joint Surveillance Target Attack Radar System (Joint STARS).Prior to conducting the test, the simulations and synthetic environment (SE) were verified and validated using the Department of Defense (DoD) Recommended Practices Guide and the Distributed Interactive Simulation (DIS) Nine Step Process.  This paper will discuss the difficulty involved in verifying and validating a complex synthetic environment involving satellite transmission of data and the need to complete some steps of the verification and validation (V&V) during the actual testing.  In addition, it will discuss the value of V&V during the conduct of the test as a measure of the synthetic environment’s test readiness.  Conducting V&V activities prior to a test demonstrates to the tester that the synthetic environmentSE can meet the test requirements.  The rRepeating of critical V&V activities during the test confirms to the tester that the synthetic environment is functioning as expected and meeting the test requirements.End-To-End Test OverviewThe End-To-End (ETE) Test was one of the three tests conducted under the auspices of the Department of Defense (DoD) Joint Advanced Distributed Simulation (JADS) Joint Test and Evaluation (JT&E).  JADS was chartered to investigate the utility of advanced distributed simulation (ADS) technologies for the support of developmental test and evaluation (DT&E) and operational test and evaluation (OT&E).  The program is Air Force led with Army and Navy participation.  Science Applications International Corporation (SAIC) provided contracted technical support to the ETE Test.The ETE Test was designed to evaluate the utility of ADS to support the testing of command, control, communications, computers, intelligence, surveillance, and reconnaissance (C4ISR) systems.  It conducted its test and evaluation (T&E) utility evaluation in an ADS-enhanced test environment, using the Joint Surveillance Target Attack Radar System (Joint STARS) as the system under test (SUT) immersed in a representative C4ISR environment.  The ETE Test also evaluated the capability of the JADS Test Control and Analysis Center (TCAC) to control a distributed test of this type and to remotely monitor and analyze test results.The ETE Test used distributed interactive simulation (DIS) to assemble a synthetic environment (SE) for testing C4ISR systems.  The intent was to provide a complete, robust set of interfaces from sensor to weapon system, including the additional intermediate nodes that would be found in a tactical engagement.  The test traced a thread of the complete battlefield process from target detection to target assignment and engagement at corps level using ADS.  It allowed the tester to evaluate the entire thread, or the individual contribution of any of the parts, and to evaluate what effects an operationally realistic environment has had on the system under test.The test concept was to use ADS to supplement the operational environment experienced by the E-8C and light ground station module (LGSM) operators by adding additional entities to the battlefield seen by Joint STARS.  Also, by adding additional elements of the command, control, communications, computers and intelligence (C4I) systems that Joint STARS interacts with, and a weapon system to engage targets, the test team could evaluate a thread of the complete battlefield environment, from target detection to target assignment and engagement.This was accomplished by adding approximately ten thousand simulated targets to the live targets seen by the radar on board the E-8C aircraft.  As a result, a battle array approximating the major systems present in the area of interest was presented to the operators both in the air and on the ground.  A network was then constructed with nodes representing appropriate C4I and weapon systems that provided a more robust cross section of players for interaction with the E-8C and LGSM radar surveillance operators.Several components were required to create the ADS-enhanced operational environment used in the ETE Test.  In addition to Joint STARS, the ETE Test required a simulation capable of generating thousands of entities representing the rear elements of a threat force.  For this purpose, the ETE Test team selected the U.S. Army’s Janus simulation.Also, a simulation of the Joint STARS radar, called Virtual Surveillance Target Attack Radar System (VSTARS), that simulated both moving target indicator (MTI) radar and synthetic aperture radar (SAR), was developed to insert the  simulated  targets  into the  radar  stream on aboard the E-8C while it was flying a live mission.The target data was were sent to the aircraft, for processing by VSTARS, using satellite transmission.  More will be said about this later.Other capabilities used to support the test included simulations or subsets of the Army’s artillery command and control process and a simulation of the Army Tactical Missile System (ATACMS).The ETE Test consisted of four phases.  Phase 1 developed or modified the components that allowed the mix of live and simulated targets at an E-8C operator’s console and LGSM operator’s console.  Phase 2 evaluated the utility of ADS to support DT&E and early OT&E of a C4ISR system in a laboratory environment.  Phase 3 moved components of VSTARS onto the E-8C aircraft, ensured that the components functioned properly, and checked that the synthetic environment properly interacteds with the aircraft and the actual LGSM.  Phase 4 evaluated the ability to perform test and evaluationT&E in a synthetically enhanced operational environment using typical operators.More detailed information on the ETE Test can be found in the ETE test Test reports available at http://www.JADS.abq.com.  (After 1 March 2000 refer requests to HQ AFOTEC/HO, 8500 Gibson Blvd SE, Kirtland Air Force Base, New Mexico 87117-5558, or SAIC Technical Library, 2001 North Beauregard St. Suite 80, Alexandria, Virginia 22311.)SAIC.ETE Test Verification, Validation and Accreditation MethodologySince Joint Advanced Distributed Simulation (JADS) was is a DoD-sponsored joint test, the overall guide for the verification and validation (V&V) of the JADS End-to-End (ETE) Test was the Department of Defense Verification, Validation and Accreditation Recommended Practices Guide.As previously mentioned,T the JADS ETE Test was required to utilize the Institute of Electrical and Electronics Engineers (IEEE) Standard 1278 for Distributed Interactive Simulation (DIS) to develop its ADS synthetic environment (SE).During the development of the IEEE Standard 1278, one of the key methodologies associated with DIS was the verification, validation and accreditation (VV&A) process for distributed simulation applications.The Defense Modeling and Simulation Organization (DMSO), in cooperation with the VV&A subgroup of the DMSO/U.S. Army Simulation, Training and Instrumentation Command (STRICOM) Workshop on Standards for the Interoperability of Distributed Simulations, sponsored a project to define the VV&A process for distributed simulation applications using DIS.This project, known as the VV&A of Distributed Simulations, developed a DIS VV&A process model that was an elaboration and expansion of the nine -step model originally accepted for consideration by the 10th DIS Workshop.This process model and its accompanying Recommended Practice for Distributed Interactive Simulation -- Verification, Validation, and Accreditation (Draft-4 November 1996) form the basis for the VV&A of the ETE Test SE.The DIS Nine Step Process Model was developed with a conventional, short-lived, DIS exercise in mind, as opposed to a test of a major system, and presupposes a full complement of funds and personnel available at the beginning of the exercise development.If one tailors the DIS Nine Step Process Model to the joint test process, then the process model would appear as shown in Figure 2-1.Figure 2-1.  JADS ETE Test Process ModelFigure 2-1.  JADS ETE Test Process ModelIn the JADS ETE Test Process Model, test events, which consist of the planning, construction and assembling of the SE; , integration and testing of the SE; , accreditation of the SE, ;  and conduct of the test, all proceed on the left side from top to bottom.  The V&V events, to include documentation, proceed to the right for each test event.The major change from the DIS VV&A Process Model is the inclusion of the SE accreditation as a part of the test process, resulting in an eight step V&V process.  When ADS is used in support of operational and developmental testing, accreditation is a management function and is a mandatory part of the test process.The V&V of the ETE Test SE was based upon the functional requirements and acceptability criteria that were taken from the test plans and other documents that describe the SE.  The V&V agent developed the V&V plan by identifying the tasks required to satisfy these requirements and acceptability criteria in a manner that matches and complements the test activity plan, test requirements, component requirements, available resources, and timelines.During Phase 1, which paralleled the development of the DIS Nine Step Process Model, several of the initial steps in the process model, such as the V&V of the conceptual model, were completed.  The V&V activities performed during the Phase 2 of the ETE Test were divided into two categories.  The first category was the V&V of the individual nodes and the simulations used at each node.  Following the V&V of the nodes and their simulations, the SE was activated in a stepwise manner, with V&V activities performed as each component was activated.  This continuous V&V as the SE was constructed made the troubleshooting relatively painless.  The new guy was always wrong until proven otherwise.Once the SE was operational, a final V&V of the entire SE was conducted and the results were presented to a multi-service accreditation board.Functionality and Integration Tests Versus Verification and ValidationOne of the premises of this paper is that V&V occurs throughout the life of a test.  One of the reasons for this premise is the continuous V&V activity that occurred during the activation of the SE.The testers on the ETE Test team, who consider V&V to be a burden that must be borne, described the activation of the SE as a series of functionality and integration tests.  The purpose of these tests was to determine if the requirements and acceptability criteria, described in the test plan, were satisfied.The testers were satisfied that their test objectives were met by the fact that the implementation of the SE under test accurately represented their description and specifications.  They were also satisfied that the SE was an accurate representation of the real world they were attempting to simulate.  These words should sound familiar, as they are the basis of the DoD definitions for verification and validation.They used as data, the same data as those collected for the V&V of the SE.  Their tests, to see if everything was working right, were the same tests used in the V&V of the SE.  Two reports were written, a test report and a V&V report.  The data and tests performed were the same in both of them.Play it Again, SamThe ETE Test team was blessed in that we got to V&V the environment twice.  As mentioned previously,  Phase 3 of the test was designed to move the radar simulation out of the lab and integrate it into the radar subsystem on board the Joint STARS E-8C aircraft.  The DIS- derived data, required by the radar simulation, would be provided using satellite transmission to the aircraft while it was flying a mission.  Details are available on the JADS  web site.Not only was this a major modification of the previously accredited environment, but there were concerns about whether it would work.  Phase 4 of the ETE Test called for several test flights using an expensive and over committed test aircraft.The Department of Defense Verification, Validation and Accreditation Recommended Practices Guide gives six benefits of VV&A.  Obviously, the first two, increased confidence in modeling and simulation (M&S) use and reduced risk of M&S use, apply in this instance.  Prior to proceeding to Phase 4, another V&V would bewas conducted and the results would were once again be presented to a multiservice accreditation board.  Once the SE was accredited, the test flights would taketook place. Murphy Knows V&VAs the title implies, problems were encountered in conducting the Phase 3 V&V.  The first and foremost problem encountered was that we could not create the test environment without flying the aircraft.  This problem was compounded by the fact that nobody was going to let us use the aircraft just for V&V.The testing and V&V configuration, shown in Figure 5-1, was as close to flight conditions as it was possible to duplicate.  System Integration integration Tests tests and V&V were conducted using line- of- sight transmission and, when satellite time was available, satellite transmission.ESPDU = entity state protocol data unitGSM = ground station moduleGNIU = ground network interface unitSCDL = surveillance control data linkT1 = digital carrier used to transmit a formatted digital signal at 1.544 megabits per secondFigure 5-1.  Test and V&V ConfigurationFigure 5-1.  Test and V&V ConfigurationEven though this environment was very close to the test environment, we knew that it was not close enough.  When the aircraft took off, the environment became dynamic.  As the aircraft proceeded towards Fort Hood, Texas, and the entity starting positions were loaded in the database, the relationship of the aircraft was constantly changing with respect to the satellite.In addition, once the aircraft arrived on station, it flew an orbit that allowed it to keep the mission area within the radar field of view.  As the aircraft banks during the turn at each end of the orbit, the orientation of the aircraft’s satellite antenna changes with respect to the satellite.  When the aircraft banks towards the satellite, reception should improve, and when it banks away, reception should worsen.  Pretest calculations indicated that there was a possibility that data would be lost during the turn that banked the antenna away from the satellite.We were able, however, to V&V the performance of the radar simulation in its new configuration and determine that it did not affect the performance of the other subsystems on the aircraft, with one major exception.Before discussing the second problem, it should be pointed out that at this stage the V&V conducted to date did give the test team a high degree of confidence that the SE would function under the test conditions.The second problem was one that we had known about all along and had planned from the beginning to verify during the test flights.  One of the requirements for the radar simulation was that it did not affect the performance of the actual radar during its operation.  This can only be determined by measuring the performance of the radar, during a mission,, using instrumented vehicles, ground reflectors, and ground emitters.  This was scheduled to be done during the second mission, and the data would be analyzed by the Joint STARS Joint Test Force (JTF).The scheduled system integration tests, required by the Joint STARS JTF, and V&V tasks were performed on Saturday, 13 March.   The first  scheduled flight  was to wastake place on on 19 March.  During the V&V, a problem with the satellite transmission was detected.  Fortunately, this problem could be corrected using laboratory facilities, and the V&V of the satellite transmission took place on 17 March.The scheduling of the test flights could not be moved because the test aircraft was scheduled for a major reconfiguration on 3 April.As a result of the scheduling constraints and the fact that the V&V was incomplete, the accreditation authority decided to proceed with the flights without accreditation.  The V&V conducted to date provided a reasonable assurance that the environment would function well enough to provide valid test data.  The data collected during the flights would be used to complete the V&V and the results would be presented to the accreditation board.  They would then say whether we were wise and sage, or just plain desperate.Phase 4 Verification and ValidationThe Phase 4 test flights occurred from 19 March through 31 March and were conducted by the Joint STARS JTF i, in support of JADS, using the E-8C test aircraft designated as T3.  There were a total of three flights flown, with the entire ETE Test SE activated during each flight.  Results of the Phase 4 operational test (OT) may be found on the JADS web site.During the flights we would need to advise the Joint STARS JTF that the system was healthy and working properly upon arrival at the mission area.  We would also need to advise them if the system went down at any time during the mission.  Like all Joint STARS missions, our flights carried personnel that who could perform other tests if our systems did not work.It was decided that the best way to determine if the system was working properly was to perform certain critical V&V checks.  These V&V steps would look primarily at the functions used during the flight and the operational testing.During the flight to the mission area, the JTF would verify that the live radar appeared to be operating properly.  Once this was completed, we would initiate the radar simulation and verify that it was operating properly.  As a part of initiating the radar simulation, we would also establish and check out the satellite communications.  When all was ready we would then proceed to load the radar simulation database, using the satellite link, with the start-up location of all of the entities.  The status of the database could be monitored and once it was loaded, provided we were on station, the scenario would be started.  Scenario data would be provided to the radar simulation for the remainder of the flight using the satellite link.All data transmitted over the satellite link was were logged at both the source and the aircraft.  These logs were used post-test to determine data dropout and latency.  This These data was were then compared to the flight logs and loss during turns was detected.  The loss was small enough, however, that it not affect the validity of the radar images.  This completed that portion of the V&V.The time over the mission area was divided into three segments. The first segment consisted of virtual radar only, with only the 10,000 simulated entities appearing on the radar screen.  During this segment, a subset of the V&V activities deemed critical for the virtual radar were was performed.  At the same time, operational test data on the Joint STARS were was collected.  The validity of this these data was was determined post-test based upon the results of the V&V performed.The second segment consisted of a virtual radar area and an actual, or live, radar area.  During the 25 March mission, instrumented vehicles, reflectors, and emitters were present in this area, and data was were collected on the performance of the radar while the radar simulation was operating.  Subsequent analysis showed that the radar simulation did not impact the radar on the aircraft.  Also, during this segment, V&V activities were performed at the same time that operational testOT data was were being collected.The third segment consisted of three areas:, the live and virtual radar areas previously discussed, and a mixed area.  In the mixed radar area, both real radar returns and simulated radar returns were mixed, resulting in a very realistic radar image.  Once again, V&V activities were performed at the same time that operational testOT data wereas being collected.Results of the Phase 3 & 4 V&V are also available on the JADS web site.A Never- Ending StoryThis paper has attempted to describe the never- ending story of the V&V conducted in support of the End-To-End Test.  There are two important premises to be taken from this saga.The first premise is that V&V can and should take place throughout the life of a test.  Verification and validation have as their purpose the determination that the simulations used in the environment, and the environment itself, meet the requirements established by the user.  This is true for any simulated or virtual environment, no matter whether it is created using DIS, high level architecture (HLA), or your own home- grown variety.Put simply, each time you do V&V, you want to know if it is working, or meeting performance requirements, and how well it is working.  Correspondingly, each time you use the environment for any part of a test, you want to verify that it is working and how well it is working.  The key to doing this efficiently is to separate your V&V activities into two groups.  The first group of activities consists of those things that need be done only once.  Compliance activities are an example.The second group of V&V activities are those that tell you if the simulations and the environment are meeting performance requirements.  These activities are critical to determining the validity of the test data collected and must be performed each and every time that data is are collected.The second premise is that occasionally there will be no choice but to conduct V&V throughout the life of the test.  Phase 4 of the ETE Test was both ambitious and complicated.  Complicated test environments, that also use scarce and expensive elements, typically only exist during actual test trials.  This is true of both conventional tests and ADS-a augmented tests.When conducting an ADS- augmented test, it will not be atypical for V&V to take place during the pretest phases, the test phases, and the post-test phases.  Sometimes the issue is not how valid will the data be, but instead, how valid was were the data.Author BiographyGARY J. MARCHAND is the technical lead for the End-To-End Test of the Joint Advanced Distributed Simulation Joint Test Force.  He retired from the U.S. Army in 1993 as a Colonel colonel after having been the Deputy deputy Directordirector, U.S. Army Training and Doctrine CommandTRADOC Analysis Command, -White Sands Missile Range.  He is currently employed as a senior analyst by SAIC.