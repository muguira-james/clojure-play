WBT RTI Independent Benchmark Tests: Design, Implementation, and Updated Results Ms. Pamela KnightMr. Ron LiedelMs. Melanie KlinnerU.S. Army SMDCP.O. Box 1500106 Wynn DriveHuntsville, AL  35807-3801Tel. 256-955-3300 HYPERLINK mailto:Pamela.knight@smdc.army.mil pamela.knight@smdc.army.mil, ron.liedel@smdc.army.mil, melanie.klinner@smdc.army.mil,  Mr. Ray DrakeMr. Paul AgarwalDr. Edwin NunezMs. Jessica GiddensMs. Carol JenkinsCOLSA CorporationAdvanced Research Center6726 Odyssey DriveHuntsville, AL  35806Tel. 256-922-1512  HYPERLINK mailto:rdrake@colsa.com rdrake@colsa.com,  HYPERLINK mailto:pagarwal@colsa.com pagarwal@colsa.com,  HYPERLINK mailto:enunez@colsa.com enunez@colsa.com, jgiddens@colsa.com,  HYPERLINK mailto:cjenkins@colsa.com cjenkins@colsa.comKeywords: Runtime Infrastructure, benchmark, HWIL, latency, throughput, HLA, distributed simulationABSTRACT: The Modeling and Simulation (M&S) environment places critical latency and throughput requirements on Hardware-In-the-Loop (HWIL) processing configurations.  The Wide Band Information Infrastructure (WBII) Program initiated an independent evaluation of four Runtime Infrastructure (RTI) implementations to study their latency and throughput performance aspects in a standard High Level Architecture (HLA) federate environment.  Initial results of that earlier benchmark were reported at the Fall 2001 SISO/SIW conference paper 01F-SIW-033.  This paper presents an expanded discussion of the RTI benchmark concept, system specification and component requirements, the Master-Subordinate federate test sequence execution process, critical loop design diagrams, the benchmark Federation Object Model, and the RTI Proxy Federation Functional Diagram used in the evaluation of the HLA-compliant RTI implementations. An update summary of all benchmark analysis and results completed since the initial Fall SISO/SIW paper will also be presented.The evaluated RTIs now include: (1) RTI-1.3NG4, (2) MÄK RTI 1.3.5-ngc, and (3) Georgia Tech Parallel and Distributed Simulation (PADS) Federated Simulations Development Kit (FDK) 3.0 Detailed RTI.The benchmark's experimental design evaluates the effect of twelve independent variables on throughput and latency. One-way throughput and round-trip latency measures have been evaluated to determine if the RTIs exhibit statistically significant performance differences.  The benchmark results are intended to provide assistance to Modeling and Simulation personnel in HLA federation design, implementation, and optimization.The most recent benchmark activity was conducted under the new Wide Bandwidth Technology (WBT) Program at the Federation Analysis Support Technology Laboratory (FAST Lab) in Huntsville, Alabama.  The Space and Missile Defense Command is the executing agent of the WBII and WBT programs, and SAIC is the Prime Contractor.DISTRIBUTION A. Approved for public release; distribution unlimited.IntroductionThis paper is a follow-on effort to our initial benchmark work presented at the Fall 2001 SISO/SIW conference (paper 01F-SIW-033).  The objective of this paper is to provide in-depth design and implementation details of the Run Time Infrastructure Benchmark (RTIBM) software program.  Updated benchmark results of some newly-available RTI versions are also included.The earlier effort was conducted under the Wide Bandwidth Information Infrastructure (WBII) program.  It now continues under the new program name of Wide Bandwidth Technology (WBT).  Like its predecessor, this program’s executing agent is the Space and Missile Defense Command (SMDC) in Huntsville, Alabama.  SAIC continues as the prime contractor, with COLSA Corporation responsible for RTIBM activities conducted at the Federation Analysis Support Technology Lab (FAST Lab) located in SMDC’s Advanced Research Center (ARC) in Huntsville.2.0  Benchmark Concept High Level Architecture (HLA) is a software architecture used for communication between components in distributed simulations.  In HLA terminology, the primary component is designated as a “federate”.  Federates participate in a “federation”.  When a federation is running it is called a federation execution  (FedEx).  The runtime infrastructure (RTI) is the software implementation for HLA.  It provides and manages the communication interface for federates.  Since federates do not communicate directly, all federation communication is performed through the RTI, using “object attributes” or "interactions".  RTI objects are persistent and interactions are not.  Since the RTI is the sole communications pipe, it potentially introduces some latency and throughput reduction.In the last few years, various implementations of the RTI have been developed by different organizations.  Each RTI implementation has its particular strengths and limitations.  To evaluate which RTI implementation most adequately suits the requirements of a particular federation, it is necessary to have objective comparisons of RTI latency and throughput under various configurations.It is not economically feasible to test all RTI implementations in every hardware and software configuration in which they are currently used or will be used in the future.  The goal of this RTI benchmark (RTIBM) application is to provide a general-purpose tool to gather latency and throughput performance data for the largest number of RTIs in configurations that are as similar as possible.  The design uses symmetrical data exchange between federates to measure performance.  Although this may not represent the most typical federation execution, it allows the effect of more independent variables to be measured individually.The federate application, bmfed is responsible for conducting the actual benchmark tests in RTIBM.  It has the capability to conduct four major types of tests, one-way throughput for objects (OWOT) and interactions (OWIT), and round-trip latency for objects (RTOL) and interactions (RTIL).  RTIBM federates perform specific federate to federate communication as federate pairs.  A federation must consist of one or more federate pairs. In each federate pair, one federate serves as the “sender” and the other federate serves as the “receiver” for throughput tests or “returnee” for latency tests.  Figures 1 and 2 illustrate generalized sender/receiver and sender/returnee configurations for one-way throughput and round-trip latency benchmark tests.Figure 1.  Four Federate – One-way Throughput ConfigurationFigure 2.  Four Federate – Round-trip Latency ConfigurationThe RTIBM uses automation techniques for execution and data collection due to the large number of independent variables, configurations, RTIs, and tests to be performed.  These automation techniques are focused on two key areas.  First, all that is reasonable within a single FedEx is supported within bmfed.  This primarily includes the ability to vary the number of instances per object and the attribute/parameter buffer sizes. Additionally, most command line input is provided to the master federate and then propagated by it to the remaining subordinate federates. This simplifies federation start up and reduces the potential for command line errors. Secondly, a PERL script, bmmanager supports additional automation between federation executions.  This script starts federates in various configurations as required by the benchmark tests.  Flexibility is also very important.  To provide the ability to run different tests under diverse configurations without any code modification, a test sequence format (TSF) has been defined to describe each test configuration in detail. The bmmanager script reads TSF files and conducts RTIBM tests accordingly. It is also capable of identifying and terminating federate processes existing after the FedEx has terminated. As illustrated in Figure 3, all federates are started in the background.  The master federate, F1 is started as the last process. Bmmanager establishes a server and waits for the master federate to connect as a client.  Benchmark progress is provided to bmmanager through this message pipe.Figure 3.  Four Federate – Round-trip Latency Expanded Configuration3.0  System SpecificationThe RTIBM uses a minimal set of RTI interface calls for three reasons.  The RTIBM is focused primarily on performance issues.  (To verify RTI functionality, an independent mechanism is already in place with the DMSO RTI verification process.)  Some RTI implementations provide the ability to disable unused features, resulting in performance gains.Existing and forthcoming RTI implementations may emphasize performance in lieu of full functionality; by relaxing the full-compliance requirement, a larger number of RTIs can be benchmarked. The RTIBM testing was conducted at the Advanced Research Center (ARC) FAST Lab, located in Huntsville, Alabama.  Two Silicon Graphics Octane computer platforms with 704 MB of RAM running the IRIX 6.5 operating system were employed for the FAST Lab RTI benchmarking activities.  The test platforms communicated through a Cisco 2924 XL 10/100 FE switch with 110 feet of Cat-5 100 Mb/s cable between each platform and the switch.  The RTIBM tests were conducted on an isolated LAN.  Pre-test preparation consisted of terminating a list of non-essential processes including the background scheduling process, “cron”.  Also, system monitors were locked with a low priority screen blanking process.  While testing, no mouse movement, keyboard input or remote access is performed except by the bmmanager, between test samples.  The benchmark program supports RTIs providing an HLA 1.3 C++ software implementation, and can transfer data using RTI objects and interactions.  In addition, the RTIs must also support “reliable” RTI interactions, which are used for the coordination and synchronization of tests.  Table 1 presents the RTI implementations tested for this paper.Table 1.  Supported RTI implementationsRTI VersionDeveloperRTI-1.3NG4DMSOMÄK RTI1.3.5-ngcMÄK TechnologiesFDK 3.0 DRTIGeorgia Tech PADS4.0  Software RequirementsThe software requirements are spread across the three major components of the RTIBM program, which is comprised of the benchmark federate, benchmark manager, and the test sequence format.Benchmark FederateThe RTIBM federate includes:Exclusive federate to federate communicationMultiple benchmark object types per federateMultiple attributes per object typeMultiple instances of each object typeMultiple benchmark interaction typesMultiple parameters per interactionMultiple federate pairs.Flexible federate-pair configurationsAbility to vary the number of object instances within a single FedExAbility to vary the size of object attributes within a single FedExAbility to vary the size of interaction parameters within a single FedExAbility to change RTI transport mode within the FedExAbility to perform multiple samples for each test.Ability to perform multiple tests in a single FedExNo use of the RTI Management Object Model (MOM) servicesAbility to synchronize the tests/samples without the use of the RTI synchronization interfaceAbility to save test sample results as comma delimited text filesBenchmark ManagerThe RTIBM includes the ability to:Parse TSF files and start each federate with the appropriate command line options.Function with RTIs selected for evaluation.Save a log file for each federateRun on a non-test platform and relocate test results to the same host. This allows off-line analysis without impacting on-going tests.Provide adequate feedback for test analysisConduct multiple federation executions.Test Sequence FormatThe test sequence format establishes:Federate namesMaster federateSystem hostnamesRTI implementation identifierPrimary benchmark execution directoryRTIBM federate commandFederation execution data fileTimeout period for all federates to join a federationTimeout period for critical benchmark dataBundling stateTransport modeHostname of the RTI executive process, when applicableMultiple test sequences within a single TSF fileDefinition of the location of each federate.Definition of the specific RTI types per federate.Definition of the sending and receiving federate pairsDefinition of the federation execution nameDefinition of multiple test sequencesDefinition of number of samples and size of samples for each testDefinition of the type of test (OWOT, RTOL, OWIT, RTIL)Definition of the number of RTI type instances.Definition of the size of attribute and parameter buffers.Definition of the max amount of data to be queued before ticking the RTIDefinition of the max number of updates to be queued before ticking the RTIDefinition of the tolerance used for calculating the mode of round trip latency tests.5.0  Master-Subordinate Federate Test Sequence ExecutionFor each benchmark, the benchmark manager (bmmanager) reads one TSF file and starts federates on one or more systems, as needed.  For each FedEx all subordinate federates are started as background processes and the master federate is started last, also as a background process.  The master federate is responsible for coordinating and synchronizing all tests by using internal reliable RTI interactions to communicate with the subordinate federates.  The master federate propagates test descriptions to the subordinates and they send results back to the master.  Figure 4 provides the general steps taken by federates during benchmark execution.Parse command line optionsSynchronize all federates joinedWhile no errors    Get test description    Perform declaration management once    Federates synchronize on declaration management    Federates prepare for test as described    Federates select proper benchmark loop    For each sample in current test        Synchronize on test ready        Synchronize on test begin        Do inner benchmark loop        Synchronize on test finished        Save sample results    End blockEnd blockFederates resign from federation one at a timeFigure 4.  Pseudocode for general federate executionA detailed description of the synchronization and coordination processes within the benchmark federation executions is provided in Figure 5.  The test sequence diagram consists of three parts: the left and right side show master and subordinate federate actions, and the center indicates the functions used.  A brief explanation of the general actions executed by the routines and the specific actions undertaken by the master and subordinate federates is presented in Figure 4.6.0  Federation Object ModelAnother major component of a federation is the Federation Object Model (FOM).  The FOM describes all objects and interactions that may be used in a federation execution.  The RTIBM FOM was created with the DMSO Object Model Development Tool (OMDT).  The primary RTI object and interaction types are briefly described below.RTI benchmark objectsThe FOM consists of two primary object types used for benchmarking RTI object updates: objects created by send federates “sendObj”, and objects created by returnee-federates, “returnObj”.  There are thirty-two subclasses of send/receive objects, and each subclass has thirty-two attribute buffers.  These objects are used to deliver the data on which measurements are taken to characterize performance in object-based tests.  One-way object throughput (OWOT) tests utilize “sendObj” objects and round-trip object latency (RTOL) tests utilize “sendObj” and “returnObj” objects. Figure 5.  RTIBM FedEx Sequence Diagram Figure 5.  RTIBM FedEx Sequence Diagram (Concluded)Table 2 shows the classes, subclasses and attributes of the RTI benchmark objects.6.2  RTI benchmark interactionsThe FOM consists of two primary interaction types used for benchmarking RTI interactions.  Send federates create interactions called “sendInt” and returnee federates create interactions called  “returnInt”.  There are thirty-two subclasses of send/receive interactions, and each subclass has thirty-two parameter buffers.  These interactions are used to deliver the data on which measurements are taken to characterize the performance of the interaction-based tests.  One-way interaction throughput (OWIT) tests utilize “sendInt” interactions and round-trip interaction latency (RTIL) tests utilize “sendInt” and “returnInt” interactions.  Table 3 shows the classes, subclasses and parameters of the RTI benchmark interactions.6.3	RTI functional interactions: coordination, synchronization, and data exchangeThe FOM consists of two primary interaction types used for coordinating, synchronizing and exchanging the results of benchmark tests: interactions created by the master federate, “masterFederate”, and interactions created by the subordinate federate, “subordinateFederate”.  The master directs the subordinates with “testDirective” interactions.  The subordinates respond with “testState” interactions.  The master propagates test descriptions with “testDescription” and the subordinates send test results to the master using “testResults”.  The master uses the “endFedex” interaction to direct subordinates to resign from the federation.  The “internalTestID” parameter is used to ensure all federates are conducting the same test and sample.  Table 4 shows the classes, subclasses and parameters for the RTI functional interactions.7.0	RTI ProxyThe rtiProxy is a singleton class that provides a minimal framework for using RTI objects and interactions through the use of proxies.  Relevant RTI information such as object class handles, interaction class handles, attribute handles, and parameter handles are retrieved from the RTI and stored within rtiProxy for quick access.  Storage for local objects and interactions is provided by the use of Standard Template Library (STL) maps, which provide access by object instance, or interaction class handles, respectively.  Proxy objects that map to RTI objects are created, and proxy attributes for the objects that map to RTI attributes are created.  The rtiProxy class does not encapsulate the entire RTI interface.  Its emphasis is towards the benchmark application and RTI calls are made directly by the benchmark federates when optimal.  Figure 6 shows a four-federate federation with the use of rtiProxy.  Calls are made to the local RTI component (LRC) directly by the federate and through rtiProxy by using the RTI ambassador (rtiAmb) calls supplied by the RTI.  The LRC makes calls to the federate through the federate ambassador (fedAmb), which provides callback functions.  These are pure virtual member functions that are overridden by benchmark federates to define specific responsive behavior to the RTI.  All RTI data is received by federates through the federate ambassador class.  As shown in Figure 6, federates exchange RTI information through the LRCs.8.0	Sender-Receiver Throughput and Latency Critical Benchmark Loop DesignEach federate plays a role as “sender” or “receiver” for one-way throughput tests.  Similarly, during round-trip latency tests, they act as either “sender” or “returnee”.  For the RTI benchmark application, federates must exist in sender/receiver or sender/returnee pairs that communicate timed benchmark data only with each other.  To accomplish this, sender federates publish and update specific objects or interactions to which only the corresponding receiver/returnee federate subscribes.Four inner benchmark loops were developed in the RTIBM to measure the performance of throughput and latency for objects and interactions.  The same loops are used for both “best effort” and “reliable” transport modes.  At this time, the RTI data distribution management and time management services are not used in the RTIBM.  By design, the RTI tag is not utilized.  An eight-byte timestamp is packed into the front of every attribute/parameter buffer.  Buffers exceeding eight bytes include the timestamp and arbitrary data in the remaining allocated buffer.  The benchmark loops consist of: one-way object throughput (OWOT), one-way interaction throughput (OWIT), round-trip object latency (RTOL), and round-trip interaction latency (RTIL).  Throughput testing measures how much data can be sent in a limited amount of time and is expressed as updates per second for the RTIBM.  Since the buffer size and duration are known, performance could also be expressed as megabits per second or some other unit of measure.  With latency tests, the emphasis is placed on the delay between the time data is sent and received.  The RTIBM measures latency using round-trip data exchange.  Rather than using an arbitrary update rate, data is propagated based on two other thresholds.  The first is the amount of data queued in the LRC and the second is the number of updates queued in the Local RTI Component (LRC).  This approach enables the RTIBM to determine the maximum frequency of the RTI as well as the latency.  For most applications the frequency of the update is as important as latency.Figures 7 through 10 present the design details of the inner benchmark loops.  These loops are comprised of source code spanning four functional areas across two federates that are represented in the diagrams by four distinct blocks.  Each federate’s main function contains code allowing data to be sent using RTI executive calls.  Each federate also contains code in the federate ambassador (denoted by LRC in the figures), allowing the RTI data to be received.  The benchmark loop is executed at the level of an individual sample.   Thus, for a test with a sample count of ten, the loop would run ten times and the results of ten samples would be saved.  Additionally, the number of updates in a sample must be defined.  Within the context of the RTIBM, sample size refers to the number of “updates” in an individual test sample.  Also note that “queuing” interactions or object attributes to the LRC refers to invoking the RTI ambassador calls sendInteraction() and updateAttributeValue() respectively. Arrows indicate the transfer of timed benchmark data from one functional area to another.  OWOT tests consist of sender/receiver federate pairs.   Preparation for the test is completed before entering the time-sensitive benchmark loop.  At this point, sender federates have already published only the object types and attributes in accordance with the test description, and have already created the correct number of instances per object type.  The receiver federate has already subscribed to these objects and discovered the created instances.The benchmark loop is described as follows.  The sender federate traverses the map of send-objects and updates the attribute buffers with the current timestamp. The timestamp merely provides unique data per object update.  The updates are queued to the LRC on a per-object basis.  The thresholds for the amount of data and number of updates are checked. If either is exceeded, pending updates are flushed from the sender federate’s LRC and delivered to the receiver federate’s LRC.  This is done by using the flushing version of the tick function.  Each time the bottom of the cycle is reached, all pending updates in the LRC are flushed regardless of any threshold. The receiver federate cycles through a loop, continuously flushing the sender’s reflections from the LRC to the federate ambassador with the RTI tick() call until the expected number of updates is received or a timeout occurs.  Updates are then counted and discarded.Figure 8 provides a similar diagram for the OWIT benchmark.  Basically, the only difference between OWIT and OWOT is that interaction parameters are used instead of object attributes to send benchmark data.  Unlike RTI objects, with RTI interactions all parameters are inherently published or subscribed.  By definition, interactions do not have instances; none are created or discovered.  Only the types of interactions and number of parameters per interaction are used as specified in the current test description. RTOL tests utilize send/returnee federate pairs.  Preparation for latency testing occurs in the same manner as with the throughput loops, but with some distinctions.  Since latency is measured using round-trip data transfer, returnee federates must produce return-objects that correspond to the sender federate’s send-objects.  The send federates subscribe to and discover the appropriate return objects.The benchmark loop is described as follows.  The sender federate traverses its map of send-objects and updates the attribute buffers with the current timestamp.  One timestamp is acquired for all attributes in a particular instance as the object is updated.As with throughput tests, object updates are immediately queued to the LRC and the updates are flushed if either the threshold for the number of updates or the amount of data is exceeded.  However, with latency tests, the sender federate must process object reflections pending in the LRC that have been returned from the returnee federate. This allows it to achieve optimal round-trip latency performance. This is accomplished by ticking the RTI until no new reflections have been received. At this point, the sender federate resumes updating send object attributes.  It is important to note that any time the RTI is ticked, data is potentially sent and received by both federates in the same invocation of tick(). Returnee federates continually cycle and process sender reflections until either the expected number of reflections occur, or a timeout expires. While in the federate ambassador, the returnee federate processes sender reflections by copying the original timestamp to the corresponding proxy return object attribute. A map of send-object handles to proxy return-objects created during the preparation phase is utilized. It reduces processing overhead while in the federate ambassador.  After the appropriate object’s proxy attributes are updated locally, a pointer to its proxy instance is pushed onto a list.  Upon returning from the federate ambassador, the returnee federate queues return reflections from the proxy objects containing the original timestamp to the returnee’s LRC for delivery.  During the next invocation of tick(), the original timestamp is delivered back to the sender federate’s LRC.  The sender federate processes return reflections occurring in the federate ambassador by storing the round-trip time, simply calculated as the difference between the current time and the original time. Figure 9 illustrates the design of the RTOL loop.While experimenting with various implementations of the round-trip latency inner benchmark loop, two versions were found to yield the best results.  Version 1 is represented as shown in Figures 9 and 10.  Version 2 uses the two function calls “usleep(1) and tick()” instead of the one tick(min,max) call.  To determine the optimal version for each RTI, ten samples consisting of ten thousand updates per sample were conducted for each RTI in three configurations using both versions.  The tests consisted of two federates using one object or interaction depending on the test type, with one eight-byte attribute or parameter buffer.  The results are shown in table 5.  Round-trip latency is expressed in milliseconds as the mean of the sample means (representing 100000) updates for each test.Average update frequencies illustrate how often each object or interaction is updated per second.  Version 1 consistently yielded the best results for RTI-1.3NGv4 for interaction and object tests whereas version 2 proved to be consistently optimal for MAK RTI1.3.5-ngc and FDK_3_0_DRTI.  The difference in performance between the benchmark loops is significant in some cases.  It is also interesting that object-based tests slightly outperformed interaction-based tests for the RTI-1.3NGv4 while the reverse is true for the other RTIs.Benchmark ResultsThe updated RTI benchmark results are presented below in Figures 11 through 36, and in Table 6.  Some results may differ from those presented in our Fall 2001 SIW Conference paper (01F-SIW-033) because they were generated using new RTI implementations and enhanced benchmark code.  Due to space constraints, only a subset of the full results is shown here.  For similar reasons, a comprehensive discussion of each figure is not practical.  We intend to release the full set of RTI benchmark results to the modeling and simulation community in the near future.  It is important to note that analysis of all test results would be required to provide the most representative assessment of RTI throughput and performance characteristics in the benchmark configuration.  The subset of benchmark results presented in the following pages provides a snapshot of RTI performance in selected test scenarios.Each figure presented below graphs the variation of a single independent variable with respect to average time and frequency.  The independent variables are: number of object types, number of object instances, attribute buffer size, interaction types, and parameter buffer size.  Dependent variables are average time and frequency.10.0	SummaryIn conclusion, this paper provides pertinent details of the design and implementation of the current RTI benchmarks.  Analysis of the benchmark results provides important information for RTI selection, evaluation, and optimization.  For follow-on activities we intend to continue, expand, evolve, and refine this RTI benchmarking effort, incorporating M&S community suggestions and feedback.Figure 11.Figure 12.Figure 13.Figure 14.Figure 15.Figure 16.Figure 17.Figure 18.Figure 19.Figure 20.Figure 21.Figure 22.Figure 23.Figure 24.Figure 25.Figure 26.Figure 27.Figure 28.Figure 29.Table 6.  Attribute Updates Dropped Per Instance Count in Figure 29RTIInstanceCountUpdates Droppedvs Total UpdatesDMSO5124,416 / 512,0001000116,708 / 1,000,000MÄK128435 / 128,00025610,132 / 256,00051240,600 / 512,000100089,117 / 1,000,000Figure 30.Figure 31.Figure 32.Figure 33.Figure 34.Figure 35.Figure 36.Author BiographiesPAMELA KNIGHT is the Acting Director for the Space and Missile Defense Command's Information Science & Technology Directorate.  Ms. Knight has more than 20 years of professional experience in physics, engineering, test and evaluations, system engineering, information technology and management. MELANIE KLINNER is an Electrical Engineer in the Space & Missile Defense Command's Information Science & Technology Directorate.  She has more than 10 years of experience in DoD programs including Modeling and Simulation, Information Technology and Information Management, Systems Administration, Software Development and Systems Integration Analysis.RON LIEDEL is a senior engineer with the Space and Missile Defense Battle Lab (SMDBL) located in Huntsville, AL, and has authored numerous SMDC and BMDO policy forming software documents.  These include the SMDC Software Development Plan, the SMDBL 10 Year Software Plan, as well as the Command Software Mission and Function Statement.  Mr. Liedel founded and chaired the SMDC/SDIO Computer Resource Working Group and has presented several papers nationally on Software Sizing for Mega Systems. He serves as the FAST Lab Director for SMDC.RAY DRAKE is a Software Engineer for COLSA Corporation in Huntsville, AL.  He has over 14 years of experience in system administration, optimization, and programming across various UNIX platforms.  He is currently COLSA's technical lead for the Federation Analysis Support Technology (FAST) Lab at the Advanced Research Center (ARC) in Huntsville, AL.  He is responsible for developing the RTI benchmark software used to conduct the timing studies. DR. EDWIN NÚÑEZ is a scientist for Research, Development, Test & Evaluation at COLSA Corporation in Huntsville, AL.  He has been involved with WBII, WBT, and projects requiring the application of innovative technologies to algorithm development.  Dr. Núñez provided the Design of Experiments for the test cases run for the RTI benchmark testing.PAUL AGARWAL is the COLSA Corporation Advanced Research Center (ARC) FAST Lab Program Manager.  He also serves as the COLSA Corporation ARC Program Manager for the ITB, a joint international missile defense simulation testbed project co-sponsored by SMDBL's Testbed Project Office.  He has over 19 years experience in public and private sector computer software and systems analysis, design, development, evaluation, implementation, and program management.CAROL JENKINS is a Senior Systems Engineer with COLSA Corporation.  She has over 20 years of professional experience in DoD programs including Modeling and Simulation, Design of Experiments, Statistical Data Analysis, Software Development, Independent Verification and Validation, Communication Analysis and Weapon System Test and Evaluation.  She earned her Bachelor of Industrial Engineering degree in 1981 and her MS in Engineering from the University of Alabama in Huntsville.  She is responsible for data analysis following the RTI benchmark testing.JESSICA GIDDENS is a Systems Analyst for COLSA Corporation in Huntsville, AL.  She received her Bachelor of Science in Mathematics in May 2001.  Since joining COLSA, she has participated on analytical, engineering, and testing simulations in support of the U.S. missile defense effort.  She has also been responsible for data analysis following RTI benchmark testing.Table 5.  RTL Inner Benchmark Loop Selection ResultsTable 4.  RTI Functional InteractionsFigure 6.  RTI Proxy Federation Functional Diagram EMBED Word.Picture.8   EMBED Word.Picture.8  ClassSubclassesParametersmasterFederatetestDirectivetestDirectivetestDescriptioninternalTestIDtestDescriptionendFedexnamesubordinateFederatetestStateinternalTestIDtestStatetestResultstestResultsTable 3.  RTI Benchmark InteractionsClassSubclassesParameterssendIntS1-S32buffer_1 – buffer_32returnIntR1-R32buffer_1 – buffer_32Table 2.  RTI Benchmark ObjectsClassSubclassesAttributessendObjS1-S32buffer_1 – buffer_32returnObjR1-R32buffer_1 – buffer_32Figure 9.  Round-trip Object Latency Sequence DiagramFigure 7.  One-way Object Throughput Sequence DiagramFigure 8.  One-way Interaction Throughput Sequence DiagramFigure 9.  Round-trip Object Latency Sequence DiagramCONFIG - TYPE - VERS     RTI -1.3NGv4  MAK RTI1.3.5-ngcFDK_3_0_DRTI-SGIMP-TCP RTLHZRTLHZRTLHZ       2 SYS 2 CPUs RTOL V14.12232.3315.3864.51179.054777.062 SYS 2 CPUs RTIL   V17.16136.5014.6958.9869.164488.47    2 SYS 2 CPUs RTOL V26.65208.635.65341.582.33470.692 SYS 2 CPUs RTIL   V210.62158.243.44364.332.46452.36    1 SYS 2 CPUs RTOL V14.20227.759.21136.97241.412826.771 SYS 2 CPUs RTIL   V17.25134.7715.7772.4294.732806.34    1 SYS 2 CPUs RTOL V26.90220.063.99348.922.00497.081 SYS 2 CPUs RTIL   V21386.77141.362.40450.572.07490.66    1 SYS 1 CPU   RTOL V17.24132.4238.2652.23594.381491.431 SYS 1 CPU   RTIL   V111.8382.4620.9751.44514.611522.89    1 SYS 1 CPU   RTOL V217083.05130.256.57282.252.00498.521 SYS 1 CPU   RTIL   V216137.9582.504.21332.111.99498.26Figure 10.  Round-trip Interaction Latency Sequence Diagram