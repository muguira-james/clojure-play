Automatic and Real-Time Visualization of NASA Constellation Vehicle SimulationsWesley N. Colley and Gregory S. ReedCenter for Modeling, Simulation and AnalysisUniversity of Alabama in HuntsvilleHuntsville, AL 35899(256) 824-4625 HYPERLINK "mailto:colleyw@uah.edu" colleyw@uah.edu,  HYPERLINK "mailto:reedgs@uah.edu" reedgs@uah.eduABSTRACT:  NASA uses several in-house and COTS M&S tools to support Project Constellation.  Many of the tools used by Marshall Space Flight Center engineers lack sufficient interoperability.  In fact, translation from one tool to another often involves time-consuming manual editing of output files.  We have developed two tools that provide a much faster, much more automated approach to this translation.  The first is a simple, low-overhead Perl code that automatically parses the raw output from NASA’s propagation tools to extract relevant data for visualization. The tool automatically communicates the data from within Perl to the visualization software via TCP/IP.  The second tool is a full-up HLA federate that communicates with NASA's Distributed Space Exploration Simulation (DSES) to gather relevant data from the federation and feed it to the visualization software in real time, again via TCP/IP. Currently, our tools link with Satellite Toolkit, a commercial visualization product in wide use at Marshall Space Flight Center, but they could be readily extended to link with other software packages with network-ready APIs.Interoperability at MSFCAt Marshall Space Flight Center, and at NASA centers nationwide, scientists and engineers have produced a tremendous wealth of modeling and simulation (M&S) resources that touch nearly all aspects of spaceflight.  Unfortunately, a substantial fraction of the resources resides in “black box,” “stovepiped,” or very highly customized software that is not interoperable with other software.  A major goal, therefore, of the Constellation Integrated Modeling and Simulation Office (IM&S Office) has been to identify models and simulations that have applicability to Constellation, and to find ways to increase their interoperability with other Constellation products and/or commercial-off-the-shelf software.In a previous effort [1], we worked to find examples of non-interoperable M&S technologies that held promise for Constellation.  We were led to the Engineering Directorate, specifically.  There we discovered that many of the engineers were engaged in “file interoperability,” meaning that they would take output files from one piece of software, and use that file as input to another.  Obviously, the two programs could not run simultaneously in this configuration.  Worse still, the programs in question did not produce files that were readable by other programs, and typically the engineers were left to hand edit files, line by line, to make them readable by the next program in the chain.  When we reported this to the IM&S office, it was agreed that we should begin work immediately to alleviate the situation at Engineering.In speaking with the EV-12 personnel, we learned that many of the engineers were using Satellite Toolkit (STK) to visualize output from several different NASA orbital models:  Maveric, POST, and OTIS.  STK, from Analytical Graphics, Inc., is a commercial off-the-shelf (COTS) software platform that offers extensive capabilities in simulation and analysis of space-oriented missions.Automated File ConversionOur first task was simply to streamline the process of “file interoperability” used by MSFC engineers.  There are many choices of environment that would be well-suited for such a task; we selected the Perl scripting environment for several reasons.Perl is specifically designed for parsing text, and it is widely used in this capacity.Perl natively incorporates regular expression matching, which is extremely useful for our purposes.STK provides Perl hooks into its API, allowing Perl scripts to send commands to STK in order to manipulate objects, animation, models, and the overall scenario.Perl, as an interpreted language, is operating system independent.Basic File ConversionMaveric, POST and OTIS all produce output files in the form of ASCII tables, in which each row represents a new time-step, and each column stores the relevant data about each spacecraft or spacecraft segment; typically, the columns of the tables contain (x,y,z) positions and possibly roll, pitch, and yaw.  Our Perl code automatically detects the meanings of each column from the column headers and proceeds with parsing the rest of the file.There are a couple of slight complications to this basic scheme.  Commonly, because of the way the orbit generators work, the time-steps between rows will be inconsistent, or even zero (i.e., two lines at the same time-stamp).  The other complication is that the coordinate systems are not necessarily consistent between the different orbital models.  For example, some software outputs position relative to the initial launch platform, rather than in a more standard coordinate frame such as J2000.0.The first of these complications we addressed with two user-specified operating modes.  The default mode is that only the final entry at a given time-stamp is reported, so that no repeated time-stamps can enter the output file.  The second mode allows the user to specify a uniform time-step for the output.  In this mode, the script transfers data at a rate no greater than that set by the time-step.  Moreover, using a large time-step enables the visualization of a very large output file in a short amount of time, albeit with less fidelity than in the default mode.  For quickly visualizing results from a specific run, this decrease in fidelity is not often a concern, as STK is capable of interpolating smoothly between fairly large gaps in position. The coordinate system problem is, fortunately for us, largely solved in STK itself.  STK has great flexibility with respect to coordinate system—its proponents like to say that at its core, it is a “vector engine.”  STK even allows the user to define a local coordinate system (such as one fixed to the launch platform).  Since our proof-of-concept goal here was output readable by STK, we made no further effort in addressing coordinate issues.Currently, the output file has the form of an STK ephemeris file; however, this would be trivial to change to many other possible formats.We should mention that while Perl is included in nearly any Unix distribution, one must download a Perl interpreter for Windows.  We have used the freely downloadable ActiveState Perl distribution under Windows XP and have encountered no problems therefromSTK’s Connect APIThe STK distribution provides (at additional licensing expense) an API that is both C/C++ and Perl compatible.  The connection into the API is made through TCP/IP and is therefore accessed via C/C++ using standard TCP/IP libraries at compile time and during execution.  The Perl compatibility comes in the form of two Perl modules (.pm files).  To use them one must first ensure that the files are in Perl’s search path (which includes the local path) and one must include the command “use STK;” within the Perl script itself.  This surprisingly straightforward interface allows one to control STK nearly completely from within Perl, remotely across the Internet.The Connect capability allowed us to create an exciting addition to the base file conversion utility.  Instead of the usual method of starting a new scenario in STK and uploading the ephemeris file by hand, our tool can automatically create the spacecraft with its correct ephemeris within STK (remotely, if desired) with no further effort from the user.To add to the appeal of space visualization, it is often desirable to include renderings of the spacecraft, and “articulations” which include engine firings and stage separations.  STK has straightforward facilities for this, allowing importation of CAD files of several different formats for the spacecraft model, and articulation files, which are essentially time-stamped scripts for the actions of the spacecraft during the mission.  Rendering the model and articulations Figure  SEQ Figure \* ARABIC 1:  Partial screen shot of STK rendering Maveric data as parsed by our Perl tool.  The spacecraft has been scaled in size for emphasis.is really just a matter of pointing STK to the CAD and articulation files.  As such, we have provided command-line options to our script that allow the user to specify these files.   REF _Ref175466780 \h  \* MERGEFORMAT Figure 1 shows a screen shot of STK as it is being driven by our script, using a Maveric input file, a NASA-provided CAD model for the Ares I spacecraft, and a NASA-provided articulation file.  Again, this requires no knowledge of the internal structure of the Maveric data file, no file editing, and no manual configuration of STK.Ultimately, the utility of the Perl script is that as soon as the Maveric, POST or OTIS file is available, one can, with a single command-line entry, render the orbit in STK.For reference, this Perl tool and its associated documentation can be found at http://cmsa.uah.edu/?downloads.Full Real-Time HLA InteroperabilityThe Perl tool described previously is very handy, but it is still one step short of full real-time interoperability—the orbit generation software (Maveric, OTIS or POST) is not running simultaneously with STK.  As described in the introduction, full real-time interoperability is the true goal of the Integrated Modeling & Simulation Office, and as such, we proceeded with facilitating that capabilityThe Case for HLAWe choose here to make a few comments about the High-Level Architecture (HLA) since many in the space community are only somewhat familiar with it.  During the late 1980s and early 1990s, the defense community found itself in a situation not unlike the current one at NASA:  it had accumulated a significant M&S capability, but most of its M&S software was locked up in stovepipe or black-box systems.  Furthermore, at the time, computer power was such that the idea of running large operational-level simulations on a single machine was simply not tenable.  One solution was to “distribute” the simulations by allowing different machines to simulate different portions of the scenario.  Quite typically, this meant something like allowing an Air Force simulation to “play” air assets and a Navy simulation to “play” maritime assets.  The problem was, of course, getting these systems to communicate the relevant data to each other reliably in real time.  To short-circuit the history lesson, we fast-forward to the mid-1990s, when simulations had matured, and some interoperability was commonplace in large training exercises, often leveraging off the Distributed Interactive Simulation (DIS) protocol.  However, this and other interoperability protocols of the time fell short of some of the needs of the increasingly large collections of simulations.  At this time, the defense community created HLA as a highly standardized, yet highly flexible interoperability protocol.  By now, HLA has been adopted as an IEEE standard, and it is commonly used to “federate” (join) up to dozens of models, each called a “federate,” running in locations separated by borders and even oceans, into a single large coherent “federation.”The main reason for the refresher is to underscore HLA’s utility for NASA.  Like the defense community of the mid-1990s, NASA finds itself replete with non-interoperable simulations spread across the continent.  The defense community has fortunately done most of the hard work of developing, testing, improving and solidifying the interoperability protocol.  NASA would be very wise to leverage this technology to enhance its own modeling and simulation capabilities.The Distributed Space Exploration Simulation—BackgroundHLA’s first major use at NASA was a classic HLA use case.  The Japanese space agency (JAXA) was preparing to dock equipment with the International Space Station (ISS).  To practice the docking, JAXA created a simulation of their equipment.  Johnson Space Center meanwhile maintained a simulation of the ISS.  The solution was to use HLA to link the Japanese simulation across the Pacific to the JSC simulation.  The effort was very successful, and training provided helped to yield a successful flight mission [2].With a successful HLA capability in house at JSC, it was natural to expand upon this capability in support of Project Constellation.  Different centers had been placed in charge of different aspects of the Ares I spacecraft:  MSFC manages the lower stages, JSC the payload, and Langley the Launch Abort & Recovery System.  Each of these centers had developed models of their components, and again, there was a natural desire to federate the simulations to run together.  Thus was born the Distributed Space Exploration Simulation (DSES).DSES Technical DetailsDSES should be familiar to the SISO’s space audience, since it has been briefed there several times (e.g., [2], [4]).  However, we will give a few of the salient technical details.  The RTIHLA is simply a protocol, not a code library.  Right now there are two vendors that supply actual code libraries that conform to the protocol.  Those are Pitch, which provides a Java interface, and MäK, which provides a C++ interface.  DSES has chosen the Pitch RTI, but uses the C++ “hooks” provided with Pitch to invoke the Java RTI from inside C++.   We therefore acquired the Pitch RTI and the C++ hooks to create our federates.The FOMThe DSES team has done a very good job of providing information about the DSES federation on a technical wiki (or “twiki”) [3]. This is where we located the DSES federation object model (FOM), which delineates the data structures that are to be passed via HLA during run-time.   The Pitch RTI uses a simple XML specification for the FOM, which we found very easy to manipulate right inside a text editor such as Emacs.Data Distribution Management and Time ManagementData distribution management is an important feature in HLA for handling federations in which thousands of entities are being simulated.  In our case, however, we have only the Ares I spacecraft segments, and forseeably no more than a dozen or so entities, so invoking this powerful feature of HLA is simply unnecessary.Time management in HLA handles the way the clock ticks in the federation.  All federates must agree on how this happens so that no federate gets behind another in logical time.  Otherwise, cause and effect could fall out of synch.  In DSES, again, the implementation is straight-forward.  All federates agree to advance 0.25 seconds in logical time for each iteration, and generally the federation will be run at either a real-time pace (logical time = wall clock time), or at double real-time (0.25 seconds logical time = 0.125 seconds wall clock time).  In either case, each federate simply time-stamps the beginning of each iteration; then, upon completing its tasks for that iteration, waits until 0.25 seconds has elapsed since the time-stamp to move to the next iteration.  However, there is a fair amount of synchronizing to be implemented at the beginning of the run, which we will discuss shortly.K-Fed and BRITNEOur goal was to prove the concept of creating a federate from scratch that would interoperate with DSES.  Given our previous experience with STK, an obvious federate would be one that simply “listens” to the DSES traffic and picks out the relevant information for visualization in STK.  However, short of asking several of the relevant participants in DSES to run several exercises while we tweaked our code, we decided to create a DSES stand-in federate that would produce valid (or at least coarsely realistic) orbital data.  The simplest choice was to create a federate that would generate classical Keplerian orbits and communicate that information to the federation in the same way the orbital generator in DSES might.  We called this DSES stand-in federate “Kepler Federate” or simply “K-Fed.” Figure 2 shows the nature of the interactions between the various applications.  K-Fed is intended to be replaced by the DSES framework, while the BRITNE application (explained below) will interface between DSES and STK.Quick and Dirty Keplerian Orbit GeneratorGenerating Keplerian orbital data is a straight-forward process.  It is something of an aside to present the process here, but we think it might be useful for some to see exactly what we have done so that they also can use a simple Keplerian orbit stand-in when desired.  The parameters are the 5 constant Keplerian orbital elements (a = semi-major axis, e = eccentricity, i = inclination,  = argument of perigee,  = angle of the ascending node).   The sixth element is the true anomaly, , which is the only one that varies in time.To locate an object in its orbit relative to the periapsis (in this case perigee), one computes the mean anomaly as simply M = 2t/P, where P is the period of the orbit, and t is time since last pass through perigee.  The eccentric anomaly, E, must be solved for according to M = E – EsinE; we used Newton’s method.  From there, the true anomaly, , which is the angle between the object and perigee, is defined as EMBED Equation.3  .The radial distance from the earth’s center is then  EMBED Equation.3  .The vector position of the object is now (x, y, z) = (rcos, rcos, 0), and we have only the three orbital elements that speak to the orbit’s orientation left to account for.  First we rotate about z by , the argument of perigee, then about x by i, the inclination, and finally again about z by , the angle of the ascending node.  Now the object is in its proper place according to the Keplerian orbital elements.  (Getting the velocity requires only a few more lines of mathematics that ensure conservation of vector angular momentum.)K-FedNow that we can generate orbital information at will, it is “simply” a matter of pushing that information out into the federation.  As mentioned previously, this federate must also serve as the master federate, since it is standing in for the DSES federation.  The main difference between the master federate and the others, at least in the DSES federation, is that the master waits for all federates to join before it sets the synchronization point that allows the federates begin running.   Most of the federation control in the DSES federation is achieved internally with synchronization flags in combination with some standard HLA calls designed to facilitate synchronization.  Fortunately, all of this has been very well documented by the DSES group [4].  In fact, we were able to reuse much of the code in this reference in the construction of our own federate.Figure  SEQ Figure \* ARABIC 2:   Outline of the proposed connection between the DSES federation and STK using tools developed by CMSA.  Currently, K-Fed serves as a stand-in for the DSES federation.Aside from the master duties, the federate must also agree to publish the orbital data it generates, which requires only a simple call to the RTI’s publication facility. BRITNEOur second federate, which is to be regarded more as the proof-of-concept, in that this is the one we would hook directly into DSES (and again, K-Fed is merely the DSES stand-in), we call “Bridging Real-time Information To NASA Environments” (BRITNE).  Having successfully created K-Fed with all of its master federate responsibilities, our implementation of BRITNE was relatively straightforward.  After initializing the RTI, BRITNE essentially only needs to subscribe to the pertinent data in the FOM, and await the go-ahead from K-Fed.  After that point, BRITNE’s only HLA function is simply receiving spacecraft location data from the RTI.We promised a visualization capability here, and again we rely on the STK Connect API.  We have discussed the Perl hooks in to the Connect API, and as mentioned above, there are also C/C++ hooks available.  The C++ calls, it turns out, are very similar to the Perl calls successfully employed previously.  Thus, with little difficulty, we were able to drop the C++ Connect calls into BRITNE and achieve a real-time visualization capability that is compatible with the DSES federation. REF _Ref171925399 \h  \* MERGEFORMAT 3 shows both federates running with the STK visualization in the background.  At upper left is the BRITNE federate running in a verbose mode in which it prints to the screen the updated position information for the spacecraft it is receiving from the RTI.  The RTI, of course, is receiving the updated information from K-Fed, and in the upper middle of the figure, we find K-Fed’s verbose screen output.  At bottom left, we find the Pitch RTI graphical user interface showing that both federates are running and communicating successfully with the RTI.  Meanwhile, shown in the background is the actual visualization of the spacecraft, which is labeled “Kepler” and shown as a yellow dot. Figure  SEQ Figure \* ARABIC 3:  K-Fed (top center) and BRITNE (top left) federation feeding visualization information to STK.Conclusions and Future ProspectsThe purpose of this project was to prove the concept that interoperability of NASA models and simulations could be greatly improved.  We have proven that concept in two important ways.  First, we have automated the process of file interoperability between NASA orbit generators and the COTS visualization package STK—users only need to enter a single command to take a data file from one of several orbit generators to see the orbit fully rendered in STK.  Second, we have created an HLA federate that will link in to the DSES HLA federation and provide real-time COTS visualization in STK.Both of these tools, at face value, should be quite useful for MSFC engineers; however, the more important result to take away is the concept proven:  within a semester’s effort, we have greatly enhanced the interoperability of NASA tools and a COTS tool.  This certainly suggests that other NASA and COTS tools can be similarly linked to enhance the interoperability of M&S in support of Project Constellation.References[1] Colley, W. N., & Componation, P., 2006, “Assessment of Opportunities for Enhancement in NASA SE&I with M&S,” Huntsville Simulation Conference, Huntsville, AL, October 2006.[2] Crues, E. Z., Chung, V. L., Blum, M. G., Bowman, J. D., 2007, “The Distributed Space Exploration Simulation,” (07S-SIW-058), Simulation Interoperability Workshop, Norfolk, VA, March 2007.[3] DSES, 2007,  HYPERLINK "http://simlabtwiki.arc.nasa.gov:16100/twiki/bin/view" http://simlabtwiki.arc.nasa.gov:16100/twiki/bin/view[4] Phillips, R. G., Dexter, D., Hasan, D., & Crues, E. Z., 2007, “A Coordinated Initialization Process for the Distributed Space Exploration Simulation,” (07S-SIW-059), Simulation Interoperability Workshop, Norfolk, VA, March 2007.WESLEY N. COLLEY is a Senior Research Scientist at the University of Alabama in Huntsville’s Center for Modeling, Simulation and Analysis (CMSA).  Dr. Colley received a Ph.D. in Astrophysical Sciences from Princeton University in 1998.  At Princeton and later at the Harvard-Smithsonian Center for Astrophysics, Dr. Colley studied gravitational lensing and cosmology.  He worked at MIT Lincoln Laboratory’s Space Control Group on mid-flight track fusion in missile defense systems (SBIRS Low, Air Force), before lecturing at the University of Virginia and examining large-scale structure in the cosmic microwave background.  He then moved to the Virginia Modeling, Analysis, and Simulation Center (VMASC) to work on validation methodologies for operational training simulations (Navy), and agent-based modeling of mass casualty events (DHS).  At CMSA, he has carried out a survey to identify modeling and simulation techniques that will enhance NASA's Systems Engineering and Integration effort for upcoming lunar missions.  Under the Army’s Aviation and Missile Command, he used discrete event simulation to optimize aviation inventory strategies for greatest uptime at minimum cost.  Colley validated analytical queuing based assessments of Air Force command and control messaging systems, and is currently leading an effort for the Naval Air Warfare Center to construct validation methodologies for Netcentric Warfare simulations.GREGORY S. REED is currently pursuing a M.S.E. in Industrial and Systems Engineering with concentration in modeling and simulation from the University of Alabama in Huntsville. He has been employed at the Center for Modeling, Simulation, and Analysis since May 2006 in various capacities: as an undergraduate Student Specialist, a Graduate Research Assistant, and in his current position as a Research Associate. He graduated summa cum laude with a B.S.E. in Electrical Engineering from the University of Alabama in Huntsville in 2006.  In his position as a Research Associate, he currently supports the development of Ares launch vehicle risk assessment simulations from several fronts; he is heavily involved in developing the baseline risk model and statistical risk templates, evaluating the risk simulation's Verification, Validation, and Accreditation (VV&A) work, and providing configuration control and proper security for the project's sensitive data.  His other projects have included the development of models and terrains for the America's Army training simulator, and the support and development of a professional development course on simulation software.  His research interests include normative and descriptive decision theory, game theory, experimental economics, artificial intelligence, and genetic algorithms. He plans to pursue a Ph.D. in the field of Computer Science, with his thesis research relating to the intersection of genetic algorithms, artificial intelligence, and modeling and simulation.