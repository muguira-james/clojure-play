 State Consistency and Event Integrity MonitorDavid L. FisherTerry S. McDermottJames L. BryanMonica A. JanesNorthrop Grumman Information Technology12000 Research Pkwy, Suite 236Orlando, FL 3282407-243-2010, 407-243-2026, 407-243-2009, 407-243-2017david.fisher@ngc.com, terrence.mcdermott@ngc.com, james.bryan@ngc.com, monica.janes@ngc.com Keywords:latency, monitor, network performance ABSTRACT:  In forming federations of distributed simulation systems whose internal protocols vary between DIS and HLA, some form of gateway may be used to resolve protocol differences.  With a system of gateways in place, they may take on additional responsibility.  For example, they might facilitate network utilization optimization or provide support services to the distributed simulations, like protocol filtering or entity state dead reckoning.  To perform these additional functions, the array of gateways must internally maintain some level of simulation state.  This implies that data packets may no longer maintain their originating identity or uniqueness as the information is transferred from one simulation system to another through the system of gateways.  This being the case, direct comparison of message content cannot be used to evaluate whether state information is maintained consistently across the simulation systems or to determine network performance issues such as latency or lost data.  This paper proposes an approach to developing a tool to overcome this problem, which we call “The State Consistency and Event Integrity Monitor”.  By continuously monitoring elements of the simulation’s entity state data at distributed sites, time-synchronized state data can be collected, compared and evaluated outside the context of the ongoing simulation.  The results can provide for the early detection of simulation, network performance, and latency issues or detecting negative training issues; and allow corrective or investigative action to be carried out while a distributed simulation exercise is occurring rather than trying to troubleshoot it after the completion of the simulation.  Software located at each simulation site will “sniff” the local network to determine critical information such as scenario entity positions and events.  This information is then sent back to a centralized “Master” state monitor program.  By comparing the collected data as a function of time among the sites to the ground truth (from the data origination site), the Master can alert simulation site engineers through a variety of alarms that anomalous conditions have been detected or that certain thresholds have been exceeded.  Analyzing the distribution and the type of simulation state error data, the troubled area can be isolated, or at a minimum, the search for the troubled area can be narrowed.  Corrective action can then be taken with reduced impact to the distributed simulation exercise.1. IntroductionThe concept for this tool is to provide distributed simulation event run-time monitoring through comparison of entity state and event interaction data among participating sites.  In forming federations of distributed simulation systems whose internal protocols vary between DIS and HLA, some form of gateway is often used to resolve protocol differences.  With a system of gateways in place, additional capabilities can be provided to support the execution of the distributed simulation.  For example, the gateways might provide the ability to optimize network utilization or provide support services to the simulations, like filtering and dead reckoning.  To perform these additional functions, the complex of gateways must internally maintain some level of simulation state.  This implies that data packets may no longer maintain their original identity or uniqueness as the information is transferred from one simulation system to another through the gateway system.  This paper assumes that all state models within the simulations are equivalent (errors are not due to the model) and that the cause of any of the issues described subsequently are not model-based.  Also, we are using the term state data in a very general sense; that is, it refers to the state of any simulation data.  For example, it might be voice communication data or the positional data of an aircraft.2. ObjectiveDue to the likely lack of one-to-one correspondence of the state data between the originating simulation site and other participating sites, direct comparison of data messages cannot be used to evaluate whether accurate state information is continuously maintained across the simulating systems.  The inability to directly compare data may be directly related to gateway functions such as protocol transformation, dead reckoning or filtering, network latency, or software restrictions.  Elements of the simulation’s entity state data at the distributed sites can be continuously monitored while time synchronized state data is collected at each of these sites for comparison and analysis outside the context of the ongoing simulation – but potentially concurrent with the simulation execution.  This allows for the early detection of network or simulation problems or for the detection of negative training issues, providing the ability for investigative or corrective action to be carried out while a distributed simulation exercise is occurring rather than canceling it or trying to troubleshoot it after the completion of the simulation.3. Technical ApproachSoftware located on the gateway (or at least in a computer at a network location logically equivalent to the gateway relative to the local area network) at each simulation site would “sniff” the local network to gather critical simulation information such as scenario entity positional and event data.  This information would be packaged and sent to a centralized “Master State Monitor” site.  Applying user defined threshold values and ground truth data (data from the site originating the simulation data),; the master site would analyze and compare the collected data.  If an anomaly iswere detected, the master site would alert simulation site engineers through a variety of alarms that an anomalous condition has been detected or a certain threshold has been exceeded.  By analyzing the type of simulation error and the routing of the data through the wide area network, an engineer can troubleshoot the problem to isolate and correct it to reduce the impact to the distributed simulation exercise.To initialize the monitoring, the master site would distribute, start and control an intrusive monitoring agent to each participating site in the simulation prior to starting the distributed simulation.  This agent would package all or part of the state data and send it back to the “Master State Monitor”.  The Master State Monitor’s purpose is to gather data from each of the distributed agents to filter, analyze and determine if there are any apparent network or simulation problems.  The master would in turn display the analysis data within a user friendly single window (for easy detection) graphical user interface (GUI) and provide a visual and audible signal if any network issues are discovered or threshold values are exceeded.  Utilizing the comparison of state data from all the participating sites against the collected ground truth data, and based on subsequent analysis, probable causes for network or simulation issues can be determined and displayed.  Comparison of state data with ground truth requires that all the collected state data be synchronized in time, to allow analysis to be made as a function of time of each sites state data when compared to the ground truth data.  In a perfect system, each site’s state data should map to the originating ground truth data as a function of time.  In addition to providing the analysis data and warnings, possible corrective actions can also be suggested.It is unlikely that there will be a desire to collect all state data and to monitor it continuously.  This would heavily tax the bandwidth of the local area and wide area networks with all the data being sent to the master in addition to the primary simulation data.  The storage capabilities of the master site would also be challenged.  A more realistic scenario for this tool would be to monitor selected state data generated by each site and only periodically transfer this data to the master for comparison and analysis.  Which data is monitored could be chosen during the event planningevent-planning phase or before the running of the event – or even interactively during the event.  An alternative would be to have the tool automatically select which data to monitor.  This selection can be done randomly or by use of artificial intelligence.  The selection of which state data to monitor can be allowed to change dynamically by the artificial intelligence software to match the needs of the system, to optimize bandwidth usage efficiencies, or in response to previous monitoring results.  This dynamic selection of acquired data can be implemented to better pinpoint arising network or simulation issues.  Since data is only collected periodically and sent to the master, bandwidth usage should be minimal, as should CPU consumption by the agents although the agents would normally be on their own computer.Certain monitoring and sampling parameters will need to be established.  The sampling period for comparison and analysis may be chosen during event planning or the software can be programmed to automatically make the selection.  As for the type of state data selected for monitoring and comparison with ground truth data, the data type can be predetermined and manually established by the master site, or the data type can be dynamically selected using artificial intelligence techniques to help match the needs of testing or to help pinpoint arising network or simulation issues.  In addition, sampling rates may vary directly with the rate of change of the state.  For example, an aircraft flying at 500 knots requires sampling at a much higher rate than a tank traveling 10 MPH.The monitor software should be able to monitor, record, and alert for simulation issues such as fair fight and negative training during runtime.  For example, shots are taken at, but miss an aircraft being simulated at one site, but according to the aircrafts positional data at the site where the shots are taken, the shots hit.  Assuming the models are correct at each of the sites, this fair fight and negative training issue might be caused by latency and/or dropped position data messages.  However, without monitoring, the trainers or participants may never know about these issues during runtime and have a false impression of actual capabilities based on the limitations of the distributed simulation environment.If during a training event these issues arise, there are several possible courses of action.  An attempt can be made to correct the problem during runtime without stopping the event or pausing the simulation to attempt to correct the problem.  If the problem cannot be corrected immediately, the decision might be made that it is not critical enough of to stop the event.  In the worst case, if the problem is of a critical nature, the event might be terminated with the loss of training opportunity and the dollar cost of the time spent on planning and executing the simulation.  In addition to the runtime analysis and comparison of state data, the agents and master can be used to help validate the distributed simulation environment and pinpoint issues by running other tests.  These tests, for example, might include ping tests from the tool agents and master to the other agents and gateways within the simulation to check for latency or could include scripted system configuration tests that can be run prior to the simulation event to ensure that every site has the same radio simulation settings.The master and agent software would be designed to:Use a combination of test methodologies to track down potential network or simulation issues.Use ping tests between gateways and test computers to separate (?) network from gateway issues.Collect workstation data to determine internal, individual computer issues.Use threshold values to alert engineers of out-of-range data.Collect network data for statistical and analytical purposes.Figure 1 presents a diagram showing tthree simulation sites sending state data to the “Master” for analysis and comparison.  The ground truth originates from the site simulating that state.  The following examples illustrated how this tool may be used to track down several potential distributed simulation problem areas.Example 1:  If the timing of identical positional state data of a simulated aircraft received at all other simulation sites is 500 ms behind the original data at the simulating site (ground truth data), the master could deduce that the latency problem is due to an issue with the gateway or the network at the site simulating the aircraft.  Additional ping tests between the sending site gateway and the other participating sites would then be used to help further delineate between network issues and problems associated with the originating gateway.Example 2:  If the positional state data of an aircraft being simulated at each site have negligible latency from all the originating simulation sites except for one, at which all the state data is 500 ms behind the ground truth data, the master would suspect the cause to be anomalies with the network or the gateway at that specific site.  Again, the use of ping testing from all the test sites to all the others could be used to further isolate the problem.Example 3:  In order to reduce bandwidth utilization, one of the expected functionalities of the gateways is to provide dead reckoning of entities.  This allows the extrapolation of some state data such as position, and reduces the need to continuously update this data across the wide area network.  Updates are only sent when changes from the dead reckoned positional data occur.  If the positional state data for a particular entity deviates from the ground truth at one site, then periodically and suddenly corrects, the master would suspect the cause to be intermittently dropped data packets causing a deviation from the dead reckoning positional data at that gateway as the cause.			Figure 1.1 The State Consistency and Event Integrity Monitor ArchitectureThe interpretations of the comparison and analysis results will depend upon the specific functions provided by each particular gateway.  Therefore, the resulting suspected cause of problems will be a function of the specifics for each gateway, which will require tailoring of the analysis for each particular application.  In order to tailor the comparisons and analyses, using the master application, the simulation engineer would define a threshold value for the position of each type of entity, fixed wing aircraft for example.  The master would determine if the aircraft was within its designated location parameters and notify the simulation engineers by alarms on the GUI if it is off course.4.0 ConclusionDue to the expense and effort associated with coordinating and executing large distributed simulations, it is important to identify issues that might produce negative training impacts or affect the training effectiveness of distributed simulation training events.  If these issues are caught and isolated during the training event run time, corrective action can be taken to prevent cancellation or negative training.  Due to the utilization of gateways and their functions such as maintaining simulation state, there is currently no ability to provide a direct comparison of data messages to determine run-time network or simulation anomalies or issues.  The alternative is to develop a tool that uses indirect comparison as described above to quickly and cost effectively provide for runtime monitoring of distributed simulation training events.Author BiographiesDR.  DAVID L.  FISHER is a software developer for Northrop Grumman Information Technology in Orlando, FL.  Since receiving his doctorate in Physics from the University of Texas at Austin in 1995, he has worked in the areas of theoretical plasma physics, laser-plasma interactions, laser wakefield acceleration, the detection of explosive materials, millimeter wave sensor technologies and applications, and in the development of an Internet business.  He is presently developing several software tools.  David has 20 professional publications and 2 patents (1 pending).DR.  TERRY S. MCDERMOTT is Senior Scientist for the Simulation Technology Department of Northrop Grumman Information Technology.  He has over 20 years of experience in systems engineering and simulation including system and software definition and development, system performance simulation, software design, realtimereal-time and constructive simulation, protocol development, standards development, and project management.  Currently, Terry is contributing to development of systems for visualizing tactical situations, training exercise planning and coordination, and distributed test environments.  Terry received his doctorate in mathematics from the University of Southern California specializing in Functional Analysis.JAMES L. BRYAN is a Systems Engineer for Northrop Grumman Information Technology in Orlando, FL.  He is the lead for Federation Integration and Test, and is responsible for standards development, systems analysis, and test/user tool development for the USAF Distributed Mission Training (DMT) program.  He has 8 years of experience in distributed simulation integration and test and simulator system development and design.  While on active duty with the USAF as a combat search and rescue helicopter pilot, he was the Test Director for the OSD sponsored JCSAR JTE’s Virtual Simulation program.  Jim has four professional publications and one patent pending.  MONICA A. JANES is a software developer for Northrop Grumman Information Technology in Orlando, FL.  She has over 7 years of experience in designing and developing all aspects of software including test descriptions, product specifications, and software integration.  Since receiving her Bachelor of Science degree in Computer Science from the University of Central Florida in 1996, she has developed several simulation software tools to provide visual awareness, data analysis, and common distributed infrastructures to the training world.SnifferSite 2 Ground TruthSite 3 TruthSite 1 TruthSnifferGatewaySite 1SnifferGatewaySite 3GatewaySite 2AlarmRun-TimeStateMonitorMaster