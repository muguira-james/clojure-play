Joint Metrics Development and Implementation inSupport of Single Integrated Air Picture (SIAP) Assessmentsin a Distributed Simulation EnvironmentLaura BennettBrett ZombroSystems Planning and Analysis, Inc.4900 Seminary Road, Suite 400Alexandria, VA  22311703-578-6395, 703-578-6332 HYPERLINK "mailto:lbennett@spa.com" lbennett@spa.com,  HYPERLINK "mailto:bzombro@spa.com" bzombro@spa.comKey Words: SIAP, metrics, truth-based assessment, distributed simulation, JDEP.Abstract: This paper surveys the recent development of a Joint methodology for assessing the capability to develop and maintain a Single Integrated Air Picture (SIAP) in the systems that collectively provide Integrated Air and Missile Defense (IAMD).  A hierarchy of quantitative metrics pertaining to various levels of IAMD performance, developed by sponsorship of the Joint SIAP System Engineering Organization (JSSEO), is described.  Particular attention is given to SIAP attribute measures derived from Joint Requirements Oversight Council (JROC)-validated Capstone Requirements Documents (CRD), to the algorithmically explicit truth-based assessment approach on which their definitions are based, and to the process by which Joint approval of the metric definitions (and the approach) has been attained.  The paper also discusses the standard digital tools and data formats which are being developed to encode the algorithms for evaluating SIAP attribute measures and other IAMD system measures of performance.  Such tools enable significant automation of post-test analysis processes over a variety of test venues, and are particularly attractive for use in Hardware-in-the-Loop (HWIL) and pure simulation testbeds.  Finally, the paper outlines JSSEO's plans to integrate these standard assessment tools within the Joint Distributed Engineering Plant (JDEP) technical framework‚Äîa state-of-the-art distributed simulation capability and a key component in the U.S. Department of Defense's (DoD) Simulation-Based Acquisition (SBA) initiative.1.  IntroductionSystems providing Integrated Air and Missile Defense (IAMD) are operationally responsible for the tracking, identification, and precision engagement of a broad spectrum of air and missile defense targets.  The trend in the evolution of aerospace threats is toward the proliferation of unmanned systems, including theater ballistic missiles (TBM), cruise missiles (CM), unmanned aerial vehicles (UAV), and large caliber rockets (LCR).  The deployment of weapons of mass destruction is a probable component of future warfare and many of these unmanned threat platforms have the capability to deliver such weapons.  The Single Integrated Air Picture (SIAP) has been identified by the Joint Requirements Oversight Council (JROC) as a fundamental capability iesrequired for the success of the air and missile defense mission as these threats emerge and evolve from the present-to-2010 time frame [1].The Joint SIAP System Engineering Organization (JSSEO) is the lead organization chartered by the U.S. Department of Defense (DoD) to, ‚Äúprovide the engineering design for the DOD‚Äôs Joint Theater Air and Missile Defense (JTAMD) integrated architecture (IA)‚Äù [2].  The JTAMD IA is to serve as the design foundation of the SIAP.  Quantification of the SIAP capability in evaluative, predictive, and prescriptive terms is a necessary step towards the fulfillment of JSSEO‚Äôs charter.  To this end JSSEO (along with its predecessor the SIAP System Engineering Task Force) has initiated a process for the development and Joint approval of a hierarchy of SIAP metrics.  The metrics quantify aspects of the SIAP capability from subsystem-level performance to force-level capabilities required by an operational commander.  These SIAP metrics have been successfully integrated into the test plans of large-scale, joint air defense training exercises involving an integrated air defense network of ground, missile, and radar systems operating in a simulated, high-threat environment.  JSSEO will use these metrics as an integral part of a Joint Distributed Engineering Plant (JDEP) simulation environment, providing performance standards for multi-platform interoperability to achieve the Single Integrated Air Picture.The SIAP metrics measure the adequacy and fidelity of information that is used to form a shared understanding of the tactical situation for the support of battle management and engagement operations.  The standard set of definitions and algorithms provide for quantitative evaluation of air picture quality within the IAMD community.  SIAP metrics are also needed to predict and prescribe SIAP improvements that translate into improved warfighting capability.  As DoD Test and Evaluation practices migrate towards rapid-feedback, distributed simulation environments, processing and analysis of results will depend upon well-defined performance metrics with aggregative properties and automated, repeatable evaluation procedures.  The SIAP metrics are applicable throughout the algorithm development and test cycle, as a guide to algorithm research and development, for software- and hardware-in-the-loop and operatorinthe-loop experiments, field exercises, and evaluations, e.g., Roving Sands (RS) ‚Äô01 and ‚Äò03, Joint Combat Identification Evaluation Team (JCIET) ‚Äô02, Joint Combat Identification Exercise (JCIDEX) ‚Äô03, and the planned Combined Joint Task Force Exercise (CJTFEX) ‚Äô04.  2.  ObjectiveThe intent is to establish a standard methodology for the evaluation of the SIAP based on a core set of metrics and measures of system/subsystem performance for designated, high-priority problem domains.  JROC has established guidance for assessing a SIAP in the form of Key Performance Parameters (KPP) in the TAMD and the Combat Identification (CID) CRDs [1,3].  JSSEO has refined and expressed in mathematical form these CRD KPPs as SIAP Attributes and their associated quantitative SIAP Attribute Measures [4].  It is the role of the Joint Theater Air and Missile Defense Organization (JTAMDO) to identify military utility measures, known as Measurements of Effectiveness (MOE), which support the derivation of CRD KPP thresholds and objectives. 3.  Formation of the SIAPA guiding purpose of the SIAP is to achieve a correct and consistent understanding between participants of air vehicle track characteristics within an operational commander‚Äôs area of influence (AOI).  As stated in [1], ‚ÄúThe SIAP is the air track portion of the Common Tactical Picture (CTP) and is the product of fused, common, continual, and unambiguous tracks of airborne objects of interest in the surveillance area.  SIAP is derived from real-time and near-real-time data, and consists of correlated air object tracks and associated information.‚Äù  The SIAP attributes are based on data held in the central track stores of the host system that are displayable to the participant.  This data may come from local sensor measurements as well as remote sources such as Link-16, Link-11, and other tactical data networks.  For purposes of metric calculations, the track set consists of actionable tracks where a track is considered actionable once it has an associated track number and is displayable to an operator.  A schematic of the track data assessment process is shown in Figure 3.1.The quantitative measures and supporting definitions apply to tracks on tactically significant airborne objects of interest in the air vehicle component of the SIAP.  For the air track portion of the SIAP, this would include any cruise missile, fixed wing aircraft, rotary wing aircraft, air-to-surface missile, large caliber rocket, unmanned aerial vehicle, or other airborne object meeting air vehicle reporting criteria, in the area of influence.  A collaboratively defined, Defense Planning Guidance (DPG)-based "Common Reference Scenario" (CRS) provides stressing scenarios and mission-level vignettes for SIAP-related integrated system evaluation to obtain a consistent baseline for evaluating current performance and proposed improvements [5].4.  Methodological Approach:  The SIAP Metrics HierarchyThe SIAP attribute measures developed by JSSEO form the pivotal mid-layer in a SIAP metrics hierarchy, linkingsystem/unit-level Measures of Performance (MOP) to warfighting Measures of Effectiveness (MOE). Definitions adopted by SIAP for the levels of the metrics hierarchy are as follows:Measure of Effectiveness (MOE) ‚Äì The measure of operational success that must be closely related to the objective of the mission or operation being evaluated [6].SIAP Attribute ‚Äì The measure of a quantifiable property of a SIAP that is derived from TAMD and CID CRD requirements and associated KPPs.Measure of Performance (MOP) ‚Äì The measure of a system‚Äôs technical performance, for example, expressed as speed, payload, range, time on station, frequency, or other distinctly quantifiable performance feature [6].  The MOPs quantify aspects of system and subsystem performance that may be used in the analysis of SIAP shortfalls or the prescription of SIAP improvements  (see Appendix, Table A-1).  The MOEs represent essential types of warfighting capability measures that are relevant to air and missile defense  (see Appendix, Table A-2).  Examples of measures at each level follow in Figure 4.1. EMBED Word.Picture.8  4.1  SIAP Attribute Core Measures [4]The attribute measures discussed in this section correspond to the SIAP and CID KPPs described in the TAMD and CID CRDs (2001) [1,3] and form the cornerstone of the metrics hierarchy.  The precise mathematical formulation of the SIAP attribute core measures appears in [4].  The qualitative definitions of the attributes are given below:Completeness:  The measure of the portion of true air objects that are included in the SIAP.  The air picture is complete when all objects are detected, tracked, and reported.Clarity:  The measure of the portion of the SIAP that contains ambiguous tracks and/or spurious tracks.  The air picture is clear when it does not include ambiguous or spurious tracks.Continuity:  The measure of how accurately the SIAP maintains track numbers over time.  The air picture is continuous when the track number assigned to an object does not change.Kinematic Accuracy:  The measure of how accurately the TAMD Family of Systems (FoS) reports track position and velocity.  The air picture is kinematically accurate when the position and velocity of each assigned track agree with the position and velocity of the associated object.Commonality:  The measure of consistency of the air picture held by TAMD Family of Systems (FoS) participants.  The air picture is common when the assigned tracks held by each participant have the same track number, position, and ID.ID Completeness:  The measure of the portion of tracked objects that are in an identified state.  The identification (ID) is complete when all tracked objects are in an identified state.ID Correctness:  The measure of the portion of tracked objects that are in the correct ID state.  The ID is correct when all tracked objects are in the correct ID state.ID Clarity:  The measure of the portion of tracked objects that are unambiguously identified.  The ID is clear if no tracked object is in the ambiguous ID state.In addition to the eight core, instantaneous attribute measures, aggregative forms of the metrics across time and across number of participants have been formulated in [4].  All of the SIAP attribute measures, with the exception of Commonality, are associated with specific, quantitative threshold and objective requirements in the CRDs.  The Commonality measure is an attempt to quantify the property implied by the term common in the TAMD CRD definition of the SIAP.  Common in the context of the SIAP is generally understood to involve a consistent understanding among all participants.  While it is not intended that all network participants must receive all data, it is desired that any information needed by a participant or a group of participants be available to them.  Therefore if a group of participants has a need for the same information, it should be held by all of them. Information needed for situational awareness (SA) is broadly needed by all network participants; thus generally speaking, the situational awareness picture should be consistent among all participants [4].  The computation of the SIAP attributes requires some knowledge of truth regarding objects.  In the absence of ground truth, such as in the case of an un-instrumented, live exercise, it is still possible to obtain valuable estimates of most of the attributes by comparing the information available to one participant with that of another, or by examining the internal consistency of each individual participant's air picture over time [4].4.2  Sample Attribute Measure:  ClarityFor illustration, the mathematical form of the clarity attribute measure is considered [4].  The quantification of the clarity attribute comprises two measures: one for ambiguous tracks and one for spurious tracks.  Both ambiguous and spurious tracks result in a loss of clarity, but the sources of the errors and the solution sets for the errors differ.   Tracks are ambiguous when more than one track, assigned to the same object and not correlated within a system, are displayable to a participant.  The instantaneous system track picture ambiguity Am(tk) at participant m at time tk is:	 EMBED Equation.3  		(1)where NAm(tk) = the number of assigned, uncorrelated tracks held by participant m at time tk  and JTm(tk) = the number of objects with at least one assigned track held by participant m at time tk.  The Ambiguity, A, can range from one to many tracks per object.  The air picture may be said to be perfectly unambiguous when A is equal to one. A track is spurious when it is not assigned to any object.  The instantaneous system measure of the percentage of tracks that are spurious, Sm(tk), as measured by participant m at time tk is:	 EMBED Equation.3    	(2)where Nm(tk) = the number of tracks held by participant m at time tk.  The percentage of tracks that are spurious, S, ranges in value from 0%, when perfect, to 100%. The TAMD CRD [1] provides a KPP for the percentage of tracks representing distinct objects.  Thus the CRD requirement bounds the percentage of all tracks that are not spurious to (100% - S).  Perfect clarity is attained when the air picture is totally unambiguous, (A=1), and there are no spurious tracks, (S=0).From the precise mathematical formulation of the attribute, the following four levels of the metric can be derived:Instantaneous system metric:  The measure of the attribute at an instant in time from the perspective of a single participant.  Time-averaged system metric:  A time average, typically weighted according to an object or track count, of the instantaneous system metric.  Instantaneous IADS metric:  An average over participants, typically weighted according to an object or track count, of the instantaneous system metric at a particular instant in time.Time-averaged IADS (‚Äúroll-up‚Äù) metric:  A weighted average of an attribute over participants and time.  The IADS metrics provide a high level assessment of the performance of the entire IADS.The aggregative form across time and number of participants for the clarity attribute in the case of ambiguous tracks appears as,	 EMBED Equation.3   EMBED Equation.3     , A ‚â• 1.  	  (3)The aggregative form across time and number of participants for the clarity attribute in the case of spurious tracks appears as, EMBED Equation.3  for, 0 ‚â§ S ‚â§100.                                                               (4)In the equation, K is the total number of scoring times over the time period of interest for the scenario; M is the total number of participants (or units to be scored) in the scenario; and, J is the total number of objects ever occurring in the scenario [4].5.  The Joint Approval ProcessJSSEO working groups of subject matter experts propose solutions that will result in improvements of system functions by developing the relevant attributes and engineering-level measures of performance (MOPs) and calculating before/after performance.  The SIAP Metrics Working Group, composed of subject matter experts from each of the Services, JTAMDO, and the Missile Defense Agency, has as its primary mission to ensure that the SIAP metrics are jointly selected, rigorously defined, and that an implementation plan is in place for automated metrics tool development.  The standardized sets of definitions provide the joint community with a common frame of reference for quantifying and assessing the aggregate performance of a given SIAP configuration.  JSSEO endorsement of formulations of SIAP metrics and of measurement techniques (reflected in publications [4,7,8]), is contingent upon their approval by all Service and Joint agency representatives to JSSEO.  In this way, a high degree of Joint involvement in both the development and sanctioning of metrics is assured.  Implementation:  Associated Software Format and ToolsJSSEO uses a collection of tools to define and develop a disciplined joint system engineering approach and methodology for the implementation of the attribute measures in a distributed simulation environment.   Standard Air Track Data FormatA standard data format for air track data input to the trackto-truth matching algorithm and data transfer from the track-to-truth matching algorithm to the metrics scorer was developed.  The intent is for the metrics scorer and the track-to-truth matching algorithm to be able to seamlessly interface with data from numerous sources in multiple reference coordinate systems.  For root cause analysis and traceability purposes, the standard format encodes track source, track status, ID source, ID status, and reporting unit responsibility.6.2  Track-to-Truth Matching An assignment procedure was defined to ensure consistent scoring, and automated for modeling and simulation purposes.  The assignment of tracks-to-truth is based on a unique optimal gated assignment algorithm in the Automated Reconstruction and Correlation Tool for Interoperability Characterization (ARCTIC) software tool designed by the Center for Naval Analysis [9] for use in air defense analysis, and later adapted specifically for SIAP assessments.  A unique assignment procedure is one in which the assignment function is determined as that which optimizes (by convention, minimizes) the instantaneous value of a pre-specified assignment cost function over the set of all assignments satisfying unique assignment (UA) eligibility criteria.  The cost function may depend only on the instantaneous assignment, or may depend as well on assignments already made over some previous time interval (unique optimal assignment (UOA)¬†with hysteresis).  A gating constraint may also be imposed, resulting in a gated unique optimal assignment (GUOA) [7]. Briefly stated, JSSEO will implement a multi-step assignment algorithm for current SIAP assessments, incorporating the following features:assignment limited to tracks specified as having scoring precedence.administrative assignment of remote-only self tracks, such as Link 16 J2.2 Air Precise Participant Location and Identification (PPLI) tracks.a position/velocity and other set of gating constraints to rule out implausible assignments and identify spurious tracks.(4)	for each track source independently, a first pass through a GUOA algorithm with all tracks and objects not excluded by gating considered eligible.(5)	for each track source independently, a second pass through a gated independent nearest neighbor assignment (INNA) algorithm with the same gating criteria as used in the first pass for all tracks not assigned in the first pass [7].The assignment steps outlined are displayed pictorially in Figure 6.1.The proposed UOA algorithm is based on a pairwise cost function that is effectively a weighted sum of squared components of two-dimensional track errors.  It is the chisquared (or Mahalanobis) distance for a two-dimensional track with a position/velocity error covariance corresponding to a case in which errors are independent but of equal magnitude in the two orthogonal directions [7].  Appendix B of the SIAP Implementation Report provides the detailed method by which the unadjusted cost function was obtained.6.3  Metric Computational Software The Performance Evaluation Tool (PET) was developed by the Naval Surface Warfare Center (NSWC), Corona Division, San Diego, CA for the visual analysis of tactical scenario data.  The PET software has been used in a range of test and analysis scenarios from live test analysis (including ASCIET/JCIET events and U.S. Navy Exercises), hardware-in-the-loop (HWIL) experiments, to selected software simulations.   PET has the capability to calculate the SIAP attribute measures from suitably formatted input data.  For this reason, the PET software and its derivatives are designated to have a key role in current and future test and analysis commissioned by JSSEO [10]. EMBED Word.Picture.8  Figure 6.1.  Track Assignment Procedure [7]7.  JDEP IntegrationThere is a plan to implement the SIAP metrics across a range of test and simulation environments.  The JDEP Technical Framework is a DOD-funded, state-of-the-art distributed simulation capability to assist system developers and the test and evaluation community.  It is broadly defined as a capability to link distributed components in user-tailored federations that may span across hardware-in-the-loop, software-in-the-loop, and simulation venues.  It provides users with the ability to identify and access existing computer hardware- and software-in-the-loop and simulation capabilities across the DoD and industry, and technical support to federate these into distributed system environments for use in development, integration, testing, and assessments.  The JDEP Technical Framework defines the components that compose a JDEP federation, interface specifications, and guidance on how to configure and apply the components to a user‚Äôs needs [11].Simulations will in most cases need to be calibrated to live or HWIL results to provide confidence, and a mature JDEP Technical Framework may provide the means to ultimately migrate from HWIL to well-calibrated and cost-efficient simulation environments [3].  Validation will be achieved through incremental JDEP builds using operationally representative Common Reference Scenarios, operational models of sensors, high-fidelity (KPP) models of communications terminals, and environmental effects.  A schematic of the planned JDEP Technical Framework architecture for a forthcoming (2004-5) distributed HWIL event involving the AEGIS, PATRIOT, and E-2C systems is provided in Figure 7.1.  The circles, ovals, and rectangles in the diagram represent the different federates making up this JDEP federation.  The JDEP Data Extraction Federate (marked ‚ÄúData Collect‚Äù in Figure 7.1) is to be noted.  The role of this federate is to extract relevant data in order to calculate specified SIAP attribute measures and MOPs in post-event data analysis.  JSSEO conducts system trade-off analysis of proposed solutions using theater force-level models and assessing SIAP performance by calculating the SIAP attributes before and after changes are made.  In the past, such analyses have been carried out through predominantly serial, manpower-intensive processes, but as the analysis tools discussed in Section 6 mature, it becomes possible to employ more automated analysis procedures.  Presently, PET is being automated to allow batch processing of SIAP metrics from data collected by the Data Extraction Federate in multiple JDEP runs.  The concept likely to be employed in the 2004-5 HWIL tests is that of rapid post-run tabulations of metrics, together with post-event processing of multi-run statistics.  As work on the JDEP Technical Framework advances and JDEP federations come into wider and more constant use, the metrics processing capability may be built into the framework as a separate federate, allowing near-real time ‚Äúmetricsinthe-loop‚Äù results to be displayed and analyzed as the test run progresses.  The quality of the SIAP could thus be assessed as the (simulated) SIAP develops.   This near-real time concept of SIAP assessment is still in a relatively visionary planning phase, but the metrics tool upgrades that will enable rapid post-run assessment (as a first step towards the realization of a true metricsinthe loop capability) are well underway.  Other tools are being brought into play as interest in the JDEP Technical Framework extends to assessment of the ballistic missile (space) component of the SIAP. The Ballistic Missile Defense (BMD) Benchmark tool provides a simulation environment that captures the salient features of multi-sensor/multi-platform BMD tracking and measures of performance, and also allows automated batch-mode calculation of a set of SIAP attribute measures relevant to TAMD CRD [1] requirements on the space track portion of the SIAP.  This tool may be employed in either a simulation or an assessment capacity in future JDEP federations, and also has potential for Verification and Validation (V&V) use in connection with specific federates.The resulting set of products is envisioned as a JDEP kit, which will be given to JSSEO partner programs for their internal testing, verification, validation, and accreditation efforts.  The SIAP attribute measures, MOPs, MOEs, and associated software tools and data formats form an integral part of the JDEP kit to evaluate JDEP test results.  JDEP addresses both new requirements and current shortfalls.  Analyses are conducted to guide Service acquisition decisions for fielded and future systems.  The JDEP Technical Framework is further described in the Integrated Air and Missile Defense (IAMD) Capstone Test and Evaluation Master Plan (TEMP) [12].  8.  Future DirectionsMeaningful and precise definitions and formulations of SIAP attributes have been presented, based upon the guidelines found in the CID and TAMD CRDs [5-7].  These definitions are quantifiable, testable, and measurable.  Having defined these attributes, JSSEO will use them as part of a system engineering process to successfully evaluate, predict, and prescribe meaningful engineering recommendations for a SIAP.  With universal SIAP attribute definitions effectively vetted through the Services and applicable agencies, the joint community will have a common reference for defining SIAP-related warfighting capability [4]. In a subsequent phase of development, sets of MOPs will be provided for evaluating the output of candidate multi-source integration processes.  Data fusion is a key, enabling technology for SIAP in order to utilize data from a variety of tactical and intelligence data networks to obtain a composite view of the position, movement, and identification of all targets within the area of observation.  Information from all sensor and intelligence data sources that impact the battlespace needs to be exploited. Data fusion aids in providing a consistent and stable track picture in the presence of intermittent, incomplete, and ambiguous data.  Data fusion metrics will allow the quantification of performance gains with respect to numerous performancecategories, including resource usage, track identification and management, and sensor coverage and management.  Network-based metrics can represent the multiplicity of local and remote fusion nodes, the interconnection of nodes, and shared states among network nodes.  Future work can account for synergies and trade-offs in the various components of the fusion process.  Due to the modular nature of the fusion process metrics can be presented for each stage of the fusion process at a given data fusion node, including data alignment, data association, correlation, and state estimation and prediction.9.  References[1]	Theater Air and Missile Defense Capstone Requirements Document (TAMD CRD), U.S. Joint Forces Command, 2001.[2]	JSSEO Management Plan version 1.7, January,¬†2004 draft.[3]	Combat Identification Capstone Requirements Document (CID CRD), U.S. Joint Forces Command, 2001.[4]	Single Integrated Air Picture (SIAP) Attributes, Version 2.0, SIAP SE TF Technical Report [ADA 418818], 2003.[5]	Single Integrated Air Picture (SIAP) Common Reference Scenarios (CRS), SIAP SE TF Technical Report, July, 2002.[6]	Defense Systems Management College (DSMC), Glossary of Defense Acquisition Acronyms and Terms, 10th Ed., Defense Systems Management College, Ft. Belvoir, VA, 2001.[7]	Single Integrated Air Picture (SIAP) Metrics Implementation, SIAP SE TF Technical Report [ADA 397225], 2001.[8]	Single Integrated Air Picture (SIAP) Measures of Effectiveness (MOEs) and Measures of Performance (MOPs), SIAP SE TF Technical Report [ADA 397221], 2001.[9]	ARCTIC (Automated Reconstruction and Correlation Tool for Interoperability Characterization) software, version 3.0 Beta, Center for Naval Analysis (CNA), 2003.  [10]	PET (Performance Evaluation Tool) software, version 6.4 Beta 52, Naval Surface Warfare Center (NSWC) Corona, 2003.[11]	J. Dahmann and J. Wilson, ‚ÄúMDA and HLA:  Applying Standards to Development, Integration           and Test of the Single Integrated Air Picture Integrated Architecture Behavior Model,‚Äù Proceedings of the Simulation Interoperability Workshop, Arlington, VA, September, 2003.[12]	Integrated Air and Missile Defense (IAMD) Capstone Test and Evaluation Master Plan (TEMP), JSSEO, January, 2004 draft.10. List of AcronymsARCTIC	Automated Reconstruction and Correlation Tool for Interoperability CharacterizationBM	Battlefield ManagementBMD	Ballistic Missile DefenseCRD	Capstone Requirements Document IAMD	Integrated Air and Missile DefenseJDEP	Joint Distributed Engineering PlantJROC	Joint Requirements Oversight Council JSSEO	Joint SIAP System Engineering OrganizationJTAMDO	Joint Theater Air and Missile Defense OrganizationKPP	Key Performance ParameterMOE	Measure of EffectivenessMOP	Measure of PerformancePET	Performance Evaluation ToolSA	Situational AwarenessSBA	Simulation-Based Acquisition SIAP	Single Integrated Air Picture TAMD	Theater Air and Missile DefenseTBM	Theater Ballistic MissileAuthor BiographiesLAURA BENNETT is a Senior Analyst for the Joint Systems Engineering Group, Systems Planning and Analysis, Inc. in Alexandria, VA.  For fifteen years she worked on the engineering design and development of integrated, intelligent systems for the U.S. Army Night Vision and Electro-Optics Directorate, Ft. Belvoir, VA, and the U.S. Army Research Laboratory, Adelphi, MD.  She holds an A.B. from Bryn Mawr College, Bryn Mawr, PA, and an M.S. from the College of Engineering, University of Arizona, Tucson, AZ.  BRETT ZOMBRO is a Principal Staff member and Program Manager for Joint Single Integrated Air Picture (SIAP) System Engineering Organization (JSSEO) support, Systems Planning and Analysis, Inc. in Alexandria, VA.  His expertise is primarily in the area of complex system dynamics.  He holds a B.E. from Stevens Institute of Technology, an M.A. from Princeton University, and a Ph.D. from Cornell University.APPENDIXTable A-1.  SIAP Functional Areas and Measures of Performance (MOP) [8]FUNCTIONAL AREAMOPTIME     Time SynchronizationAbsolute ErrorRelative Error     LatencyElement data exchange latencyTime to get track report on the airLost track persistenceSENSORS/TRACKERSTQ consistencyDATA CONNECTIVITYTime distribution of connectivity failures (radio-to-radio)Time distribution of duration of connectivity failures (radio-to-radio)Time distribution of connectivity failures (track store-to-track store)Time distribution of duration of connectivity failures (track store-to-track store)TADIL update ratesDATA REGISTRATION     Geodetic RegistrationNavigation error(s)Navigation Qpg consistency      IU RegistrationIU registration errorIU registration error covariance consistency      Sensor RegistrationSensor registration errorSensor registration error covariance consistency     Sensor GridlockNetwork-wide absolute sensor gridlock errorNetwork-wide absolute sensor gridlock error covariance consistency     Data ProcessingComputational ErrorsCORRELATION/DECORRELATIONCorrect correlation fractionCorrect non-correlation fractionIncorrect non-correlation fractionFalse correlation fractionCOMBAT ID     IFF ‚Äì Mode N AssessmentAssessment by time durationAssessment by number of responses    CategoryCategory program performanceCategory Assessment    Target/Class TypeTarget class/type program performanceTarget class/type AssessmentTHROUGHPUTDesign capacity usageNumber of tracks deleted due to storage limitsElement update ratesFORMATION TRACKINGFraction of tracks which are formation tracksFraction of tracks of unspecified strengthFormation strength to tracks ratioTRACK MANAGEMENTTrack Initiation Time     Reporting ResponsibilityR2 ShiftsTable A-2.  Measures of Effectiveness (MOE) Metric Examples [8]MOEMetric ExamplesLeakersTotal number of Hostile weapon systems (manned or unmanned) that reach their ordnance release points:  by type.Hostile AttritionTotal number of Hostile targets killed:  by target type.Friendly AttritionTotal number of Friendly targets killed:  by target type.FratricideTotal number of Friendly targets killed by Friendly forces:  by asset type and shooter type.Total number of Neutral targets killed by Friendly forces:  by asset type and shooter type.Weapon ExpendituresTotal number of weapons expended:  by type.C2Total number of engagements ordered:  by type and by target.Blue sortie rates ordered:  by type and by target.BattlespaceLocation/Time of weapon commit.Time/Distance from initial detection to commit.Time/Distance from ID (other than unknown) to commit.Location/Time of intercept (engagement). EMBED Word.Picture.8  Figure 3.1. Measurement of SIAP Attributes [4] Figure 7.1. The JDEP Technical Framework Anticipated for Late 2004 Distributed Simulation Events Figure 7.1.  The JDEP Technical Framework Anticipated for Late 2004 Distributed HWIL Event 