USING THE BAT BEOWULF CLUSTER MANAGEMENT TOOLFOR DISTRIBUTED SIMULATIONSMarioMr. Mario Espinosa andDr. , Edwin Roger Banks, Paul AgarwalCOLSA Corporation, Advanced Research Center,6726 Odyssey Drive, Huntsville, AlabamaL 35806  HYPERLINK "mailto:mespinosa@colsa.com" mespinosa@colsa.com,  HYPERLINK "mailto:rbanks@colsa.com" rbanks@colsa.com,  HYPERLINK "mailto:"  Ms. Jacqueline SteeleMr. Ron LiedelDr. Claudette OwensU.S. Army Space & Missile Defense CommandP.O. Box 1500106 Wynn DriveHuntsville, AL  35807-3801Tel. 256-955-3300jackie.steele@smdc.army.mil, ron.liedel@smdc.army.mil,  HYPERLINK "mailto:claudette.owens@smdc.army.mil" claudette.owens@smdc.army.milpagarwal@colsa.comKeywords: distributed Distributed simulationSimulation, Beowulf, PCpc clusterCluster, BATand BATABSTRACT: Many in the modeling and simulation community are taking advantage of the processing power and cost reduction of Beowulf clusters. Beowulf clusters consist of PCs made of COTS (commercial-off-the-shelf) hardware running open-source or commercial distributions of the LINUX operating system. These systems are administered and managed by the use of scripts. As the number of nodes increases, the complexity of administering these LINUX installations becomes a serious issue. This paper describes the development and characteristics of the Beowulf Administrative Technology (BAT) system. The BAT is a suite of tools designed to expedite and simplify the use of Beowulf clusters during modeling and simulation by facilitating the administration of these systems. This paper will explain how a particular verified, validated and accredited simulation suite used by the Missile Defense Agency (MDA) GMD (Ground-based Midcourse Defense) program called Flight Termination System (FTS) was parallelized utilizing the BAT technology. The FTS system was also run on a cluster using the Open-PBS + MAUI suite of tools and the results compared.DISTRIBUTION A.  Approved for public release; distribution unlimited1. IntroductionThe reduction in the price/performance of PCs (personal computers) in recent years combined with advances in networking technologies has made it feasible to build a crude approximation of an MPP (massively-parallel processor) system out of multiple PCs with a dedicated LAN serving as the processor interconnect. A Beowulf cluster is an example of this approach. Such systems provide, for a fraction of the cost of a MPP supercomputer, a significant amount of processing power with an interconnect performance adequate for many parallel applications that might not require a tightly coupled design8. There are additional benefits to executing a simulation on a cluster (across multiple computers)10:Reduced execution time.. This can be accomplished by dividing simulation computation into many sub-computations executed on different processors.Geographical distribution.. Different components of the simulation can reside on different physical sites.Integrating simulators.. Different simulations could interact better with each other on a distributed environment.Fault Tolerance.. When many processors are available there is the possibility of a processor to take over and pick up the workload for a failed processor/machine.Two main topics are discussed in this paper: (1) a description of a particular suite of cluster management and usage tools (the BAT), and (2) the results of a a comparison of twoa particular simulation runs (STEPAL) using both the BAT and the popular PBS/MAUI combination.Many in the modeling and simulation community are taking advantage of the processing power and cost reduction of Beowulf clusters1-4,7, 7. In the beginning Beowulf clusters consisted of PCs made of COTS (commercial-off-the-shelf) hardware running open-source or commercial distributions of the LINUX operating system. These systems were usually administered and managed by the use of scripts. As the number of nodes increased, the complexity of administering these LINUX installations became difficult.  This paper describes the development and characteristics of the Beowulf Administrative Technology (BAT) system. The BAT is a suite of tools designed to expedite and simplify the use of Beowulf clusters during modeling and simulation by facilitating the administration of these systems and providing end-user support for submitting and monitoring jobs.Our work on this area was originally motivated by the need to have a “one-system” view/interface to a Beowulf cluster through an easy to use API. This API allows developers to create and develop their own tools and applications without having to be concerned about the communication and task scheduling aspects of the cluster. The API takes care of representing the cluster and a single computer with multiple “processors”. We have developed that API and we have added a secondary Java-based API for a highly portable web-interface(s) to this BAT cluster.  Now the developers cannot only develop text-based applications but they can create graphical interfaces to their tools using our Java API. We used the API to create a particular suite of tools to facilitate the administration, monitoring, and development of simulations on a Beowulf cluster.In this paper we explain the approach we utilized to support an MDA GMD project called System Test and Evaluation Planning Analysis Lab (STEPAL). The STEPAL project is charge of intercept and flight termination debris field simulation, among other things. We used the BAT to parallelize their computational process and distribute it on a Beowulf cluster. We simultaneously utilized PBS using the MAUI scheduler to run the simulation in another Beowulf cluster. This paper will compare the techniques and performance of the runs in these two environments.2. Problem DefinitionThe STEPAL FTS software is a set of code and scripts that simulate the debris field associated with established off nominal trajectories associated with an anti-ballistic missile interceptor from various launch sites associated with the GMD flight test program. Two sorts of simulations were run: (1) FTS or flight termination, and (2) Intercept. Non-parallelized versions were first run. For an FTS run with 40 trajectories in one PC (personal computer) with an AMD Athlon 1.4 GHz processor and with 256 MB of RAM (random access memory), an average base time was obtained of 36 minutes and 3 seconds WCT (wall clock time). For an intercept run of 4641 possible trajectories on a PC with a dual-Pentium  ProPentium Pro MMX 233Mhz with 128 MB of RAM took 14 hours WCT. And on a dual Pentium III processorson a dual Pentium III processor at 1133 MHhz and 1 GB of RAM took 4.24 hours (4 hours, 14 minutes).Our goal was to decrease the amount of WCT runtime by parallelizing the calculations done on each trajectory utilizing a Beowulf cluster. A secondary purpose was to compare the BAT running on a small cluster utilizing its load-balance scheduling system against a bigger production mode government cluster running typical cluster tools.At our disposal we have the Advanced Research Center’s [ARC’s] IBM 128 node cluster, each with dual Pentium III processors at 1133 MHhz and 1 GB of RAM. This cluster is worth about $1M. The ARC Beowulf cluster was running the PBS (Portable Batch System) bash script submission program, along with a Maui scheduler. The PCs are networked on a 100Mb switch.Also atto our disposal, we have is a small 7 node7-node cluster, e. Each with an AMD Athlon 1.4 GHz processor and with 256 MB of RAM. This cluster is running the BAT suite. The PCs are networked on a 100Mb switch. This cluster is worth about $5,000.3. ParallelizationA large improvement in WCTTC was achieved by running each “iteration” of the code for each trajectory independently on different nodes within a cluster. This was accomplished by developing a script that will create multiple instances of itself and where each instance is responsible of for making the computation for each trajectory (or a number of )of trajectoriestrajectories). These script instances run as separate jobs on the cluster. After all the jobs are done, one last job will concatenate all the results in to a single file, for the STEPAL team analysis. See Figure 1. TThis form of parallelization did not require modifying the underlying algorithms of the code. Basically we created scripts that ran the same code with different input parameters. This technique was chosen because of the \independent nature of the trajectory-debris computations, and the government validated and verified algorithms could remain untouched.4. Beowulf Tools	BAT	The BAT system manages cluster resources, task queuing, and load balancing1. It consists of a series of daemons or agents that reside on each working node that dynamically connect to an administrative node. The administrative node queries the working nodes for general information, sends in system commands (LINUX shell commands), and submits jobs corresponding to specific tasks. The BAT system also comprises a web-browser interface for task submission and personal (authenticated user) task management. It also consists ofprovides a separate administrative interface with information about the cluster resources, the users, and the tasks the users have submitted. These tools (daemons, central server, and  weband web interfaces) are developed using the BAT System API composed of a JAVA-based API for web-browser tools and a C++ API for the cluster-resident tools. Another tool, the task composer, assists in building a ”task-file”, which can be submitted remotely through the web interface. The task-file in turn, is composed of the jobs that are to be distributed through the cluster’s working nodes. FIn figure 2 we haveshows an overview of how the BAT system works. The Task Composer creates the task file to be submitted through the web client application (web interface, browser-based). The Web Client sends the task file to the BAT cluster administration tool through a proxy server. The BAT administration tool then distributes the jobs to the cluster. BAT Distinctive Features•Task Composer Tool – Graphical application simplifies task file creation and editing.•Fail-over – Allows restart of tasks upon failure.•E-mail notifications – job status and/or results.•Administrative Task-Queue Viewer – Administrators can control the task queues.•Load Balancing Using CPU and Memory Resources – Scheduler optimizer chooses working node with the least load.•Caching – Task files and results can be saved on Administrative Node for quick resubmission.•User and Administrator Interfaces – Two different interfaces simplify system operation.  Provides administrator with detailed system control and monitoring capabilities, end-users with ability to submit and monitor jobs.•Automatic Output Handler – Job results can be sent to data repository on the cluster and/or to the user (or e-mailed).•Lightweight – Designed for minimum cluster intrusion in terms of CPU, network, and storage loads.•API – Allows development of customized tools for cluster administration and job submission. Takes care of communication, synchronization, and monitoring of cluster system.•Scheduling – Supports a number of job submission types (e.g., particular, load-balance, incremental, etc.)•Node Pool Assignment – Allows users to specify a number of nodes from a pool of cluster nodes.•Web Access – Allows users to remotely submit and monitor jobs to a cluster and get status notifications and results.        Comparison between PBS and BATThe following list of features was taken directly from the OpenPBS website5This is a comparison list based on those features. User InterfacesPBS xPBS provides a graphical interface for submitting both batch and interactive jobs, querying job, queue, and system status, and tracking the progress of jobs. Also available is the PBS command line interface (CLI) providing the same functionality as xPBS. BATBAT moved away from the X-server graphical interface to a web-based graphical interface. This allows administrators and users access to different clusters remotely. These interfaces provide a way to submit batch jobs, execute system commands, monitor system status, and track the progress of jobs. Interactive jobs and a command line interface are features to be developed. Job Priority PBS Users can specify the priority of their jobs, and defaults can be provided at both the queue and system level. BAT Job priority can be set within a task at the queue level. But currently there is no Task priority property. Job-Interdependency PBS PBS enables the user to define a wide range of interdependencies between batch jobs. Such dependencies include: execution order, synchronization, and execution conditioned on the success or failure of a specified other job. BAT Within a task, job execution order can be set as well. The BAT provides a variety of scheduling types. Cross-System Scheduling PBS PBS provides transparent job scheduling on any system by any authorized user. Jobs can be submitted from any client system or any compute server. BAT BAT requires an Administrative node to which tasks/jobs are submitted queued and scheduled from any client system.Security and Access Control Lists PBS Configuration options in PBS permit the administrator to allow or deny access on a per-system, per-group, and/or per-user basis. BAT Configuration options in BAT permit the administrator to deny access to the cluster on a per-user basis. Per-group and per-system access privilege are future features.Job Accounting PBS For charge back or usage analysis, PBS maintains detailed logs of system activity. Custom charges can be tracked per-user, per-group, and/or per-system. BAT BAT maintains detailed logs of system and user activity as well. It includes dynamic log-level setting. Comprehensive API PBS Included with PBS is a complete Application Programming Interface (API) for sites who desire to integrate PBS with their applications, or have unique job scheduling requirements. BAT The BAT API supports C++ and JAVA languages. Developers can build their own (cluster and web-based) tools and even modify scheduling parameters. Automatic Load-Leveling PBS The cluster job scheduler provides numerous ways to distribute the workload across a cluster of machines, based on hardware configuration, resource availability and keyboard activity (all of which information is available via PBS). BAT The cluster jobs scheduler provides numerous ways to distribute the workload across a cluster of machines, based on hardware configuration, resource availability (all of which information is available via web-interfaces). Keyboard activity is not presently monitored Enterprise-wide Resource Sharing PBS PBS does not require that jobs be targeted to a specific computer system. This allows users to submit their job, and have them run on the first available system that meets their resource requirements. This also prevents waiting on a busy system when other computers are idle. BAT BAT does not require that jobs be targeted to a specific computer system. This allows users to submit their job, and have it run on the first available system that meets their resource requirements. This also prevents waiting on a busy system when other computers are idle. But sSpecific computer targeting can be done by administrators if requiredneeded. Username Mapping PBS PBS provides support for mapping user account names on one system to the appropriate name on remote server systems. This allows PBS to fully function in environments where users do not have a consistent username across all the resources they have access to. BAT BAT doesn't require accounts to be created and maintained at the working nodes. Each user space in virtualized within the BAT. Administrators can fully control the users' privileges remotely from the Administrative node. Parallel Job Support PBS PBS supports parallel programming libraries such as MPI, MPL, PVM, and HPF. Such applications can be scheduled to run within single multiprocessor systems or across multiple systems. BAT Currently BAT does directly support these libraries but it will allow the use of these environments (including PBS) through the BAT interfaces or separately. Integrated support for MPI is a future addition.Fully Configurable PBS PBS can be easily tailored to meet the needs of different sites. The Job Scheduler module was designed to be highly configurable. And since PBS is currently provided as a source code distribution, each site can customize PBS if they desire. BAT BAT is also fully and highly configurable. Automatic File Staging PBS PBS provides users with the ability to specify any files that need to be copied onto the execution host before the job runs, and any that need to be copied off after the job completes. The job will be scheduled to run only after the required files have been successfully transferred. BAT BAT stages files in different places for better performance (Administrative node and proxy server). Broad Platform Availability PBS PBS is now provided on a wide range of Unix workstations, servers, and supercomputers. Several new platforms are being supported with each new release. BAT Currently the BAT system (cluster tools) is designed to work on any Linux OS. Red Hat and Slackware Linux hashave being tested. The web-interfaces are designed to work in any platform running the JAVA runtime software.Y2K Compliant PBS PBS passed NASA and DOD Y2K testing and is fully Y2K compliant when run under a compliant operating system. Testing included 1999 - 2000 year roll over, leap year, and 366 day testing. BAT The BAT software should be fully Y2K compliant when running in a compliant operating system. 5. Results From Parallelized RunsFTS SimulationWhen run in parallel, the 40 trajectories of the FTS simulation on the ARC 128 node cluster running PBS with a Maui scheduler took an average of 3 minutes, 32 seconds (averaging eight runs). This includes much time just waiting for the jobs to be scheduled, discussed later herein. This result is ~3.1 times faster than the non-parallelized version.The parallelized times are wall-clock from the time the job is submitted until the result file(s) are available. A large portion of the reported time was spent simply waiting for the Maui scheduler to start the job! 	When the same run was done on the BAT 7 node cluster, using only seven working nodes, the average wall clock time was 4 minutes and 29 seconds (averaged over 10 runs). This result is about ~2.5 times faster than the non-parallelized version.    Intercept SimulationFor the intercept simulation, we ran 4641 trajectories on the ARC 128 node cluster under the same conditions. The run took an average of 33.5 minutes (averaged over 3 runs). This result is ~7.6 times faster than the non-parallelized version.When the same 4641 trajectories run was done on the BAT 7 node cluster, it took an average of 1 hour 28 minutes.  This result is ~3 times faster thaen the non-parallelized version.6. Results AnalysisRichard S. Morrison defines performance indices for Parallel Computation6. In this section we define some of these indices (the ones most meaningful to us) metrics and report the values measured. The definitions of some of the performance indices herein are quite abbreviated compared to Morrison’s presentation. Execution Rate. This is a family of measures, such as megaflops, MIPS, LIPS, etc. Here we choose to report the clock rate for the cluster computers.Speedup. The speedup factor is the ratio of the time taken to perform the computation on one processor divided by the time on P processors. Speedup can be a function of P and generally ranges 1 <= S(P) <= P.Efficiency. This is simply S(P) / P.Utilization. This measures the degree of parallelization. It is the ratio between the actual number of Operations and the number that could be performed by P processors in a given time.The results from our cluster runs:Execution Rate. The ARC cluster computer clock rate is 1133 MHhz. The BAT cluster computer clock rate is 1..4 GHhz.Speedup. S(40) has already been reported as 3.1 on the ARC cluster and S(40) is 2.5 on the BAT cluster. S(4641) was 7.6 on the ARC cluster and S(4641) was about 3 on the BAT cluster.Efficiency. For 40 trajectoriesthe FTS runs the efficiency was 3.1/40 = ~8% on the ARC cluster and 2.5/7 = ~35% on the BAT cluster. For the 4641the Intercept runs the efficiency was 7.6/256 = ~3% and on the BAT cluster 3/7 =  ~43%. Utilization. For 40 trajectoriesthe FTS runs the efficiency the utilization was 31.25% on the ARC cluster and for the BAT cluster the utilization was 100%. For the 4641 runthe Intercept runs the utilization was 100% in both clusters.7. Analysis Discussion	We can readily note that even when the ARC cluster consists of more nodes (128), these are not utilized as efficiently as the seven nodes of the BAT cluster. Both clusters have not very different results in the runtime of the simulation. It is true that part of the efficiency difference could be attributed to the higher execution rate that the nodes of the BAT cluster have. But the vast number of nodes the ARC has over the BAT cluster should more than compensate for this, since the simulation never maximizes the CPU resources on any of the systems during execution. That is why the ARC cluster always has better speedup (see Table 1).Table  SEQ Table \* ARABIC 1Trajectories (jobs)AMD Athlon 1.4 GHz 256 MBdualDual-Pentium  Pro MMX 233Mhz 128 MBdualDual Pentium III 1133 MHz 1 GBBAT cluster(7 nodes with AMD Athlon 1.4 GHz 256 MB)ARC cluster(128 nodes with dual Pentium III 1133 MHz 1 GB)4036m 3s--4m 29s3m 32s4641-14h4h 14m1h 28m33.5m	Another important aspect is how schedulers impact cluster performance. We suspectare convinced that the reason the BAT cluster has a much higher efficiency is the fact that the BAT tools contain a more sophisticated and advanced scheduling algorithm. The concept of load balancing, in particular, is different in between the Maui and the BAT schedulers. 	Load Balance	Load balance as defined in High Performance Cluster Computing9 as the capacity of the system to “migrate work load from heavily loaded workstations to lightly loaded ones”. The problem with migrating the jobs (processes) dynamically is that this process carries substantial overhead. Over usage of this migration by the system happens when the resource utilization fluctuates considerably, which is very usual. This over usage can be exacerbated by the intrinsic load such migration adds to the system. Neither PBS nor BAT providesdo true migration in the above sense. The way the “migration” is done by Maui and the BAT very different. Instead of migrating the jobs, the BAT scheduler assigns or submits jobs to particular (or a series of) node(s) based on some load criteria that is kept as historical date inside the schedulers.  Load balancing on Maui is based on job queues of the working nodes. Whichever job queue of a node is emptier is the one getting the next job. In contrast the BAT load-balancing algorithm is based on cluster/node resources. It contains a history of CPU and memory utilization, and based on these parameters it will choose to which node to submit the next job. Basically, it is monitors all the cluster resources as a whole and schedules processes depending on the nodes’ load factors. We strongly believe this perspective is way more efficient than other load balancing schemes. This perspective is great for initial working node assignment on a loaded system without migration”.8. Conclusions	We have demonstrated a particular example of how the BAT suite of tools can help the simulation community community effectively harness effectively the cost-effective computing power of a Beowulf cluster. We believe the BAT is more effective, powerful and easier to utilize than most of the Beowulf administration and job submission tools available today.9. References:	1 Cluster Computing for Calculating Line of Sight over a Distributed Terrain, Guy Schiavone2. Computing with Beowulf,  J, J. Cohen, Raytheon Co., Greenbelt, MD.; National Aeronautics and Space Administration, Washington, DC. 19993. Construction and Utilization of a Beowulf Computing Cluster: A User's Perspective, J. L. Woods, J. S. West, P. R. Sulyma 19994. DDemonstration of cluster computing for three dimensional CFD simulations, W.S. McMillan, M.A. Woodgate, B.E. Richards, B.J. Gribben, K.J. Badcock, C.A. Masson and F. Cantariti5.  HYPERLINK "http://www.openpbs.org/features.html" http://www.openpbs.org/features.htmlCluster Computing, Architectures, Operating Systems, Parallel Processing & Programming Languages, 2003, section 3.1.2, Richard S. Morrison.Ekachai Juntasaro, Putchong Uthayopas, Boonlue Sawatmongkhon, and Khongthep Boonmee, "High Performance Computing for Compressible Turbulent Flow", in NECTEC Technical Journal, Vol2, No. 9, November 2000 - February 2001, ISSN 1513-2145, pp. 182-192.Scaling of Beowulf-class Distributed Systems, John Salmon - Christopher Stein - Thomas Sterling,  HYPERLINK "http://www.supercomp.org/sc98" Proceedings of SC98, Orlando FloridaHigh Performance Computing, Vol2 Programming Applications, Rajkumar Buyya, 1999 Prentice-Hall, Inc.Parallel and Distributed Simulation Systems, Richard M. Fujimoto, p 4-5, Chapter 1.1, Wiley-InterScienceAuthor BiographiesMARIO ESPINOSA is a Senior Software Engineer at COLSA Corporation in Huntsville, AL.  He has over 102 years of experience in system administration, optimization, and programming, across a number of hardware platforms.  He has worked on both commercial and government-focused software and networking projects.   He assisted with the review, analysis, and interpretation of the RTI benchmark data.EDWIN ROGER BANKS is a Senior Scientist at COLSA Corporation in Huntsville, AL. He has been involved with cluster computing, mobile computing, innovative software tools, a large expert system application, and a variety of programming projects over the past 35 years.  Dr. Banks received his Ph.D. in Mechanical Engineering from M.I.T. in 1971.RON LIEDEL is a senior engineer with the Space and Missile Defense Battle Lab (SMDBL) located in Huntsville, AL, and has authored numerous SMDC and BMDO policy forming software documents.  These include the SMDC Software Development Plan, the SMDBL 10 Year Software Plan, as well as the Command Software Mission and Function Statement.  Mr. Liedel founded and chaired the SMDC/SDIO Computer Resource Working Group and has presented several papers nationally on Software Sizing for Mega Systems. He serves as the FAST Lab Director for SMDC.JACQUELINE "JACKIE" STEELE is Chief of the Computer Resources Division, Space and Missile Defense Battle Lab, U.S. Army Space and Missile Defense Command (USASMDC) with over 20 years experience in engineering and simulation in government, industry, and academics.  Ms. Steele oversees the Advanced Research Center and the Simulation Center, and manages the DOD High Performance Computing Management Program (HPCMP) Shared Resource Center located at SMDC.  She holds a BME and BTE from Auburn University and MSM from University of Alabama-Huntsville.CLAUDETTE OWENS is a senior engineer in the Computer Resources Division of the Space and Missile Defense Battle Lab, U.S. Army Space and Missile Defense Command (USASMDC), in Huntsville, AL.  She has over 18 years experience in basic R&D, engineering and simulation in government.  She is the government technical monitor responsible for the day to day operations of the USASMDC Advanced Research Center.  Dr. Owens holds a Ph.D. and Masters in Physics from Alabama A&M University and BS degrees in Electrical Computer Engineering and Electrical Engineering Technology from the University of Alabama, Huntsville and Alabama A&M University, respectively.(add updated bio here)DRAFTDRAFT  EMBED PowerPoint.Show.8  Figure  SEQ Figure \* ARABIC 1. Parallelization OverviewFigure  SEQ Figure \* ARABIC 2. BAT Overview