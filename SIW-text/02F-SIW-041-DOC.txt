Event & Scenario Projection (ESP) Tool:  Improve Efficiency and Minimize Errors in Distributed Simulation Training Events by Applying Analysis of Past Event Monitor and Test Data to the Event Planning PhaseDavid FisherJames L. BryanNorthrop Grumman Information Technology12000 Research Pkwy Suite 236Orlando, FL 32826407-243-2010, 407-273-2009 HYPERLINK "mailto:david.fisher@northropgrumman.com" david.fisher@northropgrumman.com,  HYPERLINK "mailto:jbryan@northropgrumman.com" jbryan@northropgrumman.com Keywords:distributed training, scenario development, network performanceABSTRACT:  The concept of the proposed Event & Scenario Projection (ESP) tool is to provide “real time” guidance during the planning phase of distributed simulation training and exercise events.  The areas where guidance can be provided to the event developer are: in the allocation and distribution of scenario entities to specific participating simulation facilities and simulation models; in the allocation and configuration of the network resources; and in the estimation of the mean, average, maximum theoretical, and instantaneous peak network performance parameters.  By analyzing the collected data from past events, a cause-effect relationship can be determined using statistical, stochastic, empirical modeling and/or neural network techniques.  This cause-effect relationship may then be used during the event development process to proactively configure the architecture to provide for increased simulation efficiency, optimized network performance and to minimize or eliminate potential problems during the training event.  Simulation protocol, event control, network performance and monitor/test data collected during previous distributed simulation training, exercise or test events would be used to train the neural network, define the constants in an empirical equation based model, and/or determine statistical parameters.  The ESP tool can also be utilized to analyze previously run training events before they are re-executed.  This is particularly useful if “fair fight” issues arose during the event or when run-time / post event analysis indicates excessive latency, dropped data or other troubling simulation issues.   INCLUDEPICTURE "http://www.opinionjournal.com/images/storyend_dingbat.gif" \* MERGEFORMATINET 1. IntroductionAnytime you run a simulation event over a distributed network, there will be some delay or latency between the time that the system simulating an entity sends out information and the time the other systems in the simulation receives that information.  As long as the latency is below some threshold level, which may vary depending on the type of data being passed and the types of simulations participating in the event, there will be no impact on the effectiveness and execution of the event.  The reception reliability of data transmitted from one simulation system to another, may or may not adversely affect the event, based on a number of factors.  These factors include the type of data being sent (such as entity state data), what it is used for by the receiving system, and how often the data is updated.  The integrity of data as it is being transmitted from one simulation system to another can also impact a distributed simulation execution.  The routing of simulation data through encryption/decryption devices or data protocol translators such as gateways or portals, can affect the data integrity, possibly rendering the data unusable on the receiving end.  Depending on the type of information and the interaction of that information within the other simulation systems, the impact of latency, reliability, or data integrity above certain threshold levels can vary from no perceivable impact, to the manifestation of annoying “fair fight” issues, through impacts that can result in negative training or the disabling of the simulation.  For the purposes of this paper, the term “simulation” or “simulation resource” refers to either a virtual (human-in-the-loop) simulator or a constructive (computer generated force (CGF)) simulator such as a semi-automated force (SAF) simulator, participating in a distributed simulation event.  An “event” or “simulation event” is the execution of a scenario to meet a particular objective using a collection of simulations over a geographically distributed wide area network (WAN).  "Weapon" or "weapon system" will be used to identify/define a type of real world combat platform with certain characteristics and capabilities, whereas the term “entity” is a manifestation of a particular weapon system within a simulation where the simulated presence could be generated by either a virtual simulator or a constructive simulator.2. Tool ConceptThe proposed Event & Scenario Projection (ESP) tool would predict and estimate latency, reliability, integrity and other data transmission issues for simulation events by analyzing a combination of event planning and runtime data from previously executed simulation events.  This analysis can be used to help predict where data transmission errors are most likely to occur during the execution of the event being planned, and to provide “real time” guidance to planners in order to optimize the simulation event architecture.  In this paper we will discuss potential analytical techniques that may be applied to the previously collected event data and how these techniques would be applied to help an event planner predict potential runtime issues for an event during the planning stage.  The analytical techniques discussed will be statistical, neural network, and empirical in nature.The amount of data transmitted and received by a simulation resource is strongly dependent upon the weapon system being simulated, the type of simulation (virtual or constructive), and the specific situation and evolution of the scenario.  The evolution of a scenario is non-linear in nature and due to this fact variations or changes in the response of the simulation entities, and in particular the response of humans participating in a simulation event, can produce dramatic differences in results during subsequent executions of the same event scenario.  Because of this, the critical time evolution and statistical data parameters collected during the execution of one simulation event can vary considerably from the subsequent execution of the same scenario.  It is therefore impossible to predict the exact time evolution of these data parameters, and to a lesser extent but still very difficult, the statistics of these parameters.  For example in one evolution of a particular simulation event, a formation of fighter entities may be destroyed or damaged by surface-to-air fire early in the event resulting in a particular evolution sequence, data flow rate and scenario outcome.  In another running of the same scenario, that four-ship might survive the ground fire to engage enemy aircraft later in the evolution, significantly affecting the outcome.  This would also dramatically change the time evolution and data statistics for that run compared to the previous one.  The later evolution would potentially produce considerably more data transfer than the first due to the longer event duration and more intense interactions from the air-to-air engagement versus the surface-to-air engagement.  The purpose for a tool employing the concepts described in this paper is not necessarily to accurately predict critical performance parameters such as latency or bandwidth usage as a function of time, but to predict reasonable upper bounds of the peak levels of these parameters.  This prediction should have some confidence interval associated with the prediction, and both the critical value and its confidence interval would be determined from past event and test data.  As the techniques described in this paper rely on past data for their predictions, the predictions of the upper bounds of the peak levels should become more accurate as the pool of data increases.  The analytical techniques would more heavily weight the data derived from the most recent simulation events, as the latest changes in the participating simulation systems, software or training techniques would be reflected in this data.3. ApplicationsThe primary use for this tool is during the planning phase of a training or exercise event.  In particular, it will be most useful during the mapping of scenario entities to participating site simulation resources such as virtual simulation trainers or CGF’s.  As scenario entities are assigned to simulation resources, a prediction of useful parameters for the distributed simulation would be displayed.  Examples of this would be predictions of maximum theoretical bandwidth requirements or peak latency at specific network bottlenecks, such as at a router to WAN interface at a particular site with historical latency issues for particular data types or data volumes.  These parameters could be used to help more evenly distribute the loading on sites involved in the training event and warn when specified parameters approach levels where bandwidth, latency and data integrity issues might become apparent.  In addition to providing a warning, the software could also make suggestions of alternate entity mapping configurations to attempt to eliminate the predicted issues. A number of the simulation sites participating in a distributed event may have a redundant capability to simulate particular entities, either virtual or constructive.  For example, the simulation scenario might call for the use of a pair of virtual F-16 entities to be instantiated from a particular site along with a number of other constructive entities to be instantiated by a SAF at that site.  However, the ESP tool may predict that there could be a latency bottleneck at the desired instantiation site due to the number of interactions or the volume of data predicted to flow through the sites router.  The tool might suggest either transferring the instantiation of the virtual F-16’s to a similarly equipped site or to allocate the task of generating all or some of the constructive entities to a site with a similar SAF capability.  If, in the originally defined Federation of sites, a similarly equipped site is not available to accept the task of generating the reallocated entities, the ESP tool might also be able to offer the suggestion of adding an additional site that has the appropriate simulation capability to pick up some of the entity generation responsibility and provide a more level traffic load.  From previous event data, the tool would be able to suggest specific sites with the appropriate simulation capabilities.  Another application for this tool is for the monitoring of run-time data during distributed events.  If bandwidth, latency or data integrity issues show up during a simulation event, the use of this tool could potentially help eliminate the problem; or at least help pinpoint the source of the runtime problem in real-time or near real-time before it becomes a critical factor in the successful execution of the event.  This is a valuable capability, especially for large scale, multiple site distributed simulation events which take many manhours to coordinate, and are often expensive to put on due to site, personnel or other cost drivers.  Instead of canceling the training event to conduct troubleshooting, the event could be paused and the ESP tool analysis and predictions derived from the collected event data along with past event data, help guide the troubleshooting efforts to rapidly resolve the issue.  Potentially, the tool propose appropriate courses of action to affect troubleshooting, or display parameters to help guide troubleshooting activities towards an appropriate solution, such as the re-mapping of entities to alternative simulation resources.  If the tool could not provide the appropriate solution, it could at a minimum pinpoint the location of the problem.  Once the troubleshooting or reallocation has been completed, the event could continue. Many types of problems experienced during distributed simulation events are often fairly obvious, even to the casual observer such as the observation of flying tanks (enumeration problems) or entities jumping around the screens (potential latency problems).  However, there may be cases that are not as obvious and where a problem may not be perceived or observed during an event, even though a latency, reliability or other issue is actually preventing a “fair fight” environment or providing negative training, all unbeknownst to the event participants (aircrew) or simulation site personnel.  An example might be that voice radio data packets are being dropped due to overrunning data at a particular router.  This results in a situation induced by the simulation architecture (“Simism”) where radio calls that would normally be received and responded to, do not make it to the intended recipient(s) hampering the realism of the synthetic comm environment and potentially negatively influencing the event outcomes by making the ability to communicate much more difficult than it would in the real world environment.  This type of problem may not necessarily be readily apparent, and may only be detectable by using automated data collection and analysis tools running during the event runtime.  This may drive a requirement for runtime, end-to-end network performance monitoring and data collection.  In order to provide the data required to support the ESP tool to troubleshoot potential distributed simulation runtime issues or problems, visibility into network “end-to-end” performance and the collection of certain statistics would be required.  That means following scenario data from the time/location of its generation to the time/location it is used or displayed.  This may mean that participating sites may need to measure and share specific internal performance parameters or even provide access to their internal systems to provide that insight.  A range of possibilities exists to allow for the collection of data to support the ESP tool and to aid in the troubleshooting of interoperability problems.  These possibilities range from data collection schemes conducted independently at each site (and collated later as needed), to real-time network performance data collection and analysis at a central or distributed location.  Obviously, at each extreme of possibilities, the intrusiveness into the participating sites and the impacts to the performance of the network will be affected differently.  In order to provide either run-time analysis or post-event analysis using the ESP tool, the collection of parametric data at specific points in the trip along the network is required.  4. Analysis Implementations4.1 Simple Statistical:The first and most basic implementation of the ESP tool concept is the improvement of distributed events through the monitoring and analysis of multiple runs of a specific event scenario.  If periodic latency data is collected during the running of the event which supports users indications of “fair fight” or negative training issues, this data can be used to help re-map scenario entities to other simulation assets to reduce the possibility of future problems.  The time and value data of the troubling latency, along with event control data such as who is simulating those entities being affected by the latency, could be used as necessary to reallocate the simulated entities during the planning stage of the next execution of the same scenario or a similar scenario based on the original one.  An increase in efficiency and reliability for these predictions could be accomplished with additional event data, such as activity data on all actively simulated entities (the total quantity or the peak data rates being transmitted/time).  The tool would use the entity data rate activity to help predict the latency of the re-distributed training event.  Because of the simple nature of this problem (re-running of the same or similar training events) only simple statistical data on the past event along with event control data would probably be necessary to make the predictions.A second and more complex mode for the application of the ESP tool would be to use it during the planning of a new or a highly modified previously run scenario.  In this mode, as scenario entities are mapped to site simulation resources, predicted peak latency levels or required network bandwidth would be displayed for each participating site, along with pertinent warnings and suggestions for entity re-distribution if certain preset threshold levels were crossed.  There are several possible techniques that may be used for these predictions, utilizing the data collected during previous distributed events.  Initially, a simple statistical model could be used.  For example, the peak and/or average contribution to the activity (bandwidth usage) for a particular entity generated at a particular fidelity level by a simulation site during previous events can be calculated and used in the prediction.  The output in this simple example could be a simple sum of the peak values for all the entities simulated.  4.2 Neural Network:Another, and potentially complementary, method to use for the analysis of past event and test data to predict critical parameters such as bandwidth or latency is through the use of neural networks.  Since these and other critical parameters are affected by the individual interactions between the event scenario entities, the runtime system is, to a great extent, non-linear in nature.  For this reason, neural network techniques should have a reasonable success in predicting potential bandwidth, latency or other parametric issues if applied correctly.  For each participating site, the collected event statistical data for each type of entity generated by that site during an event would correspond to the analysis input, while the output would correspond to important parameters such as bandwidth usage and latency implication predictions for the entities analyzed.  EMBED PowerPoint.Slide.8  Figure 4.1: Example Neural Network Configuration.A neural network is a system that tries to model the way that humans learn.  The system is not equation based, but is trained (as in the human brain) by inputting collected event network and statistical data along with the associated known result.  Once trained (training may be a continuous process as new data becomes available), current event entity parameters are input and the predicted results are output for each entity type.  The use of neural networking as a tool for problem solving is a subject of interest in many fields.  As with most techniques, there are a number of approaches to applying neural networks.Initial versions of a neural network tool might use coarse site input data with each simulation site having inputs only at the level of the simulation, with three potential inputs per site.; virtual simulators, roll played / threat station simulators, and computer generated forces simulators.  Inputs would be the number of entities simulated at each simulation level for each site.  The output would correspond to the value of the critical parameter of interest such as peak value of the bandwidth required.  This could then be compared to the available bandwidth for that site and issue a warning if the required bandwidth approaches or exceeds the bandwidth available. The training of the neural network would use the combination of event control and network/simulation performance data collected during a distributed simulation event.  The training of the neural networks would use event control data to determine the inputs.  It would use performance data collected from the event to set the output during the training of the neural network.  In the above example, there would be three inputs and one output.  Participating sites would have their inputs set corresponding to the number of entities they simulated at that the described simulation level.  Sites that do not participate would have their inputs set to zero.  The outputs for the neural network training would be set to the critical parameter of interest determined from the data collected during the event (peak bandwidth usage for example).  The output training could be other statistical data calculated from the collected run-time data.  Training of the neural network would continue on a regular basis as newly collected data is received.Further resolution to increase the accuracy of the predictions could be achieved by subdividing the neural network inputs of event simulation entities.  For example; all virtual simulations would have their own input corresponding to the type of weapon system simulated.  Once the neural network has been trained, the output of the training could be used to guide in the event control-planning phase of a new training event.4.3 Linear Statistical Model:As in the straightforward case of using simple statistical analysis while re-running a past event scenario with the same Federation configuration, linear statistical techniques may also be applied to the more complex problem outlined previously in the neural networks approach.  One approach would be to determine from past distributed simulation events and tests, a peak critical parameter value (ex. bandwidth) contribution, both into and out of the router for a specific entity instantiated by a specific site.  The sum of all the peak values would be used as a maximum predicted value for that particular entity in bandwidth requirement predictions.  The use of a simple sum of the statistical values, in general, will yield an unrealistically high value.  During the instantiation of each of numerous entities at a specific site, it is highly improbable that each will hit the calculated upper bound value during the same event and at the same time.  The application of statistical correlation techniques could give a more reasonable upper bound, but would require time information on the critical parameters for each training event.Another more complex, but more reliable predictive method the ESP tool might utilize, would be to determine a mean and variance of the peak value for the desired parameter, and to provide predictive values based on a requested or default confidence interval (i.e. 95% confidence that peak bandwidth usage will be below X).  The accuracy of these predictions would increase over time as more distributed simulation events were run and the amount of collected data for statistical analysis increased.  Like before, the simple sum of these parameters for all the entities simulated by a site would, however, be an overestimate for multiple entities.  Again, it is highly improbable that each will hit the maximum value on the same training event and at the same time.  This value could be scaled down by a reduction factor or statistical technique such as the application of a time series model.. 4.4 Multiple Order Statistical Models:A third approach would be to use higher (multiple) order statistical based equation modeling to predict the critical parameters by applying multiple regression analysis.  This would be used to introduce coupled terms.  The coefficients of these models would be determined from past training event and test data statistics.  Again these models could be used to predict parameters such as peak bandwidth with an associated confidence interval. One implementation of this technique is similar to that described in the neural networks section.  In this simple implementation, the independent variables could be the number of assets simulated at a specific site for a specific fidelity.  The dependent variables would be the critical parameter that we are trying to predict such as bandwidth or latency at specified bottlenecks.  The coefficients would be determined by applying multiple regression analytical techniques such as a least squares fit to the statistics from past event and test data.  As the statistics of the events would be updated with new event data, the coefficients would also be updated.5. ConclusionIt is possible to develop a tool that would provide simple to complex statistical modeling for the prediction of distributed simulation event performance parameters using the techniques described above.  This capability would provide the event developer an ability to develop a scenario that would be optimized for the system architecture the event is to utilize.  An ESP tool would also allow runtime monitoring of distributed events while providing predictive fault detection and analysis in real time.  This capability would provide a tremendous tool to lead to more efficient and cost effective (time & $’s) distributed simulation executions.Author BiographiesDR.  DAVID L. FISHER is a software developer for Northrop Grumman in Orlando, FL.  Since receiving his doctorate in Physics from the University of Texas at Austin in 1995, he has worked in the areas of theoretical plasma physics, the detection of explosive materials, millimeter wave sensor technologies and applications, and in the development of an Internet business.  He is presently developing several software tools.  David has 17 professional publications and 2 patents (1 pending).JIM BRYAN is a Systems Engineer and the Information Engineering Section Lead for Northrop Grumman IT in Orlando, FL.  He is responsible for standards development, systems analysis and test/user tool development for the USAF Distributed Mission Training (DMT) program.  While on active duty with the USAF as a combat search and rescue helicopter pilot, he was the test director for the OSD sponsored JCSAR JTE’s Virtual Simulation testing program.  He presented his paper, “Conducting JCSAR Mission Execution Testing with Distributed Interactive Simulation” at the December 1997 International Test & Evaluation Association (ITEA) conference, which is published in its proceedings.  Thanks:The authors would like to give a special thanks to Theresa Clark for her help in preparation of this document. FILENAME 02F-SIW-041_020703-final.doc		8/8/02