The Combat Automation Requirements Testbed (CART) Program: Post Hoc Analysis of  Case Study 1 ResultsAuthors:Capt Jeff NagyDavid G. HoaglandEdward A. Martin, PhDMartin Anesgart, PhDAir Force Research LaboratoryAFRL/HECI2210 Eighth StreetWright-Patterson Air Force Base, OH 45433-7511937-656-7013, 937-255-8072, 937-656-7214 jeffrey.nagy@wpafb.af.mil david.hoagland@wpafb.af.mil, edward.martin@wpafb.af.mil, martin.anesgart@wpafb.af.milBryan E. BrettScience Applications International Corporation4031 Colonel Glenn HighwayBeavercreek, OH 45431937-431-4390Bryan.e.brett@saic.comKeywords:CART, goal orientation, human performance modeling, task network models, time critical targets, verification & validation, constructive simulations, JIMM, HLA, simulation-based acquisition.Abstract:  The Air Force Research Laboratory’s (AFRL) Human Effectiveness Directorate undertook the Combat Automation Requirements Testbed (CART) program to develop and demonstrate a human performance modeling environment that will provide a means for modelers and analysts to represent human decision making and tactics in a model that can interact with other simulations using the High Level Architecture (HLA).  An experiment was conducted (called a Case Study) to demonstrate the feasibility of CART’s goal-oriented approach to modeling human behavior.  The CART team evaluated an operator model against pilot-in-the-loop performance for a time critical target (TCT) scenario.  Overall the results of the descriptive and statistical analysis indicate a high degree of consistency between the performance of the HPM and that of actual pilots (60% correlation).   This paper presents the detailed results of the data analysis and examines where the HPM and the pilot differed and why.1. IntroductionRecently the Air Force Research Laboratory’s (AFRL) Human Effectiveness (HE) Directorate completed its first case study for the Combat Automation Testbed Requirements (CART) program.  The study was designed to develop and demonstrate a human performance modeling environment that will provide a means for modelers and analysts to represent human decision making that can interact with other simulations via the High Level Architecture (HLA).  The results for case study one were presented at the Spring 01 Simulation Interoperability Workshop (SIW). [1][2] At the time of the Spring conference, AFRL/HE was still in the process of completing a post hoc analysis of the data collected in case study and therefore only presented the preliminary results.  Since that time a more through look at the data has been accomplished.  This paper will describe the methodologies used in the data analysis and present the final conclusions and recommendations.2. Overview of the CART Program2.1 Program Vision, Goal, and ObjectivesThe vision of CART is to provide the Air Force with the capability to maximize total human-system performance while saving time and money in acquisition using realistic human performance models.  To achieve this vision, the CART team is developing and demonstrating an advanced human-centered modeling and simulation (M&S) technology capable of establishing objective, performance-based crew system interface requirements.  The top-level technical objectives are to:  (1) develop tools that enable creation of realistic operator models based on goal orientation, (2) develop methods that connect operator models to a joint constructive environment, (3) demonstrate the technology via Case Studies using warfighter domains, and (4) transition the technology to the warfighter, acquisition and industry. The overall purpose of the CART case studies is to 1) demonstrate the technology of the CART human performance modeling environment and the approach to model development and integration with constructive simulations, and 2) to assess the validity of the CART concept by comparing mission performance data from simulation trials that incorporate a CARTdeveloped HPM with data from trials incorporating a HITL simulator operating in the same environment.  If the CART approach and tools can be demonstrated to create and integrate HPMs that successfully represent human performance in the case study environments, CART can be offered as an expanded set of tools to the organization/modeler/analyst seeking to improve representations of operator performance in constructive simulation environments.2.2 Case Study 1 Development, Variables, and MethodologyCase Studies are a key activity of the CART program to demonstrate the feasibility of its goal-oriented approach.  Primary objectives include:  (1) successfully developing and integrating an HPM into a constructive simulation environment; (2) evaluating the operator model’s performance against human-in-the-loop (HITL) performance in the same scenario; and (3) using the Case Study results to address issues of interest to the participating acquisition program.The scenario for which constructive and virtual simulation trials were run consisted of a strike mission against a time critical target (TCT, or Scud hunting).  A part-mission scenario, in which only the acquisition, attack, and egress legs were flown, was developed based on prior virtual simulation exercises.  The scenario called for the HPM and pilots to fly at medium-to-high altitude, and employ a simulated Joint Strike Fighter (JSF) sensor suite in an effort to acquire and attack high value, mobile TCTs.  In addition, it required the HPM and pilots to react and avoid pop-up surface-to-air missile (SAM) threats.  Target acquisition sensors available to the HPM and pilots included synthetic aperture radar (SAR) with a ground moving target indicator (GMTI) overlay and a targeting infrared (TIR) imaging system.The dependent variables for Case Study 1 are listed in Table 1.  These measures are goal oriented and were selected using Rasmussen’s Means-End Abstraction Hierarchy during the of decomposition mission tasks.[3]  Thus, the overall mission task (the end:  kill TCTs) was decomposed into lowerlevel goals (the means:  navigation, threat evasion, target attack, and others) that supported that end.[4]  In like fashion, the goals were further decomposed into the tasks and notional system attributes supporting the courses-of-action associated with the tasks.  This means-ends decomposition provides visibility into the way performance at the lower levels “bubbles” upward through all levels of the particular tasks or operator performances that most directly influence the higher, mission-level outcomes.Table 1: Case Study Dependent Variables (DVs)Navigation GoalDV1 = number navigation error replans generatedDV2 = number navigation error replans acceptedDV3 = number threat-based replans generatedDV4 = number threat-based replans acceptedThreat Evasion GoalDV5 = number threat locks on ownshipDV6 = number threat launches at ownshipDV7 = percent threat missiles defeated by ownshipTarget Attack GoalDV8 = range at weapon release3.0 Case Study Results [5]3.1 Overall  Analyses There were two measures used to assess the overall ability of the HPM to accurately represent the performance of the pilots in the HITL simulation.  The first was to simply count the number of ground targets destroyed by the HPM and compare it to the HITL case.  For Case Study 1, the number of targets destroyed by the HPM was 36 out of 36 trials, or 100%.  In the HITL condition, pilots destroyed 47 targets out of 48 trials, or about 98%.  The post hoc analyses took one step further and looked at some of the underlying differences between the dependent variables.The effect of interest was whether a difference existed between the human performance model (HPM) and the human-in-the-loop (HITL) conditions on the eight dependent measures noted in the last section, taken individually and in concert. Demonstrating whether a difference existed fell to three categories of measures as produced with the aid of SPSS, release 10.0:Measures of central tendency and dispersion, inference, and internal structure, this last being derived as a byproduct of the other two. Measures of central tendency and dispersion (descriptive statistics) included means, standard deviations and intercorrelations among the dependent measures. Measures of inference included repeated measures multivariate and doubly multivariate analyses of variances (MANOVA). Measures assessing whether internal structural differences existed between the two operator types included an index developed by the Department of Psychology at the University of Akron in the 1980’s, called the congruency ratio. Congruency ratios are based on intercorrelation matrices, as well as tests of homogeneity of variance/covariance (Levene and Box’s M) drawn prior to the interpretation of MANOVA.  3.2 Descriptive Statistics: Measures of Central Tendency and Dispersion    Presenting descriptive statistics for the classified dataset used in the current study needed a means to present the classified data for this report in an unclassified format that hid the true responses of the operator, yet preserved any differences between operator types. Used was a “mean rank” procedure that transformed each of the eight dependent measures onto the same numeric scale. This method was used solely to provide means and standard deviations and not the correlations. The inferential results, that for this paper depict the model effects at a general level, are such that the actual data values cannot be defined. On the average, the HITL pilots showed poorer performance on several of the dependent measures; namely, more locks and launches on own ship as well as a greater range at release (of missile on target) and a lower percentage of threat missiles defeated The   results were not as clear cut with the remaining measures. The pilots generated and accepted more navigation error-related replans, while the HPM generated and accepted more threat-based replans. Although the percentage of variation around the mean, known as the coefficient of variation (standard deviation divided by the mean times 100), was generally less for the HPM  than for the HITL, the percentages were greater than expected for the HPM. For example, a high of 24% for range at release compared to an HITL high of 31% for the same dependent measure On one measure it was higher; the number of launches, 17% for the HPM and 12% for the pilots.    Moreover, the results were not consistent across all scenarios. Although treated as equivalent, some of the scenarios revealed results opposite to the overall average. For scenarios one and two, the model demonstrated a greater number of locks and launches on own ship (mean rank of 43.08 and 34.17, respectively for scenario one, and 67.92 and 67.58, respectively for scenario two) than did the pilots (mean rank of 38.06 and 31.81, respectively for scenario one, and 33.81 and 48, respectively for scenario two); for scenario one the pilots defeated a  greater percentage of missiles than the model; the    range at release was further for the model than the pilots in scenario three; and the model generated and accepted more nav-related replans than did the pilots for both scenarios two and four. Note that measures of central tendency and dispersion for ranked data typically involve medians and interquartile deviations, but transmuting classified data into an unclassified dataset required a mean rank procedure for the reasons noted previously, and as a consequence the provision of means and standard deviations.3.3 Descriptive Statistics: Correlations Among the Dependent VariablesUpon the total sample (N=84 for the correlations among DV1 to DV6; N=81 for correlations with DV7 and N=83 for correlations with DV8 due to missing data) rested the framework for determining the multivariate models explained in the next section. The intercorrelations by operator type also became important for a subsequent procedure, the calculation of the congruency ratio as discussed in a later section. What is important to note here is that in the majority of cases the correlations among the dependent variables for HPM exhibited similar strength and the same direction as the correlations for HITL, with some notable exceptions.  For the model number of nav error-related replans generated and accepted (DV1 and DV2) had highly positive and significant (p<.01) correlations with number of launches against own ship (DV6), while the respective correlations for the pilots were still positive and significant, but to a much lesser degree. And the number of locks and launches (DV5 and DV6) inversely correlated to a greater degree with range at release for the model than for the pilots. Also, there were two exceptions to the finding that while for a number of variables there were differences in sign, i.e., one correlation was positive while the other was negative, neither of the correlations was significant (p<05). Both number of threat-based replans generated and accepted (DV3 and DV4) showed a significantly moderate (p<.01) and negative correlation with percent missiles defeated (DV7) for the model, while the respective correlations for the pilots were nonsignificant (p>.05) and positive. In fact, DV3 and DV4 (as well as DV1 and DV2) correlated with all the other variables exactly the same for the model, whereas for the pilots, there was some, though not excessive variation.3.4 Measures of Inference   Each iteration of the human performance model and each pilot executed all of the six scenarios or trials,    Leading to the strong possibility of correlated error between trials, at least for the pilots. Simply stated, performance on one trial can color performance on a subsequent trial. One way to handle this potential bias was through the use of repeated measures analysis, an analysis that enabled generalization beyond the current sample. Performing this analysis required examining beforehand the matrix of correlations among the dependent variables, finding those sets of variables that correlate with each other significantly (p<.05) and forming models to simultaneously test the sets of variables in what is known as a “doubly multivariate” analysis. An example with five dependent variables illustrates the concept with “↔” indicating significant correlation:DV1↔DV2 DV2↔DV3 DV3↔DV4 V4↔DV5↔DV3		         DV5          ↔DV5From these intercorrelations three models would result―(1) DV1, DV2, DV3; (2) DV1, DV3, DV5; and (3) DV3, DV4, DV5. Examining the matrix of intercorrelations for the entire sample in the study revealed three sets of variables, forming the following three doubly multivariate models:Model 1―DV4, DV5, DV6, DV8;Model 2―DV3, DV4, DV7;Model 3―DV1, DV2, DV5, DV6 Used were three categories of measures to assess whether the HPM operated in the same or similar manner as the HITL. Although the measures of central tendency and dispersion evinced some differences, in many cases similar coefficients of variation (CV) existed in the model as in the pilot group. For the two dependent variables where the inferential post hoc statistical tests (with all scenarios included) found significance, CV was 9% for HPM and 12% for HITL with DV4; 17% for HPM and 12% for HITL with DV6. For DV3, found significant when scenarios two and three were deleted, CV was 10% for HPM and 16% for HITL. Further, the measure of internal structure used to assess commonality among all the dependent measures revealed good consonance between the model and the pilots (60.82%).Interaction between operator type and scenarios was not posited as part of the hypothesis tested, but evidence for differences between scenarios and the way the two groups operated within a particular scenario could not be ignored as shown by the tests of homogeneity of variance/covariance. The three doubly multivariate models were tested on each scenario separately. For Model 1, the omnibus tests resulted in no significance (p>.05) for any of the scenarios. For Model 2, significance (p<.05) was found for scenarios three, four and five (population eta-squares of .637, .346 and .411, respectively). For Model 3, significance was found for scenarios two, three and four (population eta-squares of .395, .474, and .527, respectively). Understanding or categorizing the scenarios would be useful for future case studies.4.0 Discussion4.1 HPM PerformanceDiscussion Overall the results of the descriptive and statistical analysis indicate a high degree of consistency between the performance of the HPM and that of actual pilots.  This is particularly true regarding measures of performance at the generalized function level.  In terms of target acquisition and attack, there was little or no difference in the probability that the target would be located, correctly identified, and destroyed.  Differences were observed in measures of navigation and survival function performance.  These appear to be driven by differences in the way actual pilots and the HPM used the auto-router.  Employment doctrine used in the trials dictated that with the exception of maneuvering to evade a launched missile, the auto-router was to be used exclusively for making route changes.  The human performance model followed this doctrine completely.  Pilots were less consistent.  Observation of the HITL trials and comments from the pilots indicate that the inconsistent use of the auto-router by pilots can be attributed to quirks in the performance of the auto-router.  The auto-router was a beta version of the capability.  Sometimes it created routes that seemed to be counterintuitive to the situation.  A good example of this are routes generated for target attack.  Targets were always identified using the TIR.  Because of the limited range of the TIR, the aircraft was close to the target when the identification and subsequent designation was made.  Once the target had been designated, pilots were to use the auto-router to create an attack route to the target.  Often, this route was not a straight line or short path to the target.  Indeed a relatively long route might be generated that spiraled around the target, eventually passing over it.  In these instances, some pilots would ignore the route and drive directly to the target and execute the attack. Pop-up threats was another situation that could generate unusual routes.  In these cases (particularly scenario 3), the pilot or HPM would successfully evade a threat, but once the evasion was complete the auto-routed might then offer a route that took the aircraft either back over or close to the threat so that another evasion maneuver was required (this could occur if the pop-up threat was very close to the original flight path and close to a “must fly” point through which the aircraft had to pass).  In trials flown by the HPM, this process might be repeated several times until, eventually, the aircraft “cleared” the threat.  Pilots, on the other` hand, would immediately understand the problem and manually fly the aircraft around the threat until a route would be generated that would not overfly the threat.  While agreement in performance of the HPM and pilots was basically good at the general function level, it would be appealing to know the correlation of performance at lower levels.  Unfortunately, this has proven difficult.  The challenge is in obtaining insight into the lower level performance of the live pilots.  The most interesting aspects of operator performance are those cognitive and perceptual activities that are tied to mission success (e.g., target detection and identification, shoot list prioritization).  These events are not directly observable and are difficult if not impossible to infer from the overt actions that are collected.  An option is to pause a HITL trial and question a pilot regarding factors such as situation awareness, decision making, workload, current goals, etc. However, this can disrupt performance.  A conscious decision was made in the case study to not disrupt the pilot and live with the loss of data.  The assumption was that agreement in performance at the general function level would be sufficient because this is the level at which most decision-makers would be most concerned.The differences between the performance of the HPM and the actual pilots that were observed raise some other interesting issues regarding model validity.  At first blush, it might seem that HPM was deficient because it did not predict situations in which operators would not use the auto-router.  Indeed, using data from the HITL runs, it would be possible to modify the HPM so it behaved more consistently with actual pilots.  But, is this a more valid model?  It would seem that the goal of human performance modeling within the acquisition process is to predict the performance of well-trained operators, consistently applying well defined employment doctrine and tactics.  For stealthy aircraft such as JSF, technology such as the auto-router can be crucial for creating a survivable system.  When testing the system, the auto-router needs to be applied consistently so that its effectiveness can be understood clearly.  A real-world problem, however, is that technologies that get tested early in acquisition, are not always mature.  They have flaws and quirks like the auto-router used in the case study.  Real operators are likely to respond to these flaws by using the technology inconsistently and, in the process, produce data that cloud the evaluation of that technology.  On the other hand, human performance models can be developed that are completely consistent in their use of system capabilities and that produce data that support a cleaner evaluation of those technologies.  It should be pointed out that in the case study, the human performance model demonstrated better overall survival rates and survival related performance (e.g., fewer launches) than the actual operators.  These results suggest that even the flawed auto-router produced superior survival than that of actual pilots.  It would seem reasonable to conclude that the auto-router technology has merit and is worth pursuing as part of the acquisition program.4.2 Additional Benefits Gained from Human Performance ModelingBeyond providing a realistic representation of human performance, experience from the case study suggests that the model development process as well as the model ultimately developed can provide the human system team and the broader system engineering team with other benefits as well.  One of these benefits is insight into effective tactics for system employment.  Even though it was developed based on input from subject matter experts, the initial implementation of the strike fighter pilot model proved to be fairly ineffective at finding the SCUD missile target.  The problem was that initial efforts to detect the target used SAR patches whose resolution was too gross to yield a visually detectable return.  The model development team took a step back and re-thought the entire acquisition process.  An integrated target acquisition strategy was developed in which the GMTI and medium and low-resolution SAR images were used to for initial target detection at distances beyond effective TIR range.  During this phase a list of potential targets was developed.  When the aircraft was within range, the TIR was applied to examine the potential targets and find and identify the SCUD.  As evidenced by the performance of the model, these tactics for target acquisition proved to be very effective.  Their effectiveness was further validated when pilots in the HITL trials were taught the same tactics and, subsequently, exhibited target acquisition performance close to that of the model.  The conclusion of the model development team was that human performance modeling could provide a useful context for developing tactics that most effectively employ a new system or technology.Another benefit of human performance modeling is that it provides model developers and users with an intimate understanding of the performance required of the operator.  This understanding can lead to insights such as more effective function allocation between operators or opportunities for effectively applying automation or job aiding to support the operator.  In developing and using the JSF pilot model, for example, it quickly became clear that the most difficult, task-intensive portion of the job was target acquisition.  Within target acquisition much time was spent performing the switch and control manipulation activities required to operate the sensors.  The team realized quickly that employment of the TIR especially was driven by the shoot-list and that this manipulation was repetitious and could be automated easily.  The basic concept of this automation was that:As the pilot built the shoot-list, an intelligent agent would observe the location of the airplane and the location of points on the ground of interest and compute range to those points.When the aircraft was in TIR range of a point the agent would extend the TIR, command it to look at the point of interest and generate and save a medium and high-resolution image of the point.The pilot would enter an exploitation mode on an MPD and page through the resulting imagery looking for the target and once found designate it for attack.  It is expected that this capability would increase significantly the number of potential targets that could be examined in a pass through the target area because much of the sensor manipulation activity has been off-loaded from the pilot.  Also, it would be fairly easy to build a working prototype of the capability by reusing a portion of the JSF pilot model acquisition task network.  Indeed, an important insight gained in the first case study is that a human performance model can become the basis for demonstrating an automated or aiding capability.5.0 Implications for How M&S is Applied 5.1 The Challenge of Obtaining Constructive System Representations.A key concept in CART is that integrating human performance models with constructive system and mission environments provides an opportunity to assess how operator performance can impact broader system and mission performance. Consequently, a critical ingredient for CART success is availability of constructive system models that are sensitive to variation in the performance of a CART operator model.  Our experience in the Case Study suggests that such constructive system models can be difficult to obtain and that CART and other HBR modelers might have to seek alternatives. Early in the case study, existing, available constructive models were reviewed in an effort to identify any that might be able to interact with a strike fighter pilot model.  This review determined that mission level models such as Suppressor offer the ability to model a complete range of mission functions.  However, the level at which actions and events within those functions are modeled is not sufficiently detailed.  SAM engagements and associated outcomes, for example, are modeled using probability tables.  The ability to have the aircraft interact dynamically with the threat (e.g., maneuver the aircraft, apply countermeasures) does not exist.  Thus, it would not be possible to have an operator model control a Suppressor model and play out the effects of operator decision making regarding threat evasion.  On the other hand, engagement level models such as RADGUNS and ESAMS play out the aircraft engagement by missiles and guns in great detail.  But, that is all they do.  They do not address target acquisition, attack or other important functions.  Their scope is too narrow for the CART JSF system-modeling requirement.  At the end of the review, the team concluded there were no suitable constructive system models among the existing constructive model set and an alternative approach was required.The alternative approach selected was to convert the virtual Mission Interactive Combat Station from the VSWE into a constructive simulation.  The result was a high fidelity JSF constructive representation that was sensitive to operator model performance.  Reuse of the MICS provided tremendous cost savings over developing a JSF model from scratch.  Also, because detailed visuals and operator station graphics were not required by the operator model, the constructive MICS could be ran on lower cost computing platforms.  The original virtual MICS ran on a fourteen-processor Silicon Graphics, Inc. Onyx.  The re-hosted constructive MICS runs on a two-processor Silicon Graphics Octane. There is a significant difference in cost between these two platforms.  The researchers expect that virtual simulations will provide an important source of system representation for other HBR modelers.  5.2 A New Paradigm for Linking Traditional Constructive and Virtual Simulation.  Traditional approaches to using modeling and simulation in system acquisition involve a mix of constructive and virtual simulation.  Initially, a broad range of alternative system concepts are defined and evaluated using constructive simulation.  Alternatives that generate desired levels of performance are selected for more detailed evaluation using virtual simulation.  Virtual simulations are developed which represent key capabilities of the alternatives, test plans are developed for conducting an evaluation, operators are obtained, and testing is conducted.  The alternative(s) judged to be the most cost effective following virtual testing are then pushed forward into prototyping and engineering development.  While the above process offers significant improvement in the acquisition process, it still has some flaws.  The constructive models used to screen system alternatives early in the process represent humans in a very limited fashion.  It is difficult if not impossible to systematically manipulate factors of interest to human system designers.  One cannot, for example, represent alternative function allocations between operators and/or machine to determine the optimal allocation scheme for a given system design.  Consequently, current constructive modeling is of little use for resolving crew system design issues.  Virtual simulation provides the obvious advantage of allowing potential operators to interact with a system concept.  Indeed, this is very important because the insights and information gained can be extremely valuable for crew system design.  Virtual simulation, however, has its limits too.  Because of the time and expense required to develop and modify virtual simulations, it often is not possible to implement the full range and combination of capabilities that might be of interest in a program.  Consequently, a “partial testing matrix” is implemented and only a subset of all possible alternatives of interest is marked for testing.  The challenge here is to “guess right” on factors such as what levels of performance of a capability should be tested, what combinations of capabilities are more important, etc.  Another limitation of virtual simulation is the human operators who participate in testing.  Testing often occurs over a short period (e.g., days, weeks).  For complex systems, this usually is not enough time for operators to become proficient in system employment.  It is difficult to use data from these operators to predict levels of mission performance that can be achieved by highly trained proficient operators.  For new systems, a concept of employment might not exist.  In this case, some portion of the test event might be devoted to letting operators “play” with the system in order to test different tactics and operational concepts and identify those with merit.  While this provides valuable insight into effective system employment, it also reduces the time available for testing system alternatives.  A secondary effect is that operators evolve their own tactics and procedures.  Some operators will develop more effective tactics than others.  This can lead to significant variability in performance across operators, which can, in turn, cloud the assessment and comparison of system alternatives.  Finally, it can be difficult to get operators to cooperate with tactics or employment procedures in some cases.  A good example of this is the auto-router used in the case study.  The employment concept for the simulated JSF was that pilots would always use the auto-router to control the flight of the aircraft except when under engagement by a SAM.  In actual use, pilots would sometimes ignore this directive and elect to fly the aircraft themselves.  This was driven, in part, by the fact that the auto-router was a beta version that sometimes produced a route that was counterintuitive.  Nevertheless, pilot behavior was contrary to the procedure they were given.  If the focus of the study had been to evaluate the auto-router, the pilot behavior would have made it difficult to obtain a clean assessment.The testbed developed for the case study provides an interesting mix of capabilities that offer the flexibility and economy of constructive simulation and fidelity generally associated with virtual simulation.  The result is a simulation environment that solves the problems described above and, we believe, offers a new paradigm for integrating constructive and virtual testing.  First, a CART human performance model linked to what used to be a virtual simulator provides a rich constructive environment for exploring operator issues associated with system alternatives and concepts.  This provides an acquisition program simulation team with an opportunity to resolve these issues before proceeding to virtual simulation.  Also, because of its high fidelity system representation, a CART testbed provides an opportunity to screen the system alternatives identified through traditional constructive simulation, subjecting them to more intense scrutiny than ordinarily possible with constructive simulation.  Problems with an alternative can be identified before taking on the time and expense of implementing it in virtual simulation.  Indeed, a CART testbed can be used to conduct testing on a complete test matrix prior to conducting virtual simulation.  A partial test matrix can then be applied in virtual testing in an effort to confirm results of constructive testing.  Because the HPM can be driven by data produced by the models that underlie the system simulation and does not need detailed user interfaces, the cost of modifying the constructive system simulation to represent different system alternatives can be much less than modifying a virtual simulator.  This permits implementation and testing of a greater range of alternatives.  A CART testbed also can be used to resolve tactics, concepts of operations, procedures and other system employment-related issues.  This was demonstrated in the earlier discussion of how CART was used to derive more effective tactics for sensor employment in JSF.  During virtual testing, these tactics can be taught to participating operators.  This makes their performance more uniform and proficient.  It also reduces the amount of time required for operators to train and “play” with the system, leaving more time for testing. Finally, a CART HPM can be developed that faithfully applies tactics and follows employment concepts providing a clear assessment of the technology or capability of interest.Not only is CART a feed-forward capability that bridges the gap between traditional constructive and virtual simulation, it can become a feed-back mechanism that exploits data collected in virtual simulation to improve the CART HPM and, in turn, feed data back to the traditional constructive simulations.  Once the virtual simulation runs are complete, the data generated by the operators and the information gathered in post-mission debriefs can be used as appropriate to refine performance of the CART models to better reflect real operator performance.  Also, data generated by a CART testbed can be used to update traditional constructive simulations.  For example, a CART testbed can be used to fly vectors past different SAM sites that vary parameters such as speed, altitude, and bearing relative to the site and then implement evasive maneuver when the SAM launches.  Outcomes of the engagements can be recorded and the data can be used to update SAM probability of kill tables in models such as Suppressor so the data more accurately represent  effects of a pilot on SAM survival.  5.3 Assessment of the CART HPM Tool and ArchitectureIn general, the case study team was pleased with the IMPRINT-based task network-modeling tool.  The graphical user interface made it easy to specify tasks and develop networks.  Also, the graphically based tool for mapping external variables to SIMAN interactions proved easy to use and, ultimately, will provide a significant savings to CART users because they will not have to re-write the CART middleware each time a new HPM is built.  Perhaps the most powerful feature of the tool was the goal state capability that was added as part of the CART program.  As expected, the goal states yielded a HPM that responded dynamically to changes in the mission environment.  It was interesting to observe how variations on a core scenario were able to generate significantly different model performance in terms of the number of times goals fired and the duration that goals were active.  This confirms the expectation that an advantage of being able to connect human performance models to system and mission environment models is that it provides the human factors analyst and, indeed, the entire system development team with tremendous insight into how the mission environment and system design drive performance of the operator. While the CART tool had many positive aspects, there were some limitations.  One of these was the programming language inside the model development environment.  One of the realizations gained in the case study was that CART models require much more programming than traditional IMPRINT models.  Some of this is driven by the interface with the constructive system representation.  A large number of variables that receive data (information) from the constructive system simulation and pass actions back to the simulation must be defined and managed within a CART HPM.  Equally important is the need to represent cognitive, perceptual and other processes that underlie task performance.  In the JSF pilot model, for example, extensive code was written that represented the operator’s perception of targets on the sensor displays and determined when target detection and identification could occur.  The language does not support complex conditional statements (e.g., If-Then-Else).  Also, it does not support nesting of conditional statements.  In order to create nested conditionals, individual conditional instruction  Segments are embedded in macros and one macro calls another which calls another, etc.  It is an awkward arrangement that can make debugging a challenge.  A powerful addition to future versions of the CART HPM development environment would be a more extensive, robust programming environment with good debugging tools.A limitation of the overall CART testbed was speed of execution.  The integrated CART-FRED-JIMM simulation ran three to four times slower than real-time.  Since completing the case study, the Defense Modeling and Simulation Office (DMSO) has provided funding to explore ways to make the simulation run faster. The CART development team has optimized elements of the CART runtime environment and has been able to increase performance to about 1.8 times real-time.  At this point the obstacle is the FRED-JIMM simulation which runs at 1.3 times real-time.  The remainder of the delay is driven by overhead associated the HLA RTI using time management services and running regulated and constrained.  The lesson learned here is that if real-time performance is desired in a CART simulation, all federates must be capable of achieving better than real-time performance to off-set delays imposed by the RTI.  For CART HPM themselves, this does not appear to be a problem.  These models tend to run many times faster than real-time. Beyond the tool itself, perhaps the most important insight gained is the need for efficient and effective data collection, management, and analysis tools.  The CART concept for data analysis is to develop a hierarchy of performance measures and data that can be used to trace and evaluate how low-level operator performance impacts high level functions and objectives.  While the data and measures for the case study could not be discussed in detail, it is the opinion of the CART team that the performance-measure hierarchy does provide an effective means for explaining operator effects.  The challenge is the level of effort it takes to generate the measures. The testbed developed for the case study generates massive amounts of data from the JIMM mission environment, the MICS system representation, and the CART HPM.  Specialized data reduction software had to be developed to assimilate these data into a database that could be manipulated readily.  Additional software was developed to generate summary performance measures and statistics.  Even more challenging is need to integrate the data sets so an analyst can move easily up and down the hierarchy exploring the data, developing an understanding of how lower level performance drives higher level outcomes, and developing an explanation of the results.  It is the explanation of results that that can be particularly challenging.  This process extends beyond the manipulation of data output by the simulation.  It requires that detailed knowledge of the mission environment and mission scenario, characteristics of the system being tested, and the operation of the human performance model be available during data analysis and that it is possible to access portions of these data be able to answer questions as they arise.  Within the Case Study, the activity described above was accomplished through custom developed software or by more manual processing using simple tools such as spreadsheets or documents.  The labor intensive nature of this method exhausted resources programmed for the data analysis rather quickly limiting the range of issues and questions that could be explored.  The lesson learned is that CART testbeds generate a tremendous amount of data and information about a mission environment but extracting that information and exploiting it to the maximum extent possible can be difficult.  Attention needs to be directed to developing an integrated set of tools that make the data reduction and analysis process more efficient.6.0 References[1]  Hoagland, D. G., E. A. Martin, & B. E. Brett,  “The Combat Automation Requirements Testbed (CART) Program:  Improving the DoD's Requirements Process Through Inclusion of Realistic Operator Performance,” Simulation Interoperability Workshop Paper Number 01S-SIW-137, Spring 2001.[2]  Hoagland, D. G., E. A. Martin, B. E. Brett, J. A. Doyal, N. D. LaVine, & R. A. Sargent,  “The Combat Automation Requirements Testbed (CART) Program: Results and Lessons Learned from Recent Testing of Advanced Human Performance Models Interacting with DoD Constructive Simulation,” Simulation Interoperability Workshop Paper Number 00F-SIW-023, Fall 2000.[3] Rasmussen, J., A.M. Pejtersen, & L.P. Goodstein,  “Cognitive Systems Engineering,” John Wiley & Sons, New York, 1994.[4]  Martin, E. A., B. E. Brett, & D. G. Hoagland, “Tools for Including Realistic Representations of Operator Performance in DOD Constructive Simulations,” Proceedings, AIAA Modeling and Simulation Technologies Conference and Exhibit, Portland OR, August 9-11, 1999.[5] Brett, B.E., Doyal J.A., Malek D.A., Anesgart M (2001) “The Combat Automation Requirements Testbed (CART) Case Study 1 Results,” US Air Force Research Laboratory Technical Report AFRL-HE-TR