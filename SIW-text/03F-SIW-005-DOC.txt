The Many Lessons from the RDE Command 1st Application - But did we learn anything?Ralph J. Weber IIIComputer Sciences Corporation4090 S. Memorial ParkwayM/C 4-2-06Huntsville, AL 35802(256) 885-7133 HYPERLINK "mailto:mporter@csc.com" rweber@csc.comGregory B. TackettKathryn RooseUS Army Aviation and MissileResearch, Development, and Engineering Center (AMRDEC)AMSAM-RD-SS-AERedstone Arsenal, AL  35898256-842-9501, 256-955-8407 HYPERLINK "mailto:greg.tackett@rdec.redstone.army.mil" greg.tackett@rdec.redstone.army.mil HYPERLINK "mailto:kathryn.roose@rdec.redstone.army.mil" kathryn.roose@rdec.redstone.army.milKarin LarsenScience Applications International CorporationApplied Technology Group4901 D Corporate Dr.Huntsville, AL 35805(256) 313-1845karin.larsen@rdec.redstone.army.milKeywords:M&S, FCS, VDLMS, MATREX, Objective Force, Virtual, Lessons LearnedABSTRACT: During April 2003, the Program Manager, Non-Line-of-Sight Launch System Command, Control and Communications (NLOS LS C3) collaborated with the provisional US Army Research, Development and Engineering Command (RDECOM) to conduct the First Application (1stApp). 1stApp was a distributed experiment using a mixed, HLA and DIS architecture, to successfully network four sites including more than 70 workstations, and 200 personnel using virtual and constructive simulations. During two weeks of runs, 1stApp was able to produce 18 successful runs out of 21 attempts. Virtually all the participating simulations were legacy simulations developed for different reasons. With only three weeks of integration permitted by the very aggressive schedule, clear and complete Interface Control Documents (ICD) were essential to providing each simulation owner with the tools for understanding the interactions and communications requirements of 1stApp. This paper will address the lessons of 1stApp. We do not claim “lessons learned” because no lesson is learned until it is internalized and changes to processes inculcated into normal operations. This paper will discuss the experiment and network architectures, functions, experiment purposes and processes, and some unclassified results. IntroductionThe Army modeling and simulation (M&S) vision is to design and build a simulation architecture and reference implementation using standardized components that represent key characteristics of network-centric warfighting systems.  This architecture will support Army Transformation, including the evaluation of Future Concept System (FCS) and Objective Force (OF) concepts and the performance of engineering trade studies.There were 2 major goals of the 1stApp.  The first was to put together an architecture to support analysis objectives providing insights into Networked Fires.  The second was to define a baseline architecture to move forward with a persistent distributed design and evaluation environment for bridging towards the VDLMS architecture.The 1stApp leveraged previous RDEC Federation experimentation such as the Calibration Experiment (CalEx), and ongoing activities in support of individual FCS-related demonstrations, especially the C3-On-The-Move Demo, the LSI Unmanned Combat Demo, and the Non Line Of Sight – Launch System Command, Control and Communications (NLOS-LS C3) Full System Simulation (FSS-1).  It incorporated JVB lessons learned and architectural considerations from the C4ISR Demo and the LSI Capstone Demo.  It used Depth and Simultaneous Attack Battle Lab (DSABL) FireSim networked fires representations.   1stApp was a geographically distributed experiment, designed as a superset of the NLOS-LS C3 FSS-1 experiment.  The NLOS-LS event, as a Distributed Interactive Simulation (DIS) event, was bridged into a High-Level Architecture environment to interact with legacy Virtual Distributed Laboratory for Modeling and Simulation (VDLMS) simulations, servers, and simulators to augment the NLOS-LS-focused experiment with additional sensor systems, more explicit C3 simulation, and additional NLOS fires.1stApp was designed to use established and emerging models and simulations at key RDECOM M&S facilities, linked with representative test and user communities.  Key analysis agencies also participated in experimental design and execution.Geographic distribution of the event was accomplished by linking four simulation sites with one wide area network monitoring and collaboration server site, and physically bringing resources from the other VDLMS organizations to the four simulation sites, as shown in Figure 1.1, where green circles indicate distributed sites, tan circles indicate organizations not at own sites, and blue circles indicate personnel support only. Figure 1.1 – 1stApp Distributed Network2. Architecture1stApp architectural design was based upon the requirement to use existing RDECOM facilities and simulations. This requirement caused us to use a physical backbone and simulation infrastructure derived from those currently in use to support RDECOM customer experiments.  A High Level Architecture (HLA) simulation backbone was laid across the physical connectivity to provide simulation interactions.  Sub-networks of Distributed Interactive Simulation (DIS) traffic were bridged into the HLA architecture at three sites, then the appropriate HLA federates and DIS simulations were connected to provide the overall digital architecture.In addition to the digital simulation network, 1stApp also utilized digital collaborative environment tools and analog commercial voice conference calls to support experiment control and tactical voice communications.  The entire architecture design is shown in Figure 2.1.Wide Area Network (WAN) Architecture  A robust network had to be designed that would meet the stringent requirements necessary to support the 1stApp.  The network was implemented over the Defense Research and Engineering Network (DREN).  DIS simulation traffic was hosted on the Local Area Networks (LAN) and HLA simulation traffic was supported across the Wide Area Network (WAN).  Network performance to include latency, bandwidth and multicast issues and tools used were noted.  Also, observations pertaining to the simulation network as an integration of the LAN, Metropolitan Area Network (MAN), and WAN were used to generate recommendations for future network infrastructure modifications.The 1stApp network functioned as a “flat” network in that it did not use any sophisticated routing.  This permitted each simulation federate to perceive the network, whether local area or wide area, as a LAN.  It also simplified integration of the encryption devices necessary to accommodate the Secret classification of the exercise.  The distributed simulation sites mentioned above are all connected to the DREN, and have historically used the DREN to connect their real-time simulation capabilities, particularly in support of the RDEC Federation Calibration Experiment (CalEx) [1] [2].  A detailed discussion of the 1stApp network design and performance is given in reference [3]. SHAPE  \* MERGEFORMAT Figure 2.1  1stApp Federation DiagramSimulation Architecture  Most of the existing RDECOM simulations were developed using DIS protocols for interconnectivity, and have demonstrated compliance with HLA using the Real-time Platform Reference (RPR) Federation Object Model (FOM) and DIS/HLA gateways.  During the design of 1stApp, it proved possible to configure all data exchanges into DIS compliant Protocol Data Units (PDUs) and corresponding RPR FOM data without the need for extensions.  This allowed use of commercial DIS/HLA gateways without customization.  In addition to the gateway federates, there were several native HLA federates, also indicated in Figure 2.1.  In order to control and limit WAN traffic and avoid feedback loops, only HLA traffic was allowed to pass across the WAN.  Consequently, any DIS to DIS traffic between sites had to pass through two gateways to encode and decode data packets, which impacted performance in terms of data latency.  This led to the addition of a small red cell at Ft. Monmouth to allow for local blue/red force interactions.HLA FederationWith the core of the 1st App Federation being the FSS2 DIS-based applications, we chose to leverage DIS-HLA gateways to bring in remote federates. The 1stApp architecture called for only HLA traffic across the wide area network, so the gateways were performing true routing between the WAN and Redstone Arsenal LAN. Since DIS-based messages and the gateway applications were critical to the architecture, it was natural to derive the 1stApp FOM from the Real Time Platform Reference (RPR) FOM. The RPR FOM (Version 1.0, draft 2) was pared down to the essential object and interaction classes required for the exercise, then subsequently augmented with several object and interaction classes that were specific to 1st App’s native HLA federates. These 1stApp-specific addendums were not required to be translated by the gateways. An even smaller subset of the interaction classes were retained from the RPR FOM. Most of the interesting DIS interactions were constructed in the variable datum PDUs of Action Request and Event Report.  The Routing Space table was left empty for 1stApp. All distributed sites were part of a single multicast group, since sufficient bandwidth was available for the scope of the exercise. Data CollectionA team was dedicated to collecting data for both quick-look analysis and post experiment analysis.  The team was responsible for collecting data on both the DIS and the HLA side of the 1st App network. The following is a brief summary of the tools used for data collection and their assigned role.The Data Collection and Analysis Tool (DCAT) is a tool which was developed by AMRDEC for collecting and analyzing data during experiments.  It has the capability to collect in both DIS and HLA mode.  During the 1st App exercise, DCAT collected data on the DIS side of the architecture to give a quick look analysis of data.  DCAT is based on the SQL Server database.  DCAT also had charting capabilities built into the tool for visualizing certain MOE data quickly.  For 1stApp, DCAT was mainly used to debug the architecture by being able to quickly gather and analyze data related to site, host and entity definitions on the DIS side of the network.  The team used DCAT to gather this information and then query the team until all unknown entities were resolved.The COTS hlaResults data collection tool was used as the main collection and analysis tool for 1st App.  The team used the DIS mode of this tool to collect the DIS simulation data and store it in an Oracle relational database.  Once stored in the database, the team wrote custom queries to analyze the data to answer specific analysis questions.  This data was then displayed using the charting capabilities of Crystal Reports.  These quick look reports were provided to the test director and the Army Material Systems Analysis Agency (AMSAA) representative to ensure that the proper data was being collected and that errors did not exist in the simulations which would prevent post experiment analysis. The team also attempted to collect data using this same tool on the HLA side of the network simply to prove out the tools usefulness as an HLA collector.  A small amount of data was collected on the HLA side but not analyzed.The MaK Data Logger was used to collect data from every run.  This logger data was used during the exercise between runs to playback this data for debugging purposes.  It was also used to playback data into an hlaResults database when needed.  All the logger files were also available post exercise to further analyze the record runs.Many of the simulations which played in 1st App were collecting and writing out there own log files.  Figure 2.2 is a summary of some of the data which was collected.Application LogsApplication Log NameSize(MB)NMP1 - NMP610CommServer-data.tar.bz2233CommServer-anomalies.txt0.2FireSimA492FireSimB374IDEEAS Missile Server Logs446Intelligent Ex-Monitor4SEAMS886Figure 2.2 Additional Data Sources System Simulations1stApp was scoped to represent the quantities and types of systems composing an FCS Unit of Action (UA), based on the UA Systems Book, using the then-current version 1.6 document. The primary scenario engine for 1stApp, providing platform-level performance for the bulk of the red and blue forces, was the AMRDEC-developed Interactive Distributed Engineering Evaluation and Analysis Simulation (IDEEAS).  IDEEAS is a constructive simulation which is synchronized with real-time to support virtual experimentation, with a low level of runtime user interactions.  All entities in 1stApp were generated by IDEEAS, except as identified with the other models described below.FireSim from DSABL was used to represent all red and blue indirect fire platforms, with the exception of about half of the Non-Line of Sight Launch System (NLOS LS) platforms which are otherwise represented by the NLOS LS Mission Management Applications (MMAs).  FireSim was also used to represent all counter-battery radar systems.OneSAF Test Bed (OTB) represented a small number of air sensor platforms used to populate fused sensor data.  OTB also represented the local red ground force at the Communications and Electronic Research Development and Engineering Center (CERDEC) for interaction with Reconnaissance, Surveillance and Target Acquisition (RSTA) simulations.When launch platforms fired missiles and armaments, the flights, interactions, and detonations of those entities were generated by the Missile Server from Aviation and Missile RDEC (AMRDEC) and the Armament Servers from the Armament RDEC (ARDEC). The Tank and Automotive RDEC (TARDEC) manned simulator and Human Performance Model (HPM) represented two RSTA vehicles, which controlled unmanned reconnaissance vehicles.  The CERDEC manned simulator suite represented a single RSTA vehicle, which also controlled a Class I Unmanned Aerial Vehicle (UAV), a Small Unmanned Ground Vehicle (SUGV), an Unattended Ground Sensor suite, and an Intelligent Munition System.  There were also several desktop simulators of Class I-IV UAVs, and dismounted forward observers at AMRDEC.C3 SimulationsCommand, Control, and Communications (C3) functionality was focused on those functions critical to Networked Fires, which included target reporting, sensor fusion, situational awareness, weapon-target pairing, calls for fire, fire missions, and Battle Damage Assessment (BDA).  Communications representations were limited to two key areas:  communications between ground sensor suites and the RSTA vehicle, which were simulated by the CERDEC Aggregate-Level Communications Environment Server (ALCES) model, and communications between NLOS LS missiles and their control stations and relays, which were simulated by the AMRDEC Communications Server.  Many of the existing tools from the RDECOM organizations had overlapping functionality in Command and Control (C2), so the challenge was to scope meaningful roles for these simulations so that fire missions could be executed at various levels of command.  Collectively, these products provided the functionality of a Network-Centric system without explicitly modeling the individual nodes and interfaces of that system [4].3. Force StructureThe force structure represented in 1stApp was the Future Combat System (FCS) Unit of Action (UA) structure as approved by the US Army Training and Doctrine Command (TRADOC) Commanding General, 11 March 2003.  To minimize the impact of a large number of entities, selected units, not critical to this experiment, were played notionally.  Some units from the Unit of Employment (UE) were added to the 1stApp vignettes to provide a more robust NLOS fires capability.The equipment available to the UE/UA was restricted to only the required pieces to explore the objectives properly – e.g., joint assets, engineering assets were not played.  With the exception of two High Mobility ARtillery Rocket System (HIMARS) batteries, two NLOS batteries from adjacent UA, and two radars, notional units – those units belonging to the UA other than the 1st Combined Arms Battalion (1st CAB) and NLOS Bn - were not played.3.1 Scenario VignettesBoth 1stApp scenario vignettes were based on the TRADOC Caspian 2.0 scenario.  Both vignettes were the result of the collaborative efforts of participants from all systems played in 1stApp with Subject Matter Experts from the Unit of Action Mounted Battle Lab (UAMBL) at Ft. Knox, KY; the TRADOC Analysis Center (TRAC) at White Sands Missile Range (WSMR), NM; the Depth and Simultaneous Attack Battle Lab (DSABL) at Ft. Sill, OK and TRAC at Ft. Leavenworth, KS.3.1.1 Vignette DescriptionThe 1stApp scenario was developed to support answering the key issues for NLOS-LS-C3 STO.  To accomplish this, a UA slice was set in an FCS context utilizing complex terrain as a basis for the experiment.  The scenario used was Brigade and Below Scenario Caspian Sea 2.0 (Spring; Day; Good Visibility) modified from exercise Sabertooth as run in February 2003 at TRAC WSMR.To provide the necessary elements for analysis, threat forces were organized and deployed in accordance with standard US Army opposing forces doctrine with one exception.  They retained an aggressive counterattack capability.  Threat forces employed a non-linear, decentralized defense that provided the opportunity to inflict maximum casualties and retain combat power for future operations.  The tactic of hiding from detection and drawing Blue forces into the close fight to negate Blue’s long range firepower remained a constant throughout.   In order to set the proper conditions to investigate network load requirements, a very high ratio of red forces were used and typical threat tactics were deviated from to provide the necessary type and number of target sets required.  Blue forces were employed in offensive operations with only one maneuver battalion scripted for play.  With IDEEAS as a baseline driver for simulation, tactical concepts from the O&O were reflected whenever possible.  The Blue force mission was to attack to clear a route through the area of operation in order to facilitate a forward passage of lines with a follow-on brigade.  Army Material Systems Analysis Agency (AMSAA) FCS Systems Book version 1.6 data was used for the foundation for all system physical and functional descriptions.  AMSAA performance data was used for stochastic accuracy and lethality.3.1.2 Vignette A Vignette A began with a 90 minute Battlespace shaping phase, during which the Brigade, NLOS Bn, and 1st CAB concentrate on setting conditions for tactical success.  Comanches and UAVs were employed per the Intelligence, Surveillance and Reconnaissance (ISR) plan to find and affect enemy High Payoff Targets (HPTs).  Recon units then advanced to prevent tactical surprise and to confirm the location and condition of enemy along the route.  The 1st CAB, using multiple, dispersed routes along its designated axis of advance, rapidly moved to positions of advantage near two objectives.  Once these objectives were secured, 1st CAB units continued east to secure points along a canal to enable movement toward the final objective.  Vignette A lasted approximately 3.5 hours.3.1.3 Vignette BVignette B was a small scenario developed to enable less robust simulations to participate in the exercise.  At the beginning of each iteration, 1st CAB coordinated fires to isolate the two intermediate objectives.  UAVs continued to search for HPTs and prevent enemy interdiction.  The two MCS companies were located in positions of advantage and provide BLOS fires.  Recon units conducted aerial and ground RSTA in the area immediately in and around the objectives.  Simultaneously, route reconnaissance was conducted on two additional intermediate objectives.  This vignette lasted approximately one hour.4. LessonsThe authors have identified two types of lessons from 1stApp.  First are lessons which are specific to this experiment.  We expected to and achieved some notable results which will assist architecture builders to refine their product.  But we also gleaned some lessons which may be applicable to the larger simulation community.  These lessons can serve to illuminate the needs of the RDE community and the structure of standards required to serve that community’s interests.4.1 1stApp Specific LessonsVDLMS baseline metrics were identified for cost, schedule, and performance.  Cost is not addressed in this paper.The 1stApp completion date was fixed due to availability of DSABL personnel, and the requirement to conclude prior to the FCS Milestone B decision.  The schedule included two weeks of record runs and one week of VIP briefings, as is shown in Figure 4.1.  Given the fixed schedule endpoint, the length of time required to gain approval for classified operation, and the late arrival of funds, the distributed site integration time was delayed by one month from the original plan.Figure 4.1  1stApp ScheduleThe VDLMS performance results from 1stApp are based upon the size of the scenario, the complexity of the federation, the number of successful record runs, and statistics regarding network and HLA performance.The maximum size of the FCS UA slice represented within stable simulation runtime was for Vignette A.  Vignette A runs executed over the course of approximately 3 hours, and peaked at roughly 2500 entities, including munitions in flight.  Each of these runs produced two million PDUs, logger files of 450MB, and 1.3GB HLAresults database files.  Vignette B was a 1 hour slice of Vignette A, scaled down to about 500 entities.  These runs each produced 300,000 PDUs, 55MB logger files, and 500MB HLA Results databases.The eighteen record runs were out of a total of twenty-one attempts.  For each record run, outstanding simulation problem reports were documented to ensure that the proper analytical insights could be drawn from each run.  Since final integration completion overlapped the first few record runs, the later runs tended to be more robust than the earlier ones in terms of analytical validity.  Along with the free play of the 1stApp role-players, this gave each run a unique personality which lent its utility towards addressing the various networked fires metrics.  The total stable runtime to execute these eighteen record runs was approximately 40 hours.Network latencies for the 1stApp WAN ranged from 2ms to 36ms based on ping tests between sites across the DREN.  Throughput losses ranged from 0% with 18Mb connections to 0.29% with 4Mb connections, measured from each site, with AMRDEC as the server.  Bandwidth usage, measured at AMRDEC, was as shown in Table 4.2.Vignette AAverageMaximumDIS0.37Mb/s9.67Mb/sHLA0.13Mb/s2.67Mb/sVignette BAverageMaximumDIS0.00Mb/s2.84Mb/sHLA0.02Mb/s1.43Mb/sTable 4.2   Bandwidth UtilizationPerformance for the simulation layer proved to be very stable and robust.  The MaK RTI and Gateway demonstrated high reliability and stability.  Joining and resigning the federation took under two minutes as a rule.  The HLAcontrol tool allowed the federation to monitor object updates to assist in federation management.The MaK RTI Executive GUI did cause a problem with joining and rejoining, requiring federates which dropped out of the simulation to change their names before rejoining.  This bug was avoided through the use of the RTI Exec from the command line interface.  The RTI Spy was also disabled for similar reasons.It was the team’s first experience with using hlaResults for collecting data. The team had limited experience with the tool before 1stApp.  One of the main issues which arose was the time it took to process the data into the Oracle database.  Originally the team had 2 data collection machines.  One machine was dedicated to collecting the data and one machine to do quick look analysis on the data between runs.  It became apparent during the first week of record runs that this was not going to be sufficient.  For the large scenario runs, it took approximately 8 hours to completely input the data into the Oracle database when parsing the variable datum records which existed in Action Request PDUs and Event Report PDUs.  It should be noted that when running without the 1st App-specific parsing code for the variable datum, hlaResults was able to load the Oracle database in near real-time. However, this simply moved the parsing problem to a post-execution activity. When running 2 large scenario runs per day, a separate machine was needed for morning and afternoon runs.  A third machine was then needed for analyzing the data.  The team was able to get these resources but struggled to catch up with the data collection. During the initial integration testing of 1stApp, only a limited subset of the FOM information was set to reliable transport. These include: all attributes and interactions of the Management Object Model (MOM); the 1stApp interactions (i.e. ActionRequest, EventReport, MunitionDetonation, WeaponFire, StartResume, StopFreeze and interactions of the Lethality Services); and the object attributes of the Designator class (and its parent, EmbeddedSystem). The latter was set to reliable due to the critical nature of target designation for the exercise.As integration testing proceeded, we discovered that during heavy loads both positional and damage state information would get out of synchronization between the distributed sites. This was due to the nature of HLA, and how object attribute information is only transmitted when it changes. Whenever a gateway missed a change in damage status or velocity vector, it would continue to perform heartbeat operations on the DIS side of the gateway with stale information. The dead-reckoning algorithms used on the DIS side caused the scenario information to quickly diverge from one site to another. This was rectified by setting certain key attributes of BaseEntity and PhysicalEntity to reliable transport, including: EntityType, EntityIdentifier, WorldLocation and VelocityVector (for BaseEntity) and DamageState, EngineSmokeOn, FirePowerDisabled, FlamesPresent, Immobilize, and SmokePlumePresent (for PhysicalEntity).  4.2 1stApp Lessons for the RDE CommunityMost distributed RDE events, including 1stApp, have a set of common characteristics.  They are under-funded, schedule intensive aggregations of predominantly legacy simulations derived from many sources.  RDE exercises are exciting, stimulating, collections of “ad-hocery” with high expectations as to their outcomes.1stApp was successful in spite of those conditions because:The Test Director was able to avoid “mission creep,”The initial architecture decisions adopted a tiered structure to plans for on failures and provided for graceful degradation and contingencies,The parameters normally maintained for DIS and RPR FOM simulations were modified to limit the impact of C3 and entity data,All simulations were monitored, during execution, for unreasonable system loading and modified between runs,The test team was able to climb a steep learning curve to work with a reasonably robust tool set.1stApp was intentionally sized to fit the available infrastructure.  Six months prior to startex the test team decided that about 3000 entities was as many as could be accommodated; while leaving bandwidth for the C3 messaging which was the point of the exercise.  The test director was able to insist that scenario vignettes be sized accordingly and that the C3 message traffic be held to reasonable levels.  He was able to avoid “good ideas” from all sources.The initial architecture chosen for 1stApp provided for failure of the HLA component or the WAN.  Since the MaK tools were new and untried by the test team, it seemed wise to plan for problems.  Similarly, WAN encryption was an unknown.  Due to the schedule slips, the encryption systems were not installed until very late in the integration process.  Planning for WAN failure provided alternatives.  Figure 4.3 graphically shows the failure modes considered for 1stApp.Figure 4.3  Tiered ArchitectureTo reduce the normal state update traffic and increase the scenario size, we chose to reduce the state update rates.  The following parametric changes were enforced:Each simulation provided a process to equalize the PDU output rate over the specified heartbeat.No articulated parts were represented.All entities used an automatic update rate of 60 seconds.  This value should have been placed in a parameter file to allow experimentation to determine the optimum value.Static entities transmitted entity state PDUs, once at initiation and for every appearance change.  This was in addition to the automatic heartbeat.Moving entities transmitted an entity state PDU, once at initiation, for every appearance change, and whenever they exceeded the dead reckoning threshold parameters.  These thresholds were ten (10) degree and ten (10) meters. This was in addition to the automatic update rateThese parameter changes required changes to those system which incorporated out-the-window visual systems.  They routinely chose to increase the “time-out” parameter to 300 seconds.  This caused some minor visual anomalies.  The effect of the parameter changes was to reduce the state PDU loads from approximately 3K/sec to about 1K/sec.  Additionally, C3 PDUs from sensors, a large bandwidth user, were restricted to rates that could be rationalized as reasonable.  The alternative was to allow unconstrained reporting.  The systems could easily send more than 10K PDUs /sec if unconstrained.  That traffic, from more than 300 sensors would quickly flood the network.  But the question quickly arose, “How do we know what the right rate is?”  The answer depended on the team’s ability to sense the traffic and identify the source of problems during run time.  A Real-Time Monitor system, developed for 1stApp, allowed us to make decisions during each run’s “hot wash”.  For instance, a simulation was observed to be sending 14K PDUs/sec from just one simulated sensor.  Since it was simulating a manned sensor, this seemed excessive.  It seemed unlikely that any soldier would send over 14,000 Spot Reports per second, regardless of the situation.  The simulation owner was told to reduce the rate by 90% by the next run.  During that test, we were able to see that they had, in fact, restricted the PDU output as required. The test team had to bring all the simulations together and integrate their communication in less than 30 days.  The right tools, discussed above, were critical to that effort.  But they were not automatically effective.  By drawing on a vast pool of expertise, the 1stApp team could learn the tools while trying to integrate the systems. And, in fairness the MaK and VTC tools were fairly easy to use.Federating simulations designed for radically different uses demanded detailed interface specifications.  The entity data was fairly straight forward.  Enumerations for new and prototyped systems were defined.  The C3 messages, however, were more difficult.  The content of the messages had to be determined and then bit encoding values assigned to each message.  Since most of the legacy applications were DIS, a set of Interface Control Documents (ICD) were developed to inform each simulation developer of the required message content.  The variable datum fields of the DIS Event Report PDU were used to encode the Message to Observer, Observer Mission Update, Spot Report, System Status and Target Data messages.  The variable datum fields of the Action Request PDU were used to encode the Command and Control System Fire Mission Processing (C2SFMP) message used to direct a fire unit to either engage a target or to plan and fire a loitering missile mission.  This message filled the role of a tactical Call For Fire (CFF) message.  The C2SFMP message record was used to distinguish between two types of missions.  An Order To Fire was used to order a missile engagement while a Lam Loiter Mission ordered a Loitering Attack Missile (LAM) loitering mission.  Other ICDs were written to define Data Collection methods and processes and NLOS-LS internal messaging.The ICDs were iterated until each was judged to be mature enough to support the test.Adherence to all the ICDs was rigidly enforced.  Each player system was required to comply and tested to ensure compliance prior to joining the 1stApp test federation.  This provided the test director with the only means of determining that each system could communicate accurately with others.  Given the compressed schedule, this was the best way to confidently compress integration opportunities.  5. ConclusionsPerhaps the greatest lessons learned from 1stApp are that the systems used in RDE experiments should have the following characteristics:Easy to integrate with well defined interfaces,Inexpensive to procure,Plug and play whenever possible,Stable, robust.The standards that define the interfaces and communication between simulations should meet the same criteria.In the area of HLA technology and data collection, a couple of lessons were apparent.  The HLA RTI technology seems to have come a long way in the last couple of years.  The problems during encountered during 1stApp on the HLA side were a result of lack of integration time more than capability of the tools. The RTI was very stable but many of the HLA federates were not stable.   In the area of data collection, the team found that tools exist to collect the data but analysis tools are lacking and the analysis usually needs to be customized for the experiment.  As a result, this needs to be planned for very early in the experiment planning process.  More development and research needs to be done into easy to use and easily modifiable analysis tools.  1stApp provided the participants with ample lessons.  The question, of course, is whether we will learn anything from those lessons.  For learning requires that we inculcate the lessons into the follow-up experiments.  Only time will tell.6. References[1]	Lorenzo, Max; Tackett, Greg; Roose, Kathryn; Larsen, Karin; and VanBebber, James: “Network Challenges and Observations Noted During an HLA Distributed Simulation Exercise”, Simulation Interoperability Workshop (SIW) Paper 02S-SIW-059, March 2002. [2]	Bentley, Tim; Larsen, Karin; Roden, Angela; Fann, Joey; Gonzalez, Gilbert; Roose, Kathryn; VanBebber, James; and Tackett, Greg: “HLA Lessons Learned from the RDEC Federation Calibration Experiment”, SIW Paper 02S-SIW-042, March 2002.[3] Birmingham, Jacob; Dykstra, Phillip; Kile, Tom; Heck, Mark, Roose, Kathryn; Tackett, Greg; and Valls, Miguel:  “The Architectural Design, Implementation and Observations of a Network, Built over the DREN, in Support of the Distributed HLA Simulation Federation of the RDE 1st Application”, SIW Paper 03F-SIW-029, September 2003.[4] US Army AMRDEC:  "RDE Command First Application (1stApp) Final Report", 25 July 2003.Author BiographiesRALPH J. WEBER III is Deputy Director of CSC’s Modeling and Simulation Center of Excellence (MSCoE), headquartered in Huntsville, AL.  He is responsible for supporting internal CSC requirements and US government customers in more than 12 locations world-wide.  Mr. Weber was designated a Certified Modeling and Simulation Professional (CMSP) by the Modeling and Simulation Professional Certification Commission (M&SPCC) in 2002.  His current memberships and affiliations include the Simulation Interoperability Standards Organization (SISO), where he has been Vice-Chair and Chair of the RDE Planning and Review Panel since 1996.  He is also a member of the Conference Committee for the Huntsville Simulation Conference (HSC) and a member of the Army Aviation Association of America (AAAA).  Mr. Weber retired from the US Army after 24 years service in 1992.GREGORY B. TACKETT is the Distributed Simulation Manager with the US ARMY Aviation and Missile Research Development, and Engineering Center (AMRDEC), where he is responsible for federating AMRDEC distributed simulations at the Advanced Prototyping, Engineering, and eXperimentation (APEX) lab with other simulations within the Army, DOD, and the commercial sector.  His current and past memberships include the Army Distributed Simulation Working Group, AMC M&S IPT, the Military Applications Society (MAS), Simulation Interoperability Standards Organization (SISO), Military Operations Research Society (MORS), DMSO Architecture Management Group, charter membership in the Tennessee Valley HLA Users Group, and chair of the Technology Working Group of the SPACE FACT.  He is an associate editor of the Transactions of the Society of Modeling and Simulation International.  He has received an invention award for the Pseudo-Random Urban Feature Entity Server (PRUFES), and as a member of the RDEC Federation, received the 2002 RDA Domain DMSO Award.KATHRYN ROOSE is a Computer Scientist currently providing support for the APEX Laboratory at the U.S. Army Aviation and Missile Command Research, Development, and Engineering Center (AMRDEC). She received a M.S. in Computer Science from the University of Alabama. Ms. Roose has 22 years experience in system and network management, and in the development of software applications for the U.S. Army and NASA.KARIN LARSEN is the Simulation Software Architectures Directorate Manager in the Applied Technology Group for SAIC.  Ms. Larsen has been involved in the software development of military systems since 1983 and has been supporting the Distributed Simulation Center for the U.S. Army Aviation and Missile Command Research, Development, and Engineering Center (AMRDEC) since 1999.  Ms. Larsen also has an extensive background in Program Management and software process improvement.  Ms. Larsen received a BS degree in Computer Science from the University of Missouri in 1983Aberdeen - ARLOrlando – STCRedstone Arsenal - RTTCRedstone Arsenal - APEX IIIFt. Belvoir - CERDECServerComms= DIS Application= HLA FederateSponsor/LabLEGENDDSite of ExecutionACE clientIP VTCPolycomACE clientACE clientMultipleCommercial PhoneConference CallsFor Unclass Voice...............IP VTCPolycomHLA RTI DRENVirtual Meeting ServerACEDCATAMRDECVTI HPMServerArmamentARDECConfiguration)MRMServer (DISArmamentOTB MC2LoggerDataGatewayMakALCESMFS3ViewDriverViewCommand(GEC)UGS view(CMDIS)viewUAV/UGV(OCU)controlUAV/UGVCERDEC/B DIS NetworkRecon VehicleCERDEC/MMC2CERDEC/MSTCDCATAPEX III DIS NetworkFireSimFt. Sill(4x)StealthXIGCDASARDECObserverForwardStealthXIGLoggerMak DataCommandProtoIDEEAS(IDEEAS)ServerMissileDisplaySensorPlannerMissionFSS2OTBSTCmode)(DISResultsHLAGatewayMakDCATOTBHLA ResultsFederateCERDEC/BCERDEC/BV/L ServerARLSTCHLA ResultsVTI SITLTARDECHLA ControlSEAMSMC2federatecoreFSS2-basedCERDEC/MonmouthAMRDEC