The Combat Automation Requirements (CART) Program:  Results and Lesson Learned from Recent Testing of Advanced Human Performance Models Interacting with DoD Constructive SimulationsAuthors:David G. HoaglandEdward A. Martin, PhDAir Force Research LaboratoryAFRL/HECI2210 Eighth StreetWright-Patterson Air Force Base, OH 45433-7511937-656-7013, 937-255-8072david.hoagland@wpafb.af.mil, edward.martin@wpafb.af.milBryan E. BrettJeff A. DoyalScience Applications International Corporation4031 Colonel Glenn HighwayBeavercreek, OH 45431937-431-4390, 937-431-4429Bryan.e.brett@saic.com, jeff.doyal@saic.comNils D. LaVineRobert A. SargentMicro Analysis & Design, Inc.4900 Pearl East Circle, Suite 201EBoulder, CO 80301303-442-6947nlavine@maad.com, bsargent@maad.comKeywords:CART, human performance modeling, HLA, task network models, SIMAN, RPR FOMAbstract: This paper discusses recent work stemming from the integration and test of advanced human performance models interacting with a mission-level simulation environment.  This is made possible by a technology development and demonstration program within the Air Force Research Laboratory/Human Effectiveness Directorate called the Combat Automation Requirements Testbed (CART) Program.  CART has been briefed at past SIW conferences and this paper, along with the accompanying briefing, will update the modeling and simulation community on key advances and lessons learned.  Specifically, this paper covers accomplishments in building high fidelity models of operator tasks for an advanced, single-seat fighter aircraft attacking and destroying ground threats.  Using goal-oriented task network models of human decision making, tests have shown these models are capable of flying, navigating, performing threat evasion maneuvers, and successfully attacking ground targets in a typical time-critical target scenario.  This paper also discusses some of the advantages and pitfalls of trying to mesh new human performance models with legacy and emerging constructive simulation environments given CART’s implementation of the SIMAN interactions and RPR FOM.1.  Introduction“All weapon systems, including uninhabited vehicles, depend on Human Systems technologies - there are no unmanned systems.” [1]Analysts and decision-makers rely heavily on constructive simulations of a system to help translate mission requirements identified by the warfighter into critical system performance requirements.  Within constructive simulations, trade-offs are conducted on key subsystem attributes by selectively varying capability levels and then measuring the results on mission performance.  In this way, weapon system performance requirements can be selected based on simulation results that yield expected levels of mission performance.  A major benefit arising from the use of objective, quantitative requirements derived in this manner is that they provide explicit criteria against which subsystem designs can be built and tested.Unfortunately, consideration of the operator as part of the system has usually been avoided in these requirements generation efforts.  The acquisition community relies on subjective, and often expensive and time-consuming, man-in-the-loop simulation partly because of their inability to model crew member behavior within today’s constructive simulations.  Hence, cockpit designs are not quantifiably linked to a set of overarching requirements of weapon system effectiveness levied by the warfighter as are the other subsystem-attribute requirements, such as radar range, weapon CEP, etc.  The result is an acquisition process that develops and field cockpits developed apart from other subsystem development but yet which must be integrated together in a coherent form to maximize operator performance for training, wartime, and contingency operations.As the combination of today’s modern subsystems becomes more complex, development of the interface displays and crew station controls becomes a major integration task for contractors who must provide pilots, soldiers, and seamen with an ‘optimized interface’ (the requirement term commonly used today to describe what the contractor is to build).  Not only does the lack of quantifiable operator performance lead to crew interface inadequacies, it can also drive up costs unnecessarily because the system was employed inappropriately in simulation.  For example, military analysts have noted that, "Every single analysis that I have ever seen has suffered from the lack of capturing smart tactics.  Mistakes such as pursuing an attack when the tactic should have been to 'run away' lead to mission outcomes (aircraft losses) that seem to indicate system deficiencies when in fact the system was misused tactically."[2]  Analysts and decision-makers need a means to readily model and understand the effects of human performance as part of  total weapon system effectiveness measures.One of the primary inhibitors to advancing integrated models of human performance is the lack of a single architecture suited for multiple users with various needs.  In addition, different simulations today use separate underlying databases thereby relying on different algorithmic approaches for human representation.[3]  Like models of subsystems which can be aggregated to different levels of fidelity, the solution to making human models more widely accepted and used by the larger M&S community must evolve through an accepted interface (e.g. the Run Time Infrastructure (RTI)).2.  The CART Program: A Brief Overview2.1  CART Program ObjectivesThe Air Force Research Laboratory/Human Effectiveness Directorate (AFRL/HE) is currently engaged in developing and demonstrating a human modeling software environment as part of the Combat Automation Requirements Testbed (CART) program.  The objectives of the CART Program are to:  (1) advance the state-of-the-art in human modeling using interoperable simulations and practices, such as HLA, (2) demonstrate a robust human modeling architecture that is compatible with current and future DOD simulations, (3) link operator performance with mission effectiveness, and (4) provide the capability to trace cause-and-effect relationships during or after simulation runs.2.2  CART CapabilitiesThe CART program will extend current constructive modeling and simulation (M&S) capabilities by providing new tools for generating crew station requirements whether for conceptual systems or for modifications to existing interfaces.  The first capability that CART seeks to demonstrate is a human performance modeling capability that analysts can use to create operator task network models which correspond to interactions between the operator and system.  Analysts will also be able to stochastically parameterize the models to reflect different levels of operator performance.  Human performance models developed using CART’s graphical user interface can be integrated with constructive models of a system and interact with the system in the context of a simulated mission (as described in greater detail later).  The second tool provides a performance assessment capability.  This supports generation of measures of operator performance whereby operator measures can be linked to measures of system performance and top-level mission effectiveness requirements levied by the warfighter.  With CART, relationships between operator performance and mission performance can be visualized and traced back to demands of the interface and mission on the operator as well as the assumptions embedded in the model of operator performance levels.  In turn, levels of operator performance that are required to produce desired mission outcomes can be identified and used to iterate upon crew system design, function allocation, and for a variety of other analyses.3.  CART Approach and ResultsThe current phase of the CART program consists of six tasks and each is briefly discussed in greater detail in Hoagland et al. [4] and Brett et al.[5], therefore the top level objectives and results of work obtained thus far are summarized briefly below.3.1  Task 1:  Crew System Requirements Establishment.The CART program characterized the current acquisition process for DoD acquisition programs, highlighting the nominal extent to which crew system requirements are analyzed and established.  In addition, the CART team has identified the potential ‘users’ of our modeling environment as well as the processes employed today to generate requirements using M&S.  It was found that CART should be more than simply a human performance modeling technology.  It should also be a process for applying that technology in conjunction with other system modeling and evaluation activities to generate weapon system requirements (including crew system requirements).In the process of identifying the current state-of-the-art for constructive simulations and how system requirements are currently derived, Task 1 results recommended the need for three fundamental capabilities.  These are needs for:  (1) techniques for decomposing missions that support linking operator performance metrics (MOPs) to mission-level measures of effectiveness (MOEs), (2) techniques that allow human performance models to interact with constructive simulations, and (3) tools that allow analysts and decision makers to visualize and trace performance metrics from the operator level through mission outcomes.CART’s implementation of human performance representation within a constructive simulation environment will not change the high-level goals and approach of the traditional data analysis used in ultimately generating requirements today.  The analysis will still focus on identifying differences in mission outcomes (reflected in missionlevel MOEs) achieved by the various system concepts being addressed.  The CART approach will simply allow a more detailed data analysis, providing traceability of mission results to particular task performance and crew system components.  If differences in mission-level outcomes are identified, the analysis then turns to the lower levels of the measure hierarchy to identify which function, task, or subtask performance measures best explain the differences in mission outcome.3.2  Task  2:  Human Modeling Architecture.This key task is currently wrapping up the definition of a human modeling architecture capable of developing and integrating human models with the mission simulation called the Joint Interim Mission Model (JIMM).  Using recent releases of the CART model development software, the CART team has successfully developed and demonstrated pilot models interacting with JIMM through an interface that characterizes a human-centered flow of information across the HLA RTI (see Figure 1).Figure 1.  CART ArchitectureThe CART team is adapting an architecture that builds upon a tool developed by the U.S. Army Research Laboratory called the Improved Performance Research Integration Tool or IMPRINT.[6]  IMPRINT, together with the CART extensions, is a human performance modeling architecture that supports:  (1) representation of information seeking and attention sharing, (2) representation of workload effects on performance, (3) systematic manipulation of key performance attributes (e.g., time, accuracy, task prioritization) by the user, and (4) HLA compatibility.  The human performance-modeling environment shown in Figure 1 is a hybrid of two approaches to human performance modeling:  task network modeling and first principle modeling.Task network modeling is CART’s core human-performance modeling method.  Task network modeling breaks the human performances of interest into a series of tasks characterized in terms of performance times, accuracy, and probabilities.  Tasks are linked together into networks that represent sequences and paths performance can take.  Within CART, the IMPRINT tool is used to provide baseline task network modeling capabilities, and then extended to provide representation of the goal-oriented nature of human performance and to communicate with external models via the HLA.[2][4][5]With first principle models, users will also have the capability to augment the core task network models with models that provide high-fidelity representations of human performance.  Essentially, tasks in the task network will call first-principle models that represent relevant aspects of task performance.  The first-principle model will then execute, and return the parameters required by the task network model.The HLA provides the communications link between CART-developed human performance models and external simulations.  Data is passed between architecture components using the Run-Time Infrastructure (RTI).  The human model can receive data regarding system and mission status from the constructive system simulation and data about the external world (e.g., SAM launches) which typically come from mission environment models.  In return, actions to be implemented by the operator (e.g., maneuver, target designation, weapon launch) are passed to the constructive simulation by the task network model.  Similarly, the task network model uses the RTI to pass data to first principle models to initiate them as required and receive the results during model execution.3.3  Task 3:  Conduct Trade Studies.AFRL/HE and its prime contractor (SAIC) are to conduct trade studies to select two operational contexts from among various predefined military domains of interest.  The purpose of this task is to conduct a structured assessment to select an appropriate simulation topic area for the CART’s Case Studies as described in Task 5 below.  Selection criteria for the trade studies include human performance complexity, availability of existing system and environmental models, the team’s simulation-domain expertise, and program affiliation with Simulation-Based Acquisition (SBA) initiatives.  The first trade study has been completed and we selected a single seat fighter attacking targets which are time critical and which impose a variety of high workload tasks on the operator including threat detection and evasion, detecting and attacking ground targets, and in-flight replanning.  The second Case Study domain will be selected later this fiscal year based upon the results from the second trade study.Task 4:  Real-Time Operational Mission Simulation.The technical approach for each Case Study is to modify and prepare a human-in-the-loop simulator for testing.  As a result of the selected domain for the first Case Study, the CART team chose to use the formidable resources already in place at the Wright-Patterson Air Force Base SIMulation and Analysis Facility (SIMAF).  The SIMAF is a state-of-the-art modeling and simulation facility used to aid program offices in deriving new concepts of operations, aircraft requirements, and performance trades in a system-of-systems simulated environment.  Virtual, constructive and distributed connections to other players in the DoD are available in the SIMAF for acquisition-related simulation exercises.  Based on locality and the cutting-edge methods and tools being applied in the SIMAF, the CART team elected to baseline its CART simulation environment using SIMAF’s mission-level simulation (the Joint Interim Mission Model) and the crew system interfaces reflected in their manned simulator stations.  During this task, engineers also added the needed data collection capabilities for computing human performance measures and mission effectiveness metrics.  Preparing and modifying SIMAF components for the purposes of CART’s constructive and manned testing has been basically completed for Case Study 1.  Task 5:  Conduct Case Studies.  The heart of the CART program is the accomplishment of both constructive and virtual simulation tests for identical mission tasks within the selected mission context.  This important steps represents the verification and validation of the CART technical approach and human performance models developed thus far.  CART engineers and scientists are currently in the midst of comparing constructive and virtual simulation measures to demonstrate the efficacy of the human performance modeling architecture and our approach to integrating human modeling data to JIMM using the HLA (see Figure 2).For the constructive tests, six different scenarios constituting various initial conditions and pilot “workload” have been developed to serve as the basis for our human model-to-JIMM testing environment.  For each scenario run, the human model developed within CART’s software environment is essentially expected to detect and avoid threats, perform in-flight mission replanning, finding and identifying mobile targets, and collating avionics and munition information when prosecuting the target.  While the purpose of this battery of tests is to evaluate the ability of the model to realistically act and react to environmental situations, the overarching purpose of these tests are to demonstrate the feasibility of CART’s technical approach, i.e. does integration of task network modeling and goal orientation work  successfully to pass data to and from an external simulation?  This step effectively constitutes verification of the human performance models generated thus far. By the time this article is published, the first half of Case Study 1 (constructive tests) will be completed with more lessons learned available by the time of the Fall 2000 SIW conference.Figure 2.  Testing ApproachFor virtual simulation, between six and eight pilots are planned for man-in-the-loop testing to be completed no later than the middle of October 2000.  The pilots will have fighter experience and may have to be trained on the SIMAF’s cockpit switchology and mechanization to bring them to a level of competency felt comparable to that of the model run during the constructive tests (which was based on subject matter expertise).  After initial training is completed, the pilots will run through the same scenarios, initial conditions, and will be expected to perform the same series of tasks as embedded in the human performance.  Measures of operator performance and mission effectiveness will be collected identical to that collected in the constructive tests.  A multivariate repeated measures experimental design will be used to compare model and human-in-the-loop performance data.  The analysis will assess the effect of experimental variables on dependent measures such number of target kills, survival rates, time and accuracy to accomplish crew station tasks, etc.  Our hypothesis (goal) is there will be no statistically significant difference between the model’s performance measures and the human-in-the-loop performance measures.  In addition, the costs in terms of funding and labor hours to get both constructive and virtual test results will also be assessed to discern the relative impacts of using models versus man-in-the-loop simulation.  Even though this will be the first time a comparison will have been accomplished using human performance models, the data will still be useful as a rough order of magnitude for future CART demonstrations.  The final results and conclusions will be available by the Spring 2001 SIW conference and the 2001 CGF/HBR conference.Task 6:  Prepare Testbed Definition.To document all the research and development activities involved with this testbed concept in a form useable by the larger M&S community, SAIC will develop a CART performance specification that defines an implimentable testbed.  The definition specification will include guidance regarding how to integrate equipment or instrumentation in a simulation for the purposes of human data collection, the overarching CART software architecture description, Case Study RTI implementation methods used, and a reconfigurable hardware architecture methodology.4.  Lessons Learned4.1  Tactics:  Identify the General Case.When developing human models, the modeler must always incorporate a set of tactics the simulated human will employ while performing a mission.  To arrive at these tactics, the modeler has to consult with multiple subject matter experts (SMEs) and references.  Unfortunately, it is rare that two SMEs employ (or articulate) the exact same set of tactics.  Therefore, the modeler must try to find the “general case” tactic, or the elements of the tactics that most SMEs seem to agree upon.  Once he/she has arrived at this general case tactic, the modeler should then present it to the SMEs for final validation to ensure that it is a reasonable set of procedures for the particular mission in which the human model will be employed.4.2  Integrate The Human Models And Constructive Simulation Early.The CART human model will act/react to a number of variables from the constructive environment as discussed previously.  As more and more external variables are used by the human model, it becomes increasingly difficult to thoroughly test it in a stand-alone mode.  That is, using only internal software stubs or drivers, it is nearly impossible to stimulate the model to the degree it will be once integrated.  It is only when operating in an integrated mode and all of the data is passed in realistic sequences that the human model is sufficiently exercised to allow the modeler to uncover all of the bugs.  The earlier this integration can take place, the faster model bugs can be corrected and/or avoided.  We favor parallel development efforts for CART human models and the constructive system simulation in which common simulation components are developed, integrated and tested in isolation and later integrated into a complete, fully functioning whole.  Not only does this ensure that the data required for testing the HUMAN MODEL is available, it provides the opportunity to resolve data access and availability issues within the constructive system simulation and ensures that data required by the human model can, in fact, be provided.4.3  Software Allocation of Human Model Functionality.CART engineers are currently working in the SIMAF facility to understand the hardware and software that is in place, and to develop the middleware necessary to allow the human model to run within the SIMAF’s constructive simulation environment.  This aspect presents some major challenges, and requires the development of specific software since simulation implementation details will vary from facility to facility.  For example, how does one determine whether the operator can detect a target within a SAR image?  In the virtual environment, this is accomplished by the pilot directly observing and interpreting SAR imagery on a display.  In the constructive environment, however, issues such as occulting and clutter need to be addressed mathematically, and will require the processing of data from the facility’s image generator and simulator.  On the output side, some means has to be provided to allow the human model to maneuver the simulated aircraft.  In the virtual environment this is accomplished by the human pilot applying pressure to a stick transducer; however, some algorithmic workaround is necessary for implementing CART’s model of  pilot control.Like all modeling environments, the CART human modeling development environment has its strengths and weaknesses.  While our general goal is to develop as much of the human performance model as possible using the CART software, there were a few instances in which the capabilities of the CART software was insufficient and we had to develop our own custom algorithms to provide needed functionality.  One of these custom algorithms was shoot list management.  The shoot list is the list of points on the ground the pilot wants to image during the target acquisition function.  It begins with an initial set of points based on terrain and features in the target area (e.g., roads, bridges, open fields) but changes over time based on results of the search process, on/off-board data, and dynamics of the simulation (e.g., ground moving target hits).  Shoot list management is an integrated process that represents the operator’s plan and tactics for sensor employment and also his memory functions (e.g., where he wants to look, which objects he has examined already, his identification and classification of those objects).  The shoot list management algorithms were a bit complex and involved extensive use of arrays whose complexity exceeded that of arrays that could be used in the CART software.  Consequently, our shoot list algorithms were custom developed and embedded on the constructive simulation side.  This gave the shoot list algorithms direct access to sensor data on the constructive simulation side without having to arrange for additional data communications through the RTI.  Though separate from the main CART human model, these algorithms were controlled by the CART software by initiating execution of the shoot list algorithm when a shoot list update occurred in the main human model.The second algorithm developed, separate from the main CART human model, was a control function for a high G maneuvers (up to 7 G) as part of the threat evasion goal state.  While the representation of pilot control of the airplane was generally accomplished by manipulating the auto pilot (a baseline part of our aircraft system), intensive threat evasion required a rapid onset, high-G turn typically away from the threat.  The constructive aircraft’s auto pilot could not provide the turn rates required, therefore it was necessary to develop our own control algorithm.  Because of the sensitive, closed-loop-control nature of the algorithm, it was embedded in the constructive simulation side.  Like the shoot list management algorithm, it was under the control of the main CART human model which initiated the algorithm, directed a heading to be flown to next, and “occupied” time while the algorithm executed.4.4  Test Models Thoroughly Under All Anticipated Scenarios.The way in which a single human model performs can vary significantly within and across the scenario(s) under which it is run.  Variation in the mission environment (e.g., threat situation, target position, route) can greatly influence the way in which the goals, functions and tasks in a model interact.  For example, changes in the scenario may cause operator goals/functions/tasks to trigger in a different order and thus greatly change the behavior of the model.  Similarly, model behavior can change even within a single scenario.  If an entity within a mission environment is subject to vary across trials (e.g., an air defense entity behaves in a probabilistic fashion, launching weapons based on a random number draw) the human model behavior will vary as well.  Therefore, it is critical to conduct thorough, repeated testing of the model within and across all potential mission scenarios to ensure model robustness.4.5  RPR FOM Selection and Implementation.The Real-time Platform Reference Federation Object Model (RPR FOM) was chosen because it is available, tested, reliable, and compatible with existing RTI’s and VR-Link.  The CART team chose to utilize the RPR FOM over creating a new FOM, or adopting an Agile FOM Framework (AFF) for the following reasons:  (1) in CART, the Simulator Object Model (SOM) can change with each human performance model developed; (2) the AFF only works if a Federate’s SOM is similar to the FOM of the Federation (for the mapping to work the SOM must have a corresponding concept within the Agile FOM); and (3) with the RPR FOM, CART users do not have to possess the programming skills to rewrite and maintain middleware code. [8]  For data not represented by specific RPR FOM object attributes and interactions, a table of enumerated data names and numbers is created.  This enumerated data is mapped to SIMAN Interactions, such as “ActionRequest” and “SetData.”  This facilitates federation interoperability for entities, object attributes, and interaction parameters in a CART federation not represented by other RPR FOM elements.Additionally, many simulations that will eventually join the CART federation that may also utilize the RPR FOM.  By designating some variables as “external” in the CART model, a user sets up the model to interact with other members of a CART federation.  Prior to federation run-time, these external variables are mapped to HLA RPR FOM object attributes and interaction parameters.  They are also mapped to human model “actions” to be taken (whether the CART model wants to “receive” or “send” the data represented) when the variables are encountered during federation run-time.  During federation run-time, when these variables are used in a CART model, their values update accordingly or a simulation action (such as suspending a task or starting a new goal state) may be triggered.4.6  Middleware Development.CART human models act as a federate within a High Level Architecture (HLA) compliant federation.  The data sent across the federation (entities, object attributes and interactions) has been mapped to the RPR FOM.  In order for this interaction to occur, certain “middleware” had to be developed in the CART software where the components of the CART middleware are shown in Figure 3.  Figure 3. CART Middleware DescriptionThe user-defined external variables are the interface between CART models and other HLA federates prior to and during federation run-time.  During federation run-time, Component Object Model (COM) services are used to communicate between the “middleware” and the Micro Saint Run-Time Engine (MSRTE) which executes CART models and we use Mäk Technologies’ VR-Link product to provide an interface to the RTI.  The middleware interface is needed because it allows the CART user to map external variables to RPR FOM object attributes and interaction parameters prior to run-time.  During run-time, this middleware code allows the RTI with VR-Link to interact with COM, and then to human model.4.7  Time Management Issues Using HLA 1.3 V6.We've found that implementing a Time Management scheme to link an event driven simulation tool, like the CART software environment, to a time-stepped simulation commonly used by human-in-the-loop simulations, has presented us with the largest challenge.  Our approach is to make all of the federates in a CART federation both time regulating and time constrained.  Because time synchronization between the human model and the constructive simulations is critical, we employ the time management service of the RTI 1.3 V6 to ensure time synchronization of all the simulations.  Specifically, we are using the “nextEventRequest” and “timeAdvanceRequest” services from the RTI.  All of our interactions are coming across in correct Time-Stamped Order (TSO), and the federation is in fact interacting as we would expect it to.Under time management service, a federate determines the simulation time to which it would like to advance and requests a time advance from the RTI.  The RTI receives time advance requests from all federates, determines the shortest advance requested, and then sends a time advance grant to all the simulations to that time.  Processing time for the grant varies and a simulation should not proceed until it receives a grant and knows how far in time it can advance.  We have had to impose a 50ms delay in the CART simulation following each time advance request to ensure that the subsequent grant will be captured.  As a result, this has slowed the CART simulation significantly such that we are running slower than real time (approximately 4:1).  DMSO has indicated that the next generation RTI (RTI NG) will process time advance requests and grants more efficiently, possibly alleviating the problem.  With the support of DMSO, a significant segment of CART activity over the next year will be focused on improving the speed of CART execution when coupled with other simulations.  This will involve migrating our RTI interface to RTI NG as well as optimizing our middleware which communicates with CART’s simulation engine.  Additional insights should be available by conference time.5.  SummaryAs M&S technologies continue to make rapid advancements, their utility and value to the acquisition process continue to increase.  However, there is still room for improvement in both the tools and processes being implemented to support acquisition specifically in the way human behavior and tactics are represented.  In addition, care must be taken not only to assure that the human modeling tools provide a useful product, but that the tools themselves are useable by the target population - and that the human modeling process itself does not impose undue additional burden on the user (ideally the process should simplify the user’s job).  The purpose of CART is to permit analysts and decision makers to consider the human early during constructive simulation-based trade studies and to understand how performance at the crew station level impacts mission outcomes - a major challenge in itself.Results and valuable lessons learned are starting to come together as we progress through our first major demonstration of the CART technology.  To summarize lessons learned thus far:A human modeler must try to find the “general case” tactic, or the elements of the tactics that most SMEs seem to agree upon.We favor parallel development efforts for CART human models and the constructive simulations in which common simulation components are developed, integrated and tested in isolation and then later integrated into a complete, fully functioning whole.  This aspect presents some major challenges, and may require the development of specific middleware software since simulation implementation details will vary from facility to facility.  As a result, human modeling functions may actually be required to be implemented on the constructive simulation side as opposed to the human model side due trade-offs in speed and simplicity.The Real-time Platform Reference Federation Object Model (RPR FOM) was chosen because it is available, tested, reliable, and compatible with existing RTI’s and VR-Link.  For data not represented by specific RPR FOM object attributes and interactions, a table of enumerated data names and numbers is created.  This enumerated data is mapped to SIMAN Interactions, such as “ActionRequest” and “SetData.” The use of the SIMAN Interactions provides data flexibility without compromising the structure and interoperability gained by using a pre-defined FOM. In short, we have a tool which is very adaptive to sending and receiving data to/from other simulations using the concept of external variables mapped to the RPR FOM SIMAN interactions.During federation run-time, Component Object Model (COM) services can be used to communicate between the CART “middleware” and the Micro Saint Run-Time Engine (MSRTE) which executes CART models and we use Mäk Technologies’ VR-Link product to provide an interface to the RTI.We've found that implementing a Time Management scheme to link an event driven simulation tool, like the CART software environment, to a time-stepped simulation commonly used by human-in-the-loop simulations, has presented us with the largest challenge.  Our approach is to make all of the federates in a CART federation both time regulating and time constrained. We have had to impose a 50ms delay in the CART simulation following each time advance request to ensure that the subsequent grant will be captured.With the migration to the RTI NG and with the support of DMSO, a significant segment of CART activity over the next year will be focused on improving the speed of CART execution when coupled with other simulations.6.  References[1]  Director of Defense Research and Engineering,  Defense Technology Area Plan, Chapter IX Human Systems, February 1999.[2]  Brett, B. E., E. A. Martin, & D. G. Hoagland,  Tools for Including Realistic Representations of Operator Performance in DOD Constructive Simulations, AIAA Modeling and Simulation Technologies Conference and Exhibit, Portland OR.  Paper Number AIAA-99-4027, August 1999.[3]  Pew, R. W. and A. S. Mavor  (Eds.) (1998).  Modeling Human and Organizational Behavior. Washington, D.C.: National Academy Press.[4]  Hoagland, D. G., E. A. Martin, & B. E. Brett,  The Combat Automation Requirements Testbed (CART) Program:  Improving the DoD's Requirements Process Through Inclusion of Realistic Operator Performance, Simulation Interoperability Workshop Paper Number 99S-SIW-129, Spring 1999.[5]  Brett, B. E., J. A. Doyal, D. A. Malek, E. A. Martin, & D. G. Hoagland:  “The Combat Automation Requirements Testbed (CART) Task 1 Final Report:  Implementation Concepts and an Example”  US Air Force Research Laboratory Technical Report Number AFRL-HE-WP-TR-2000-0009.[6]  Improved Performance Research Integration Tool (IMPRINT) User’s Guide Version 4.0 (April 1998).  US Army Research Laboratory, Human Research and Engineering Directorate, Aberdeen Proving Ground MD 21005-5425.[7]  Hendy, K. C. & P. S. E. Farrell,  Implementing a Model of Human Information Processing in a Task Network Simulation Environment, DCIEM Number 97-R-71, Defence and Civil Institute of Environmental Medicine, North York, Ontario, Canada, December 1997.[8]  SISO-STD-001-1999: Guidance, Rationale, and Interoperability Modalities for the Real-time Platform Reference Federation Object Model, 14 September 1999.Author BiographiesDAVID G. HOAGLAND received his Bachelor's of Aeronautical and Astronautical Engineering degree from the Ohio State University in 1985, where upon he joined the US Air Force as a civilian engineer at Wright-Patterson AFB.  As a crew systems engineer working in the Aeronautical Systems Division, he was involved in the design, development, testing and acquisition of major weapon systems, namely the F-16 Fighting Falcon, the 767 AWACs, the F-117 Stealth Fighter, and the E-3 JSTARs.  He moved to the Air Force Research Laboratory in 1995 as a plans and programs engineer, and later became the deputy program manager for the Crew-Centered Design Technology (CCDT) Advanced Technology Demonstration 6.3 program.  He has served on numerous unmanned vehicle initiatives in the Air Force including the 1996 SAB UAV Summer Study team and the DARPA/AFRL UCAV program.  He currently serves as Program Manager for Combat Automation Requirements Testbed (CART) Program.EDWARD A. MARTIN is a biomedical engineer in the Air Force Research Laboratory's Human Effectiveness Directorate where he is currently Technical Director of the Combat Automation Requirements Testbed (CART) Program.  He earned his MS in Electrical Engineering from Syracuse University in 1971, and a PhD in Biomedical Engineering from the Ohio State University in 1985.BRYAN BRETT received a Bachelor degree in Psychology and a Masters degree in Experimental Psychology, both from the University of Florida.  He currently manages the Crew Systems and Simulation Business Area of SAIC's Aeronautical Systems Division in Dayton, OH.  He has experience as a senior analyst, principal investigator, and currently serves as Chief Scientist for the CART Program.JEFF A. DOYAL received a Bachelors degree in Psychology from Indiana University in 1989.  He received his Masters degree in Applied Behavioral Science/Human Factors at Wright State University, 1991.  He currently works as a Principal Research Psychologist, Aeronautical Systems Division, Science Applications International Corporation (SAIC) where he serves as the chief human modeling developer in support of CART program.NILS D. LAVINE received a Bachelor degree in Engineering from the United States Military Academy, West Point, NY.  He received his Masters degree in Civil Engineering from Colorado State University.  He is a Principal Systems Engineer with Micro Analysis & Design, Inc. (MA&D).  He has over eight years experience working on and managing projects in areas of Distributed Simulations, Computer Generated Forces, and Human Performance Modeling.  He is the MA&D Program Manager for CART.ROBERT A. SARGENT received a Bachelor degree in Mechanical Engineering from Colorado State University.  He worked for 12 years in the Maintenance Engineering organization at the Boeing Commercial Airplane Group in Seattle, Washington.  During Bob’s last three years at Boeing he was a Senior Specialist Engineer in their Maintenance Human Factors group, helping over 30 airline maintenance organizations around the world implement a human error reduction program aimed at decreasing the occurrences of maintenance related operational events and accidents.  Currently, he is a Staff Human Factors Engineer at Micro Analysis & Design, working as a systems analysts and test modeler for CART.PAGE  1PAGE  1