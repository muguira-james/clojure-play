Integration of Engineering Level Sensor Federation into a Brigade Level C4ISR Experiment Using RTI V1.7Mr. Maximo LorenzoCECOM Night Vision and Electronic Sensors Directorate10221 Burbeck Road, Building 309Fort Belvoir, VA 22060703-704-3185lorenzo@nvl.army.milBill Riggs, Dave Schell, Mike CarusoScience Applications International Corporation4001 N. Fairfax Drive, Arlington VA 22203briggs@nvl.army.mil,  HYPERLINK mailto:mcaruso@nvl.army.mil,dschell@nvl.army.mil mcaruso@nvl.army.mil,dschell@nvl.army.milMajor Joseph BurnsMounted Manuever Battlespace Battle Lab2010 Black Horse Regiment RoadFort Knox, KY 40121Keywords:Battle Labs, HLA, DIS, network latency, ATMABSTRACT CECOM Night Vision and Electronic Sensors Directorate (NVESD) collaborated with the Mounted Maneuver Battlespace Lab (MBBL) during the Battle Command Reengineering Experiment (BCR) IV in April 2000, to assist in the testing of operational concepts relating to the use of sensors and improved command and control assets in future maneuver brigades. This joint venture took the form of a distributed sensor simulation using the “Paint the Night and Swiss Together” (PST) Federation, which uses wide area network ATM links to control realtime graphics displays rendered and filtered at Fort Belvoir Virginia, and displayed in the Reconfigurable Scout Simulator at Fort Knox, Kentucky. This distributed sensor simulation used an DIS/HLA gateway to convert DIS 2.0.4 PDU traffic to the HLA representation used by the sensor federates. The implementation of the PST high resolution sensor visuals in conjunction with lower resolution daytime visuals in the reconfigurable simulator offered useful insights into exploitation of a high resolution sensor simulation, in conjunction with ModSAF, which hosted a significant number of entities during the exercise. Lessons Learned included the proven capability of the DREN and the DMSO RTI to support this demanding real-time simulation, the potential for sharing high performance computing resources across remote sites, network design demands, and the warfighter’s demand for high quality, high fidelity sensor simulation. The CECOM Night Vision and Electronic Sensors Directorate (NVESD) made distributed simulation history from Monday, April 10 through Wednesday, April 19, 2000, by integrating a high-resolution engineering level sensor simulation site into the Battle Command  Reengineering (BCR IV) experiment from a remote site at the CECOM Night Vision and Electronic Sensors Directorate (NVESD) at Fort Belvoir. NVESD implemented this remote simulation across an ATM backplane using a High Level Architecture (HLA) federation. Federate components of the sensor simulation, consisting of the controls and display were located at Ft. Knox and communicated sensor status and viewing direction over Defense Research and Engineering Network (DREN) OC-3 lines to synthetic image generator and sensor  effects federates at Ft. Belvoir. The Ft. Belvoir federates then transmitted high resolution synthetic sensor imagery back to Ft. Knox in real-time. I. Scope and Purpose of ExerciseBCR encompasses the challenges the Army faces in order to incorporate rapidly changing electronic communications, computer, and sensor technology into the 21st century force structure. The purposes of BCR IV were as follows: To examine the effects of digitized command and control at brigade and lower levels of command. These effects included the impact of improved Battlefield Visualization on Battle Command, and implications of near-perfect situational awareness.Develop  more effective tactical operations centers. Among the futuristic capabilities represented in this far-reaching experiment are a common-database command and control system, video-teleconference white-board capability, robotic unmanned air and ground platforms with advanced sensor packages, and uplinks to synthetic aperture radar and satellite imagery. Past experiments have shown that while these advances provide improved information to commanders, staffs and soldiers, they can also overload them with information and increase their workload. BCR IV provided a comprehensive environment in which to assess these issues.BCR IV included six re-configurable man-in-the-loop simulators representing a future scout platoon armed with futuristic sensors, robotic platforms, and communications equipment. The “Paint the Night” sensor simulation was configured to operate with the platoon leader or platoon sergeant’s simulator, a physically reconfigurable future scout vehicle simulator with two seats (commander and mission specialist stations), a control stick, three monitors, a sound system, supported by an SGI Onyx 2 (for “out-the-window” visuals), and several SGI O2s providing 2D display, controls, and simulation host functions.  A driver’s station was located adjacent to the commander/mission specialist compartment. One monitor (with  touch screen/controls) was used for  fire control), a second monitor (also with  touch screen/controls) was used for the sensor view, and a third monitor was used for a plan view display of ModSAF.  This simulator was connected to the Fort Knox exercise network through DIS, and to the Paint the Night HLA federation through the ATM network, using a DIS to HLA converter that was positioned at Ft. Knox. ModSAF was used to populate the battlefield with both friendly and enemy forces driven from Blue and Red cell workstations located at Fort Knox/MMBL. The ModSAF version used during BCR IV communicated with the simulation network via DIS protocols, and updated the BCR commander and tactical operations center situational awareness displays based on ModSAF intervisibility and constructive sensor algorithms.The NVESD sensor simulation provided a valuable and realistic component to the BCR IV exercise. While much of BCR IV simulated sensor effects either constructively, or through low fidelity “stealth” displays, the NVESD sensor simulation gave the battalion task force a taste of the information which can actually be accessed through a realistic, advanced sensor display to support mounted reconnaissance and security operations at brigade and battalion levels. NVESD provided the scout platoon leader the ability to “attach” the sensor to unmanned aerial and ground  vehicles, as well as sensor packages mounted on manned systems. One of the advanced electro-optical sensors planned for use on these systems is the Long Range Advanced Scout Surveillance System (LRAS3). In the LRAS3 configuration, the Paint the Night sensor simulation represents the following:2nd Generation. B-Kit LongWave Scanning FLIRDay TV (Charged Coupled Device)Laser Range Finder (Far Target Locator)Global Positioning SystemCommon Aperture Optics (Narrow and Medium Fields of View)FLIRTVThe BCR IV experience points the way to the future use of engineering level experiments in conjunction with high performance computing resources on behalf of DoD battle labs. AMC labs are already planning a Pan RDEC capability which will allow for digital representation of sensors as well as other battlefield and platform specific systems before metal is bent, in order to explore system and battlefield performance, life cycle, and cost issues.  This capability will utilize resources of the DREN, HPCMO, and other lab facilities to connect RDEC, TRADOC Battle Lab, and Army Corps of Engineers modeling and simulation capabilities,  Plans are underway to extend this effort to Fort Benning, Fort Lewis and Fort Rucker in the near future, and could extend to other facilities desiring access to this capability. NVESD is also discussing the development of a bulk transport service in the RTI to facilitate these goals and demands. .In the meantime, improvements to the PST Federation and the PTN simulation are continuing.  Planned improvements include simulation support for automated target recognition (ATR), microsensors, uncooled IR, passive millimeter wave, 3rd Generation FLIR and other spectrally-based sensors. Research is ongoing to employ high resolution computer graphics based on ray-tracing of BRL-CAD models to represent shadows, reflections, 3D atmospheres, smoke and 3D signature prediction for ground, targets, and trees.II. Simulation Architecture Over the past twelve months NVESD has collaborated with Army Research Laboratory in creating a high fidelity engineering simulation that represents sensors and realistic target effects in a common, HLA-based simulation environment. This federation is called the “Paint the Night and SWISS (Synthetic Wide-band Imaging Spectra-photometer and Environmental Simulation) Together”, or PST Federation. The underlying philosophy, as shown in Figure 1, is to implement the PST Federation as a set of composible federates  with standardized interfaces (e.g. “slots”) across the HLA RTI “backplane”. This permits the remote distribution of simulation components across local area and wide area networks, such that the simulation system is location independent, given sufficient bandwidth and sufficient host computer performances.   EMBED PowerPoint.Show.8  Fig. 1 Backplane PhilosophyThis innovative approach allows the visual system to be remoted from the displays it supports and the controls which drive it. In BCR IV, the following PST component federates were integrated with the Reconfigurable Simulator as depicted in Figure 2 below:The Device Federate monitors man-in-the-loop interface hardware through a low-level device driver and publishes the device’s mode using query/response interactions as an array of generic analog and digital signals. This federate may be  configured to work with any device and control all aspects of the FOM.The Sensor Controller Federate reads the output of the device federate for device mode, sensor position, orientation, and configuration. This federate publishes the sensor location, mount point (e.g. position of the sensor relative to the vehicle of which it is a component), and configuration to a sensor image generator federate.The Sensor Image Generator (SIG) Federate subscribes to the sensor controller for “attached” vehicle data (location, type, id) and camera position and attributes (FOV, view portsize), and to the SAF Interface and other federates which generate remote entity and platform data. The final rendered frames are published by the SIG Federate and piped to the Datacube via a digital video port, using a database pager to manage the scene graph.The Sensor Effects (Datacube) Federate subscribes to sensor parameters (FOV, magnification, electronic zoom, gain, level, and polarity) for proper sensor effects generation,  and to sensor symbology and reticle data for overlays. The Datacube transfers video via the ATM (X/Open Transport Interface) to the remote display (e.g. the sensor display in the future scout simulator).The SAF Interface Federate subscribes to vehicle objects from SAF and converts SAF entities from external FOM (including DIS exercises) into the PST FOM. This federate publishes entity/platform data to sensor controller and SIG federates. The current implementation consists of a DIS/HLA gateway, which converts and publishes DIS PDUs as HLA objects and attributes. In the future, this federate will also convert PTN entities to external FOM; efforts are ongoing at NVESD to improve the reliability of the DIS to HLA conversion. The SAF Federate was not used during BCR IV, whose ModSAF applications used DIS to publish entity state information. However, PST Federation supports Joint SAF applications using RPR FOM. In the future, Agile FOM interface will also be employed. The Far Target Locator Federate subscribes to vehicle/sensor location and orientation, and will subscribe to Global Positioning System data from a solar system federate, when available. It uses intersection to determine range and calculates target position in a military grid EMBED PowerPoint.Show.8  Figure 2: PST Federation As Implemented to Support BCR IVThe PST Federate components used a sensor-specific terrain database of the BCR IV exercise play box, a 180 by 130 kilometer area around Tuzla, Bosnia. PST interactions with the ModSAF were conveyed using the DIS/HLA gateway previously discussed, which performed the necessary coordinate transformations from the Geocentric Coordinates (GCC) required by DIS to the PST Federations internal coordinate system.IV. Lessons LearnedIn BCR IV, the following lessons were learned:Long haul communication among image generation, controls and displays over the DREN does work with minimal latencyRTI latency was not noticeable in real time simulationExtensive snap-to-ground calculations did impede simulation performanceGood network design is critical to successLong haul connectivity among RDEC simulation facilities and TRADOC Battle Labs permits sharing of high performance comuting resources for the most demanding simulation applications, including high fidelity sensor simulation Warfighters desire the high fidelity virtual simulations to support a variety of requirements including the evaluation of C4ISR concepts and systems.a. Network Latency BaselineNVESD has been connected on the DREN to the Army Research Laboratory at Aberdeen Proving Ground for almost a year at OC-3 bandwidth (155 mbits/sec).  NVESD and the High Performance Computing Management Office (HPCMO) have invested in upgrading Ft. Knox to an OC-3 connection on the DREN in support of these efforts. This has enabled a 35 frames per second (fps) scene refresh capability from Fort Belvoir to Ft. Knox over OC-3 several times over the past year. b. BCR IV Network PerformanceDuring BCR IV, this rate was lessened to an average bandwidth consumption of about 60mbits/sec or 21 fps for 640 by 480 images. Factors influencing this included transport delay through the Datacube federate and internal routing of traffic on the Fort Belvoir leg of the network. The Scene performance varied between 15 - 60 Hz, depending on the number of total entities in the exercise, and the number of entities filtered out through the DIS/HLA gateway.  Network Latency between Ft. Belvoir to Ft. Knox latency varied from 11-20 milliseconds (ms) for each leg (down and back) with most transmissions falling within the 11-13ms range, giving less than one frame of video delay.  (33ms/frame) The goal for total end to end latency was less than 250ms; an estimated latency of  200ms or less was achieved, broken down as follows:16ms -- Grip digitization11ms -- DREN path Ft. Knox to Ft. Belvoir66ms -- SIG33ms -- Sensor Effects (DataCube)11ms -- DREN path Ft. Belvoir to Ft. Knox33ms -- Video displayWhile RTI latency was not directly measured during BCR IV, the real time image transfer was sufficiently responsive to the remoted controls to indicate minimal RTI latency; if the RTI had exhibited latency in the 50 – 80 ms range, this lag would have been noticeable to the user. BCR IV supported about 2400 entities, roughly half of which were filtered out through the DIS/HLA gateway, using a spatial criterion (e.g. distance from the sensor locations) to eliminate irrelevant data. Even with this relatively minimal filtering, The SIG federate was able to handle up to 1253 simulated vehicles using RTI 1.3v7. It should be noted here that the SIG Federate’s performance was degraded with high entity counts (e.g. above 600) due to the necessity of performing snap-to-ground calculations to ensure visual correlation between the ModSAF and PTN terrain databases. In the future, it is recommended that, when sensor and ModSAF terrain skins are not correlated, snap-to-ground should only be implemented for those objects that are potentially within the sensor field of view. Even better would be to derive ModSAF terrain databases from the Paint the Night T-mesh terrain skin, since this would eliminate the need for ground clamping altogether. Also it was discovered that the network was better able to handle higher levels of traffic if entities were added slowly, in increments, since adding them all at once overwhelmed the Cisco Router CPU, resulting in significant packet loss. These experiences led the PST team, including CECOM NVESD and ARL engineers, to explore the desirability of point to point messaging, and to recommend improvements to the local CECOM NVESD network design, needed to take full advantage of ATM bandwidth and to reduce network inefficiency. Figure 3 depicts the network topology that existed during BCR IV.Figure 3: BCR IV Long Haul Network TopologyOperating in the standard, administrative CECOM NVESD configuration, the simulation network traffic was routed through the common laboratory network on the way to the DREN gateway. A LAN emulator was used to support ATM traffic, resulting in bidirectional collisions and packet loss. This is clearly not an optimal posture for an HLA-based simulation network. Figure 4: Recommended Long Haul Network TopologyFigure 4 represents the proposed modifications to the CECOM NVESD and DREN to support full scale real time distributed component simulation. Significantly, point to point communications is established between  remote simulation nodes, eliminating router connections. Administrative traffic is segregated from simulation exercise traffic, a move which is designed to reduce untimely loss of service to both classes of user. The entire long haul simulation network is upgraded to OC-12 standards, ensuring sufficient bandwidth to support up to eight graphics channels.c. Potential for High Performance Computational Resource SharingWhile long haul communications have been available since the heydey of SIMNET to allow remote sites to interact in a “peer-to-peer” simulation architecture, the full exploitation of HLA allows us to decompose and recompose this “DIS-like” simulation architecture into one which can optimize the use of high performance computing resources to improve hardware utilization rates, as well as permit increased simulation fidelity. This is especially significant for real time graphics systems, which have traditionally acted as a high cost bottleneck to simulation scalability. The BCR IV experience offers a first glimpse of a very attractive alternative for simulation designers and users, that being a “rent a graphics pipe” solution to the scalability problem. This paradigm could also be useful in distributing and parallelizing other computationally demanding applications, such as computer generated forces, whose internal architectures have varied from “massive parallelism” to RISC-based uniprocessing. While desktop real-time graphics applications based on Pentium III architectures augmented by graphics acceleration are quite impressive from the standpoint of processing polygons, (recent experiments with PTN terrain databases in Performer on a Pentium III/Linux/Gforce configuration have yielded as many as 5 million textured polys per second), inadequate texture memory and lack of antialiasing still represent hurdles to be overcome, especially when very large terrain databases with geospecific textures must be processed.d. Desirability of High Resolution Sensor Simulation (Soldier Comments)CECOM NVESD conducted a soldier survey of the scout vehicle crews that used this system during BCR IV. Overall, the soldiers’ reaction to the high resolution sensor view was outstanding. They found the quality of the target signatures for the purpose of  recognition and identification friend or for (IFF) to be excellent, with realistic detection probabilities. They were also impressed by the long range situational awareness afforded by the simulation of on-board sensors, as well as the ability to attach to robotic ground and unmanned aerial vehicles. The crews determined that the system stability, reliability, and responsiveness needed improvement, and that further training would be beneficial to successful simulation use. Table 1 summarizes these survey results.Crew Experience Yrs/MonthsSA and BDA AccuracyRealistic TerrainHigh Res. to Sim. ViewUse in Future ExperimentsHigh Res. Terrain In Future SimulatorsPlt. Ldr1.8IIIIIGunner2.4IIIIIPlt Sgt14NIIIIGunner10IIIIITotal28.2Average7.05IIIIII=ImprovedR= ReducedN= No Change/ImpactS= SufficentNS= Not SufficentTable 1: BCR IV Operator SurveyV. ConclusionsThe BCR IV experiment represents a significant milestone for the evolution of both Paint the Night simulation technology and the aggressive exploitation of HLA capabilities. Among the technical insights achieved by utilization of PTN’s HLA prototype during BCR IV, was the performance cost associating with abstracting out the RTI using the “HLA Wrapper” approach. Future PST Federation rewrites will enable us to make better use of RTI functionality and services. With the recommended improvements to CECOM NVESD network topology, the PST Federation and VPS testbed offer an excellent environment for testing RTI performance in a highly demanding, high resolution distributed component simulation. Because RTI latency was not directly measured in BCR IV, it would be highly desirable to do so using the PST Federation, in conjunction with other HLA compatible federations, to collect performance data and enhance the insights gained during BCR IV.References:[1] Riggs, W. Unpublished Article to DMSO News, June 16, 2000[2] Stoudenmire, E. A II, Lorenzo, M. and Escobar, T. “The Remotely Locating of Simulator Display and Controls Through High Speed Video Transfer and HLA/RTI, SIW Paper 99F-SIW-124, September 1999MAX LORENZO is the Chief of the CECOM Night Vision and Electronic Sensors Directorate Virtual Prototyping Systems Branch. Mr. Lorenzo is the government sponsor and project manager for Paint the Night, a high fidelity real-time EO/IR sensor and scene simulation used to support NVESD and TRADOC Battle Lab experiments which require Paint the Night’s unique high fidelity visual system, HLA-based architecture, correlated terrain and model databases and experimental support of target acquisition, battle command and control, countermine and automated target recognition functions.  He has a B.S. in Geology from The College of William and Mary.BILL RIGGS is the SAIC Team Lead for the development team supporting the NVESD Paint the Night Program. Mr. Riggs has eleven years experience in developing computer generated forces and virtual terrain databases as part of the SIMNET, Warbreaker, STOW, and Virtual CIC programs sponsored by DARPA, STRICOM, and the Naval Research Laboratory. He has a M.S.F.S. degree from the Walsh School of Foreign Service, Georgetown University.