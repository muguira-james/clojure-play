Managing the Increasing Complexity of HLA FederationsMichael L. WalkerRaytheonBldg 805, MS B3P.O. Box 11337Tucson, Arizona  85741-1337(520) 794-9779mlwalker@raytheon.comKeywords:Grid/Cluster, HPC, Rendezvous, Zero-Conf, RAID, SANS, HLAABSTRACT: The HLA community has been using RTI technology to connect multiple federates together across networks to obtain larger system simulations.   This technology has allowed for us to solve many issues and questions that we where not able to address using standalone applications.  However, over the past few years, this technology has started to produce a whole new set of problems that must be controlled.These new testbeds span multiple machines, operating systems, languages (such as C/C++/Java), geographic regions, and organizations.  Traditionally, these systems have been managed with the human-in-the-loop approach.  Now, these testbeds are becoming too large and the direct human intervention into testbed management is becoming the limiting factor in terms of cost and schedule.  This paper‚Äôs focus is an overview of research at Raytheon Missile Systems (RMS), into how to manage these ever increasingly complex testbeds.  The various issues that are being observed in testbed design and implementation will be addressed, along with external drivers that will effect how testbeds need to evolve.  The paper will focus on technologies, which on the surface might seem unrelated (Rendezvous, IPv6, LDAP, Clusters/Grids, etc.), that RMS is exploring to address these new testbed issues.1. External Factors That Effect Our Path ForwardHLA Technology has allowed Operational Research, System Engineering, and other trade studies to be performed across the entire weapon system.  While in the past, tradeoffs had to be made in the level of detail that each component of a system would be modeled, we now have the ability to take the ‚Äúbest of breed‚Äù for each component in our federations.However, the increasing complexity of our weapon systems is generating corresponding complexity in our testbeds that represent those systems.  Investigating the interconnectivity of our traditional weapon systems‚Äîthrough Net Centric Operations (NCO) and the Global Information Grid (GIG)‚Äîwill only lead to an ever-increasing complexity in accurately modeling these systems inside a federation.2. The ProblemOver the past few years as we continue to build ever-larger testbeds, a few truths have emerged that effect our progress.  The challenges we face in designing, deploying, and maintaining these testbeds are not Moore‚Äôs Law (limit on data density), network speed, or storage space.  These issues will be solved and are being solved by commercial industry. This growth in technology, which helps us solve a new class of problems, produces an unavoidable byproduct: complexity.Until now we have relied mainly on human intervention and administration to manage this complexity.  However, it is becoming increasingly evident that this complexity is starting to become the major limiting factor in our progress in meeting our testbed requirements.  If we are to continue down the path of building larger testbeds to simulate our systems, this complexity issue needs to be addressed.  Complexity is not limited to the effects just on the setup and maintenance of a testbed. The large number of dependencies and application interactions will only make pinpointing root problems more difficult.Thus, Raytheon has invested in research into technologies that will help us manage this complexity.  The goal is to make federations that are simpler to develop, deploy, analyze, and maintain, so that we can focus our engineering talents on the real problems at hand.3. Critical Areas That Impact Our TestbedsInvestigating the lessons learned from past federation implementations shows five broad areas that need to be addressed and managed in reducing the effects of complexity in our testbeds.  The first and most obvious issue is the number of computers that are required in our federations.  This growing problem produces a whole host of issues that continue to increase our cost and schedule.  Typically, with the human intervention method of system administration, these machines are treated as individual ‚Äúunits‚Äù.  Individual management of these systems produces a diverging set of machine access (accounts), inconsistent tools (compilers, operating system versions, etc.), reactive disaster recovery plans, and so on.  As each machine continues to diverge, and as requirements differ from testbed to testbed, we get the byproduct of redundant hardware for each testbed instance.  This replication of hardware further results in a continuing exponential growth in the number of machines and thus complexity.The replication and ownership of hardware to support each instance of a testbed has produced another set of problems every time we need to build a new testbed.  By replicating hardware, we quickly approach the condition of running out of either money and/or floor space.  Even if, in the near term, money and floor space are no object, it will be quickly discovered that duplication in the maintenance costs will become a dominating factor.The third issue that must be addressed is the historical nature of how legacy simulations and new federates are designed and implemented to run in a federation.  Too many federates exist around the concept of a single federate per CPU.  To make matters worse, many federates are tied to CPU and operating system, limiting federation design flexibility.  At the same time, either through necessity or convenience, our federations are designed and built to run in real wall-clock time.  These constraints can impact the choice of federates that are used (as primary high-fidelity simulations typically don‚Äôt run in real-time), limit the scalability of the federation, and increase the money required to buy or maintain special CPUs to meet federate requirements. The next issue affects our federation designs once we are required to move to a distributed testbed.  The increased transmission latency, variation of delivery times, etc. can impact the federation‚Äôs synchronization and repeatability.  Addressing these issues may require non-real time execution, Monte Carlo setup and execution, or data synchronization (both local and distributed).The distance and number of hops a federation is required to operate over, may lead to other problems.  Long-haul latencies may introduce a new set of simulation artifacts that impact how the testbed can be used.  Along with these latencies, data sharing and data archiving increase the demands on bandwidth if a centralized data collection scheme is used.The last broad category of complexity is in using current HLA technology to meet the demands and requirements of many emerging testbeds.  Simulation of GIG and NCO requirements will affect the way we approach testbed design.  The Government mandate for IPv6, protecting of proprietary data in a competitive environment, and priority messaging are only a few examples of issues that need to be addressed in using federations in these new areas. 4. Design GoalsAs we look at how to reduce the complexity of our federations, there exists a set of goals that we would like to employ in our quest for faster/better/cheaper.  These design goals, along with the critical areas talked about above, must be addressed as a whole for them to be truly effective in their impact on reducing complexity.The first area to be addressed is that the solution(s) need to work in a heterogeneous world.  Technology will continue to evolve making any solution that is not adaptable to the next generation of hardware/operating system/software obsolete by design.This basic premise leads to the design goal that the system needs to adapt to its environment.  This adaptation to the changing environment suggests three additional requirements.The first of these three requirements is that the system needs the ability to automatically configure and reconfigure itself under varying conditions.  The first condition is a change in the environment.  An addition, subtraction, or change in hardware or networking requires a change in the configuration.  The other change in the environment could exist from external (system level) requirements.  Thus, as the federation requirements change, so should the configuration to meet these new requirements.Thus, with this requirement it would only be natural to assume the second of the three requirements is that the system should be self-monitoring.  This proactive approach to monitoring has at a minimum the ability to automatically notify the system administration when it determines it is ‚Äúsick‚Äù.   However, for self-monitoring to be truly useful, the system must be able to automatically recover (as best it can) from failures in hardware, disk drives, networks, etc.In being able to adapt to its environment and reconfigure itself, one would also expect that the system should be able to perform automatic optimization of its resources.  While performing automatic optimization, we want to keep the complexity of adapting (load balance, network balance, etc.) hidden from any user-level requirement or intervention.The last design goal is probably the most crucial of all in reaching the goal of faster/better/cheaper development and implementation of federations.   These changes must be easy to use.  We must hide the use of technology to allow the engineer to think about the problem she needs to solve, and not how it must be designed and managed at the machine level.5. Proposed TechnologiesTo address the five broad issue areas along with the six design goals of the complexity issue, Raytheon has identified seven critical technologies and areas of research that are being investigated to meet these requirements.  Currently, these seven critical areas are linked to each other and affect the total solution and goals.It is the intention of this paper to introduce the reader to these seven areas and provide a short justification for their selection.  By no means will we be able to cover the full scope and breadth that these technologies bring to the table; we will leave it to future papers to explore the use of each technology in greater detail. The first and most obvious technology that is being employed is IPv6.  The simple reason this technology is on the list is the government requirement.  However, this technology has many features that make it a natural fit into how we design and deploy testbeds.  Some of these technologies that we are investigating (and some we have already implemented) are: 1) multicast DNS (mDNS), 2) routing tables, 3) quality of service (QoS), and 4) authentication.IPv6 has been integrated into Raytheon‚Äôs E-RTI, and is active in all of the latest operating systems providing dynamic networking capability.  Leveraging the mDNS capability of IPv6 in a federation removes the need for most network configuration and maintenance.  Future work is looking into the routing tables, QoS, and authentication features to meet the demands of message priority and the protection of intellectual property.   The next technology that works hand-in-hand with IPv6 mDNS is Rendezvous [1], also known as Zero Configuration or ZeroConf.  Rendezvous is an Internet Engineering Task Force (IETF) proposed standard that is finding its way into many major operating systems, printers, games, televisions, etc. today.  This technology is also being explored inside the Navy with ‚ÄúSelf-Healing Networks on Ships‚Äù [2], and inside the Army with ‚ÄúAutomatic Networking of Soldiers in Combat‚Äù [3].Rendezvous is the basis for many of the changes in the way we think about managing testbed complexity as well as how we design and build federates and federations.    The concept of Rendezvous is simple; producers of services publish to the network what services they can provide.  Consumers of services send requests for those services over the network.  Rendezvous provides the mechanism to introduce these producers and consumers.  The classical use of Rendezvous is printers that publish that they can provide a print service, and Microsoft Word ‚Äúlooking‚Äù for those services.   This concept can directly be applied to federations where there are producers and consumers of information. Rendezvous can facilitate this mapping to produce dynamic federations.  This technology has been integrated into the E-RTI and the ERTI Framework, reducing the complexity of the setup and deployment of testbeds [4].The next two areas of investigation are in the setup and configuration of machines in a heterogeneous environment.   The first is in information storage and retrieval.    By using technology to store a single boot image/Machine-Type/Testbed, we can provide configuration control at the testbed level and not at the machine level.  At the same time, the storage of the entire set of data in a ‚Äúvirtually‚Äù central location makes finding and retrieving information simple. This concept of a central operating system configuration is not new, and has proven itself in High Performance Computing (HPC) cluster environments.  Some of the technologies that are being investigated are: 1) SANS for heterogonous network boot and file systems, 2) RAID for redundancy and performance, and 3) distributed file systems to provide virtual file systems for large scale distributed testbeds. While providing a single configuration, it is only natural to provide a single user account management system.  This feature would reduce the requirement to maintain different access lists for each operating system (if not machine), and reduce machines to glorified terminals, as people need to use them.  Thus, the employment of secure LDAP (and other solutions) is being investigated to meet these requirements.As testbeds continue to grow in size and complexity, the need exists to be proactive in monitoring the system.  This monitoring needs to occur on three different levels.  The most basic is at the hardware layer.  If we don‚Äôt know the state of the machines, hard drives, and network connections, it limits our ability to determine the root cause analysis of federation execution failures.The second level of monitoring needs to occur at the system infrastructure level.  Monitoring needs to include the CPU, hard drive, memory and network loading that testbed (as well as non-testbed) objects are having on the infrastructure.  Again, unless we understand what is happening at this level, finding the root cause of any anomalies will be much more difficult.A third level of monitoring needs to occur at the federation layer itself.  This not only includes tools that report the health and status of each federate, but also monitors the flow (and correctness, if possible) of the data traffic between federates.  A classic example is that a federate can report that everything is fine, yet if it is not receiving critical messages, then we might not be aware of a failure in the federation. The sixth area of research is in the area of synchronization and timing.  As we are required to integrate detailed (non-real time) federates, Monte Carlo trials, and large latency networks, the ability to effectively control and maintain repeatable results must be explored in some detail.The last technology that brings the results of these other technologies together into an easy-to-configure and easy-to-use solution is in the area of Grid and Cluster computing.  Raytheon is exploring how we can take advantage of these technologies to set up and control our federations.  These technologies give us the abstraction away from hardware and provide a level of fault recovery, and research is underway into automatic optimization of resources.  To date, Raytheon has demonstrated Grid and Cluster technologies to deploy and monitor testbed execution.6. How These Technologies Are EmployedThese technologies will be used to meet the goals and requirements of managing complexity as we build federations.  As we develop and share our computational resources between multiple federations, the information is stored on a series of RAID disks employing technologies like SANS.  This gives us the ability to insert our set of disks into the RAID machines and (re)boot the entire set of computers.  As the computers are powered up, they are automatically loaded with the standard configuration for that federation instance.There is thus no need to track the number of machines, machine names, IP addresses, etc.  By using IPv6 mDNS service, each machine assigns itself a unique IPv6 address that is used for communication.  Since we now have a series of computational units, without the traditional mapping (X service runs on Y machine), Rendezvous is used to automatically provide the mapping between the service provider and the service consumer.  Once the machines are booted and the mapping of services is complete, user accounts are maintained in a single manner using secure LDAP, as an example.    Thus, all machines (of a given operating system) are in effect equivalent from a setup and resource capability point of view. Logging into any machine (of a given operating system) gives the user his standard environment; he can now use the Grid/Cluster software to run his Monte Carlo trials for that federation setup.The Grid/Cluster software will also use Rendezvous to find out what machines exist and are available on the network that can provide computational services to the operator.  This Grid/Cluster software will deploy each federate to an appropriate machine, monitor the virtual cluster, and provide a real-time status to the user.  As the federates are deployed to available resources, this same technology is used by the E-RTI and ERTI Framework to establish a dynamic ‚Äúvirtual‚Äù federation network to run and complete the task at hand.  As the Grid software determines that other computational resources are available, subsequent Monte Carlo trials may be executed using these ‚Äúvirtual‚Äù federations with no additional setup.As we look into connecting a set of local machines with other machines distributed throughout the country, the above solution gives us the flexibility we need to take advantage of the increase in computing resources with little to no change in our setup.   The same technologies discussed above to configure and run locally will be used to configure and run the distributed system.  Where bandwidth, priority messages, protection of intellectual property and authentication are issues, the emerging standards of IPv6 in routing, QoS, and authentication are being investigated to meet these challenges.As research continues in the HPC community and internal to Raytheon, the ability to monitor machine health and network status and loading will lead to auto re-deploying of federates dynamically to meet the challenges of optimization and fault recovery.  Where long latency and synchronization becomes an issue, Raytheon has implemented and continues to investigate synchronization and timing issues (along with Grid technologies) to meet these challenges.7. ConclusionsRaytheon is continuing its research into managing the complexity of HLA federations.  Many technologies and approaches have been demonstrated in federation setup and management.  This capability includes the deployment of an IPv6 RTI (E-RTI) that takes advantage of mDNS, along with Rendezvous to provide a dynamic federation execution environment that reduces the complexity discussed in this paper.Research has also occurred in the use of Grid and Cluster technologies for the setup and execution of testbeds.  By using Grid/Cluster technologies along with the capabilities of Rendezvous and IPv6, Raytheon has demonstrated the ability to run federations with little to no setup as we move from design to deployment.Work continues in the area of machine setup and management, along with new methods to abstract, communicate, and monitor machines on the Grid.  As commercial technologies evolve, we will continue to evaluate these technologies for inclusion into how we build and operate testbeds in the future.8. References[1]	www.zeroconf.org[2]	‚ÄúSelf-Healing Networks on Ships‚Äù[3]	‚ÄúAutomatic Networking of Soldiers in Combat‚Äù[4]	Walker, Michael L., ‚ÄúOverview of the Raytheon E-RTI,‚Äù Paper 03F-SIW-016, SISO Fall 2003.Author BiographyMICHAEL L WALKER is a Senior Principal Engineer with 15 years of Commercial and Military experience in System Engineering, Network/Internet Technology, and Large Scale Simulations. His current assignments include military application requirements in DARPA CPU architectures/design effort, embedded processing frameworks, and HLA-RTI design for embedded and large scale federations. Mike has presented papers at the AIAA International TMD Conference, SIW/SISO, and Raytheon symposiums on System Simulation Design and Methodology. He received his BSEE from New Mexico State University and his Masters in EE from the University of Southern California.