The Role of the DARPA Communicator Architectureas a Human Computer Interface for Distributed Simulations*Alan GoldschenDan LoehrThe MITRE Corporation1820 Dolley Madison Blvd.McLean, VA  22102703-883-6005, 703-883-6765alang@mitre.org, loehr@mitre.orgKeywords:Human Computer Interface, DARPA Communicator Architecture, simulation, speech recognition, speech synthesis, natural language processing, dialogue management, conversational system, componentsABSTRACT: We describe an emerging architecture, the DARPA Communicator Architecture (DCA), that defines human computer interface (HCI) standards for advanced spoken dialogue systems [1]. We describe how interoperable components specified by the DCA support new HCI capabilities in distributed wargame simulations, such as Modular Semi-Automated Forces (SAF) simulations.The DCA is expected to emerge as an HCI standard for advanced spoken dialogue systems specifically promoting the use of interoperable plug-and-play components. The DCA is expected to define interoperability standards for different spoken dialogue components such as speech recognition, natural language processing, dialogue (discourse) management, natural language generation, and speech synthesis. We anticipate applications (such as distributed simulations) that use the DCA for its HCI to obtain benefits not previously available, such as reducing the time to integrate HCI spoken dialogue components into applications. The application has the opportunity to leverage off HCI component advancements in a variety of emerging research and commercial products with minimal integration efforts using the plug-and-play paradigm.  Additionally, the DCA's component framework allows for more flexible higher-level dialogue strategies than is possible with traditional "forward pipeline" architectures. For instance, the discovery of a previously unknown entity in a SAF is disseminated via the DCA for the speech recognition component to dynamically update its vocabulary for later recognition [2] [3]. We describe our work using the DCA to support a spoken language HCI for an Army-based SAF simulation. We describe the integration effort involved to make this SAF simulation compatible with the DCA.  Additionally, we describe how the DCA differs from other SAF system architectures that use HCI components, such as CommandTalk [4] [5] that uses the Open Agent Architecture [6].IntroductionWe introduce the DARPA Communicator Architecture (DCA) and describe its role for distributed simulations, such as Modular Semi-Automated Forces (SAF) simulations.  The DCA is expected to define conversational Human Computer Interface (HCI) component standards so that various applications can incorporate a ‘plug-and-play’ paradigm to integrate an HCI. We describe our transition from an existing SAF-simulation application [3] that uses the Open Agent Architecture (OAA) [6], to one that uses the DCA [7].  We refer to these systems respectively as the OAA-based system [3] and the DCA-based system [7].  The OAA-based system is quite similar to CommandTalk [4], a successful conversational HCI to distributed SAF simulations, developed for the DARPA-sponsored Synthetic Theater of War (STOW-97) exercise [5].  As motivation for changing a simulation HCI based on the OAA to one based on the DCA, we note that although the OAA-based. CommandTalk was highly successful at STOW-97, it was the result of several years’ development effort [4], [8], [9], [10], [11], [12].  Furthermore, CommandTalk’s components were designed to work with the OAA, and are not easily interchangeable with similar components from other developers.  A primary goal of the DARPA Communicator program, in contrast, is to speed development of conversational HCIs, by promoting ‘plug-and-play’ interoperability of spoken dialogue components. We begin (Section 1.1) by describing some components of a conversational HCI, and then (Section 1.2) describe the components of the OAA-based system.  In Section 2, we review the DARPA Communicator program and architecture, and in Section 3 we describe the DCA-based system.  This prototype is functionally equivalent to the OAA-based system, but uses the DCA instead of the OAA.  We then present initial comparisons between DCA and OAA (Section 4), and our conclusions (Section 5).1.1 Conversational Human Computer InterfaceFigure 1 depicts our primary goal: to provide an HCI that supports a bi-directional spoken conversation between a commander and a SAF simulation.Figure  SEQ Figure \* ARABIC 1:  Commander and SAF Simulation with HCI.Figure 2 depicts some of the components for the conversational HCI in Figure 1.  The components in Figure 2 typically follow the ‘forward pipeline’ execution flow control.  The commander’s input, for example, is processed sequentially by recognition, natural language interpretation, discourse processing, pragmatic adaptation, and back-end interface components.  Likewise, output from the SAF simulation to the commander uses the following components sequentially: back-end interface, pragmatic adaptation, situational awareness, natural language generation, and synthesis.Figure  SEQ Figure \* ARABIC 2: Some HCI ComponentsThe recognition component, such as a speech or gesture recognizer, converts the commander’s input into the most likely text string. The natural language interpretation component creates the corresponding linguistic content representation of the text string.  The linguistic content is in the form of logical forms [4] or frames (tokens) [7].  The discourse-processing component resolves ambiguity in the internal linguistic content representation using context tracking, historical information, previous dialogue interactions, rules of human-computer dialogue, and the current state of the simulation.  The pragmatics adaptation component converts the internal linguistic form into a set of specific application (SAF simulation) commands. Essentially, the pragmatics component contains knowledge of the application.  The back-end interface component communicates with the back-end application (the SAF), sending commands in SAF-specific syntax and semantics. Likewise, the back-end interface component converts application (SAF simulation) feedback to the internal linguistic form. The pragmatic adaptation and situation awareness components use previous dialogue history and the current state of the application to determine the appropriate response. The natural language generation component converts the internal linguistic content representation form into a sentence, which is then converted into synthesized speech for the commander’s ears.Figure 2 illustrates the components of a conversational HCI system.  The purpose of the DARPA Communicator program is to define interface standards for conversational HCI components.  The DARPA Communicator architecture does not define specific components; rather, it provides a standard interface for HCI components.   The benefit of a standard interface for conversational HCI components is to reduce their integration time and cost with large applications (such as distributed SAF simulations).1.2 The OAA-Based SystemFigure 3 describes the original OAA-based system [3].   The speech-processing components consist of the Speech Recognizer and the Text-To-Speech Agents, which respectively provide speech-to-text and text-to-speech capabilities.  The natural language processing components consist of the Dialogue Manager, Natural Language Understanding, Discourse Processing, Pragmatics, and Back-End Interface Agents.  The back-end components consist of the SAF simulation and the CCSIL Agent [13], which converts English commands to CCSIL messages.  Figure  SEQ Figure \* ARABIC 3: OAA-based HCI Components with SAF SimulationAlthough [3] describes the OAA-based system in detail, we briefly mention some highlights.  The Dialogue Manager Agent controls the system flow and monitors the SAF for events.  The Dialogue Manager Agent uses these events to create an internal discourse history representation so that subsequent references to these events are understood.  For example, the Discourse Processing Agent keeps track of all noun phrase entities by maintaining a discourse representation [2].  This representation allows the Discourse Processing Agent to resolve pronouns such as ‘it’ and ‘that’ by determining the proper noun.  The Pragmatics Agent translates the logical form from the Discourse Processing Agent into specific commands for the Back-End Interface Agent.  The Back-End Interface Agent communicates with the SAF directly or via the CCSIL Agent.  The CCSIL agent is used to send Command and Control messages to the SAF simulation.  All other messages (such as ‘create’, ‘zoom’) are sent directly to the SAF using the OAA.  We illustrate another example in which the Dialogue Manager Agent governs the execution control flow.  The Speech Recognition Agent has the capability to dynamically update its vocabulary, switch grammar regions, and generate pronunciations.  The Dialogue Manager Agent controls the first two capabilities. The dynamic grammar feature allows new words to be incorporated into the speech recognition grammar.  In the process of adding a new word to the grammar, the Speech Recognition Agent automatically generates phonetic pronunciations for the new word.  The Dialogue Manager Agent controls the grammar region of the speech recognizer to obtain good recognition performance.Figure 4 illustrates an execution flow control sequence for the OAA-based system.   These agents use the OAA for inter-agent communications.Figure  SEQ Figure \* ARABIC 4: Execution Flow Control for the OAA-based HCI ComponentsThe Speech Recognizer Agent converts the spoken utterance into a text string that is sent to the Dialogue Manager Agent. The Natural Language Agent then converts the text string into a logical form representation, which is sent to the Dialogue Manager.The Discourse Processing Agent resolves pronoun and noun phrase references and sends the updated logical form to the Dialogue Manager Agent. The Pragmatics Agent translates this logical form to a set of SAF-specific commands, which is sent to the Dialogue Manager Agent.The Back-End Interface Agent interprets the set of SAF-specific commands.  The Back-End Interface Agent uses the CCSIL Agent to send appropriate ‘Command and Control’ CCSIL commands to the SAF.   The other commands are sent to the SAF directly.A similar execution flow control sequence occurs for SAF events sent to the commander.  The Agents are executed in reverse numerical order than that depicted in Figure 4.  However, the natural language understanding and speech recognition agents are skipped in this reverse flow, and a brief acknowledgment of the user’s action is synthesized by the Text-To-Speech Agent.  We next describe the DARPA Communicator architecture.2.  The DARPA Communicator Architecture2.1 The DARPA Communicator ProgramThe goal of the DARPA Communicator program is to advance the state of the art of conversational human computer interfaces.  Current commercial interfaces support simple spoken commands and replies, in a limited domain, with strict interaction rules. The Communicator program will support complex conversational interaction, where both the user and the system can initiate interaction, provide information, ask for clarification, signal non-understanding, or interrupt the other participant.  It will operate across multiple domains and will support multiple modalities, including graphics, pointing, and gesture [1].Underlying the Communicator program is the DARPA Communicator architecture, designed to support rapid and cost-effective development of multi-modal interfaces.  The architecture will support a plug-and-play approach to interface development, so that multiple developers, from industry, government, and academia, can combine architecture-compliant commercial software and research components. Architecture development will include an evolving infrastructure and software repository that supports the rapid tailoring and integration of new interfaces. The architecture will build on emerging commercial standards in speech and language, and will evolve to support multi-participant interaction over a variety of devices (telephone, mobile wireless communicator, PDAs). The expectation is that the development of systems conforming to this architecture will create a market for components, which will encourage active commercial participation.2.2 The Communicator ArchitectureThe Communicator architecture is not intended to be fully defined at the start of the Communicator program.   Rather, it will itself be one of the results of the program. An initial architecture has been presented to the user community [1][7] to provide feedback resulting in revisions of the initial architecture.  This process will repeat several times, at nine-to-twelve month intervals, before the final architecture is stabilized.For an initial reference implementation, the program uses architecture based on Galaxy, a spoken language interface developed by MIT [7]. The initial architecture is based on a central process called the Hub, connected with a variety of processes called Servers.  Each server is responsible for a specialized function, such as speech recognition, natural language parsing, or interfacing with a back-end application such as a database. Figure 5 shows a sample configuration of the DARPA Communicator architecture, facilitating a conversation between a human and a back-end application.Figure  SEQ Figure \* ARABIC 5: Sample configuration of the DARPA Communicator ArchitectureThe Hub has three major functions:•	Routing: Handles message traffic among the distributed servers•	State Maintenance: Provides a means of storing and accessing state information for all servers•	Flow Control: Manages the progress of an utterance through its processing stages, server by server.The first two functions above are common to many hub-and-spoke architectures.  The last, flow control, is important for allowing complex dialogue interactions.  Control need not simply flow down a prescribed ‘pipeline’, starting, for example, at speech recognition, moving to language parsing, and continuing onwards until it eventually reaches the back-end application.  Rather, the Hub can be programmed to ‘fire’ (execute) the servers in a variety of orders, supporting a variety of dialogue interaction types.  If any server detects a problem, for example, the Hub has several options.  It can re-consult an earlier server, perhaps requesting alternate hypotheses from speech recognition as an aid to language parsing.  Alternatively, it can initiate a sub-dialogue with the user, such as “Did you mean Portland Oregon, or Portland Maine?”, in cases where the back-end application needs more information.To allow for such flexible flow control, the Hub is programmable via Hub ‘scripts’.  These scripts contain rules that dictate conditions when a server should be fired.3.  The DCA-Based SystemA first step in the DARPA Communicator program is the development of a conversational HCI to an existing application.  As mentioned earlier, we chose to replace the OAA inter-agent layer of the OAA-based system with the Communicator Hub Architecture.  Essentially, this step follows the Hub’s ‘plug-and-play’ paradigm since it uses existing HCI components to integrate with the DARPA Communicator Architecture (DCA).3.1 DCA-Based ArchitectureFigure 6 depicts the DCA-based architecture, which uses the DCA instead of the OAA.  These systems are functionally similar.  The major changes in switching from an OAA to the DCA Hub have been to the Dialogue Manager, the Natural Language Understanding, and the Pragmatics Servers, and the addition of the Playbox Server.Figure  SEQ Figure \* ARABIC 6: DCA-based ArchitectureThe OAA-based ‘agents’ have been changed to Hub ‘servers’.  The Hub script, described in the next section, performs the execution flow control instead of the Dialogue Manager Agent, which still manages tasks such as modifying the speech recognizer grammar region.  In addition, the DCA-based system includes a Playbox Server, as depicted in Figure 6.  The Playbox Server manages the audio communications on the commander’s host.  This separation is possible because the DCA Hub supports the transmission and storage of binary (audio) data.  Thus, the commander’s host platform may be a separate machine from those hosting any of the other servers.3.2 Flow of ControlFigure 7 illustrates an execution flow control sequence for the DCA-based.   As part of handling the execution flow control, the Hub stores global data in the form of frames (or tokens).   We first describe this control flow and then illustrate how the control flow works with the Hub script in the next section.Figure  SEQ Figure \* ARABIC 7: Execution Flow Control for the DCA-basedThe Hub first gives the Playbox Server a token to start listening to capture the audio samples of the commander’s spoken input.  The Playbox Server then adds the binary audio input data to the token returned to the Hub.  Since the token contains audio input data, the Hub passes the token to the Speech Recognizer Server to convert the spoken utterance (audio input data) into a text string.  The Speech Recognition Server, if a successful recognition occurs, adds the recognized text to the ‘speech recognition hypothesis’ field of the token.  Since the token contains a ‘speech recognition hypothesis’, the Hub passes the token to the Natural Language Understanding Server to create the corresponding ‘logical form’ representation on the token. Since the token contains a ‘logical form’, the Hub passes the token to the Discourse Processing Server to resolve pronoun and noun phrase references in the logical form.  The Discourse Processing Server then places the ‘resolved logical form’ onto the token. Since the token contains the ‘resolved logical form’, the Hub invokes the Pragmatics Server.  This server creates a set of ‘SAF-related command sequences’ on to the token. Since the token contains a ‘set of SAF-related command sequences’, the Hub invokes the Back-End Interface Server to interpret the set of SAF-related commands into specific SAF commands. The Back-End Interface Server does not return the token to the Hub until step 8.The Back-End Interface Server uses the CCSIL Agent to send ‘Command and Control’ CCSIL commands to the SAF.   The other commands are sent to the SAF directly. The Back-End Interface Server then receives a response (feedback) from either the SAF or the CCSIL Agent.  It places the ‘SAF-specific response form’ on the token, which is now returned to the Hub. Since the token contains the ‘SAF-specific response form’, the Hub invokes the Pragmatics Output Server to generate an ‘output logical form’ on the token.Since the token contains an ‘output logical form’, the Hub invokes the Natural Language Generation Server to create an equivalent (or appropriate) ‘text output string‘ on the token.Since the token contains an ‘text output string’, the Hub invokes the Text-to-Speech Server to create the corresponding ‘output binary (audio) form’ to the token. Since the token contains ‘output audio data’, the Hub invokes the Playbox Server to play the output audio data on the default audio device such as the speaker.  Finally, the Playbox Server instructs the Hub to delete the token. The execution flow control as depicted in Figure 7 follows a forward ‘pipeline’ flow control.  With the DCA, the DCA-based can follow a different execution flow control.  For example, if the Pragmatics Server (step 5) cannot find a set of SAF commands that corresponds to the given ‘logical form’, it can place an error message in the form of an ‘output text string’ onto the token. The Hub skips directly to step 11 to synthesize the error message for the user.  (The Playbox Server deletes the token after the synthesis of the error message.)A more interesting scenario is that the DARPA Communicator Hub allows for ‘what-if’ scenarios by creating additional tokens.  Assume that the Natural Language Understanding Server cannot create a ‘logical form’ from the ‘input text string’.  The Natural Language Understanding Server creates a new token that asks the Speech Recognition Server to give the next most likely recognized utterance.  The processing then continues with the newly created token containing a new ‘input text string’.  This process continues until the Natural Language Understanding Server can successful create a ‘logical form’ from the ‘input text string’ or until the Speech Recognition Server detects an error in that it has no other possibilities.  In this latter case, the Speech Recognition Server can either create an error message by creating an ‘output text string’ (where processing continues with step 11), or it can delete the token.3.3 Flow of Control (Hub Script)The DCA-based system uses a single Hub script to define the servers in Figure 6 and Figure 7, and to define the execution flow control.  A DCA Hub script consists of two main sections: one defining available servers, and the other defining rules for the execution flow of control.  Table 1 shows the definition of two of the servers in the DCA-based: the speech recognizer and the natural language understanding server.A Hub script contains five fields for each server.  The SERVER field is the name of the server.  The HOST field is the name of host where the server is currently executing.  The PORT is the TCP port number that the SERVER on HOST uses to communicate with the Hub.  The OPERATIONS field contains the names of specific routines (procedures) to invoke within the SERVER executable.  The INIT field contains values to initialize the SERVER.SERVER: recognizerHOST: rec-hostPORT: 15003OPERATIONS: reinitialize RecognizeAudio            UpdateGrammar ChangeRegionINIT: :grammar "stricom"SERVER: NLHOST: localhostPORT:15030OPERATIONS: reinitialize Do_NL_UnderstandingTable 1: Sample Hub script server definitionsMost of the information is Table 1 is self-explanatory, but we describe the OPERATIONS field of Table 1 in more detail to show how the DCA-based uses its servers:Recognizer Server – The Recognizer Server routine RecognizeAudio receives a token with the binary data and finds the most likely recognized string.  The routines UpdateGrammar and ChangeRegion are invoked when the Dialogue Manager Server has created tokens to add new words to the grammar or to switch grammar regions, respectively.NL – The Natural Language Understanding Server uses the routine Do_NL_Understanding to convert the text input string (from the speech recognizer) into a logical form representation.The next section of the Hub script uses rules to provide the execution flow control.  The Hub script uses a ‘rule-ordering’ conflict resolution strategy - which means that the first rule to trigger is fired (executed).   Table 2 depicts the rules used for the same two servers (recognition and natural language understanding). The consequent of each rule corresponds to the name of a routine (procedure) to invoke when the antecedent is true.  The IN parameter indicates the parameters (variables) that the routine may access within the token.  A server routine may choose to change, add, or delete hub tokens.;; this rule takes the output of recognition,;; and passes it to NL;; it also echoes something to synthesis, for;; acknowledgement to the userRULE: :sr_hypothesis --> Do_NL_UnderstandingIN: :sr_hypothesisOUT: :nl_output :synthesis_text;; this rule takes the output of NL, and;; passes it to Context TrackingRULE: :nl_output --> Do_CTIN: :nl_output :sr_hypothesisOUT: :ct_outputTable 2 – Sample Hub script rules4.  A Comparison Between the OAA and the DCAThe previous section described a spoken language interface to an Army ModSAF, using the DARPA Communicator Architecture (DCA).  As mentioned, we have also earlier implemented the same such interface using the Open Agent Architecture (OAA).  The two implementations differ only in the inter-process mechanism used – the DARPA Communicator Hub or OAA.  All other aspects of the two implementations are identical. That fact that a single system has been implemented using two inter-process mechanisms offers a unique opportunity to compare and contrast the two mechanisms.  Both are touted as desirable solutions for building complex systems out of heterogeneous distributed components.  What are the benefits of each?We discuss the differences between the two on a number of points.•	Module Types: The DCA refers to its modules as servers, while the OAA refers to them as agents.  OAA agents may exhibit all the autonomy the name agent implies.  In contrast, the servers of the DCA function as their name implies–as processes, which simply provide services.  In practice, however, both OAA agents and DCA servers are alike in that each can independently initiate or respond to messages.•	Central Processing Module: While the DCA has its Hub, the OAA has a central “facilitator” agent called the Blackboard.  The Blackboard shares two of the Hub’s three functions: inter-process routing and global data storage.  Like the Hub, the Blackboard also facilitates the flow of control.  However, it does not do so explicitly, while the Hub does  (see next bullet).•	Flow Control: Both schemes provide for a flexible flow of control. However, while flow of control is explicitly programmed in the DCA, in the OAA it is determined autonomously by interactions between the agents.  The OAA’s Blackboard merely facilitates the interactions, by providing a central registry for agents, and by matching requests for services with agents capable of providing those services.  In sum, the DCA allows for programmable but pre-determined flows of control, while the OAA allows for dynamic, but not pre-determined flows of control.•	Interprocess Language: The Hub is programmed via a Hub script, a set of rules that determine which server to fire upon which conditions.  Essential to a Hub script is the idea of tokens.  Tokens are passed, along with accompanying information (stored in frames, or hierarchical structures of key-value pairs) between servers and the hub.  The Hub uses these tokens, and the frames they contain, to determine the rules to fire.  The Hub script, tokens, and frames use a syntax and semantics unique to the DCA.  In the OAA, agents communicate via a language called the Interagent Communication Language (ICL) [6], an extension of Prolog.•	Support for Complex Dialogue Interactions:  Because the flow of control in the DCA is explicitly programmable, complex dialogue interactions (such as nested sub-dialogues) can be directly implemented in the Hub.  In the OAA, this must be implemented by the presence of a “super-agent”, which intercepts all communications with the Blackboard, and, based upon the state of affairs, makes specific requests to the Blackboard for specific agents to be invoked.  Doing this, however, takes away the dynamic agent interactions for which the OAA was designed. •	Dynamic Module Addition: Another dynamic feature of the OAA is that agents may register and participate “on the fly”, while servers in the DCA must be known to the Hub at startup. Note that in both schemes, developers of a system typically know which modules will be present ahead of time, and tailor their system accordingly.  No module will be invoked (whether via a Hub script or the OAA) unless its capabilities match those needed by other modules.  In reality, then, it is not possible for just “any old” OAA agent to show up dynamically, and contribute to a system’s performance.  However, the OAA does allow redundant agents to attach dynamically, in cooperation with existing agents that provide the same service.•	Interprocess Debugging: A small but important point concerns debugging interactions between modules.  In the DCA, because the Hub script must know about servers and their capabilities, the meta-information about information to be exchanged must be kept in two places: the server source code, and the Hub script. (By meta-information, we mean information such as is published in Application Programmer Interface (API); for example, the number and types of parameters that may be passed.)  Any change in one location necessitates a corresponding change in the other.  Unlike an API, however, there is no compiler that will automatically detect incompatibilities for the programmer.  In the OAA, such central meta-information is not required.  Note, however, that the OAA does allow such central meta-information, with agent-specific triggers (similar to Hub rules) placed on the Blackboard.•	Binary Data: Another small but important point concerns passing binary data (such as audio) between modules.  Typically, all communications pass via the central module: the Hub or the Blackboard.  Neither of these, however, can handle binary data directly.  The Hub has some limited support to establish binary data transmission, while the OAA requires a separate connection between the other modules through which the binary data flows.  This lack of full support for binary data defeats the generality of the central module.Table 3 summarizes the above points, along with several other miscellaneous items.OAADCAModule TypeAgentServerCentral Processing ModuleThe BlackboardThe HubFlow ControlDynamic, Not Pre-DeterminedProgrammable, Pre-DeterminedInterprocess LanguageICL, with unique semantics and standard syntax (Prolog)Hub script, tokens, and frames, with unique syntax and semanticsSupport for Complex Dialogue InteractionsCan be implemented in “super-agent”, at expense of dynamic flow of controlCan be implemented in Hub using Hub script rules.Dynamic Module AdditionYes - But in practice, only redundant modules will be effectiveNo - Modules must be known at run-timeInterprocess DebuggingMeta-information kept only in agentsMeta-information must be kept in two places (Hub and servers)Binary Data through Central ModuleServers must set up direct binary connection between each otherLimited support; Hub sets up direct connection between agents, which then pass binary data between each otherModules can be distributed across networksYesYesMaintain central state informationYesYesTable 3. Comparison of the OAA versus the DCA5. ConclusionWe describe a system that uses the DARPA Communicator Architecture (DCA) to interface conversational HCI components with a SAF simulation.  We discuss how the DCA differs from the Open Agent Architecture (OAA), which has been successfully used in similar applications such as CommandTalk.  We mention lessons learned in the conversion from an OAA-based HCI system to one based on the DCA.  Although the DCA is currently available only as an initial draft implementation, its goal is to arrive at a mature architecture that will define a standard for spoken dialogue component interoperability. The existence of such an HCI standard will allow application developers to use the ‘plug-and-play’ paradigm to select appropriate HCI components and rapidly develop spoken dialogue interfaces.6. References[1] 	DARPA Communicator Web Page, http://fofoca.mitre.org/, 1998. [2]	S. LuperFoy, D. Loehr, D. Duff, F. Reeder, L. Harper, and K. Miller, “An architecture for dialogue management, context tracking, and pragmatic interpretation in a spoken dialogue system", in proceedings, Association for Computational Linguistics '98, August 1998.[3]	M. Peet, P. Varma, and D. Loehr, “Strategies for Effective Human Computer Interaction,” MITRE Technical Report, 1997, (not in public domain).[4]	R. Moore, J. Dowding, H. Bratt, J. Gawron, Y Gorfu, and A. Cheyer, “CommandTalk: A Spoken Language Interface for Battlefield Simulations,” SRI International Report, May 30, 1996.[5]	A. Goldschen, L. Harper, E. Anthony, “The Role of Speech in a Distributed Simulation: The STOW-97 CommandTalk System,” in Proceedings of the Seventh Conference on Computer Generated Forces and Behavior Representation, Orlando, FL, 1998.[6]	P. Cohen, A. Cheyer, M. Wang, and S. Baeg, “An Open Agent Architecture,” in AAAI Spring Symposium Series, Software Agents, Stanford, CA, pp. 1-8, 1994.[7]	S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, and V. Zue, "Galaxy-II: A Reference Architecture for Conversational System Development, " in Proceedings of ICSLP 98, Sydney, Australia, November 1998. [8]	H. Bratt, J. Dowding, M. Gawron, Y. Gorfu, and B. Moore, “Technical Overview of the CommandTalk System,” SRI International Report, January 30, 1996. [9]	J. Dowding, J. Gawron, D. Appelt, J. Bear, L. Cherney, R. Moore, and D. Moran, “Gemini:  A Natural Language System for Spoken-Language Understanding,” in Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, Columbus, Ohio, pp. 54-61, 1993.[10]	J. Dowding, R. Moore, F. Andry, and D. Moran, “Interleaving Syntax and Semantics in an Efficient Bottom-Up Parse,” in Proceedings of the 32nd Annual Meeting of the Association for Computations Linguistics, Las Cruces, New Mexico, pp. 110-116, 1994.[11]	Dowding, J., “ModSAF Agent Language,” SRI International Report, September 8, 1996.[12]	The Nuance Speech Recognition System, Version 5, Nuance Communications, Menlo Park, CA, 1996. [13]	A. Goldschen,  “The Role of the CCSIL Agent for Distributed Simulations,” 1997 Spring Simulation Interoperability Workshop, Orlando, Florida, March 1997.Author BiographiesALAN J. GOLDSCHEN is a Lead Engineer with The MITRE Corporation Signal Processing Center and holds a Ph.D. in Electrical Engineering/Computer Science from George Washington University.DAN LOEHR is a Senior Artificial Intelligence Engineer at The MITRE Corporation, working on a variety of projects involving speech and natural language processing.  He is a Ph.D. candidate in Computational Linguistics at Georgetown University, with interests in computational discourse.* This work has been partially supported by DARPA contract number DAA-B07-99-C-C201. We use the term ‘frames’ as the natural language internal data structure since that is what the DARPA Communicator Architecture currently uses.  Command and Control Simulation Interface Language The Speech Recognition Agent uses the Nuance Speech Recognizer [12]. A requirement for this task is that the Dialogue Manager Agent notifies the Discourse Processing Agent when SAF simulation events occur.