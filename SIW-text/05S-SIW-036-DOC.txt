DTE 4: Verification and Validation Considerations for Distributed Test CapabilitiesRalph LiebertUS Army Electronic Proving Ground Ft. LewisCSTE-DTC-EP-TT-LFt. Lewis, WA 98433253-967-8106 HYPERLINK "mailto:liebert@epglewis.dtc.army.mil" Ralph.liebert@epglewis.dtc.army.milJennifer ChewUS Army Developmental Test CommandCSTE-DTC-TT-MAberdeen Proving Ground, MD410-278-1338 HYPERLINK "Jennifer.Chew@dtc.army.mil" Jennifer.Chew@dtc.army.milWilliam FredericUS Army Electronic Proving Ground Ft. LewisCSTE-DTC-EP-TT-LFt. Lewis, WA 98433253-967-5644  HYPERLINK "William.frederic@epglewis.dtc.army.mil" William.frederic@epglewis.dtc.army.milMichael J. O'ConnorITT Industries6767 Old Madison Pike, Suite 160Huntsville, AL 35806256-964-1470 HYPERLINK "Michael.oconnor@itt.com" Michael.oconnor@itt.comKeywords:Synthetic Environment, Distributed Test and Evaluation, Verification & Validation, Future Combat SystemABSTRACT: Army Test & Evaluation Command (ATEC) is maturing a distributed test capability to enable testing of complex Army systems in increasingly complex mission environments.  Several ATEC programs have been leveraged to help create the basis for integrating test capabilities throughout the command’s remote sites.  These integrated test capabilities have been demonstrated most recently in an event conducted 30 AUG to 02 SEP 04, the Distributed Test Event 4 (DTE 4).  In DTE 4, ten geographically dispersed sites were linked with the Defense Research Engineering Network (DREN) and common infrastructure developed through the ATEC Test Integration Network (ATIN) program.  Live, virtual and constructive (LVC) test systems and environments were brought together to realize the benefits of synergistic effects from the integrated test environment and stimuli to enable testing complex Army systems.  Simulated environment representation were provided from the Synthetic Environment Integrated Testbed (SEIT), which is implementing high quality test environment representations and stimuli in a doctrinally correct tactical battlespace for common application throughout ATEC testing.  Paramount importance to the effectiveness, acceptance and use of distributed testing relying on live and simulated elements is Verification and Validation (V&V) of the overall environment leading to Accreditation.  When conducting V&V in an integrated and distributed LVC test environment, there are many key simulated/stimulated elements and unique physical assets to be considered.This paper will discuss the DTE 4 capability and the novel requirements for V&V of distributed test capabilities.  This paper will also discuss the unique considerations that should be accounted for in V&V process and accreditation planning due to the unique aspects of distribute testing environment.  Specifically, components that enable distributed capabilities such as architecture & network and the impact of simulated and live test element interaction and integration are some examples.1. IntroductionCurrent and Future US Forces along with Coalition Partners are challenged to operate in increasingly dynamic and challenging battlefield environments, while facing opposition forces employing asymmetric threats. The US Army is developing the Future Combat System (FCS) that is based on Network Centric Warfare (NCW) tenants and is challenging the acquisition model currently in use. To face this challenge, the United States Army Test & Evaluation Command (ATEC) is maturing a mixed Live, Virtual and Constructive (LVC) distributed test capability to help enable testing of complex Army systems in increasingly complex mission environments to help inform evaluations and acquisition decisions. Paramount importance to the effectiveness, acceptance and use of distributed testing relying on live and simulated elements is Verification and Validation (V&V) of the overall environment leading to Accreditation.  When conducting V&V in an integrated and distributed LVC test environment, there are many key simulated/stimulated elements and unique physical assets to be considered.To develop the distributed test capability ATEC is engaged in a program to tie its geographically dispersed test centers together to benefit from the resultant synergies realized from the integration of multiple test capabilities functioning in an interactive and dynamic test framework. Several ATEC programs have been leveraged to help create the basis for integrating test capabilities throughout the command’s remote sites. These integrated test capabilities have been demonstrated most recently in an event conducted 30 AUG to 02 SEP 04, the Distributed Test Event 4 (DTE 4).  In DTE 4, ten geographically dispersed sites were linked with the Defense Research Engineering Network (DREN) and common infrastructure developed through the ATEC Test Integration Network (ATIN) program. The distributed test capability is supported by LVC test tools developed through DTC investment in instrumentation and modeling and simulation (M&S), developed through the Virtual Proving Ground (VPG). Many of these synthetic environments and test capabilities are integrated into the standardized and interactive Synthetic Environment Integrated Testbed (SEIT), which is an implementation of high quality test environment representations and stimuli in a doctrinally correct tactical battlespace for common application throughout ATEC testing.  The DTE 4 capability enables the integration of dispersed test capabilities currently in use at the ATEC test centers supporting Army materiel Test and Evaluation (T&E), providing a standard and optimized natural and man-made testbed. High-resolution environmental representations include the relevant radio frequency (RF) environment for testing Army communications equipment, an infrared (IR) environment for testing IR-based targeting sensors, and a chemical and biological (CB) threat environment for testing CB agent sensors. Also integrated into the DTE is a mobility server for determining platform mobility effects due to terrain and a robotics intelligence model to emulate the actions of an autonomous Unmanned Ground Vehicle (UGV). In addition 15 live systems emulating FCS manned and unmanned platforms and sensors as well threat systems are integrated seamlessly into the overall LVC test environment. These high-resolution and live emulated test components and environment representations are immersed and interact with a constructive semi-automated forces (SAF) model, OneSAF Testbed Baseline (OTB), providing an overarching environment that provides blue and red force structures and doctrinally correct tactical scenarios as well as low fidelity tactical platforms to associate high-resolution virtual or live test systems. Table 1.1 contains information about the ten DTE 4 participating sites and the LVC capabilities.The DTE 4 capability integrates high-resolution representations of the natural and man-made environment from physics-based M&S and live test capabilities using the High Level Architecture (HLA) and legacy Distributed Interactive Simulation (DIS) architecture to facilitate dynamic interactions among components. The DTE 4 employs a distributed test framework founded on the High Performance Computing Modernization Program’s (HPCMP) Defense Research and Engineering Network (DREN), which extends to the ten geographically dispersed DTE 4 participant sites.Particularly, the DTE 4 is focusing on the integrated contractor/government testing planned for the US Army’s Transformation program the FCS. The DTE 4 team is working closely with the FCS Lead System Integrator (LSI) (Boeing-SAIC Team) through the Combined Test Organization (CTO). The CTO is a tripartite of FCS LSI, ATEC, and FCS Program Office personnel responsible for planning and coordinating integrated T&E for the FCS. DTE 4 team is coordinating with the FCS LSI T&E and M&S personnel to insure that development of M&S and live test capabilities supporting distributed testing for ATEC supports the FCS test requirements. The original requirement is to support the FCS LSI-lead Integration Phase System Development and Demonstration (SDD) testing (IPS-1) scheduled for 3Q FY05 and documented in the Integration Phase Plan 1 (IPP-1) [1].  The IPP concept of testing has evolved and now the emphasis for the DTE capability is to be focused on the Integration and Verification (IV) testing and the experimentation.  IV 0 phase testing is documented in the FCS SDD IV0 plan [2] and experimentation we plan to support is documented in the FCS SDD Experiment Increment 1.1 [3].SITETEST MISSIONLIVEVIRTUALCONSTRUCTIVEAberdeen Proving Ground (APG)Ground MobilityStrykerVehicle DynamicsMobility ServerRobotic InteligenceLethality/VulnerabilityAviation Technical Test Center (ATTC)Rotary Wing Platform / RSTA Sensors Flight DynamicsIR Sensor Dugway Proving Ground (DPG)Chemical and Biological DefenseStrykerChem/Bio SensorWeather Effects Electronic Proving Ground (EPG)Command, Control, Communications,Computers, andIntelligence (C4I)Shadow 200(UAV simulator)RF NetworksC2 NodesBattle CommandNight Vision Electronics and Sensor Directorate (NVESD)*RSTA Sensor R&DGround Platform Simulator Platform Sensors Operational Test Command (OTC)ArmyOperational TestingNetworked ObserverRedstone Technical Test Center (RTTC)Small Missiles /RSTA SensorHelicopter (UAV)IR SensorsPlatform SensorsAerial SensorsOTBWhite Sands Missile Range (WSMR)Large Missiles5 T-72sHelicopter (UAV) TOC, JANUSYuma Proving Ground (YPG)Ground MobilityEnvironmentalStrykerNLOS-C Emulator  * NVESD is not in ATEC, it is part of the Research, Development and Engineering Command (RDECOM)Table 1.1. DTE 4 Participants, Missions, and LVC capabilities1.1 BackgroundThe DTE program started as an initiative by the US Army ATEC Developmental Test Command (DTC) VPG, which includes personnel from the seven DTC test centers, and has the mission to integrate M&S technologies into the Army T&E. VPG chartered its Synthetic Environment Focus Group (SEFG) to develop an integrated and standard environment representation to support testing. The SEIT program was developed to bring together mature test environments in an interactive and dynamic framework that contribute to a distributed test capability. As noted in addition to SEIT several other ATEC programs and VPG Focus Group activities, notably the Tools FG, are brought to bear to develop ATEC distributed testing. In addition a distributed test infrastructure, the Inter-Range Control Center (IRCC) at WSMR and the Distributed Test Control Centers (DTTC) implemented at ATEC test sites, provide a robust distributed test framework.DTC completed three IOC developments and demonstrations during CY03 with increasing levels of capability. Previous papers and reports describe IOC 1, 2, and DTE 3. The IOC 1 Final Report [4] and a paper for the 2003 US Army Simulation and Modeling for Acquisition, Requirements and Training (SMART) Conference [5] describe the IOC 1. The technical details for IOC 2 can be found in the Demonstration Control Document for IOC 2 [6].  In addition to the aforementioned IOC programmatic and technical descriptions, two Simulation Interoperability and Standards Organization (SISO) papers, 04S-SIW-082 [7] and 04E-SIW-022 [8], describing DTE 3 were prepared and presented at the Spring 04 SISO Integration Workshop (SIW S04) and at the European 04 SIW (SIW E04).  The most recent demonstration of the distributed test capability, DTE 4, was conducted 30 August to 02 September 2004.1.2 DTE 4 OverviewDTE 4 built on the foundation established in IOCs 1 / 2 and DTE 3 and occurred in August 2004. Technologically, DTE 4 was similar to DTE 3. This event reused the threads, meteorology inputs, and architecture previously described; although the DTE team optimized some of the underlying thread capabilities to support FCS test requirements and new terrain for the WSMR ranges was developed.  DTE 4 is unclassified. In addition, the scenario was changed to reflect nine Mission Execution Threads (METs) developed by the FCS LSI to support the first integrated testing for FCS and centered around the Future Force element scheduled to employ the FCS materiel, the Unit of Action (UoA).Through the decomposition of the UoA mission and nine METs into tasks related to FCS required capabilities, a TOEL is developed to drive the distributed test and provide requirements traceability to ensure the generation of test data that result with valuable information to support FCS evaluation. This test framework and process generates data for the assessment of capabilities and limitations of components, subsystems and the overall SoS in an appropriate mission context. Additionally, the TOEL provided a “script” to conduct distributed testing by indicating simulation activities, operator activities, and data collection points and drove some data collection elements for C4ISR testing. Table 1.2 shows some the statistics from the demonstration and this paper and others submitted to SISO for the S05 SIW provide greater detail.ItemValue1.  Duration of scenario:89 minutes2.  Number of Mission Execution Threads:9 METs3.  Number of sites participating:12 sites4.  Number of different time zones for sites:5 time zones5.  Number of entities:334 entities, 217 blue, 117 red6.  Number of computers:140 computers7.  Number of simulations:35 different simulations, multiple instances8.  Number of operators:70 operators / ~ 300 Total9.  Number of live platforms:13 live platforms10.  Number of test ranges involved:6 test ranges, operating concurrently11.  Amount of bandwidth used over network:80 Mbps at IRCC, 40 Mbps at RTTC, 10 to 20 Mbps at each site12.  Number of days to establish site connections:3 days for DTE 4 vs. 21 days for DTE 3 and 50 days for IOC 213.  Number of pages in Time Ordered Events List:350 pagesTable 1.2. DTE 4, 30 August to 02 September, Statistics 2. Models and Simulations and Distributed Test CapabilitiesTable 2.1 shows the M&S and live elements integrated into interacting threads. Integrated components for DTE 4 are distributed across the DREN from the ten sites using an HLA architecture backbone. A detailed description of these capabilities is provided in a paper in this forum by this author, “DTE 4: ATEC Distributed Test Capability to Support FCS Testing”, 05S-SIW-035.M&S / Test Tools						ProponentC4 CapabilitiesRole Player Workstation (RPWS)						EPGDigital Army USMTF/VMF Stimulator (DAUVS)				EPGSimulation Application Suite (SAS)						EPGOrion						EPGInfrared (IR) Capabilities	Infrared Scene Generator Using MPI Vega					RTTCOpenSceneGraph Scene Generator						RTTCPaint the Night						NVESDChemical / Biological (CB) Capabilities	Nuclear, Chemical, Biological and Radiological Environment Server (NCBR)	DPGChemical/Biological Dial-A-Sensor (CB DAS)				DPGDetonator						DPGMeteorological Capabilities	Four Dimensional Weather (4DWX) System					DPGOcean, Atmospheric and Space Environment Server (OASES)			DPGDistributed Test Support	MÄK Stealth 						AllMÄK Logger 						AllMÄK DIS/RPR FOM Gateway						AllMÄK Plan View Display (PVD)						AllStarship						EPGRemote Reconfigurable Intelligent Instrumentation to Control,Collect, Stimulate, and Simulate (RICS)2					EPGDigital Collection, Analysis, and Review System (DCARS)			EPGTest Conduct and Reporting System (TCRS)					EPGGeneral	One Semi-automated Force (OneSAF) Testbed Baseline (OTB)		RTTCVehicle Dynamics Mobility Server (VDMS) 				ATCRobotic Intelligence 						ATCLethality Server						ATCYPG Live Interface						YPGWSMR Live Interface						WSMRDPG Live Interface						DPGFLIGHTLAB								ATTCAdvanced Concept Research Tool (ACRT)					NVESDComprehensive Munition and Sensor Server (CMS2)				NVESDUniversal Controller (UC)							ATCMissile Simulator								YPGDrone Formation Control System (DFCS) UAV/Threat Tank Simulation	WSMRTable 2.1 M&S and Test Tools used in DTE 43. DTE 4 V&V / Configuration ManagementTwo areas of importance for the effectiveness, acceptance and use of distributed testing relying on LVC elements are Verification and Validation (V&V) and Configuration Management (CM). Both V&V and CM are necessary elements to support the use of test data generated in the distributed test environment for evaluation and to inform milestone decisions. Both V&V and CM are addressed at for DTE 4.3.1 DTE 4 CMThe DTE 4 CM plan is for the overall environment to include simulation components, test systems and requisite documentation.  Although the CM plan is not fully implemented due to time and resource constraints, revolving around the fact that the DTE 4 is a developmental demonstration of maturing distributed test capability and not an actual test.3.2 DTE 4 V&VThe DTE 4 V&V plan is to rely on the existing Verification, Validation and Accreditation (VV&A) available for many the component capabilities due primarily because many of the capabilities are in use for testing at various test centers and require accreditation to support T&E.  Although this component VV&A is hardly appropriate for a VV&A for an overall integrated environment. The basis for the V&V plan for DTE 4 is also driven by the resource and time limitations and the developmental aspect of the program. When conducting V&V in an integrated and distributed LVC test environment, there are many key simulated/stimulated elements and unique physical assets to be considered. Now that an appreciation for the complexity of the distributed test capability developed in DTE 4 is provided, the remainder of this paper will deal with V&V considerations.4. V&V GeneralThere are numerous regulations and standards governing requirements for VV&A and there is much variability in the interpretation V&V tenants related to application and scope of these regulations.  The development of the ATEC distributed test capability, DTE 4, is primarily governed by the VV&A requirements set down in Department of Defense (DoD), Department of Army (DA) and ATEC regulations.4.1 M&S in T&E V&VArmy Regulation 5-11, Management of Army Models and Simulations, mandates that all M&S used for system acquisition be verified and validated to support accreditation for specific applications.  In addition, Army Regulation 73-1, Test and Evaluation Policy, requires that ATEC conduct and/or support the V&V of all M&S used in T&E and accredit any M&S used to support system evaluation.  In the event that an M&S tool is modified and/or used for a new purpose, or after five years since the last accreditation, and it is used to support the Milestone decision or T&E activities, the VV&A process will apply.  The approach is to verify and validate the M&S tools based on their risk and use impact of the tools to meet acceptability criteria and program requirements.  The risk and use impact of the M&S for the acceptability criteria will help prioritize and focus on critical V&V activities, reducing risks and resources.  Accreditation certifies that an M&S tool is suitable for a particular use, as defined by the use case.4.2 Distributed Test Environment V&V ConsiderationsThe regulations set down by DoD, DA and ATEC govern the typical requirements for VV&A of M&S capabilities used in T&E.  They do not particularly address V&V requirements for distributed test environments, although many of the rules governing the accreditation of M&S for use in acquisition and T&E still apply.  There are several unique considerations in a distributed test environment that must be accounted for in the V&V plan to ensure proper accreditation.  Some of the most important are discussed but this is likely not inclusive and will hopefully serve as starting point for community discussion and ultimately development of standards and amendment to regulations.4.2.1 Network and ArchitectureThe network and architecture are required components to link capabilities to enable distributed testing.  Quality of Service (QoS) or delivery of information and latency due to the network are critical factors in V&V for distributed testing.  In DTE 4 the DREN was carefully configured to maintain reliable distributed traffic at a high level of performance and an Active Measuring Program (AMP) was instituted allowing for assessment of network performance both locally at participating sites as well as globally across the DREN.  Another factor for networking is security. Depending on the level of security the mechanisms to enable secure environments like encryption devices will have effects on latency.  Network latency and performance measurements are critical to perform prior to an actual distributed test event as part of V&V testing using appropriate network loads to ensure QoS and low latency.The architecture and object model used to support data transfer among distributed test components must be well organized and constructed to enable proper transfer of data among components and for data collection.  Artifacts due to architecture, such as irregularities in data collection, must not be introduced and this needs to be assed during V&V testing.  Comprehensive documentation is a must for the set up and implementation of the architecture.  In DTE 4 a mixed HLA and DIS architecture with gateways was used and the characteristics are well documented.Linked to the network and architecture implementation is the use of reliable versus best effort data transfer.  Based on the importance to evaluation of the data transfer will drive weather information between components in a distributed test environment will be reliable or best effort.  The decision will drive latency budgets and will need to be accounted for during V&V.4.2.2 LVC Interaction EffectsAnother factor that is somewhat unique in distributed testing is the opportunity to integrate typically disparate LVC test elements and environmental stimuli.  The capability to make components in a distributed test environment sensitive to one another effects or outputs is important to realizing the synergistic effects from bringing different test elements together.  The outcomes of these fusions must be anticipated, documented and measured for proper V&V.  One such integration in DTE 4 is the real time meteorological effects on chemical agent dispersion, which is well documented and understood.  As more of these environmental inputs are implemented a good appreciation for there outcomes will need to be assessed in V&V.Interface and injection of digital stimuli to actual (live) and virtual test item prototypes must be well understood for valid data generation in any test environment but particularly for distributed testing which relies to greater extent on these interactions than typical testing.  Both the digital stimuli and physical interference from a simulation to a live device must be well characterized, understood and documented during V&V.  In DTE 4 an example is the stimulation of live IR RSTA sensors with realistic IR energy from and IR projector which has been well documented and undergone V&V for testing at RTTC.  Also, the use of high resolution chemical agent dispersion concentration prediction stimulus to a virtual representation of chemical agent standoff sensor, which is also well understood and documented for testing.  As the number of these interactions between the digital stimuli and test representations or systems increases we will continue to need to assess these in V&V.4.3 Solutions for Distributed Test V&VV&V of an integrated distributed test environment is performed incrementally throughout the development process as lower level models are integrated into higher-level composition of models or simulations.  When large number of models and test infrastructure components are composed into complex and distributed environment, it is imperative to decompose each of the model and test infrastructure components to better understand their contribution and use impact level to the overall mission.  Their contribution and use impact level are keys to determining the V&V efforts needed and to addressing how each component plays out in the overall integration.  A decomposition structure can be used for determining how the model V&V could be expanded and traceable to support a federation of LVC elements.  In order to accomplish the V&V objectives and adequately prepare for accreditation of a large and federated distributed testing environment, V&V effort must be performed at many levels and should be performed as the lowest-level possible.  Some of the decomposed elements to be considered include, but not limited to, Input/Output parameters, architecture, network, structure, and dynamic behaviors in order to validate the significant aspects of a model that is a component of a federation.In the planning stage, it is also important to develop a template for documenting the V&V evidence of the models, data, test infrastructure, and simulation results. [9]  The structure should compose of acceptability criteria, reasoning for the decomposition, and V&V evidence needed for traceability.  The V&V template will provide a means to document the V&V evidence, including administrative information, summary of individual and composite plans and/or results, V&V evidence, and V&V resources.  Special V&V consideration must also be given to distributed testing environment where exchanging of accurate and comparable data between the elements and the latency factors for a combination of components are important. V&V must assess the overall performance, correctness, and realism of the integrated environment, and must determine if the integrated environment will provide sufficient and meaningful data for the intended application.  This V&V strategy requires that appropriate V&V has been performed and documented at the lower level before integrating into the next higher-level of model or federation.  It encourages the reuse of artifacts to further support higher level and lower level as appropriate.  The ultimate goal is to support the accreditation of the totally integrated environment by providing a complete body of evidence throughout the incremental process.4.4 Risk Analysis for V&VRisk analysis should be conducted for each model based on its contribution to the overall federation integration.  Table 4.1 shows examples of models used in a federation, and their use impact and contribution risk to the federation.  Once those risks are determined, then the method for performing the V&V and the data and evidence for support the V&V are identified.   It is preferred to conduct verification and validation separately. Verification focuses on software correctness, consistency, and completeness while validation focuses on the appropriateness of the intended application. [10] V&V EvidenceRisk/ImpactAnalysis (H/M/L)VerificationMethodValidationMethodLive TestDemonstrationAnalysisInspectionIntegrated FederationSensitivity AnalysisSub-model/ Module TestXXCommunication Network ModelHStructural AnalysisRegression TestXXOTBHFunctional TestComparison TestXXPropagation ModelMTraceability AssessmentAcceptance TestXAttenuation ModelLData AnalysisPredictive ValidationXXTransport Layer Modeling (Protocol)LInspectionFace ValidationXXNW Layer Modeling (IP)LStatic Code AnalysisInspectionXXIR Sensor ModelHSensitivity AnalysisFunctional TestXXEtc.Table 4.1 Example of risk analysis for V&V4.5 Distributed Test Environment V&V Use CaseAs noted, V & V requirements in a distributed test environment require additional tests over the traditional V &V of a system. Multiple iterations of the verification are necessary, and while those organizations that have conducted complex experiments, tests, and military training events, understand the requirements, traditional V&V documentation (DA Pam 73-1, etc) are primarily oriented to single system verification. Distributed test environments require a phased or spiral approach to verification. Table 4.1 shows an example of a typical V&V schedule.  Note that validation needs only to be accomplished twice, since the value of the output is typically not changed, but the veracity of the data is subject to many variables, often not under the control of the owner of a specific system.IterationDistributedVerificationValidationNotesStand-aloneNoYesYesTraditional V&V appliesPair-wiseNoYesNoMultiple Verification tests with each system producing data for use by your system.System of Systems – Engineering levelYes, by necessityYesNoMultiple Verification tests conducted with technicians to fully verify each data path.Integrated Mission TestingYesYesYesOne or more V&V events with intended users using the system of system as intended.Table 4.2 Shows four levels of Distributed Test Environment V&V4.6 Explanation of the four levels of verification testingStand-alone. V&V uses the traditional methods of verification and validation of a single system.Pair-wise. Verification entails a test of the primary system interacting or receiving data from each system it is designed to interact with, independently from any other system.  This is typically accomplished in a non distributed mode using the actual system or log files.  Distributed pair-wise testing can be accomplished, but it is more time consuming and requires robust communications between the participating laboratories.System of Systems –Engineering level. This is a test or series of tests (more typical) where all of the participating systems are activated, one at a time until all systems are active and passing their appropriate data.  Engineers are then permitted to test different inputs and outputs which may occur frequently or infrequently during an integrated mission test.  This approach, while somewhat undisciplined, permits all development teams to verify that they are using the data received properly and to verify operations and data with engineers on active systems.  This process typically spans several days and, not infrequently, may have several iterations.  Data latency in a distributed environment is identified and reduced if excessive.System of Systems - Integrated Mission Test. This phase is the official V&V of your system and the system of systems.  All systems and networks are made operational and operational data is created by the humans and systems that are intended to use it after publishing or distribution.  This step is necessary to introduce the human factor to the system of system, verify that the data and traffic loads can be handled properly, and validate with users that the information or product produces has value to the intend user. 5. Path AheadThe DTE program is still optimizing its environmental and test system representations, distributed test capabilities and V&V and CM plans to enhance its capability to support Army testing, in particular the FCS. The next capability demonstration is planned for the Distributed Test Event 5 (DTE 5) scheduled for August 2005. 5.1 DTE 5 PlansDTE 5 will leverage on the technologies, processes and expertise developed in DTE 4.  DTE 5 will incorporate capabilities from other Army commands and other US services. The Cross Command Collaborative Effort (3CE) is a program established to integrate Army M&S from ATEC, the Research, Development and Engineering Command (RDECOM) and the Training, Requirements, Analysis and Doctrine Command (TRADOC) to support Army acquisition requirements. In addition, DTE 5 will incorporate distributed test and M&S capabilities from the US Air Force (USAF) and US Navy (USN) being developed in the Multi-Service Distributed Event (MSDE) program to support Joint Test and Evaluation (JT&E) Roadmap requirements.  The basic concept for DTE 5 is to provide risk mitigation to the FCS program for the Experiment 1.1, assessing integration of existing and future C4ISR systems, in an overall Joint Task (JT) scenario with USAF and USN elements.6. SummaryThe SEIT / DTE development phases employ an evolving and increasing level of capability to support distributed testing for increasingly complex test requirements, culminating with a demonstration. The SEIT / DTE demonstrations utilize HLA federates, DIS applications, physical test equipment, sensors, and human operators represent the operational battlefield. Tactical scenarios include blue/red forces, C2 messaging, CB hazards, CB and IR sensors, platform mobility effects, autonomous UGV activities and live ground and aerial platforms coupled with near-real time meteorology data. Additionally, the SEIT / DTE demonstration use several simulation and test tools including a simulation logger, a DIS-to-HLA (RPR FOM) gateway, a 3D stealth visualization tool, several test planning and execution and data collection and analysis tools to complete the distributed test environment.  This complex environment requires a well planned V&V process to support the typical and unique requirements for accreditation that a distributed test environment bear.10. References[1]	Remmick, Nathan: “Future Combat System System Development and Demonstration Phase Integration Phase Plan”, Future Combat System Lead System Integrator Test Plan, January 2004.[2]	Remmick, Nathan: “Future Combat System System Development and Demonstration Phase Integration and Verification Phase Plan”, Future Combat System Lead System Integrator Test Plan, October 2004.[3]	Basciano, Tom: “Future Combat System System Development and Demonstration Phase Experiment Increment 1.1”, Future Combat System Lead System Integrator Test Plan, November 2004.[4]	Clardy, Timothy; Dennen, Kevin; Liebert, Ralph; Valentine, Peter; Carr, Stewart; Hanaway, Michael; Meyer, Hal; Music, Mark; Hernandez, Ruben; O’Connor, Michael; Briscoe, Derrick; Ryan, Robert: “Formal Report For The Demonstration of the Synthetic Environments Integrated Test Bed Initial Operating Capability 1,” US Army Developmental Test Command Report, June 2003.[5]	Liebert, Ralph; Carr, Stewart; Hanaway, Michael; Clardy, Timothy; Dennen, Kevin; Will Clayton; Meyer, Hal; Music, Mark; Hernandez, Ruben; O’Connor, Michael; Jones, Dennis; Ryan, Robert: “US Army Developmental Test Command Virtual Proving Ground (VPG) Synthetic Environments Integrated Test Bed (SEIT) IOC 1,” Simulation and Modeling for Acquisition Requirements and Training (SMART) 2003 Conference, September 2003.[6]	O’Connor, Michael: “Demonstration Control Document For The Synthetic Environment Integrated Testbed Initial Operating Capability 2,” US Army Developmental Test Command Report, September 2003.[7]	Liebert, Ralph; Clardy, Timothy; O’Connor, Michael; “Synthetic Environments Integrated Test Bed (SEIT): Building a Simulation-based Distributed Test Capability” Spring 04 Simulation Interoperability and Standards Organization Integration Workshop (S04 SIW), 04S-SIW-082, April 2004.[8]	Liebert, Ralph; Clardy, Timothy; O’Connor, Michael; “Synthetic Environments Integrated Test Bed (SEIT): A Distributed Test Environment” European 04 Simulation Interoperability and Standards Organization Integration Workshop (E04 SIW), 04S-SIW-022, July 2004.[9]	Chew, Jennifer; Sullivan, Cindy; Brain, Colin; McGuire, Richard; Brade, Dirk; Lagerstrom, Hokan; Cegla, Ingo; Pohl, Siegfried; Chagneau, Stephane; Peyras, Gerard; “General Procedures for M&S V&V Information Exchange” International Test Operations Procedures (ITOP) WGE 7.2, 18 March 2004.[10]	Chew, Jennifer; “V&V In Support of Distributed Test Environment” International T&E Association, Dec 04. Author BiographiesRALPH LIEBERT, Chief of the Electronic Proving Ground Ft. Lewis, has been with the civil service for 10 years in Army Test & Evaluation (T&E) and has been involved with instituting Modeling & Simulation (M&S) for use in T&E for seven years. Mr. Liebert started his tenure with the Army as a test officer and laboratory technician working in Chemical and Biological Defense T&E at Dugway Proving Ground and is currently managing C4ISR T&E at EPG Ft. Lewis.  He is managing M&S for EPG to support T&E for Future Combat System (FCS) and is the co-chair for the Synthetic Environment Focus Group part of the Developmental Test Command's Virtual Proving Ground. He has B.S. in Microbiology from the University of Louisiana Lafayette and has completed graduate studies in Microbiology from the University of Montana.JENNIFER CHEW is an Electronics Engineer in the Technology Management Division, HQ U.S. Army Developmental Test Command, Aberdeen Proving Ground, Maryland.  She oversees the organization’s M&S efforts for supporting the Army’s Future Combat System (FCS) test program and provides guidance on Verification and Validation (V&V) of M&S.  She works with the V&V International Test Operations Procedure (ITOP) Working Group to identify procedure for identifying and documenting V&V information from testing supported by M&S.  In her previous assignments, she served as independent evaluator of Army Electronic Warfare and Communications Systems and as reliability engineer for Army Ordnance Systems.  She holds a B.S. in Chemical Engineering from University of Maryland and a M.S in Electrical Engineering from Loyola College.WILLIAM FREDERIC is a Program Management Specialist at Electronic Proving Ground Ft. Lewis and has been responsible for large scale battle simulations operations, C4I interfaces for simulations and data collection for LVC events.  He is currently responsible for Government Furnished Materiel (GFX) to the FCS LSI for EPG controlled Modeling and Simulation products; and for GFX support to the FCS LSI System of System Test, Integration and Execution team.  He has 18 years experience in software development in DoD constructive battle simulations, C4I linkages, and data collection in system of system environments.  Previously, he completed 20 years service as a US Army combat arms officer, with a focus on combined arms operations.MICHAEL O’CONNOR is a Principal Simulation Architect with ITT Industries Advanced Engineering & Sciences Division.  He received a bachelor of computer engineering from Auburn University in 1987 and a masters in computer science from the University of Alabama in Huntsville in 1991.  Mr. O’Connor has been involved in the distributed simulation community for over ten years.  In that time he as served as the Editor of the Real-time Platform Reference Federation Object Model (RPR FOM), Chair of the SISO Standards Activity Committee, and currently serves as the SISO Executive Committee Vice Chair.  Mr. O’Connor is currently applying the standards developed by SISO in his role as simulation integration lead for the US Army Test and Evaluation Command’s Virtual Proving Ground Distributed Test Event initiative.