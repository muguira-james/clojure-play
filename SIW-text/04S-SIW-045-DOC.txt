Modeling and Simulation to Support Developmental and Operational Testing of Integrated Air and Missile Defense CapabilityLCDR Paul Ghyzel, USNJoint SIAP System Engineering Organization1931 Jefferson Davis HighwayArlington, VA 22202703-602-6441x228 HYPERLINK "mailto:paul.ghyzel@siap.pentagon.mil" paul.ghyzel@siap.pentagon.milCOL Gregory Gross, USAFOffice of Director, Operational Test and Evaluation, Strategic & C3I Systems4850 Mark Center Drive, Suite 10000Alexandria, VA 22311703-681-1440x103 HYPERLINK "mailto:gregory.gross@osd.mil" gregory.gross@osd.milMr. James SlaightSystems Planning and Analysis, Inc.2000 North Beauregard Street, Suite 400Alexandria, VA 22311703-413-2893 HYPERLINK "mailto:jimslaight@msn.com" jimslaight@msn.comKeywords:Operational Test and Evaluation (OT&E)Director, Operational Test and Evaluation (DOT&E)Developmental Test and Evaluation (DT&E)Joint Distributed Engineering Plant (JDEP)Integrated Air and Missile Defense (IAMD)Single Integrated Air Picture (SIAP)Joint SIAP System Engineering Organization (JSSEO)Integrated Architecture Behavior Model (IABM)Modeling and Simulation (M&S)ABSTRACT: A strategy of high-fidelity modeling and simulation (M&S) can be used successfully as an adjunct to actual production-representative combat systems to accomplish many of the objectives of developmental and operational testing.  Successful incorporation of M&S in the test approach will reduce the test program’s dependence on the availability of operational units and lower overall test program costs without sacrificing test program rigor.This paper describes the end-to-end test and evaluation process for the Integrated Air and Missile Defense (IAMD) capability and the incorporation of M&S in the test approach as captured in the IAMD Capstone Test and Evaluation Master Plan (TEMP).1. IntroductionThe fundamental purpose of test and evaluation in the Department of Defense is to identify levels of performance and assist the developer in correcting deficiencies [1]. An effective test and evaluation approach can be used to drive errors out of architectures, designs, and implementations early in their development.  A fundamental tenet of traditional test and evaluation is testing production-representative systems in realistic operational conditions.  As the United States Department of Defense (DoD) moves toward acquiring capabilities with an emphasis on net-centric concepts and joint interoperability, traditional test and evaluation would require the simultaneous participation of many units of several different systems from multiple Services.  Developmental and operational testing of net-centric capabilities that are manifested in varied combat, weapon and command and control systems that are geographically dispersed will be cost and logistically prohibitive if the test strategy relies exclusively on combat, weapon, and command and control systems operating in the field for test assets.Last year, DoD revised a set of instructions and acquisition regulations that directed the Department to acquire joint warfighting capabilities instead of specific weapon systems in isolation.  The Department’s vision for warfighting beyond 2010 emphasizes net-centric warfare and heavy reliance on Command, Control, Communications, Computers, Intelligence, Surveillance, and Reconnaissance (C4ISR) systems. One such capability is Integrated Air and Missile Defense (IAMD), which includes, as enablers, Single Integrated Air Picture (SIAP), Combat Identification (CID), Integrated Fire Control (IFC), and Automated Battle Management Aids (ABMA).The Joint Single Integrated Air Picture (SIAP) System Engineering Organization (JSSEO) mission is to provide the engineering design for the DoD’s Joint Theater Air and Missile Defense (JTAMD) Integrated Architecture (IA).  This IA will serve as the design foundation for giving the Joint warfighter a clear understanding of the battlespace and the confidence to engage air and relevant ground targets within the full kinematic range of US and coalition weapon systems. The Integrated Architecture Behavior Model (IABM) is a superset of Joint Battle Management Command and Control (JBMC2) functions that have been developed by or in coordination with JSSEO to support IAMD.  The IABM, when implemented in Partner Program combat systems, will meet Joint Requirements Oversight Council (JROC) validated Theater Air and Missile Defense (TAMD) and Combat Identification (CID) capstone requirements and support the spectrum of offensive and defensive operations (e.g., attack operations, suppression of enemy air defenses, air and missile defense, intelligence preparation of the battlefield) by US, allied, and coalition partners in the airspace within a theater of operations.2. JSSEO Test and Evaluation Strategy2.1 Test and Evaluation Strategy and GoalsJSSEO is developing an endtoend test and evaluation strategy for IAMD with assistance from the Office of the Secretary of Defense (OSD), Director of Operational Test and Evaluation (DOT&E), the Joint Staff, the US Joint Forces Command, the Services, and the Joint Interoperability Test Command (JITC).  This end-to-end strategy relies on a test environment that includes the components of the Joint Distributed Engineering Plant (JDEP) Technical Framework, re-usable hardware-in-the-loop (HWIL) representations of actual combat systems, and live test events involving actual weapon systems.  As the manager of the JDEP Technical Framework, JITC is currently developing the framework to support the use of digital representations of developing systems’ mission computers, sensors, and weapons using tactical software and common services to ensure realistic, repeatable tests.  The improved infrastructure will be a cost-effective venue for developing and testing the ability of future systems to support their required family of systems’ capabilities.In developing the test and evaluation strategy, one of JSSEO’s objectives is to provide tools to developers and program offices that will enable their system to pass Operational Test and Evaluation (OT&E) the first time.  In order to accomplish this, one of JSSEO’s goals is to improve the JDEP Technical Framework to provide engineering fidelity for Developmental Test and Evaluation (DT&E) and realistic operational conditions for OT&E preparations.  Another goal is to provide a certified Integrated Architecture Behavior Model (IABM) to developers and program managers.  Since the strategy is ultimately directed toward Operational Test (OT), JSSEO and DOT&E are working together at every step of the process to identify shortfalls, improve the JDEP Technical Framework, and develop the roadmap for IABM Partner Program OT&E.2.2 Operational Test Community ConcernsIn June 2003, JSSEO hosted a meeting for DOT&E and the Services’ OT representatives.  The purpose of the meeting was to introduce the OT community to JSSEO’s plans for JDEP and solicit the OT communities’ concerns regarding the use of modeling and simulation in OT.  The following concerns were brought forward:Warfighter InvolvementThere was general concern that JDEP testing has an engineering focus and does not meet the needs of the OT community to test operational requirements and capabilities.Threat RepresentationsThere was concern over the source and accuracy of threat information and the threat approval process.Configuration ManagementThere was concern as to the configuration management of the HWIL and digital representations that would be implemented for SIAP in JDEP.Verification, Validation, and Accreditation (VV&A) There was concern over the process that would be employed to validate HWIL and digital representations for Developmental Test/Operational Test (DT/OT) events.Atmospherics There was concern that currently, there is no common environmental data.  Each component of the JDEP federate imposes different environmental information.Level of Fidelity and LatenciesThere was concern over the ability of JDEP Technical Framework to provide the appropriate fidelity and sufficiently realistic latencies for DT and OT.Human System Integration (HSI)There was concern that operator-in-the-loop (OITL) testing (user acceptance testing) is usually the last step before system approval.  The issue arises when data from automated sources differ from what the actual warfighter discovers in testing.As a result of the meeting, JSSEO instituted a series of discussions with DOT&E in an effort to ensure that the OT community’s input was integrated with detailed planning for implementation of the JSSEO Test and Evaluation (T&E) goals.  DOT&E recommended that JSSEO detail their planning in a Capstone Test and Evaluation Master Plan (TEMP).  DOT&E and JSSEO are now working together to develop the IAMD Capstone TEMP [2].3. Test and Evaluation Process3.1 IntroductionThe major steps in a typical end-to-end test and evaluation process are shown in Figure 3.1.  The following paragraphs will expand each of these steps to show how JSSEO and DOT&E are working together to ensure the modeling and simulation infrastructure addresses the concerns of the OT community and will provide acquisition executives and program managers with the tools required to cost-effectively field tested systems that meet warfighter needs.Figure 3.1 – End-to-End Developmental/Operational Test and Evaluation Process3.2 Test ObjectivesRequirements for the IABM and its implementation in Partner Programs are derived from the Capstone Requirements Documents (CRD) for Theater Air and Missile Defense (TAMD), Combat Identification (CID), and Global Information Grid (GIG).  These CRDs form the basis for the IAMD IA and provide a set of Key Performance Parameters (KPP) for the family of systems to form the objective SIAP.  It was recognized that a full CRD-defined SIAP capability would not be achievable in a single development effort and that an iterative, spiral development would be necessary.  The Integrated Architecture represents that goal.  JSSEO will baseline current performance by taking measurements in live events and comparing them to the current IABM configuration and Partner Program implementations.  Designed performance improvements should be noticeable and fielding of the interim capability will establish the new baseline. JSSEO developed performance attributes necessary to mathematically define SIAP performance and is currently developing metrics that can be measured in an operational or test setting for use in calculating the attributes.  Testing the IABM and Partner Programs against the metrics provides a true indication of their performance in the SIAP.Warfighter involvement is critical to the foundation of the T&E process, establishing test objectives.  Test objectives are derived from two primary sources, the IAMD Integrated Architecture (IA) and from analysis of live events such as Operation Iraqi Freedom and Joint exercises.  The IA identifies the overall family of systems’ operational capabilities that are required for the objective SIAP, and the live events provide emergent, critical operational requirements and a baseline of current performance.  Live events also provide measurements for the environment in which the system will be expected to perform and are used by JSSEO for common services.JSSEO has developed a set of four scenarios, called Common Reference Scenario (CRS) vignettes, that will be provided to system developers and testers.  These scenarios are based on Defense Planning Guidance.  Threat characteristics are based on the latest intelligence estimates and are periodically updated.  The Services and Joint Commands, through the JTAMD process, approve the CRSs.  Excursion scenarios are developed to support specific test objectives.  In addition to a warfighter-approved scenario and accurate, up-to-date threat representations, the CRSs provide a repeatable sequence of events keyed to testing specific capabilities.  Repeatability provides the means to cost effectively isolate system problems and ensure that proposed fixes work.  Interoperability requirements and standards conformance requirements are placed in the context of the vignettes, and test cases are developed to support specific capability and conformance objectivesTest objectives are established for the IABM configuration or Partner Program designed capability, and form the bedrock for the test plan as shown in Figure 3.2. Figure 3.2 – Test Objectives and Test Planning3.3 Test PlanningThe JSSEO test planning process produces a Test Plan  and a Test Readiness Report (TRR).  The Test Plan includes a Data Management and Analysis Plan (DMAP).  The DMAP provides detailed instructions for collecting and analyzing test data to support the test objectives.  The Test Plan includes specific vignette, excursion, and test scenarios.  The objectives are provided to the test planners and included in the Test Plan.  The Test Plan also provides detailed test schedules and participant roles and responsibilities.  The Test Planning process shown in Figure 3.2 forms the basis for development of the test environment.3.4 Test EnvironmentThe JSSEO test environment is composed of three key elements: the JDEP Technical Framework, HWIL representations of legacy and production systems, and live events.  JITC, JTAMDO, and the Services are making a considerable investment of resources to improve the JDEP Technical Framework to support the JSSEO T&E objectives.  On completion, the JDEP Technical Framework will have the ability to conduct both developmental and operational tests in a highly realistic environment and with sufficient fidelity to support its engineering goals.  JDEP Technical FrameworkThe JDEP Technical Framework consists of 1) a network to provide connectivity for tactical communications, event data, control messages, status messages, and any other communications that the event participants require, 2) a common architecture with common data elements and definitions, a common data exchange and broker mechanism, a common definition of the modeling and simulation environment, a common definition of time and time management, 3) a federation development and execution process and 4) common services and digital representations of combat systems or their components (e.g., sensors).  The current architecture is based on a High Level Architecture Run Time Infrastructure (HLA RTI) that provides the common data elements, exchange and broker mechanisms, and time management structures.  One of the primary elements of realism for OT is real-time execution.  There are challenges with the current HLA RTI in running real time events with a large number of participating units, or federates, in HLA.  JSSEO and DOT&E are working together to define the real time requirement and upgrade the architecture to accommodate large families of systems, real time events.Common ServicesJSSEO is collecting information on applicable distributed simulations of the physical environment that can stimulate representations of Joint Tactical BMC2 system components.  The components necessary for this analysis include existing communication systems (e.g., Link 16, Link 11), navigation and Identification Friend or Foe (IFF) systems in various war fighting units, radar systems, other sensors and host mission computers used to generate and process measurements and observations. These components should support a composable M&S architecture, so specific requirements for these simulations fall into the categories of (1) mobile ad hoc network and Tactical Data Link communication services and effects, (2) sensors, (3) radiation propagation, (4) observables, (5) environmental effects, (6) navigation systems, (7) IFF systems (including Mode 5 and Mode S), (8) host mission computer programs (applications), (9) weapons and (10) scenario driver.Common sensors are currently planned to provide two-dimensional and three-dimensional rotating radar information and fixed and mobile phased array radar information over the HLA RTI.  Common services for communications and environmental effects are under development.  DOT&E is currently involved with JSSEO in developing the environmental server requirements.The common services will be verified and validated to ensure that each one meets the design requirements.  Requirements and test results will be developed and shared with the user community as well as other interested activities.Figure 3.4 - Test Environment ComponentsHardware-in-the-Loop (HWIL)Hardware-in-the-Loop (HWIL) systems are mission computers from legacy and developmental systems already in place.  “Hardware” refers to the actual production mission computer for the fielded weapon system under discussion.  HWILs are expected to be utilized for IABM test and evaluation for many years.  All HWILs will be certified for each test event as true representations of the system by the responsible program office as part of the Test Readiness Review (TRR).  This ensures that the system version required for the test is accurately configured.  The appropriate sensor server will provide sensor data for the HWILs. Digital RepresentationsDigital representations of developing systems’ mission computers, sensors, and weapons using tactical software will be utilized to ensure realistic, repeatable tests.  Digital representations are composed of three major parts.  The main component is the tactical software that is hosted on a commercial operating system.  Software representations of the system sensor or sensors and the fire control system are the other components.  The digital representations will be verified and validated for each event and be certified by the responsible program office.  JSSEO anticipates that multiple digital representations of the same system can be added to the federation to simulate the large number of systems that would be expected to participate in the SIAP.  3.5 Verification, Validation, and Accreditation Modeling and simulation is used throughout the test and evaluation continuum, from early IABM testing through integration testing of Partner Program implementation of the IABM to support portions of operational testing.  The ultimate objective of this testing continuum is to demonstrate that the SIAP capability is implemented in distributed systems in operationally realistic scenarios.  For the test results to be credible, the test environment must demonstrate close fidelity with the real world, be accredited by the appropriate authorities, and be acceptable to the operational community.Verification, Validation, and Accreditation (VV&A) will be performed on all components of the JDEP federation for a given test event.  The Services and Joint commands approve the Common Reference Scenarios through an established process.  Program managers will certify that their respective sensor system (e.g., radars, IFF, navigation sensors) models, HWILs, and digital representations are true representations of the actual systems.  JSSEO has an ongoing effort to characterize performance of fielded systems as they participate in live exercises.  By instrumenting these systems’ sensor and combat systems and analyzing system data as collected under actual conditions in the field, JSSEO has been able to characterize time, location, range, and range rate biases of many of the participating sensors.  This understanding of true performance of fielded systems under actual operational conditions is invaluable in validating the models used in simulation. JSSEO is currently developing requirements for a communications server to provide a high fidelity model of the communication architecture.  To provide a realistic environment, anticipated communications effects (bit error rate, unexpected communications loss, latency, etc.) will be modeled and simulated.  The Space and Naval Warfare Systems Command (SPAWAR) will accredit the communications server.  The JSSEO Test Readiness Report (TRR) provides written certification that the test infrastructure, including the participating HWILs and digital representations, can support the test objectives.  3.6 Developmental TestThe IABM is subjected to developmental test and evaluation (DT&E) by JSSEO prior to implementation in a specific weapon system, and by Service Partner Program offices during implementation and integration in their specific weapon systems.  During JSSEO DT&E the JDEP Technical Framework is used to verify the status of engineering development progress, substantiate achievement of technical performance requirements, and certify the readiness of products for dedicated operational test in support of Service and Industry efforts.  Once the certified IABM is delivered to Service Partner Program offices for implementation, DT&E will be conducted on the Service system to test its conformance to the standard established by the IABM.  Service program offices will use a combination of JDEP-based test events and live events to conduct DT&E of their systems.  IABM DT&EA dedicated JSSEO test team performs DT&E of the IABM in support of the spiral development.  Increments of functionality are delivered to the test team every four weeks as time boxes, with additional capabilities implemented in subsequent time boxes.  Time box testing is an iterative test process that uses test cases based on known IABM performance requirements.  The outcome of the test cases with respect to a time box test event will not necessarily measure a decisive pass/fail, but rather will be used as a gauge to measure the cumulative effort as the time boxes evolve to meet the requirements defined for the next planned release of the IABM.  The first delivery is planned for September 2005 and is designated Configuration 05.Requirements for the functionality contained in the time boxes are derived from Joint Requirements Oversight Council (JROC)-validated requirements documents, JSSEO documents (e.g., IABM System Segment Specification, Time Box Requirements Description Document (RDD)), communications standards and protocols (e.g., Link 16, JRE, IPV6), and Service-specific documents (e.g., sensor and command and control (C2) systems) documents.  Because the IABM will be implemented in multiple combat systems of all the Services and the performance standards of these systems vary, JSSEO uses the most stringent of currently available requirements.  These criteria are used to baseline thresholds.  Objective values will be determined in cases where they are not already specified in approved CRDs.  Ongoing JSSEO analysis of SIAP performance may drive higher objective values.  The functional areas, sub-functions, threshold and objective values, specific references to the documents they are derived from, and corresponding test cases are recorded in a traceability matrix.  This matrix provides traceability between the requirements and test cases.  A library of test cases, mapped to requirements, will provide a means to verify the testable requirements for Configuration 05 [3].  Test cases, scenarios, and test tools are developed based upon this process, which allows testing to focus on the final set of requirements while allowing “churn” at the time box level.One JSSEO goal is to automate the test and evaluation process as much as possible in order to maximize objectivity and reduce the cost of demonstrating conformance to MILSTD6016C [4] and STANAG 5516 [5].  Automated verification of both the IABM and existing systems is being investigated by building a modest set of automated stimulus/response pairs and assessing how well the analysis tools work toward detecting and isolating conformance shortfalls.As of March 2004, the IABM contains a sufficient level of functionality to warrant further DT&E using JDEP.  JSSEO has built a platform specific implementation of the IABM that is targeted specifically to the High Level Architecture (HLA) Run Time Infrastructure (RTI) and the Federation Object Model (FOM) that is being developed and validated within the JDEP effort.  This implementation will be used to verify that all requirements have been captured and are executable. This implementation of the IABM will then be compiled to create an executable program, which is the Reference Implementation. This Reference Implementation will run on the HLA RTI in a realistic operational environment provided by the common services, HWILs, and digital representations.  HLA RTI testing will allow the execution of runs where the output results will represent how well systems perform a specified function as mandated by the current requirements.Following successful DT&E of the HLA RTI implementation of the IABM, the IABM will be made available to the Services and Industry, along with the JDEP Technical Framework kit.  Program managers will receive a Platform Independent Model, two or more Platform Specific Models, associated reference implementations (one of these will be targeted to the HLA RTI for use in a JDEP construct), and V&V tests and results.Service Implementation DT&EOnce the Services receive the IABM from JSSEO, program managers will implement it into their combat systems’ tactical computer programs.  The resulting computer program will contain that system’s complete set of functionality, which is comprised of the common Joint BMC2 functionality provided by the IABM implementation as well as the additional system-specific functionality not provided by the IABM.  The JDEP Technical Framework will be used extensively to provide a realistic test environment that supports Industry integration efforts.  Program managers will continue to perform DT of their integrated systems, using both the JDEP and live test events.  This combination of testing venues is expected to provide the most cost-effective means to certify that the Service program is ready for operational test.Independent Verification an d Validation (IV&V)The IABM will undergo IV&V at three distinct points from development through integration in combat, command, and control systems to operational test of the combat system.  JSSEO is responsible for ensuring that the IABM Platform Independent Model (PIM) passes independent verification and validation (IV&V) prior to delivery to Service Partner Programs.  JSSEO is currently negotiating a Memorandum of Agreement (MOA) with JITC to designate JITC as the IV&V agent for the PIM.  The IABM will undergo IV&V for the second time during developmental test of a Platform Specific Model (PSM) as an integrated component of a combat, command, and control system.  The third IV&V will be performed as part of the combat, command, and control system operational evaluation.  The system program manager is responsible for the second and third IV&Vs.3.7 Operational TestSection 139 of Title 10 United States Code, states that Operational Testing is “the field test, under realistic combat conditions, of any item of (or key component of) weapons, equipment, or munitions for the purpose of determining the effectiveness and suitability of the weapons, equipment, or munitions for use in combat by typical military users; and … the evaluation of the results of such test.”  In layman’s terms, this requires that production of representative systems be tested under operationally realistic conditions while being used and maintained by the type of soldier, sailor, airman, or marine who will use or maintain the system in the field. Operational Testing is the last step in a sequence of seamless verification that ranges from contractor testing to developmental testing including combined DT and OT with an independent period of OT, usually referred to as Initial Operational Test and Evaluation (IOT&E). Followon Operational Test and Evaluation may be required depending upon the outcome of the IOT&E/OPEVAL. This trend toward seamless verification has reduced testing costs while compressing the testing timeline.  It is important to note that seamless verification requires close coordination between the operational and developmental testers to ensure that each gets the data required and that the data is useable.Systems are operationally tested to determine the operational effectiveness and suitability of the system. IOT&E also determines how well the system operates with other systems, and assesses the system’s information assurance attributes.  Operational effectiveness is how well the system under test accomplishes its assigned missions when used by the personnel intended to operate/maintain the equipment in the environment planned or expected for operational employment of the system as defined in the Operational Requirement or Capability documents.    A system is operationally suitable if it is sufficiently available, compatible, transportable, reliable, maintainable, safe, and supportable when used in the field.  The system must have appropriate human factor consideration, account for natural environmental effects, and meet training and documentation requirements.  Interoperability is included in the definition of suitability, but due to the increasingly netted/integrated nature of today’s systems, it is rated separately.  Our dependency on the flow of information led Congress to require that DOT&E assess the level of information assurance of new systems.As systems and families of systems become more complicated, interdependent, and expensive, modeling and simulation in testing takes on even more importance.  DoD Directive 5000.2 [6] requires program managers to plan for modeling and simulation throughout the acquisition cycle.  System models must integrate into the battle space and be able to interoperate with models of the systems with which the system will be employed.  To be used in operational testing, the models must be validated, verified and accredited, and then accepted by the operational testers.  DOT&E policy is that modeling and simulation does not replace testing, but is used to augment, or as an adjunct, to testing.  Section 141 (a) of Title 10 United States Code prohibits IOT&E/Beyond Low-rate Initial Production (LRIP) reports based solely upon modeling and simulation operational assessments.Aside from the DOT&E policy and legal restrictions mentioned above, there are several concerns with the use of modeling and simulation in operational testing.  Given the interdependent nature of current and future families of systems, it is critical that these systems meet a common standard that allows them to provide the robust input/output to be tested in an operationally relevant, simulated environment.  One important question with regard to the VV&A of a model of a family of systems is who will pay for testing the integration of these models.As each system moves through its life cycle, the models that represent the system will evolve.  One major difficulty in combined DT/OT will be in synchronizing the testing of the various elements of a family of systems to ensure that the DT community gets the required data while the OT community gets what it needs while meeting the legal requirements vis-à-vis contractor participation.  It will be imperative that the DT, OT, and program office members work closely on all facets of test planning and execution.  DOT&E and JSSEO are working together on the IAMD Capstone Test and Evaluation Master Plan (TEMP) [1] to make sure that IABM developers, JDEP infrastructure developers, and Service program offices have a well coordinated T&E roadmap.  Finally, multiple security levels can make adequate testing extremely difficult and prohibit data dissemination.  The JSSEO standard test planning process ensures that security will be addressed early and often.Operational testing, in the pre-IOT&E phase, will provide feedback to the program office on effectiveness, suitability, interoperability, and information assurance.  The results of the simulations and real world tests will also provide feedback to the simulation federation to enhance the tools and procedures used to digitally represent the given systems.4. Implementation Schedule and Status4.1 ScheduleThe goal is to provide the tools for the first program to implement the IABM to pass IOT&E in FY08.4.2 StatusTwo HWILs, E-2C, and PATRIOT, were successfully integrated into stand-alone JDEP Federations in 2002 and 2003, respectively.  Testing of the AEGIS HWIL integrated into a stand-alone Federation is currently underway.  Planning is underway for E-2C and PATRIOT to form an integrated federation in FY04 as well as for AEGIS to join that federation in FY04.  This is part of a larger effort to test all of the systems’ SIAP performance in August FY04.  The IABM is currently undergoing DT&E in preparation for integration into the combined HWIL Federation in early FY05.5. ConclusionDOT&E and JSSEO are closely coordinating efforts to implement a disciplined process for test and evaluation of a system’s performance within a family of systems as shown in Figure 5.1.  When completed, this capability will 1) ease the burden on our operational forces to support multiple test events, 2) ease the burden on Service program offices in developing their systems, 3) increase realism, and 4) be more cost-effective.  Modeling and simulation will never replace live evaluations, however with proper planning and coordination with the test community, it can be used to improve the expectation of success in a cost-effective manner.  Figure 5.1 - JSSEO T&E ProcessReferences[1] Test and Evaluation Management Guide, 4th ed., Defense Acquisition University, November 2001.[2]	DRAFT JSSEO Technical Report 2004-007. Integrated Air and Missile Defense (IAMD) Capstone Test and Evaluation Master Plan (TEMP), February 04., JSSEO.[3]	Integrated Architecture Behavior Model (IABM) Configuration 05 Description Document Version 1.0, 10 December 2003.[4]	MIL-STD-6016C Department of Defense Interface Standard Tactical Data Link (TDL) 16 Message Standard, Jointly approved at the TDL CCB 12 December 2003, due to be signed March 2004.[5]	STANAG 5516, Edition 1, Tactical Data Exchange – LINK 16, Ratified 15 January 1997. [6]	Department of Defense Directive 5000.2, The Defense Acquisition System, 12 May 2003.Author BiographiesCOLONEL GREGORY GROSS, USAF is a Military Staff Assistant to the Director, Operational Test and Evaluation, Office of the Secretary of Defense.  As part of his responsibilities, he provides oversight on testing of airborne C4I, CMD, and associated acquisition programs.  His prior assignment was as commander of the 55th Operations Group, Offutt AFB, NE.LIEUTENANT COMMANDER PAUL GHYZEL, USN is the Test and Evaluation Lead for the Joint Single Integrated Air Picture System Engineering Organization (JSSEO) Arlington, VA.  He leads the development of JSSEO's Capstone Test and Evaluation Master Plan.  He has previously served in systems engineering assignments with the Space and Naval Warfare SystemsCommand Space Field Activity, Chantilly, VA, and the Naval Air Warfare Center, Aircraft Division, Patuxent River, MD.  He is a graduate of the Naval Postgraduate School and the US Naval Academy.JAMES SLAIGHT served on active duty with the US Navy and was the Program Manager for the Advanced Combat Direction System Block 0 (ACDS Block 0), New Threat Upgrade (NTU), the Radar Display and Distribution System (RADDS) and the final upgrade to the Naval Tactical Data System (NTDS).  After leaving the Navy, Mr. Slaight managed several shipboard communications projects before joining Systems Planning and Analysis Inc. in 1995.  Mr. Slaight is currently the lead SPA Inc. manager for JSSEO Test and Evaluation Strategy.  The JDEP Technical Framework kit contains Attributes Technical Reports, Common Reference Scenarios, the Common Reference Scenario Driver, ARCTIC, PET, Environment services, Communications services, Sensor representations, and Weapon representations.