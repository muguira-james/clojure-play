Development and Documentation of a Simulation Conceptual ModelDale K. PaceThe Johns Hopkins University Applied Physics Laboratory11100 Johns Hopkins RoadLaurel, Maryland 20723-6099(240) 228-5650;     (240) 228-5910 (FAX)dale.pace@jhuapl.eduKey Words:    Conceptual Model, Conceptual Validation, VV&AABSTRACT:  The conceptual model for a simulation addresses the application domain context of pertinence for the simulation, the idea for how the simulation will satisfy its requirements, and the way that entities and processes will be represented within the simulation.  This paper addresses how a simulation conceptual model is developed and documented.  Conceptual model implications for simulation correctness and credibility are discussed.  This paper draws upon ideas for revision of DoD Recommended Practices Guide for Verification, Validation, and Accreditation (VV&A) and upon ideas of the Conceptual Model Tiger Team established at the March 1999 SIW by the VV&A Forum.1.  Introduction The Verification, Validation, and Accreditation (VV&A) Forum at the Spring 1999 Simulation Interoperability Workshop (SIW) approved formation of a Tiger Team to address conceptual model issues.  In late-March 1999, a simulation Conceptual Model Tiger Team (CMTT) was established by Simone Youngblood, VV&A Technical Director for the Defense Modeling and Simulation Office (DMSO) and SIW VV&A Forum Chair.  Participants in the CMTT were Ms. Candace Conwell (SPWAR), Mr. Robert-Jan Elias (TNO-FEL, the Netherlands), Dr. Reuben Jones (Boeing), Dr. Averill Law (Averill Law & Associates), Mr. Bob Lewis (TecMasters), Dr. Michael Moulding (Cranfield Univeristy, UK), Dr. Dale Pace (JHU/APL, CMTT Lead), Mr. Terry Prosser (Logicon), Mr. Bob Senko (DMSO), Mr. Jack Shehan (DMSO), Ms. Susan Solick (TRAC), Mr. Dave Thomen (SAIC), Dr. Bernard Zeigler (Univeresity of Arizona).  Significant comments about CMTT ideas were also received from Mr. Gary Coe (IDA).  This paper draws upon both the work of the CMTT and ideas developed for revision of the DoD VV&A Recommended Practices Guide.  Discussion of all simulation conceptual model issues would take more than a single paper.  Consequently, this paper does not address the process by which one abstracts reality to create the simulation context and simulation elements, a province of simulation theory.  This paper focuses upon conceptual model documentation in supporting conceptual validation, not because other uses of the conceptual model are unimportant but because of space limitations.  This paper addresses three main topics:  conceptual model development, conceptual model documentation, and conceptual validation. Many have different perspectives about terms and concepts used in this paper.  One can quibble about whether simulation “design” begins with the first choice about how the simulation will be built or only later when many more design decisions have been made.  Likewise, one can quibble about whether there should be a single conceptual model for a simulation (the view of this paper) with many parts or a collection of conceptual models (one or more for each part) and a composite for the whole simulation.  The basic ideas of this paper should be valuable regardless of how one sees such issues.What is the Simulation Conceptual Model?  A conceptual model is a simulation developer’s way of translating modeling requirements (i. e., what is to be represented by the simulation) into a detailed design framework (i. e., how it is to be done), from which the software that will make up the simulation can be built.  A conceptual model is the collection of information which describes a simulation developer’s concept about the simulation and its pieces.  That information consists of assumptions, algorithms, relationships (i. e., architecture), and data.  Taken together, these describe how the simulation developer understands what is to be represented by the simulation (entities, actions, tasks, processes, interactions, etc.) and how that representation will satisfy the requirements to which the simulation responds.  Many different descriptive mechanisms have been and may be employed for conceptual modelsA simulation conceptual model can be a primary mechanism for clear communication among simulation developer design and implementation personnel (systems analysts, system engineers, software designers, code developers, testers, etc.), simulation users, subject matter experts (SMEs) involved in simulation reviews, and VV&A personnel.  A simulation’s conceptual model addresses the simulation context, simulation elements, and the simulation concept.  Each of these is discussed in more detail below, and the relationship among these is illustrated by Figure 1 at the end of the paper.1)  The simulation context provides authoritative information about the domain which the simulation is to address.  Often this part of the conceptual model is merely a collection of pointers and references to sources that define behaviors and processes for things that will be represented within the simulation.  Special care must be used when algorithms are taken from more than one source to ensure that those sources do not employ contradictory assumptions or factors (such as different models for the shape of the earth, the characteristics of the environment, etc.).  The information contained in the simulation context establishes boundaries on how the simulation developer can properly build the simulation.2)  A simulation element consists of the information describing concepts for an entity, a composite or collection of entities, or process which is represented within a simulation.  A simulation element includes assumptions, algorithms, relationships (especially interactions with other things within the simulation), data, etc. that identify and describe that item’s possible states, tasks, events, behavior/performance, parameters and attributes, etc.).  A simulation element can address a complete system (such as a missile or radar), a subsystem (such as the antenna of a radar), an element within a subsystem (such as a circuit within a radar transmitter), or even a fundamental item (such as an atom).  It can also address composites of systems (such as a ship with its collection of sensors, weapons, etc.).  It should be noted that a person, part of a person (such as a hand), or a group of people can likewise be addressed by a simulation element.  A simulation element can also address a process such as environmental effects on sensor performance.3)  The simulation concept describes the simulation developer’s concept for the entire simulation application (all the federates and other pieces in a distributed simulation, everything that comprises the simulation) and explains how the simulation developer expects to build a simulation that can fully satisfy user defined requirements.  The simulation context (discussed above) establishes constraints and boundary conditions for the simulation concept.  If the simulation is concerned with realistic representation of missile flight, then laws of physics and principles of aerodynamics are part of the simulation context, making the simulation concept accommodate conservation of momentum, etc.  Unrealistic, cartoon representation of missile flight would not necessarily be so constrained.  The simulation concept includes simulation elements, i. e., the things represented in the simulation.  The simulation concept is the total of all simulation elements, specifies how they interact with one another, and includes all additional information needed to explain how the simulation will satisfy its objectives.  A primary function of the simulation concept is to serve as the mechanism by which simulation requirements are transformed into detailed simulation specifications (and associated simulation design) which fully satisfy the requirements.  The conceptual model for a simulation has to address both simulation space (the simulation operational and functional capability) and mission space (the representational capability of the simulation).Ambiguity exists in description of the simulation context, simulation elements, and simulation concept because some parameters are generic and others are application (run) specific.  A simulation element for a sensor may contain parameter values and algorithms that fully characterize sensor states, behavior, performance, and characteristics.  For a particular application, it may be desirable to change some parameters from run to run.  One could have a parameter set by simulation inputs at the beginning of each simulation run.  Or one could have a slightly different simulation element for the sensor, one with the appropriate parameter value, for each run.The High Level Architecture (HLA) Federation Development and Execution Process (FEDEP) for distributed simulation discusses three products (Federation Requirements, Federation Conceptual Model, and Federation Scenario) in development of a Federation Conceptual Model.  It is helpful to explain the relationship of the conceptual model aspects (simulation context, simulation element, and simulation concept) to these.  Federation Requirements and the Federation Scenario together in a HLA distributed simulation are roughly equivalent to the simulation context in the way that term is used here.  They establish the constraints for the specific application of the distributed simulation, what it is intended to do.  The Federation Conceptual Model which addresses what simulations (federates) will be used together in the federation is equivalent to the simulation concept, in which the simulation elements correspond somewhat to the federates of the federation.  The simulation conceptual model provides a generic capability that can be applied to a number of applications.  The simulation inputs for a particular application (scenario, parameters, controls, etc.) drive the exact results for that application.  A HLA federation is oriented toward a specific application, with the federation scenario being similar to “simulation inputs” in the previous sentence. It is helpful to think of the conceptual model as the means by which simulation requirements are transformed into simulation specifications that then drive simulation design.  A simulation conceptual model may precede most simulation design and implementation decisions, allowing the conceptual model to be largely design (and implementation) independent.  In other situations, a simulation conceptual model will include design considerations, especially when portions of the simulation are re-used from a previous simulation or when it is decided a priori to use a particular hardware or software environment for the simulation or when an iterative simulation development approach is used in which the simulation concept as well as its implementation are evolving concurrently.  The conceptual model may even be expressed in the descriptive environment chosen for simulation development, such as the Unified Modeling Language (UML) associated with object-oriented developments or one of the formal method paradigms employed when mathematically provable correctness is required (as in safety-critical applications) [1].Some simulation developments fail to have distinct documentation for the simulation conceptual model.  This invariably leads to difficulties later.  When a legacy simulation for which conceptual model documentation is not available or is grossly inadequate is used, construction of a presumed conceptual model can significantly increase the cost of validation endeavor.  Adroit simulation sponsors and developers insist upon distinct and current documentation of the simulation conceptual model.2.  Conceptual Model DevelopmentOnce simulation objectives have been established, development of the simulation conceptual model may begin. Simulation requirements and conceptual model development are a classic “chicken-egg” pair.  They each stimulate and derive from the other.  Conceptual model development may even begin prior to completion of simulation requirements.  The first step in conceptual model development is to collect authoritative information about the intended application domain that will comprise the simulation context.  Development of the simulation concept and collection of authoritative information for the simulation context is likely to occur iteratively as the entities and processes to be represented in the simulation are more clearly defined.  DMSO is developing authoritative descriptions of various military activities as part of the Conceptual Models of the Mission Space (CMMS). The CMMS authoritative descriptions (“the first abstraction of the real world . . . an authoritative knowledge source for simulation development . . . capturing the basic information about important entities involved in any mission and their key actions and interactions” [2]) will help to ensure commonality of perspective among various Defense simulations and should facilitate reuse of simulation components.  Authoritative descriptions of military activities can be used in the simulation context when appropriate for a simulation’s intended application, as can the laws of physics and similar principles.It is unlikely that the formal, documented simulation context will address everything needed to fully describe the domain that a simulation is to address.  CMMS endeavors emphasize a disciplined procedure by which the simulation developer is systematically informed about the real world and a set of information standards that simulation subject matter experts (SMEs) employ to communicate with and obtain feedback from military operations SMEs [3].  Keys to removing potential ambiguity between the ideas of the warfighting SMEs and the simulation development SMEs are common semantics and syntax, a common format database management system (DBMS), and data interchange formats (DIF).  Significant progress has been made in developing a CMMS Toolset to provide the keys noted above, but information beyond that likely to be obtained in the first level abstraction (i. e., CMMS) may be required for a simulation conceptual model.  SMEs may be “called upon to fill in details needed by [simulation] developers” that are “not provided in doctrinal and/or authoritative sources.” [4]  The more complete and clearly stated a simulation context is, the easier it will be to understand how one simulation entity (or simulation in a federation) may differ from another in its assumptions about the domain which is addressed.  This becomes very important when questions of compatibility among simulations (federates) considered for a distributed simulation implementation (federation) are addressed as well as in assessment of coherence among simulation parts.Sometimes it becomes obvious that additional information is needed about the simulation context if the simulation is to achieve its objectives when the available information is lacking, not just that it is not included in the authoritative description of the application domain.  This often occurs for simulations used to support system design.  Test programs may be established to generate such information.  Sometimes the missing information consists only of parameter information; other times, the missing information concerns the theory (or algorithms) used to describe entity behavior or performance.  When significant information about critical aspects of a simulation is unknown or uncertain, development of the simulation conceptual model will be difficult since algorithms and data will be incomplete. Sometimes inadequate attention is given to potential problems with the quality (correctness and comprehensiveness) of information upon which the simulation conceptual model is based.  Experimental (test) data also have limitations and uncertainties [5].The second step in conceptual model development is to identify the entities and processes that must be represented in the simulation to accomplish its objectives.  This enumeration process is fundamental in conceptual model development.  It is here that basic decisions are made about the level of detail and aggregation that is appropriate to support simulation requirements.  These are the decisions that determine whether a system (such as a radar) will be represented as a single entity, as a composite of subsystem entities (such as an antenna or receiver), or as a composite of composites of ever smaller entities (to whatever level of detail is needed for the purpose of the simulation).  This is where decisions are made about the level of representation of human decisions and actions.  For example, in the movement of a platform (tank, aircraft, ship, etc.), are the decisions and responses of all the people involved (the crew) represented implicitly as a single aspect of the movement control process – or does each individual involved get represented explicitly (as in a tank simulator with a position for every member of the tank crew)?The third step in conceptual model development is development of simulation elements.  A simulation element is needed for each entity or process (or composites of such) identified in step two above.  It is here that decisions are made initially about the level of accuracy, precision, resolution, etc. needed in the representation of the entity or process.  Simulation elements determine most of the functional and behavioral capabilities of the simulation.  Simulation fidelity is a function of both the scope of representation in a simulation (the entities and processes identified in step two) and the quality of entity and process representation in terms of accuracy, precision, etc.The  final step in conceptual model development is to identify relationships among simulation elements, to ensure that constraints and boundary conditions imposed by the simulation context are accommodated, and that they articulate the simulation concept.These steps will be iterated often in most developments.  Conceptual model development should always be done within the context of simulation theory, such as Application Domain Modeling (ADM) [6] or Discrete Event System Simulation (DEVS) methodology [7], to ensure that conceptual model development has coherence and can be related directly to simulation development.Conceptual model development may reveal problems with requirements for the simulation, especially if there has not been a rigorous validation of simulation requirements prior to initiation of conceptual model development.  As the simulation conceptual model is developed to fully satisfy simulation requirements, inconsistencies among requirements and lack of balance among the requirements (some very lax and others very stringent in the same general area) may become apparent.  Likewise, development of the conceptual model may also reveal serious holes in the requirements, areas where the simulation developer is left to his own initiative about what the simulation should be able to do.  A good simulation development program will encourage early formal and rigorous validation of simulation requirements, and ensure that requirement deficiencies uncovered during conceptual model development are corrected with appropriate modifications to simulation requirements.3.  Conceptual Model DocumentationThe varied nature of simulation conceptual models makes it impossible to provide a simple cookbook approach that will work well for all simulations.  Flexibility must be allowed in order to accommodate the variety of simulation conceptual models.  The structure presented in this section is intended to ensure that all needed information is available in the conceptual model description.  The objective of the descriptive format for a simulation conceptual model is to provide a coherent set of information that fully and correctly describes the conceptual model so that its capabilities, limitations, and characteristics can be readily understood by simulation development personnel, by VV&A personnel, and by SMEs involved in simulation assessments.Many misunderstand the many roles that a conceptual model description has to play.  A key to conceptual validation, it is also essential for those who later modify the simulation to ensure that they do not introduce faults inadvertently.  Many different people, and often many different organizations, may be involved in a simulation’s development and modification over its life.  Without thorough documentation of the conceptual model, it is easy to introduce unintended faults in a complex simulation.  Thorough conceptual model description is also essential to support those who develop user guides and analyst guides for the simulation.The goal of conceptual model documentation is to provide adequate information for a scientifically compelling understanding of the conceptual model when possible.  The conceptual model documentation must provide adequate information for logical, complete, and factual assessment of the conceptual model.  Several basic descriptive formats have been used for simulation conceptual models.  This section identifies and discusses four such descriptive formats:  1) ad hoc method, 2) design accommodation, 3) CMMS paradigm, and 4) scientific paper approach.  These labels hint at the approach taken by each of these descriptive formats.  The most common descriptive method for simulation conceptual models is the ad hoc method -- ad hoc means for the particular end use or case at hand without consideration of wider applications.  No formal attention normally is given to consistency (or completeness) of conceptual model descriptions using the ad hoc method.  Many legacy simulations employed this method and, thus, it may be difficult to find even partial documentation of the simulation conceptual model.  Those who must use legacy models (or perform conceptual validation of a legacy model) that does not have adequate documentation of its conceptual model must develop some level of understanding of the conceptual model upon which the simulation was built.  Sometimes this re-engineered conceptual model is developed by analysis of the simulation’s code coupled with documentation that may exist.  Sometimes those who developed and those who used the simulation are interviewed to obtain insights about the simulation’s conceptual model.  Sometimes resources required to re-engineer a conceptual model for a simulation are not available and the simulation must be treated as a “black box” whose results must be suspect except where they agree with other reliable information.A second descriptive method for conceptual models is the design accommodation method.  In this, the simulation developer employs descriptive formats that have been selected to support simulation design to describe the conceptual model.  However, there may be no standard method for capturing assumptions and limitations inherent in the conceptual model.  Portions of the conceptual model for a particular system (such as a sensor) may be scattered throughout the simulation design.  This can make it very difficult for a conceptual validation review SME to have confidence that materials gleaned from the mass of design description information describe all aspects of the conceptual model being reviewed.  Some design accommodation descriptive formats do not easily capture all critical elements of a conceptual model, such as temporal dependencies and relationships among simulation elements.A third descriptive format is the CMMS paradigm.  CMMS approaches emphasize database management system structures.  They focus upon information sets about simulation entities, actions, tasks, and interactions (EATI) [8].  A set of tools have been developed to help transform information from military (warfighter and other operational) specialists and SMEs about an application domain into reusable EATI items of information [9].  The infrastructure which DMSO has established for CMMS provides a valuable resource for managing information parsed in this way and may also be helpful for managing other kinds of descriptions of conceptual models.A fourth descriptive method for conceptual models is the scientific paper method.  This approach employs the standard construct of a scientific paper (or report).  This descriptive method tends to be more complete in its identification of assumptions, more explicit in its statement of algorithms in accord with standard mathematical and technical conventions, and more rigorous in its specifications of limitations associated with the simulation conceptual model.  Because this method of conceptual model description is the most amenable to robust support for conceptual validation reviews (and most accommodating to simulation reuse and modification), it is the recommended approach.  The following shows how this approach can be applied to conceptual models for new simulation developments, for major simulation modifications, and for re-engineered conceptual models of legacy simulations.  Nine items should be part of the description.  These nine items can be applied to all simulation conceptual model aspects: to the simulation context, to individual simulation elements, and to the simulation concept.  Using it to describe simulation elements is very important since a major portion of most conceptual validation review is assessment of simulation element representation.Nine Element Conceptual Model Description1.  Conceptual Model Portion Identification.  This item should be unique and easily comprehended by a human – a short version for easy indexing may also be used.  Both identification versions should include a date for the conceptual model. 2.  Principal Simulation Developer Point(s) of Contact (POCs) for the Conceptual Model (or part of it).  This item identifies the specific individual(s) associated with the conceptual model (or a part of it) so that it will be easy for a SME, or anyone else involved in a conceptual validation review, to know whom to contact for clarification, additional information, or discussion of the representation.  Contact information should include phone, fax, and email for each person identified.  In some cases, it may also be appropriate to indicate the person’s specific area of responsibility.3.  Requirements/Purpose.  This item should be a brief, but specific and detailed description of what the conceptual model (or the particular part of it being described) is supposed to do.  This item sets the specific perspective that should be employed when that representation is reviewed.  This item should also reference specific simulation requirements to which each simulation element responds.  It is may be useful to parallel the CMMS data structure in this so that there can be more direct linkage between the conceptual model and items in the CMMS repository.  However, in some cases where there are multiple levels of resolution in the simulation (as in the federation of a HLA distributed simulation), it may be appropriate to use a different data structure [10].  Community standards for how to deal with this issue do not yet exist.4.  Overview.  This item provides a general description of the simulation and explains how this part of the conceptual model relates to the larger application domain expected for the simulation as a whole.  This is where interactions and interfaces of the simulation elements within that the larger application domain for the simulation must be described and specified. 5.  General Assumptions.  This item identifies general assumptions that apply to the part of the conceptual model being described.  Assumptions that are specific to a particular algorithm should be stated again in association in association with that algorithm (see item 7 below).  Implications of an assumption should be stated with the assumption – normally such implications are a significant part of simulation limitations.  Assumptions can address many factors.  They may relate to the nature of an algorithm.  They may relate to how other parts of the simulation (or federation) function.  They may relate to sources and availability of information and data.  They may relate to significance of fidelity for different parts of the simulation (such as display realism).  Bad assumptions are a major source of simulation faults.  Law and Kelton note:  “We have never seen a structured walk-through [of the conceptual model] where all of the model assumptions were found to be correct.” [11]6.  Identification of Possible States, Tasks, Actions, and Behaviors, Relationships and Interactions, Events, and Parameters and Factors for Entities and Processes being described.  This item is where the basic ingredients of the conceptual model are identified.  These ingredients are not mutually exclusive.  Many find it helpful to use several orthogonal, complementary views to ensure completeness in appreciation of these ingredients.  Three such orthogonal, complementary views are the functional view (with a focus on the flow of data between elements), the data view (with descriptions of the hierarchy and static relations between elements), and the behavioral view (which addresses dynamics such as states, transitions, and interactions).  The collection of ingredients should identify the totality of the conceptual model.  Identification of dependencies and independence among actions, events, processes, etc. is important.  This item is the basis for assessment of completeness of the scope of the representation during a conceptual validation review.7.  Identification of Algorithms.  Algorithms define ways that an entity or process behaves and how it may interact with other things in the simulation.  This item should identify all algorithms and relate them to entities and processes.  Sources (pedigrees) of algorithms (and the sources and pedigrees of the data to be used in them) should be specified and the relationship of the algorithm selected to similar algorithms used elsewhere should be noted (if known).  Assumptions embedded in algorithms should be noted.  Algorithms should be expressed in standard scientific and technical notation where possible. Some algorithms may only be easily expressed in the jargon of a particular technical field (such as that employed with decision tables), whereas other algorithms may be capable of expression in the more traditional algebraic forms normally employed for calculus, statistics, and differential equations.  Data elements of algorithms should be specified (e. g., the power and antenna gain of a radar representation that uses the standard radar range equation).  If the precise values of data elements are not available, then the expected data sources should be identified and a postulated value for the data parameter (or range of possible values) should be presented.  This kind of information is essential if judgments are to be made about representational accuracy and fidelity.8.  Simulation Development Plans.  This item addresses plans for evolutionary development of a conceptual model over the lifecycle of a simulation.  For example, the initial version of a sensor representation may not take into account the aspect dependence of target signature strength.  There may be plans to more fully develop the simulation element conceptual model for that sensor later during the simulation lifecycle.  This item would identify such a plan.  It must also indicate when such expansion of the simulation element conceptual model would occur and what its implications might be. The development plan should provide as much detail as available about what the portion of the conceptual model being described will be for its future evolution.9.  Summary and Synopsis.  This item wrap-ups the conceptual model (or part of it being described).  It should clearly identify limitations of (that part of) the conceptual model as well as summarize its expected capabilities.  This item should also note explicitly any parts of the conceptual model which are incomplete, and indicate when those parts can be expected to be completed.  This item is where the developer of a conceptual model should express caveats about the conceptual model that should be known and understood by those who perform conceptual validation (and by those who modify the simulation later).  Description of the simulation conceptual model may employ more than one of these approaches.  Conceptual model description may employ design accommodation formats (such as describing specifications by Yourdon Structured Analysis to facilitate precise specifications and then expressing those ideas in UML to reduce ambiguity for code developers as well as to facilitate use of automated consistency and traceability checks) coupled with the scientific paper approach to ensure comprehensive description of assumptions, algorithm sources, and other critical factors that are important elements of the conceptual model and which are needed to more fully support conceptual validation and subsequent simulation modification..4.  Conceptual ValidationIt is helpful to discuss conceptual model implementation independence and availability of conceptual model documentation before proceeding with the  general discussion of conceptual validation.4.1  Design and Implementation Independence.Simulation Context.   Information in the simulation context is normally implementation independent, not tied to particular software or hardware.  However, this is not always the case.  When a simulation context item is reused from a previous simulation development, that item may be expressed in a manner that is implementation dependent (such as being in a particular software paradigm) to maximize compatibility with that previous developmentSimulation Element.  It is usually desirable for description of a simulation element to be implementation independent.  This provides the greatest flexibility in simulation element use.  Description of a simulation element will not always be implementation independent.  Sometimes the simulation element will be described in the software paradigm selected for simulation development.  Sometimes final algorithms in a simulation element will be simplified approximations of more correct algorithms, selected because only they can be run within the time constraints of the particular implementation.  Use of a common descriptive format for all simulation elements within a particular simulation development will facilitate comparison among them to ensure that there are no unknown inconsistencies or conflicts to interfere with satisfaction of simulation objectives.Simulation concept:  Seldom will a simulation concept be totally implementation independent, particularly if something from a previous simulation development is going to be reused in the simulation.  That “something” may exist only in a particular software paradigm (or in a particular hardware version, if a real piece of equipment is to be used in the simulation), which will (at a minimum) define interfaces with that “something.”  If the simulation concept specifies how something within the simulation will be represented (such as a plan to run the simulation on a certain class of computers or with a particular kind of operating system), the simulation concept becomes implementation dependent.HLA Federation Conceptual Model (FCM).  For HLA distributed simulation, the FCM is identified as “an implementation independent representation, which serves as a vehicle for transforming objectives into functional and behavioral capabilities, and provides a crucial traceability link between the federation objectives and the design implementation” [12].  “Implementation independent” can not normally be understood in an absolute or rigorous sense in this FCM description since identification of even one specific federate as the way to satisfy some of the functional and behavioral objectives for a federation has implementation implications – and even the earliest versions of a FCM may include identification of specific federates.4.2  Availability of Conceptual Model Documentation: Sometimes descriptions of conceptual models precede initiation of simulation design.  In other paradigms, design is begun prior to completion of the conceptual model.  The earlier descriptions of simulation elements are available, the more valuable feedback will be from conceptual validation review in preventing (or correcting) faults in simulation design and implementation.  The importance of this is illustrated by the frequency with which serious problems (faults) are found during conceptual validation reviews. 4.3  Conceptual Validation Perspectives.  Conceptual validation is assessment or evaluation of the simulation conceptual model (or part of it).  Conceptual validation consists of conceptual validation reviews performed on parts of the conceptual model (one of the simulation elements or the simulation context) and the accumulation of these reviews coupled with a conceptual validation review of the simulation concept.  A conceptual validation review performed on a simulation element determines appropriateness of the representation of that item in the simulation.  A conceptual validation review of the simulation concept assesses the overall capability of the simulation.  Conceptual validation reviews of simulation elements and the simulation concept are the only basis for judgment about simulation capabilities for any condition other than those specifically tested.  This makes conceptual validation extremely important in simulation assessment since only a small part of simulation capabilities are tested for any large simulation.  A conceptual validation review may even be performed on the simulation context to ensure that the constraints and boundary conditions imposed upon the simulation concept are appropriate.  The following comments generally apply to all conceptual validation reviews.SME Reviews.  Conceptual validation is normally based on SME review.  Quantitative assessments such as sensitivity analyses, comparison with data from various sources, etc. may be employed in the review as well as SME expert opinion. Review Scope and Criteria.  Conceptual validation reviews tend to ensure simulation correctness and enhance its credibility most when the scope of the review and criteria that will be used in the assessment are stated explicitly.  Generally it is best if the review scope and evaluation criteria are defined before conceptual validation reviews commence.  The review process works most smoothly when the review scope and evaluation criteria are agreed to by the simulation sponsor and developer as well as by reviewers, simulation users, and accreditation authorities.  The conceptual model for a simulation has to address both simulation space (the simulation operational and functional capability) and mission space (the representational capability of the simulation).  Comments thus far about conceptual validation have focused on mission space.  However, conceptual validation reviews also should address simulation space (“Sim Space”) issues.  For example, new Defense simulations are required to be HLA compatible [13].  A conceptual validation review would comment on this aspect of the simulation.  Such comments would indicate which version of the Real Time Infrastructure (RTI) was being assumed, what HLA compliance testing/demonstration was planned, which other simulations (HLA federates) were expected to interact with the simulation, etc.  Other Sim Space considerations might have to do with data collection capabilities for analysis of simulation results, for allowing user/operator observation and manipulation of the simulation while it is running, for capability of the simulation to interact with real systems or other specific simulations not included in the HLA compatibility requirement, etc.  Sim Space considerations that must be addressed in conceptual validation reviews are those covered by simulation requirements and criteria specified for the conceptual validation review.Review Format.  All reviews of a particular simulation should use similar reporting formats, using where possible reporting formats compatible with reviews of other simulations applied to the same kind of application.  Reports of evaluation reviews should include information and rationale as well as conclusions.  Conceptual validation evaluations should always be performed within the context of expected simulation application.4.4  Timing of Conceptual Validation Reviews.  The timing of conceptual validation reviews is dependent upon several factors.  First, a description of the simulation conceptual model must exist.  In the past, some simulation developments did not require distinct and complete documentation of the simulation conceptual model.  This severely hampered conceptual validation reviews of those simulations, with the result that simulation problems that could have been discovered earlier in simulation development were not discovered until they manifested themselves in simulation use.  The later in simulation development that a simulation fault is discovered, the more expensive it will be to correct.  Cost for correcting a software fault can be 100 times as much when the fault is not detected until late in development (compared to early fault detection and correction) [14].  A majority of software faults can be traced to faulty requirements [15].  This kind of fault in simulation development becomes readily apparent in a conceptual validation review.  Simulation development often requires significant expenditures to correct problems which could have been prevented by early and rigorous conceptual validation review.Depending upon the simulation development paradigm used, a “final” and full simulation conceptual model may be available prior to high level (or detailed design) or the final (full) conceptual model description may not be available until after design and implementation has begun. Preliminary conceptual validation reviews may be performed on a partial and preliminary conceptual model.  This kind of conceptual validation review can help to detect ideas that will cause simulation faults, but conceptual validation of a preliminary conceptual model should never be used as a basis for evaluation or assessment of the simulation because only the final conceptual model can be the basis for that judgment.  When validation review resources are limited, discretion must be employed in their use to ensure both that a sound basis exists for judgment about simulation suitability (i. e., conceptual validation of the final conceptual model) and that the simulation development also benefits from as much early conceptual model review as resources allow.A second factor affecting timing of conceptual validation reviews is availability of appropriate reviewers.  An appropriate administrative structure through which conceptual validation review personnel, especially SMEs who are outside the simulation development team, can be engaged may not exist early in simulation development. Such lack of appropriate administrative structure can prevent timely validation review of simulation requirements, allowing the simulation development contract to be issued based upon faulty requirements.  This can have major cost implications for simulation development.  Similar consequences may result if the simulation development contract fails to require that distinct documentation of the simulation conceptual model be provided in a timely fashion, and instead leaves the simulation conceptual model to be deduced from simulation design documentation.Resource limitations for conceptual validation reviews may restrict reviews to the final version of simulation elements and the conceptual model.  Perhaps only the more critical parts of the simulation will be reviewed.  Because of this, it is very important that experienced VV&A personnel be sought to provide advice about how to accomplish as much conceptual validation as possible within available resources.  Normally one can not do as much V&V as desired to reduce the risk of not fully satisfying simulation requirements.5.  The Conceptual Validation ProcessConceptual validation has two purposes: 1) increase simulation correctness and 2) enhance simulation credibility.  To enhance simulation credibility normally requires conceptual validation  to be performed (at least in part) by those outside the simulation development team and include all with vested interests in the simulation.The first step in conceptual validation is establishment of review scope and assessment criteria.  Ideally the scope would include everything, but in practice the scope of conceptual validation is often restricted to the more significant aspects of the simulation.  Assessment criteria come in two flavors.  The first concerns capability of the conceptual model to satisfy simulation requirements and is part of the general V&V of the simulation,  The second concerns capability of the conceptual model to support a particular application of the simulation and is oriented toward support of an accreditation decision. Establishing the review scope and assessment criteria must be done authoritatively by the simulation sponsor, user, or accreditation authority – otherwise the simulation developer may not be responsive to findings of the review.  Normally the scope and criteria document will have been drafted by the V&V or IV&V team and will incorporate simulation developer perspectives appropriately.The second step is identification and orientation of conceptual validation review personnel.  The subject matter determines technical expertise required.  Vested interests (such as that of a program office whose system is to be represented in the simulation) also impacts who should be included in the conceptual validation review team.  An ideal situation would have a review team that both represented all parties with vested interests and also contained other qualified experts (who have no vested interest) for objectivity.  Some simulation developments use a formal SME nomination/application form (somewhat similar to a resume) to capture relevant information about prospective conceptual validation SMEs in a structured and common format.  This helps to limit criticism about SME appropriateness when the reviews uncover problems.  Normally SMEs need orientation about the simulation, its intended applications, the criteria for their review and assessment, and, in some cases, the descriptive format for the conceptual model (this is most often the case when a design accommodation method for describing the conceptual model is employed).The third step is developing the conceptual validation review process.  This involves determining how the review will be conducted (via documents only, from documents supplemented by some interaction with the simulation development team, mainly by interactive dialogue between the reviewer and the simulation development team, by experiments with a legacy code to help deduce its underlying conceptual model, etc.) and how the review will be reported.  A structured review report form helps to ensure consistency, comprehensiveness, and comparability for reviews of different parts of a simulation when a variety of review personnel are used.  The conceptual validation review process also includes how the description of the conceptual model is collected and passed to review personnel, arrangement of meetings to support the review process, managing the reports and other documents, etc.The fourth step is the review itself.  This involves scheduling review personnel (members of the simulation development team required to support the conceptual validation reviews as well as SMEs and others from outside the simulation development team), getting appropriate materials (conceptual model descriptions, review orientation and report forms, etc.) to those involved, monitoring review processes, collecting reports from the reviews, etc.  Conceptual validation may require review of all major systems represented by the simulation (sometimes multiple reviews so all vested interests are accommodated) as well as review of the overall simulation.  Including adequate resources in V&V planning for both administration and performance of the review is essential.  It is wise to begin with review of the more critical parts of the conceptual model so that adequate time and attention will be given to the more important aspects of the conceptual validation review.The fifth step is response to the review.  It is not uncommon for the simulation developer to have a different perspective than that of the initial review.  It is wise to provide opportunity for the simulation developer to respond to the review.  Sometimes a misunderstanding reported by the review has to be corrected.  Sometimes a fault is identified and the simulation developer devises (and implements) a way to correct it.  The purpose of this kind of iteration between the reviewers and the simulation developer is to eliminate unnecessary differences about the review and to ensure that final versions of the review reflects the current situation (such as faults corrected).The sixth step is synopsis and conclusions from the review.  Here multiple reviews related to the simulation element are consolidated.  For example, the simulation representation of a radar may be reviewed by SMEs from the program office developing the radar and by SMEs from elsewhere.  The overall conclusions from all the reviews are brought together in an integrated and coherent fashion and used as input into accreditation reports.  Typically this synthesis is performed by the leader of the conceptual validation effort for the simulation.6.  Conceptual Validation Review ReportsReport of a conceptual validation review should contain the following information.  a)  It should clearly identify the conceptual model part which is being reviewed – by name, version, date, etc. of the conceptual model.  b)  It should identify the review personnel completely (name, contact information, etc.) and specify the information used by the review personnel: documents, interaction with simulation development team members by name and date, etc.  c)  It should identify the scope and criteria for representational assessment employed in the review.  d)  It should address the representational enumeration explicitly: Are all elements and aspects of the item (entities, states, behaviors, actions, tasks, etc.) to be represented included?  If not, which ones were omitted?  And are they pertinent for intended simulation applications?e)  It should assess assumptions.  Are all assumptions identified?  Are implications of these assumptions clearly and correctly identified?  What assumptions were omitted?  What implications need clarification?f)  It should address algorithms used.  Do the algorithms provide adequate fidelity (as expressed in terms of accuracy, resolution, etc.) for the simulation to support the intended applications, to satisfy simulation requirements, and to comply with criteria given as guidance for the conceptual validation review?  Are the algorithms correct, appropriate, and do they have acceptable and authoritative pedigrees?  What is the relation of these algorithms to “standard” algorithms used elsewhere within the Defense community?g)  It should provide a conclusion and synopsis of the review findings, clearly separating fact from interpretation, and explaining the significance of the findings. h)  It should include recommendations which might improve simulation correctness or credibility, or which might improve the conceptual validation review process in the future.7.  Costs of Conceptual Validation and Limits on Conceptual ValidationResources required to perform conceptual validation depend upon the size and complexity of the simulation being reviewed, the quality and correctness of the conceptual model documentation, and the level of validation required.  Experience has shown that there are three basic levels of simulation validity: inspection level, review level, and demonstration level.Inspection Level Validation can not go beyond face validation.  Data about the subject may be lacking.  Resources for more thorough VV&A may not be available.  The expected application of the simulation may not justify more extensive VV&A.  Whatever the cause, the validity of the simulation is a statement  of expert opinion that declares the simulation responses to be as expected.  The entire class of “un-validatable” simulations (simulations for which real world data are inadequate to support meaningful judgments or conclusions that simulation results and real world data are comparable and compatible) are limited to this level of simulation validation.  Lack of data about the subject represented for this kind of simulation restricts it to inspection level validation regardless of how many resources are applied to simulation VV&A.  All theater level and campaign level military models, as well as models of national economic and political behavior, fall into this unvalidatable category.  While parts of these models may be validatable, the total models are unvalidatable.Review Level Validation establishes that the simulation has structural validity and acceptable predictive validity for test cases considered.  It often also establishes that simulation behavior is sufficiently well-behaved from replication to replication that it has mathematical stability and replicative validity [16].  Both adequate data about the subject being modeled and adequate resources for extensive VV&A have been available to support this level of validation.Demonstration Level Validation is reserved for those simulations that must work correctly all the time.  These are simulations involved in safety critical systems, such as simulations supporting medical diagnostic software and nuclear power plant control or aircraft flight control systems.  Simulation performance and response can be predicted to consistently perform correctly according to objective criteria so that adequate confidence can be placed in them for the critical functions they serve.The higher the validation level, the greater the cost for conceptual validation  [17].  The more complete and perspicuous the conceptual model documentation, the easier it will be for conceptual validation review personnel to understand and correctly assess the conceptual model, which reduces the resources required for conceptual validation reviews.The level of validity attributed to a simulation is limited by the level of knowledge about the real world aspects to be represented.  The logical consistency and completeness of a simulation conceptual model may be established, but without confirming data about the real world, it will not be possible to claim more than theoretical validity (inspection level validation) for the simulation.8.  References[1] Volumes 1 & 2 of NASA-GB-002-95 (Release 1.0), Formal Methods Specification and Verification Guidebook for Software and Computer Systems, July 1995, NASA Office of Safety and Mission Assurance.  This document can be downloaded from URL:   http://eis.jpl.nasa.gov/quality/Formal_Methods/index.html  [2] Robert O. Lewis and Gary Q. Coe, “A Comparison Between the CMMS and the Conceptual Model of the Federation,” 97 Fall Simulation Interoperability Workshop Papers, September 1997, Volume 1, pp. 1-11.[3] Jack Sheehan, Terry Prosser, Harry Conley, George Stone, Kevin Yentz, and Janet Morrow, “Conceptual Models of the Mission Space (CMMS):  Basic Concepts, Advanced Techniques, and Pragmatic Examples,” 98 Spring Simulation Interoperability Workshop Papers, March 1998, Volume 2, pp. 744-751.[4] Thomas H. Johnson, “Mission Space Model Development, Reuse and the Conceptual Models of the Mission Space Toolset,: ,” 98 Spring Simulation Interoperability Workshop Papers, March 1998, Volume 2, pp. 893-900.[5] Patrick J. Roache (Verification and Validation in Computational Science and Engineering, Hermosa Press, 1998, ISBN 0-913478-08-3). [6] G. N. Hone and M. R. Moulding, “Developments in Application Domain Modelling for the Verification and Validation of Synthetic Environments:  Training Process and System Definition.” (Proceedings of the Spring 1999 Simulation Interoperability Workshop, March 15-19,1999, Orlando, FL.[7] B. P. Zeigler, T.G. Kim, and H. Praehofer, Theory of Modeling and Simulation. 2 ed. 1999, New York, NY: Academic Press.[8] Jack Sheehan, Terry Prosser, Harry Conley, George Stone, Kevin Yentz, and Janet Morrow, “Conceptual Models of the Mission Space (CMMS):  Basic Concepts, Advanced Techniques, and Pragmatic Examples,” 98 Spring Simulation Interoperability Workshop Papers, March 1998, Volume 2, pp. 744-751.[9] Thomas H. Johnson, “Mission Space Model Development, Reuse and the Conceptual Models of the Mission Space Toolset,: ,” 98 Spring Simulation Interoperability Workshop Papers, March 1998, Volume 2, pp. 893-900.[10] Paul K. Davis and James H. Bigelow, Experiments in Multiresolution Modeling (MRM), The RAND Corporation National Defense Research Institute Report MR-1004-DARPA, 1998.  ISBN 0-8330-2653-4.[11] A. M. Law and W. D. Kelton, Simulation Modeling and Analysis, 3rd Edition (McGraw-Hill), 1999.[12] HLA Federation Development and Execution Process Model, Version 1.3, dated 09 December 1998.[13] USD(A&T) Memorandum, “DoD High Level Architecture (HLA) for Simulations,” 10 September 1996.[14] Miller, L.A., Groundwater, E., and Kirsky, S.M., Survey and Assessment of Conventional Software Verification and Validation Methods, U.S. Nuclear Regulatory Commission Report NUREG/CR-6018, Washington, D.C., 1993.[15] R. O. Lewis, Independent Verification and Validation:  A Life Cycle Engineering Process for Quality Software, John Wiley & Sons, New York, 1992.[16] For explanation of what structural, predictive, and replicative validity mean, see B. Zeigler, “A Framework for Modeling and Simulation,” Chapter 3 in Modeling and Simulation:  An Integrated Approach to Development and Operation, Cloud, D.J. and L.B. Raines (eds.), McGraw-Hill, 1998, pp. 67-103.[17] Dale K. Pace, “An Aspect of Simulation Cost,” PHALANX, March 1997, pp. 12-16.9.  About the AuthorDale K. Pace, a member of the Principal Professional Staff of The Johns Hopkins University Applied Physics Laboratory, is a specialist in operations research, modeling and simulation, analysis, and wargaming.  An initial member of the Simulation Interoperability Workshop (SIW) Conference Committee, Dr. Pace has been involved in SIW’s current simulation fidelity endeavors.  He was co-chair of the Military Operations Research Society (MORS) Simulation Validation (SIMVAL) 1999 Workshop and is a member of the Defense Modeling and Simulation Office (DMSO) Verification, Validation, and Accreditation (VV&A) Technical Working Group and its VV&A Technical Support Team.  He was the initial lead for the validation part of the independent verification and validation (IV&V) team for Wargame 2000.  And he is Simulation’s Associate Editor for Validation. This paper was prepared under Defense Modeling and Simulation Office (DMSO) sponsorship, with oversight by its Verification, Validation, and Accreditation (VV&A) Technical Director, Ms. Simone M. Youngblood.  The views of this paper are those of the author and should not be construed to represent views of DMSO or of any other organization or agency, public or private. EMBED PowerPoint.Slide.8  