The Dynamic Adaptive Threat Environment ArchitectureMartin R. Stytz, Ph.D.Sheila B. Banks, Ph.D.Air Force Research LaboratoryAir Force Research LaboratoryWright-Patterson AFB, OH  45431Orlando, FL  32828 HYPERLINK "mailto:mstytz@att.net" mstytz@att.net,  HYPERLINK mailto:mstytz@acm.org mstytz@acm.org HYPERLINK mailto:sbanks@calculated-insight.com sbanks@calculated-insight.comKeywords:software architectue, component software, gauges, software frameworks, computer-generated actors, containerization, actor migration, rapid prototyping, exploratory prototyping, migration assistantAbstract: We describe the software architecture of the Dynamic Adaptive Threat Environment (DATE).  DATE supports the development and deployment of different types of computer-generated actors (CGAs) in large-scale Distributed Interactive Simulation (DIS) and High Level Architecture (HLA) based simulation environments. To address critical issues in software architecture and rapid prototyping, the architecture employs object-oriented techniques, component software, software frameworks, gauges, migration assistants, aspect oriented programming, and containerization to our goals of achieve composability, flexibility, re-usability, and generality.1.	IntroductionIn this paper we describe the architecture of the Dynamic Adaptive Threat Environment (DATE) software.  DATE supports the development and deployment of different types of computer-generated actors (CGAs) for large-scale Distributed Interactive Simulation (DIS) and High Level Architecture (HLA) based simulations. The architecture exploits the technical advantages provided by object-orientation, component software, software frameworks, aspect oriented programming, gauges, migration assistants, containerization, and rapid prototyping to achieve its goals of composability, flexibility, re-usability, and generality for CGAs.  The DATE object-oriented architecture supports the Rapid Evolutionary and Exploratory Prototyping (REEP) rapid prototyping software development approach,37 thereby helping to minimize costs and aid in requirements refinement.  The DATE architecture consists of highly modular components and two frameworks where interdependencies are well-defined, documented, and minimized.  The DATE architecture and design is an open architecture and supports an open design.  DATE is a completely composable system and permits components and objects to be independently developed and then integrated into DATE without disturbing or distressing existing DATE software.DATE supports assembly of a distributed threat environment composed of CGAs operating as surface threats, air threats, and jamming systems within a distributed virtual environment (DVE) also called a distributed simulation environment (DSE).  To be useful and provide a realistic environment, DATE CGAs must mimic the observable behaviors of their real-world counterparts; therefore DATE CGAs must perceive the environment, maintain a model of the environment and tactical situation, plan actions, react to situations, monitor their own action(s), and communicate with other actors (both human and computer-controlled).  However, the technologies needed for the capabilities that are required are still being developed; hence, the system architecture must support experimentation, prototyping, and continuing software and CGA development.  Nevertheless, the prime concern for the software architecture must be addressing the run-time challenges for a CGA that arise when computing human-like behaviors and reactions to a complex dynamic environment at a human-scale rate of time.  Most important of all, the CGA’s behavior must be realistic and accurate enough so that it triggers tactically and behaviorally correct responses by other humans and computer-controlled actors.  As a result of the complexity of computer-generated actor applications and the uncertainty of the specific requirements, we designed DATE to be able to exploit evolutionary software development40 and REEP37. In this paper we discuss the DATE architecture.  In the next section we present a background on related work.  In Section Three we describe the architecture and its constituent elements and their functionality.  Section Four contains a summary and suggestions for future work.  2.	BackgroundIn this section we review the background information we used when formulating the DATE architecture.  The next subsection contains definitions of terms used in the architecture.  Subsection Two contains a brief review of previous related work.  Subsection Three contains a description of the foundational architectural technologies used in DATE.  Subsection Four contains a brief discussion of Aspect Oriented Programming.  Subsection Five contains a discussion of the CGA considerations that had an impact upon the architecture.2.1	DefinitionsThroughout the remainder of this paper, we will use the terms architecture and system architecture.  The term architecture refers to the organization of a software system as expressed in its components, organization, and communication used within an application.  The architecture for a system documents the set of decisions that have been made concerning the organization of a software system, the structural elements in the system, the interfaces between the elements, and the behavior of the elements and the system.  The architecture for a system describes how the structural and the behavioral elements are composed, and in essence describes all of the components of a system and how they cooperate to achieve the functionalities desired for the system.  An architecture is much more than a simple enumeration of system capabilities; an architecture consists of a specification of the capabilities and the pieces of software that are needed to achieve these capabilities.Within the DATE architecture XE "architecture" , the following terms are defined39.  An entity is a computer model in use within DATE that can change its state.  An entity can be a computer model of a manned aircraft, a manned armored vehicle, an unmanned combat air vehicle, the weather, solar activity, a command and control network, a computer network, or any other actual or theoretical thing in the real world.  An actor is an entity that has intelligence (either computer-based or human-based). An information stream is a logical path through the architecture from an information source to a designated information sink.  A container XE "container"  is a permanent, unvarying software object that consists of a data structure plus software methods.  Containers are used to carry data along information streams.  Every container holds data that is exported from an object or a component within DATE.  There are six types of containers used in DATE, as discussed in Section 3.3.  Containers are structured into pallets and slots.  A pallet XE "pallet"  is a major category of information about entities within a container.  For example, in a military simulation, there would be pallets defined for Red entities (for entities that belong to enemy forces), Blue entities (for entities that belong to friendly forces), Green entities (for entities that belong to neutral forces), and Yellow entities (for entities that belong to unknown forces).  Pallets within a container can be nested hierarchically.  The data for an individual entity XE "entity"  is assigned to a pallet within a container according to the entity’s type and within a pallet each entity has its own slot XE "slot" .  A slot contains all of the data related to an entity for a given container.  An entity has only one slot in a container.  All of the information for an entity needed by any recipient on a given information stream is contained within its slot in the container.An incoming data stream contains data destined for an entity.  An outgoing data stream contains data that originated at a local entity and is headed toward the DSE.  Incoming and outgoing are global views of container and information stream operation.  An inbound container XE "inbound container"  is a container that is carrying data into the Common Object DataBase (CODB) or entity that the CODB/entity must read.  An outbound container XE "outbound container"  is a container that is carrying data away from the CODB or an entity and the CODB/entity must write to the container.  Inbound/outbound are information centric views of container operation.  A gauge is software that converts data collected by a software probe into a measure that is meaningful for a particular system for the purpose of performance tuning, information assurance, functional validation, compatibility, or assessment of operational correctness. Gauge outputs are written in XML.  A software probe is software that interacts with an operating system, operational application, or subset of an application to collect data for a gauge.  Software probe output data are written in XML.2.2	Previous WorkThe run-time challenges for a CGA lie in computing human-like behaviors and reactions to a complex, dynamic environment at a human-scale rate of time. A large body of work has been developed that discusses these and other issues that must be addressed when assembling a CGA.  The architectural aspects of CGAs have been addressed often and a wide variety of approaches have  been reported 1, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 31, 33, 34, 35, 36.  The architectural approaches range from modular software libraries and interacting processes to closely coupled objects, data-flow architectures, and clusters of agents.  We will discuss a few seminal approaches in the following paragraphs.The system described by Braudaway7 uses a single blackboard for all of the CGAs hosted on a single computer. The blackboard has three components: the blackboard data structure, knowledge sources, and a controller.  Each knowledge source is a self-contained specialist intelligent agent.  Each agent can be implemented as a rule-based program, procedural program, a logic program, or a functional program.  The blackboard database holds all of the information, hypotheses, partial solutions, and the problem solving state.  The controller determines the next action to be performed based upon actions nominated by the agents.Bokma and Slade6 describe the Consensus Method, which also takes an agent and blackboard-based approach to implementing CGAs.  Their architecture consists of knowledge sources, local databases, local controllers, and communication channels at each host computer.The ITEMS system, (Interactive Tactical Environment Management System) as described by Siksik36, was designed to simulate Army company-level activities in a battlespace.  In ITEMS, actors are defined using libraries that specify the sensors, weapons, communications, dynamics, and knowledge for the actor.  ITEMS uses rule-based expert systems and frames to perform decision making.  Within ITEMS, knowledge is encapsulated in frames, with each frame responsible for representing facts about a specific type of knowledge.  The ITEMS expert system has three separate knowledge representations: 1) a database, 2) a rules database, and 3) a common database.  The database holds all of the information required to represent a scenario and also holds the rules database.  The rules database holds the knowledge needed to support decision-making for tasks such as motion, navigation, communication, tactics, and mission execution.  The common database holds transient knowledge concerning the state of the DSE.Calder10 discusses a software architecture for a command forces command entity system that can simulate company and above level commanders.  The main software components of the system are the Situation Awareness class, the Terrain class, the Planner class, and the Event Processor class.  In this system, knowledge is organized into frames, which they call events.  Events are managed by the Event Processor.  The Situation Awareness class assembles information concerning mission objectives, location and strength of known enemy and friendly formations, the location of known obstacles, and remaining mission time.  The Terrain class computes mobility corridors, battle positions, and routes.  The Planner class generates, evaluates, and selects courses of action using the knowledge contained in Constraint Sets.  The Mission Tracker class monitors the progress of the unit in executing the mission. Calder9 also discusses the ModSAF command and control system.  ModSAF is composed of a set of extensible software modules that were functionally defined using object-oriented techniques.  The system exploits four software engineering techniques: 1) layering, 2) object-based programming, 3) object interface specification, and 4) data-driven execution.Reece and Kelly33 describe a software architecture for individual human combatants in a DSE.  The architecture consists of four levels: 1) physical, 2) feedback control, 3) action selection, and 4) problem solving.  The physical level describes the physical interaction and perception of the combatant and contains their fatigue model.  The control level provides feedback control to the activities/actions in the physical level that require feedback.  In the action selection level knowledge is used to select actions to be performed.  The problem solving level performs planning.Butler8 describes the Joint Simulation System (JSIMS) software architecture.  The JSIMS reference architecture has three tiers: the virtual tier, the model tier, and the physical tier.  The virtual tier contains the description of the synthetic environment, performs rendering, and handles networking tasks.  The model tier organizes the model.  The physical tier describes the physical resources, such as computers, networks, facilities, and software, that support the operation of the simulation environment.  JSIMS uses a core software subsystem to support the architecture and to support composition; composition efforts are based upon attaching changed parts to the stable core.  The layers of the architecture are the basic unit of decomposition.2.3	Foundational Architectural TechnologiesA focus of for the software development community has been the assembly of complex software systems from simpler software components.  Software component technology is becoming a key element of achieving this vision and in enabling software reuse coupled with cost-effective maintenance of legacy software systems.  A software component15 can be defined as a nontrivial, nearly independent, and replaceable part of a system that fulfills a clear function in a well-defined architecture or defined as a unit of composition with contractually specified interfaces and explicit context dependencies.  A component can also be defined as a physical packaging of executable software with a well-defined and published interface.  In general, a component is an amalgam of objects and encompasses more complexity than is found in a single object.  The emphasis in component software is on well-defined interfaces that are separate from the underlying implementation.  Component design decisions are driven by a number of factors, the leading factors being those that define component granularity.  Intercomponent communication is typically expensive in terms of resources consumed, which encourages the use of large components.  However, larger components have more complex interfaces, are more affected by change, and are more challenging to reuse, which encourages the use of small components.  Additionally, as components increase in size, the flexibility of the underlying system structure decreases.  Additional design tension arises from the need to balance conflicting demands between the need for high cohesion and for low coupling between components and objects.To operate properly, components need a reference model for their interface definitions, message passing, and data transfer.  A framework performs these functions within a component-based software architecture. A framework is a reusable part of a system that is represented by a set of abstract classes and a description of the manner in which these class instances interact.  In DATE, the framework is a software skeleton for an application that can be customized by a developer.  The frameworks that are used in DATE provide a support infrastructure, interface standards, and execution scaffolding for DATE components and objects and serve to support aspect oriented programming (AOP) techniques as discussed later in this paper.  A framework is more than a single object, a framework is generally composed of a number of objects knitted together to express a larger unit of functionality.  Conceptually, a framework serves as a backplane that interconnects the components and objects that form the application.  A framework provides an execution environment for implemented domain components and domain objects and provides services and facilities to support a set of primitives for a group of components.  Frameworks coupled with AOP are an ideal conceptualization for enabling re-use and experimentation because they allow functionality to be captured at multiple levels of abstraction and enable re-use at multiple levels of encapsulation.There are several key tasks for a framework designer.  These tasks are the specification of the contents of the framework (which are the methods and variables), the specification of the data and the control flows between the components via the framework, the specification of the interface to the framework, and documentation of the procedures used to insert a new method, variable, or functionality into the framework.  The framework specification should also contain a description of the critical functions performed by the framework and of all the variables used in the framework.  One of the chief motivations for and attractions of components and frameworks are their capability for supporting the assembly of systems through composition.An architectural framework must enable low coupling and high information hiding between components and objects with the goal of enabling system software composition at all levels and of enabling rapid interchange of components and objects.  The framework should support experimentation and evolutionary development of functionality for all components and objects.  In a CGA, a framework should support the operation of any type of decision mechanism or hybrid decision mechanism, be able to integrate any type of knowledge, and inherently support actor migration and data logging.Software gauges are constructs used to enable rapid assembly of systems from components.  A software gauge is a display system for data collected using a software probe.  A software probe collects information by intercepting data in transit between components/objects in a system.  Software gauges arose out of the need to be able to assemble new systems from existing software and out of the realization that current and future complex software systems can never be adequately tested in their intended environment.There are a number of objectives for software gauges.  The objectives include a reduction is software debugging time, reduction in the size (number of megabytes) of compiled software, simplification of software evolution and maintenance, a need to measure satisfaction of system configuration and performance requirements, and a need for detection of events that indicate that system errors have occurred or that the system is behaving in an unacceptable manner.  Software gauges allow a designer and implementer to rapidly answer questions such as: 1) are the versions of the software compatible, 2) what data is moving across the seams in the system, 3) where is most of the computational activity in the system, 4) is the data being correctly translated, 5) are the components/objects in the system receiving the data that they need in the proper format, 6) what components are connected and exchanging data, and 7) does the implementation of a component satisfy the design requirements.  A software gauge can help to determine if changes made to a system have introduced error and where errors are located.  Gauges can also be used to aggregate and visualize configuration and usage information.  A software gauge allows a designer or implementor to view the configuration of a system at multiple levels of abstraction and to conduct experiments with different configurations.  Gauges can be used to assess the suitability of two components for interaction before, during, and after software architecture transformations.  Gauges can provide data concerning temporal usage patterns, component activity, and differences in configuration between different versions of the same component.  Gauges help to insure architectural conformance and help to determine the functional similarity between different components.There are seven basic types of gauges:  1) event gauges, 2) component gauges, 3) connector gauges, 4) configuration gauges, 5) constraint gauges, 6) runtime gauges, and 7) dependability gauges.  An event gauge measures the performance of interaction protocols and can aid in assessing usage patterns.  A component gauge is designed to enable functional compatibility and prevent functional saturation.  A component gauge measures a component's semantic fit in a system using ontological information embedded in both the system and the component.  Connector/conformance gauges define and provide terminators for connectors, and thereby insure data and infrastructure compatibility.  Connector/conformance gauges allow a system architect to determine when it is meaningful and safe or unsafe for two or more components to interact.  A data compatibility gauge is used to measure the amount of work required to transform data as it moves from component to component, where work is defined as the number of intermediate functional components and data converters used.  This type of gauge helps to determine if there will be difficulties in replacing or combining candidate components that are caused by differences in functions that they can perform or by differences in the content of the data that they manipulate.  A configuration gauge is used to measure component interaction and usage and is used to measure quality of service between the components in a system.  A configuration gauge enables up-front prediction of the architectural suitability of systems/components being integrated to form an application.  A configuration gauge can be used to measure the information needed to perform component architectural merging, determine inconsistencies, and calculate the percentage of essential and optional functionality remaining within a system once a given type of component is integrated.  A constraint gauge is used to verify that an architecture meets design needs and component resource needs and is also used to compute bounds for runtime gauges.  A runtime gauge is used to determine if a system’s dynamic behavior satisfies expectations and requirements.  A dependability gauge is used to automatically measure and determine if a component and an overall system conforms to specified security policies.In addition to the seven gauges discussed above, other types of gauges have been suggested.  Some of these gauges are the connectivity gauge (which measures how changes were made to accomodate a new component), the functional gauge (which enables automatic system tuning for performance or other uses), the intercomponent compatibility gauge (which measures the risks/challenges associated with inserting a new component into a system), and the real-time performance gauge (which measures the real-time performance and quality of service for a system).  Gauges should be inserted at the seams/interfaces of a system.  An object-oriented analysis and the resulting documentation are crucial for locating these seams and interfaces.The Extensible Markup Language (XML) is a meta-language that permits the definition of a special-purpose language, including syntax, data types, vocabulary, and operators by using a Document Type Definition (DTD).  XML also supports the definition of customized markup components.  The customized markup components, called tags, are also documented within a Document Type Definition.  The DTD describes a vocabulary and syntax for use within a document to be transmitted. Several factors supported our decision to use XML within DATE for certain specific communication tasks.  First, XML supports a flexible approach to formatting.  The XML capability to define and use custom tags and the minimal requirements for the language indicated that we would be able to express any format needed within the boundaries of the language. Second, XML is widely used and is standardized and the basic components of the language are stable and well understood; as a result, the language supports re-use and extension of formats.  Finally, XML is precise; it has a well-defined set of rules for describing a document and for ordering the contents of a document without imposing semantics on an application.2.4	Aspect-Oriented ProgrammingA final consideration that influenced our architectural approach, even if only indirectly, is aspect oriented programming (see Elrad, Lieberherr, Ossher, Bergmans, Kiczales, Pace16,22,28,29,30,32).  Aspect oriented programming (AOP) is a software development paradigm that is based on the idea that computer systems are best programmed by separately identifying the various areas of interest for the system, describing their relationships, and then using mechanisms in the underlying environment to compose the individual areas of interest into a system.  Aspect oriented programming does not replace object-oriented programming, rather it looks to increase the expressiveness of object-oriented techniques by identifying areas of interest that do not fit naturally into an object hierarchy and then treating them as separate, first-class objects within the system.  Areas of interest, called concerns, can range from high level notions like security to low-level concepts like caching and buffering.  Aspect oriented programming consciously makes all concerns (no matter how scattered) first-class elements and separates them from the object structure in the system thereby allowing them to cut across multiple object hierarchies. AOP is not a replacement for object-oriented techniques, rather it is intended to address the perceived shortcomings of object-oriented software development.  A key concept in AOP is the use of aspects as a mechanism to localize the expression of a concern that spans several objects or object hierarchies.  As a result of this aggregation of concerns, re-use is enabled and the complexities of the software system are, by and large, hidden from the software developer and maintainer.  When applying AOP, a few of the issues that must be addressed are the interface between the aspects and the remainder of the system, encapsulation of aspects, decomposition of the system to yield aspects, aspect visibility, dynamic and static aspect realization, decoupling between aspects, decoupling between aspects and the remainder of the system, and fitting the development of aspects into the methodology used for developing the overall system.  In DATE, we managed these issues associated with AOP and incorporated this technology into the architecture by using frameworks and components.  The two frameworks capture aspects at two levels of the architecture.  The decomposition of the knowledge base and decision mechanisms were also guided by AOP considerations.2.5	Additional CGA Architectural  ConsiderationsIn the development of the architecture, a prime consideration was support for CGA migration because of the difficulties migration poses, the need to bound the costs imposed on a host by an incoming CGA, and because of migration’s importance to future simulation efforts.  Migration is important because it is the chief means by which a distributed simulation environment can enable a CGA (or other actor or entity) to satisfy real-time interaction performance requirements for a geographically dispersed set of actors.  Migration is important and possible because CGAs are persistent within the environment and because CGAs have long-lived goals (such as mission objectives, tactical plans, and doctrine) that drive and constrain their activity.  Migration requires that a computer host capture the state of the CGA and transmit that state to a recipient host where the CGA is re-instantiated in a manner that makes the migration undetectable to other distributed simulation participants.An issue that arises as a consequence of actor migration is the computational burden imposed upon the recipient host by the arriving CGA.  The current common approaches to migration either statically assign the migration times and locations or permit migration to occur whenever it is required to whatever host is convenient.  We find both of these approaches to be deficient.  The first approach is too brittle for effective use because as additional hosts are added to the system, the computational load can vary widely during the course of an environment’s/federation’s execution, and the cost of hand-crafting the performance of an environment will become prohibitive.  On the other hand, an approach to migration that permits a CGA to move to a host at any time without consideration for the cost imposed upon the recipient host is clearly untenable because of the potential impact upon the performance of other actors at the recipient host by the incoming CGA.  We believe that for migration to be effective and useful, the migrating CGA should be permitted to dynamically determine the host it will move to based upon the costs it would impose upon a recipient, the CGA’s performance requirements, and the performance capability and unused capacity of potential hosts.As suggested by Jamali19, an economic model of the costs and requirements for the CGA appears to be an effective means for guiding the selection of a recipient host for a migrating CGA.  This cost assessment can also provide a bounds for the computational costs imposed by a CGA and thereby permit a host to determine if it should permit a CGA to migrate to it.  To make use of this or any other cost model for migration, a protocol for the exchange of information between the CGA and a host is required.  The protocol must allow the CGA to provide information concerning the computational, memory, network bandwidth, security, and processing priority costs it will impose upon the recipient host.  The protocol should also allow the CGA to describe the granularity of the costs it will impose (how often it must be scheduled to run, the amount of incoming and outgoing information required, etc.), its expected duration at the host, and the CGA’s publication and subscription requirements (for the HLA).  Because of the complexity of the communication (or negotiation) and of the protocol and because the migration communication should be used by every CGA, we believe that these functions should be separated from the CGA and provided by the supporting software infrastructure at each host.  In AOP terminology, this is an obvious crosscutting concern and should form its own aspect.  We call this aspect a migration assistant.The migration assistant operates by first gathering a migrating CGA’s requirements.  With the requirements in hand, the migration assistant then begins the search for a host that can satisfy the requirements for the CGA.  The search should be guided by the priority that the migrating CGA assigns to each of its requirements, so that the highest priority item for the CGA serves as the initial discriminator in the selection process for a new host.  Once a host commits to accepting the CGA, the migration assistant at the current host negotiates with the potential recipient to determine if the potential host can support the CGA and if it will agree to accept the CGA.  Once a host is located, the migration assistant informs the CGA and the host of the information each needs to have in order to successfully perform the migration and also informs the simulation environment manager(s) of the migration and of the new host for the CGA.  The CGA then fills in the migration container and dispatches it to the recipient host.  Once the migration completes, the migration assistant at the recipient host informs the simulation environment manager(s) that the migration is complete, thereby permitting all affected participants in the simulation environment to update their publication and subscription information for the CGA.  Other considerations and requirements have been discussed in depth by us in a number of papers2,3,4,38.3.	The DATE ArchitectureThe DATE architecture is a data-handling architecture that exploits the technical advantages offered by object-oriented techniques, the CODB, component software15, object frameworks, gauges, information streams, migration assistants, aspect oriented programming, and containerization37. The DATE architecture supports assembly of composable applications and permits components and objects to be independently developed and then integrated without disturbing or distressing existing software.Components define the major aspects of the architecture, objects are used to flesh out the specification of the components.  The degree to which component software supports composition relates to the degree to which the architecture/design supports “plug-in” of components and the degree to which components adhere to predefined constraints and conventions.  To be useful, the constraints and conventions must specify the functionality that each component offers to the application as well as the architecture/component interface properties.  These properties, constraints, and conventions are captured within a framework.  The DATE framework, which is specified at two levels within the architecture, provides the communication and coordination services needed to assemble applications from components and acts as the plumbing that interconnects the components.  The DATE framework provides an execution environment for implemented domain components and domain objects and provides services and facilities to support a set of semantic primitives for a group of components15.  The DATE framework also guarantees message delivery and performs transaction management.Two levels of frameworks are used in DATE.  One framework is at the highest level of the architecture (CODB and containers) and the second level framework supports the individual actors.  The highest level framework provides the information routing (information stream) and data management services required by the major system components.  The second level framework provides a set of services that all CGAs require (such as data filtering, sensor filtering, and data management) and de-couples the individual CGA components from each other and from the remainder of the system.  The CODB functions as the central data repository and information router between all of the DATE components and also insures that all of the information publication and subscription requirements in the applicable Federation Object Model or Simulation Object Model are met.  During operation, the CODB receives inbound information for all of the data streams that it services, determines the recipients of the data, and stores the information until requested, at which time the information is dispatched in a container.  From the perspective of the high-level framework, each actor is a component in DATE.  Each actor receives the information that it requires (both data and control) from the CODB along a dedicated information stream and sends data and control information back to the CODB on the same dedicated information stream.  Data is transported on an information stream using containers.  Each CGA component is, in turn, composed of a framework consisting of a set of components and two data interfaces that perform data management within the actor component.The logical view of the DATE architecture, highlighting its major components, is presented in Figure 1.  In the architecture, the Network Interface and Network component is responsible for the transmission of information between DATE and the other computers in the DSE.  This component encapsulates the HLA Run Rime Infrastructure (RTI).  As information arrives at a DATE host, it is forwarded from the Network Interface software to the World State Manager (WSM).  The World State Manager maintains the entire state of the DSE based upon the information it receives across the network.  The World State Manager takes incoming data and uses it to update its information about all the entities in the DSE and places the information into a container for transmission to the CODB.  In addition, the WSM performs dead-reckoning for entities in-between receipt of state updates for each entity.  Whenever the CODB requests an update from the WSM, the WSM is required to have a container with current DSE state information ready to be dispatched.  Once in the CODB, the data is routed to every resident entity via information streams.3.1	The Common Object DataBaseOnce the DSE state information reaches the CODB, the data is repackaged into outgoing containers and dispatched on an outbound information stream.  The repackaging is accomplished by methods in the CODB that perform coordinate conversion, filtering, and routing.  The data from the CODB either moves directly to a recipient or the data moves to a sub-CODB, which is an information stream specialized form of the CODB.  Once information reaches a sub-CODB, the data is dispatched from there to the actors or entities serviced by the sub-CODB.  The containers that depart the CODB or a sub-CODB along an information stream for a recipient are customized for the entity(s) and actor(s) on the stream.  The migration assistant in the architecture resides in the CODB.  The migration assistant gathers the requirements for a migrating CGA from the CGA in an XML formatted message.  These requirements are used to determine the candidate target hosts for the outbound CGA.  Once a target host is selected, the migration assistant transmits the required information in an XML formatted message to the migration assistant at the target host.  All communication and negotiation between migration assistants is performed using XML formatted messages.  Once an intended recipient host indicates its agreement to accept the migrating CGA and its ability to satisfy the CGA’s operational requirements, the migration assistant at the sending host signals the recipient host that migration will commence and signals the CGA that it can start migrating by sending the CGA an XML-formatted message in a control container. Performance considerations constrain the information that is allowed to be sent during a migration, in addition to basic state information about the CGA, the CGA is permitted to transmit only a limited portion of its knowledge base.  The software and almost all of the knowledge base required for CGA execution must reside at the target host before migration occurs.  The migration assistant is responsible for insuring CGA software and knowledge base availability at the recipient before migration commences.The containers that depart a CODB or sub-CODB hold the DSE information required by the actors/entities on  the stream or they hold control XE "control"  information targeted at one or more actors/entities on the stream.  The CODB, and all of its sub-CODBs, is also used to store and forward state information from actors/entities hosted by a DATE instantiation to the network environment through the WSM XE "World State Manager" .  The CODB and WSM components XE "components"  work together to insure that each DATE instantiation satisfies its data transmission requirements XE "requirements"  by consolidating the output from the actors/entities and then transmitting data to the rest of the DSE.Whenever a new entity XE "entity"  appears in the networked environment, the CODB XE "CODB"  is informed of this event by the WSM XE "World State Manager"  via a message from the WSM that is placed in a control XE "control"  container XE "container" .  When the CODB is informed of the new entity, the WSM must supply the entity state, including ID, alliance, type, class, and location, at a minimum in addition to the container that the new entity will be assigned to, its pallet XE "pallet" , and its slot XE "slot" .  Finally, the CODB determines which of its outbound container XE "outbound container" s require information about this new entity and then makes the appropriate container assignments and instantiates a new container if it is required.  When an entity is removed from the environment, the WSM informs the CODB of this event.  The CODB then destroys any containers occupied only by this entity and informs the entities served by any affected containers that the entity was removed.3.2	DATE Information StreamsIn the DATE architecture XE "architecture" , the inbound and outbound information stream XE "information stream" s organize the information transportation activities and the services provided by the highest level framework XE "framework" .  Within the architecture, all of the information (data and control XE "control" ) required by an entity or actor XE "actor"  comes to the entity or actor via its inbound information stream.  All of the information (data and control) produced by an entity or actor and destined for the network environment or for another local DATE component XE "component"  departs the entity or actor via its outbound information stream.  By using information streams, we optimize the volume of information transported from the CODB XE "CODB"  to the entities or actors in a DATE instantiation and simplify the construction and operation of the actors/entities.  Information streams also allow us to simplify the information flows and Figure 1:  The DATE System Architecture.control flows within the architecture.  Information streams also serve to explicitly specify the information and control flows within the DATE architecture.  There is generally one information stream for each type of entity/actor within a DATE-architecture application operating on a given host.  There can also be one dedicated information stream used to transport “hard” real-time data from an actor or entity or component to the CODB and a separate dedicated information stream to transport “hard” real-time data from the CODB to any actors or entities or components that require it.  Containers on an information stream are double-buffered; that is, there are two containers on each information stream, one for reading and one for writing.  These two containers switch roles when the readers complete their read function.3.3	DATE Containers and Their OperationThe data in the DATE containers is written XML, which insures that any component that is attached to an information stream can access the data in the stream.  Container access is straightforward.  A component uses its internal methods to access the container on the stream(s) that service it, retrieves the data in the container in XML, and then translates the data from XML to whatever internal format(s) that the component may require.  The CODB is responsible for translating the data from the format used in the DSE into XML and for placing the resulting data into the proper containers on the information streams that service the intended recipients of the data.  A single piece of data can be placed into more than one information stream at any given time, data recipients determine the content of their streams and the CODB is responsible for servicing the recipients and placing the required data into whichever streams require the data.The methods portion of a container is composed of software routines that provide gauges, handle the movement of the data in the container along the information stream, and insure that the data remain uncorrupted during transmission.  The gauges allow a DATE-based application to assess its own health, assess the accuracy of its performance, and provide other information concerning the operation of DATE and the accuracy of the data.  The containers also contain intelligent agents that are used to verify the accuracy of connections (at run-time and during assembly) and select the gauges that should be enabled.  The output of the agents is written in XML.The main CODB XE "CODB"  has six types of container XE "inbound container" s that are inbound from the WSM XE "World State Manager" : 1) entity XE "entity" , 2) phenomenology XE "phenomenology" , 3) emissions XE "emissions" , 4) transient XE "transient" , 5) control XE "control" , and 6) migration XE "actor migration" .  The information in these six containers comes from the WSM or in the case of a control container it can also depart from a DATE object or component.  The entity container contains state information for all entities in the DSE.  The phenomenology container holds information about all phenomenology in the DSE except for sensor emissions.  The emissions container holds all sensor emission data, such as radar, infrared, sound, etc. and their status, operational wavelength, orientation, waveform, and power for every sensor in the DSE.  The transient container holds information about transient DSE events such as missile launchings, weapon firings, or other occurrences that are known to have a brief existence within the DSE.  The control container holds information concerning filtering XE "filter"  or other object control information, like halt, migrate, or resume.  The migration container contains information concerning the state of an entity that is either migrating to or from a DATE host.  The CODB has five types of outbound container XE "outbound container" s that carry data to the WSM: 1) entity, 2) phenomenology, 3) emissions, 4) transient and 5) migration.  The functionality of the outbound containers mirrors the functionality of the corresponding inbound containers.  The same types of containers are also used on the inbound and outbound information streams from the CODB to the actors/entities on its information streams.Note that because all reader side components of a container share the same copy of the virtual environment’s state, we insure that they share a consistent description of the world.  When a reader finishes with a container, the reader switches to a newly filled container of data provided by a writer, such as the WSM, once the writer signals that the new container is ready.3.4	DATE Actor FrameworksWithin each DATE actor framework, we identified six major components.  These six components are the Physical Representation Component, the Cognitive Representation Component, the Skills Component, the Physical State Information Interface, the Sensor Interface and the Threat Knowledge DataBase (or Knowledge Base).  The Physical Representation Component (PRC) contains the description of all of the physical attributes and properties of an individual CGA and has three major sub-objects, the Dynamics, Sensor Interface, and the Sensor objects.  The PRC encapsulates one or more physical models for the operation of a dynamics unit or sensor within a single package for the CGA, and each CGA can access one or more dynamics models or sensor models.  The PRC’s Dynamics sub-component includes the information and models that define CGA-specific motion properties, performance capabilities, weapons load, damage, and physical status.  The Sensor sub-component contains the sensor model(s) used by the CGA.  The other component of the PRC is the Sensor Interface (SI).  The Sensor Interface is responsible for extracting information from incoming containers and providing each sensor model for the CGA with the information that it needs in order to function.  To increase the fidelity of the operation of DATE CGAs within the DSE, each CGA within DATE has its information restricted by filtering the incoming data so that the CGA operates only upon a realistic set of information.  The sensor-filtered information is then forwarded for use in the CRC.  Data filtering is performed by the Physical State Information Interface, the Sensor models, and in the Sensor Interface. The decision-making system consists of two components, the Skills Component (SC) and the Cognitive Representation Component (CRC).  The SC models the skills and ability of the simulated operator of a CGA’s vehicle.The movement of data through the DATE architecture at the actor framework level is precisely specified.  The Sensor Interface is the data warehouse and data router on the information stream to an entity/actor and its Physical Component models.  The output of the Physical Component (which is the motion and sensor model outputs) is sent to the decision-making component via the Physical State Information Interface (PSII).  The PSII stage routes the information from a Physical Component to the decision engines that require the information produced by a particular computational model.  The incoming data is used by the CRC, which consists of the Long-term Decision Engine (LTDE), Mid-term Decision Engine (MTDE), and the Critical Decision Engine (CDE), in conjunction with the information contained in the knowledge bases to perform long-range, mid-range and immediate decision-making operations.  The LTDE, MTDE, and CDE send the outputs of their computations, written in XML, to the Arbitration Engine (AE), which selects the action to be performed.  The action to be performed is moderated by the actor’s skill level and combat psychology model.Each DATE CGA accesses a knowledge base that was constructed specifically for its type.  The architecture makes no assumptions about the human behavior model or human behavior representation that a CGA will use, that decision is left to the CGA designer.  There are two sub-components of the Knowledge Base for each actor type: the Environment Database and the Mission, Strategy, and Tactics Database.  The Environment Database contains the specification of the terrain and other static portions of the DSE.  The Mission, Strategy, and Tactics Database contains the information about a CGA’s mission, the tactics for the CGA, and the strategies to be employed by the CGA.4.	Summary and future workDATE supports the development and deployment of different types of CGAs in distributed simulation environments.  The architecture exploits the technical advantages provided by object-oriented techniques, component software, software frameworks, gauges, migration assistants, aspect oriented programming, and containerization to achieve composability, flexibility, re-usability, and generality.  The DATE architecture supports an open source development approach for individual actor components and objects, such as dynamics models, sensor models, knowledge bases, or reasoning systems in parallel with continued development of the main components of the architecture and ongoing refinement of its services.We are currently addressing issues related to the evaluation and refinement of the architecture.  We intend to improve the two DATE framework’s capacity for enforcing behavioral specifications for interface and component operation.  Another area to be addressed is re-casting the architectural and design specifications into the Unified Modeling Language (UML) to improve the foundation for future DATE development.  We also would like to determine whether the application of aspect oriented programming to the DATE architectural system can improve DATE.  We would like to determine if there are additional crosscutting concerns (beyond migration assistants) that can be identified and managed as aspects for DATE.  To date, AOP has been useful in aiding in the definition of the operation of the migration assistants.  Recasting the architectural specification into UML is the necessary first step in applying AOP to improve the DATE architecture.Two additional research areas for DATE are the refinement of the operation of the migration assistants and the XML formats needed by DATE.  One format, or set of formats is needed for the communication and negotiation between migration assistants and the second set of XML formats is needed to perform provide support for intra-DATE communication and to describe migration requirements.  A final area of research that we would like to pursue is the definition and development of intelligent agents that can perform correct and automatic component integration, especially for CGAs, given a set of UML-based requirements and desired functionality.References Adkins, M.K.  (1996) “Polling vs. Event-driven Computer Generated Forces (CGF) Architectures,” Proceedings of the 6th Conf. on Computer Generated Forces & Behavioral Rep.,  Orlando, FL, 23-25 July, pp. 313-318.Banks, S.B.; Stytz, M.R.; Hutson, L.J.; & Silver, S.M. (1998) “A Computable Combat Psychology Model for Computer Generated Forces,” The 1998 Fall Simulation Interoperability Workshop, Orlando, FL., 13-18 Sep., pp. 35-45.Banks, S.B.; Hutson, L.J.; Stytz, M.R.; & Santos, E. Jr.  (1998) “Incorporation of Multiple Skill Levels into a Domain-independent Computer Generated Force Architecture,” 7th Conf. on Computer Generated Forces & Behavioral Rep., Orlando, FL, 12 - 14 May, pp. 497-508.Banks, S.B. & Stytz, M.R. (1999) “An Approach to Enhance Human Behavior Modeling for Computer-Generated Actors,” Proceedings of the 4th International SIMTECT Conf., Melbourne, Australia, 29 Mar – 1 Apr, pp. 199-204.Becket, W. & Badler, N.I. (1993) “Integrated Behavioral Agent Architecture,” Proceedings of the 3rd Conf. on Computer Generated Forces & Behavioral Rep.,  Orlando, FL, 17-19 March, pp. 57-68.Bokma, A. & Slade, A. (1993) “Developing Large-Scale Agent-Based Systems:  An Example from Air Traffic Control,” Proceedings of the 3rd Conf. on Computer Generated Forces & Behavioral Rep.,  Orlando, FL, 17-19 Mar, pp. 21-32.Braudaway, W. (1993) “A Blackboard Approach to Computer Generated Forces,” Proceedings of the 3rd Conf. on Computer Generated Forces & Behavioral Rep.,  Orlando, FL, 17-19 Mar, pp. 11-20.Butler, B. (1998) “Simulation Composability for JSIMS,” Proceedings of the 7th Conf. on Computer Generated Forces & Behavior Rep., Orlando, FL, 12-14 May, pp. 393-406.Calder, R.B., Smith, J.E., Courtemanche, A.J., Mar, J.M.F., & Ceranowicz, A.Z.  (1993) “ModSAF Behavior Simulation and Control,” Proceedings of the 3rd Conf. on Computer-Generated Forces & Behavioral Rep., Orlando, FL, 347-356.Calder, R.B.; Carreiro, R.L.; Panagos, J.N.; Vrablik, G.R.; Wise, B.P.; Chamberlain, F.L.; & Glasson, D.P.  (1996)  “Architecture of a Command Forces Command Entity,” Proceedings of the 6th Conf. on Computer Generated Forces & Behavior Rep., Orlando, FL, 23-25 July, pp. 19-30.Calder, R.B. & Drummey, J. (1999) “Definition of a Military Intelligent Agent Architecture,” Proceedings of the 8th Conf. on Computer Generated Forces & Behavioral Rep., Orlando, FL, 11-13 May, pp. 551-562.Ceranowicz, A. (1998) “Evolutionary CGF Development,” Proceedings of the 7th Conf. on Computer Generated Forces & Behavior Rep., Orlando, FL, 12-14 May, pp. 421-431.Coradeschi, S.; Karlsson, L.; & Torne, A.  (1996) “Intelligent Agents for Aircraft Combat Simulation,” Proceedings of the 6th Conf. on Computer Generated Forces & Behavioral Rep.,  Orlando, FL, 23-25 July, pp. 93-99.Courtemanche, A.J. (1999) “Design Patterns for Computer Generated Forces,” Proceedings of the 8th Conf. on Computer Generated Forces & Behavioral Rep. Orlando, FL, 11-13 May, pp. 25-36.Digre, T. (1998)  “Business Object Component Architecture,” IEEE Software, vol. 15, no. 5, Sep./Oct., pp. 60-69.Elrad, T,; Filman, R.E.; Bader, A. (2001) “Aspect-Oriented Programming,” Communications of the ACM, vol. 44, no. 10, October, pp. 29-32.Ge, Z.; James, J.; & Nerode, A.  (1995) “A Multiple Agent Hybrid Control Architecture for Automated Forces:  Design and Software Implementation,” Proceedings of the 5th Conf. on Computer Generated Forces & Behavioral Rep.,  Orlando, FL, 9-11 May, pp. 45-52.Howard, M. & Lee, C. (1998) “Architecture of a Generic Command Entity,” Proceedings of the 7th Conf. on Computer Generated Forces & Behavior Rep., Orlando, FL, 12-14 May, pp.569-579.Jamali, N.; Thati, P.; and Agha, G.A. (1999) “An Actor-Based Architecture for Customizing and Controlling Agent Ensembles,” IEEE Intelligent Systems, March/April, pp. 38-44Jennings, N.J. (2000) “On Agent-Based Software Engineering,” Artificial Intelligence, no. 117, pp. 277-296.Jones, R.M.; Tambe, M.; Laird, J.E.; & Rosenbloom, P.S. (1993) “Intelligent Automated Agents for Flight Training Simulators,” Proceedings of the 3rd Conf. on Computer Generated Forces & Behavioral Rep., Orlando, FL, 17-19 Mar, pp. 33-42.Kiczales, Hilsdale, E.; Hugunin, J.; Kersten, M.; Palm, J.; and Griswold, W.G. (2001) “Getting Started with Aspect J,” Communications of the ACM, vol. 44, no. 10, October, pp. 59-65.Kuokka, D.R. (1993) “A Framework for Integrating Autonomous Agents,” Proceedings of the 3rd Conf. on Computer Generated Forces & Behavioral Rep.,  Orlando, FL, 17-19 Mar, pp. 181-190.Kwak, S.D. (1998) “Cognition Oriented Emergent Behavior Architecture,” Proceedings of the 7th Conf. on Computer Generated Forces & Behavior Rep., Orlando, FL, 12-14 May, pp. 445-454.Laird, J. E., Newell, A., & Rosenbloom, P.S. (1987) “SOAR:  An Architecture for General Intelligence,” Artificial Intelligence, vol. 33, no. 1, pp. 1-64.Laird, J.E.; Johnson, W.L.; Jones, R.M.; Koss, F.; Lehman, J.F.; Nielsen, P.E.; Rosenbloom, P.S.; Rubinoff, R.; Schwamb, K.; Tambe, M.; vanDyke, J.; vanLent, M.; & Wray, R.E. III.  (1995) “Simulated Intelligent Forces for Air:  The SOAR/IFOR Project 1995,” Proceedings of the 5th Conf. on Computer Generated Forces & Behavioral Rep.,  Orlando, FL, 9-11 May, pp. 27-36.LaVine, N.D.; Kehlet, R.; & Peters, S.D. (1999) “A Client-Server Approach to CGF Behavioral Rep.,” Proceedings of the 8th Conf. on Computer Generated Forces & Behavioral Rep., Orlando, FL, 11-13 May, pp. 411-421.Lieberherr, K.; Orleans, D.; and Ovlinger, J. (2001) “Aspect-Oriented Programming with Adaptive Methods; Communications of the ACM, vol. 44, no. 10, October, pp. 39-41.Netinant, P.; Elrad, T.; Fayad, M.E. (2001) “A Layered Approach to Building Open Aspect-Oriented Systems,” Communications of the ACM, vol. 44, no. 10, October, pp. 83-85.Ossher, H. and Tarr, P. (2001) “Using Multidimensional Separation of Concerns to (Re)Shape Evolving Software,” Communications of the ACM, vol. 44, no. 10, October, pp. 43-50.Oztemel, E. & Kocabas, S.  (1996) “Design Principles for Intelligent Agents in Distributed Interactive Simulation,” Proceedings of the 1st International SIMTECT Conf., Melbourne, Australia, 25-26 Mar, pp. 103-106.Pace, J.A.D. and Campo, M.R. (2001) “Analyzing the Role of Aspects in Software Design,” Communications of the ACM, vol. 44, no. 10, October, pp. 67-73.Reece, D.A. & Kelly, P.  (1996) “An Architecture for Computer Generated Individual Combatants,” Proceedings of the 6th Conf. on Computer Generated Forces & Behavioral Rep.,  Orlando, FL, 23-25 July, pp. 337-344.Sale, N.; Usher, T.; Page, I.; & Wonnacott, P. (1999) “Multiple Representations in Synthetic Environments,” Proceedings of the 8th Conf. on Computer Generated Forces & Behavioral Rep., Orlando, FL, 11-13 May, pp. 349-361.Sherman, R.H.  (1994) “Using Computer Generated Forces to Support Cooperative Mission Planning,” Proceedings of the 4th Conf. on Computer Generated Forces & Behavioral Rep.,  Orlando, FL, 4-6 May, pp. 37-49.Siksik, D.N. (1993) “Intelligent Computer Generated Forces Through Expert Systems,” Proceedings of the 3rd Conf. on Computer Generated Forces & Behavioral Rep., Orlando, FL, 17-19 Mar, pp. 3-10.Stytz, M. R., Adams, T., Garcia, B., Sheasby, S. M., & Zurita, B.  (1996) “Rapid Prototyping for Distributed Virtual Environments,” IEEE Software, vol. 14, No. 5, Sep-Oct, pp. 83-92.Stytz, M. R.; Banks, S. B.; & Santos, E.  (1996) “Requirements for Intelligent Aircraft Entities in Distributed Environments,” 18th Interservice/Industry Training Systems & Education Conf., Orlando, Florida, 3 - 5 Dec., on CD-ROM.Stytz, M.R. & Banks, S.B. (1999) “The Distributed Mission Training Integrated Threat Environment System Architecture, Rules, & Design Overview,” The Spring Simulation Interoperability Workshop, Orlando, FL, 14-19 Mar, pp. 858-867.Woodward, S. (1999) “Evolutionary Project Management,” IEEE Software, vol. 32, no. 10, Oct., pp. 49-59.Author BiographiesMartin R. Stytz is in the Air Force Research Laboratory and is a retired Lieutenant Colonel in the U.S. Air Force.  He received a Bachelor of Science degree from the U.S. Air Force Academy in 1975, a Master of Arts degree from Central Missouri State University in 1979, a Master of Science degree from the University of Michigan in 1983.  Stytz received his Ph.D. in Computer Science and Engineering from the University of Michigan in 1989.  He is a member of the ACM, SIGGRAPH, SIGCHI, the IEEE, the IEEE Computer Society, AAAI, and the Society for Computer Simulation.  His research interests include virtual environments, distributed interactive simulation, modeling and simulation, software architecture, intelligent agents, cyberwarfare, and computer-generated forces. Sheila B. Banks is in the Air Force Research Laboratory and is the past president of Calculated Insight.  Dr. Banks received her Bachelor of Science from the University of Miami, Coral Gables, FL in 1984 and a Bachelor of Science in Electrical Engineering from North Carolina State University, Raleigh, NC in 1986.  Also from North Carolina State University, Raleigh, NC, she received a Master of Science in Electrical and Computer Engineering in 1987 and her Doctor of Philosophy in Computer Engineering (Artificial Intelligence) from Clemson University, Clemson, SC in 1995.  Her research interests include artificial intelligence, intelligent computer generated forces, associate and collaborative systems, distributed virtual environments, intelligent human computer interaction, and man-machine interfaces. A concern is a particular, specific concept, goal, or purpose that is addressed using software. An aspect is a mechanism for expressing a concern without the use of subroutines, subprograms, or inheritance but instead the concern is expressed as a first class object that provides a service to the object class structure.		PAGE  