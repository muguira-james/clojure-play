Performance Measurement Challenges in Distributed Mission Operations EnvironmentsEric A. WatzBrian T. Schreiber Loren KeckLockheed MartinAir Force Research Laboratory Human Effectiveness DirectorateWarfighter Training Research DivisionMesa, AZ 85212-6061904 491-0732 HYPERLINK "mailto:eric.watz@williams.af.mil" eric.watz@mesa.afmc.af.mil HYPERLINK "mailto:brian.Schreiber@mesa.afmc.af.mil" brian.schreiber@mesa.afmc.af.mil HYPERLINK "mailto:loren.keck@mesa.afmc.af.mil" loren.keck@mesa.afmc.af.milJames M. McCallSimulation Technologies Inc.Air Force Research LaboratoryWarfighter Training Research Division6030 S. Kent St. Bldg 558Mesa, AZ 85212480-988-6561 x231HYPERLINK "mailto:James.mccall@williams.af.mil" james.mccall@mesa.afmc.af.milWinston Bennett, Jr.Air Force Research Laboratory Human Effectiveness DirectorateWarfighter Training Research DivisionMesa, AZ 85212-6061480-988-6561 x297 HYPERLINK "mailto:Winston.bennett@williams.af.mil" winston.bennett@mesa.afmc.af.milAbstract:  The Air Force Research Laboratory, Warfighter Training Research Division is performing continuing research on training technologies to measure and track performance on critical knowledge and skills for individuals and teams within high fidelity distributed simulation environments and live training environments.  The goals of the research are to identify the performance metrics for competencies that must be acquired and sustained, improve the ability to assess and structure individual and team training, and to provide consistent collection and tracking of performance data in both distributed mission operations and live training.  The program has developed an initial implementation of a system which robustly captures the objective data necessary for competency evaluation, end-user performance feedback, real-time graphics for instructor support, simulator technology developer validation, and for researcher and program manager evaluation of training techniques and technologies.  This system has been implemented in the Distributed Interactive Simulation-based Distributed Mission Operations Testbed consisting of four high-fidelity F-16 simulators, one Airborne Warning and Control System console, Distributed Control Station, and various environment generators.  This paper will describe the initial implementation and the lessons learned which drive the requirement for a new data collection architecture.  Additionally, the paper will identify challenges we expect from implementing the capability under HLA and on the live range at Nellis AFB, NV. 1.	IntroductionMission Essential Competencies (MECs) are “higher-order individual, team, and inter-team competencies that a fully prepared pilot, crew or flight requires for successful mission completion under adverse conditions and in a non-permissive environment” [1].  A warfighter possessing all the MEC experiences and demonstrating high degrees of the MEC skills and supporting competencies (e.g., situation awareness) will likely be very successful in combat missions.  A principal function of distributed simulation training is to increase the pilots' MECs through experiences and practicing the skills critical for successfully performing air combat missions. The goal of Distributed Mission Operations (DMO) training is to use repetition of a particular set of scenarios designed to exercise certain MEC skills. DMO provides the warfighter with the unique opportunity to operate in a team-oriented, multi-player environment, and thereby train higher-order and more varied skills than would normally be trained in a single-simulator environment.  As an example, a distributed simulation environment provides pilots with the opportunity to work on skills like target sorting, team communication, and maintaining team formation and tactics throughout the engagement. 1.1 Distributed simulation developmentThe development of distributed simulation networks can be viewed in terms of two fundamental questions: "What data needs to be available on the network?” and "How can the simulation be made more effective, realistic, lifelike, etc.?" In 1992, the "What" question was addressed by the Distributed Interactive Simulation (DIS) standard, which defined the data necessary for heterogeneous entities to maintain their own states on the network, and to communicate any changes in their internal states (location, velocity, orientation, articulated parameters, etc.) to other entities on the network. Having this data available allowed simulation entities to interoperate and thus we had distributed simulation.Recently, significant amounts of product design and development work have been done in the distributed simulation field, primarily on the previously stated "How" question.  The engineering aspects of distributed simulation development have focused on how to improve the network Quality of Service (QOS), and how to improve the user experience by upgrading the fidelity of components such as computer-generated forces (CGFs).  For example, areas such as packet delivery and transmission reliability have been addressed by subsequent DIS standards. In addition, much work has been done in improving the quality and fidelity of Image Generators (IGs) and terrain databases. Organizations throughout the simulation community have invested substantial amounts of time and money into developing simulation networks, aimed at providing a better user experience. However, a more important issue than how the technology works is how does all of this affect the individuals who participate in man-in-the-loop simulations? Meaning, how do we measure and quantify the training that takes place when an individual participates in DMO exercises? What is the improvement in human skill and on what tasks as a function of the technology? After all, the purpose of a DMO network is not solely about how to develop a better simulation network, it is really about how to make a better warfighter as a result of training time in the network environment.  Oftentimes, technology advancements are made without thought as to whether or not they should be made.  Emphasis and priority should be placed upon those environments and technologies that most improve the warfighters’ skills.  So how do we quantify human performance?Currently, there are no standardized data collection and measurement tools available that can objectively measure human performance in distributed simulation networks. An objective measurements platform would have a wide range of potential operating environments throughout the simulation community. Such an application would be capable of listening to the network traffic on any DMO network and providing objective, quantitative results that indicate whether or not training is taking place.1.2 DMO Assumptions DMO facilities share a common primary purpose: to train the warfighters to be better prepared to complete their mission. Most pilots, instructors, and subject matter experts will agree that when warfighters participate in DMO exercises, positive training is taking place. However, little effort has been made to objectively quantify the measure of improvement resulting from DMO training.The community currently makes two assumptions regarding the effectiveness of warfighter training in distributed learning systems.  First, we assume the training received in the simulated environment will be carried through to future missions. While subjective evaluations and pilot feedback can verify this is happening, the simulation community lacks a quantifiable method to verify that knowledge transfer is in fact taking place as a result of participation in the DMO environment [3].The second assumption of DMO systems is that the warfighters are increasing their skills, competencies, and knowledge, and that these increased abilities are demonstrated in subsequent training sessions. For example, subjective evaluation could observe a group of warfighters throughout a week-long training session, and note that by the end of the week the group as a whole is spending less time within close proximity to the enemy aircraft, thereby implying the group is deploying their munitions more effectively and at longer ranges. While subjective observations appear to confirm these results, we are left without a means of objective analysis to confirm the differences in the exact times and deployment ranges. 1.3 The need for objective measurementsThe DMO community can benefit from an objective measurement system for several reasons. First, although it is possible to subjectively evaluate a number of skills and MEC competencies, subjective evaluation does not provide the level of precision and detail in measurement that are obtained by using objective measurement software. Additionally, in the absence of objective measurements, there is no way to “rate the raters”, meaning, the possibility exists for a Subject Matter Expert (SME) rater evaluating a group of pilots to have a vested interest in their success.In the case of air-to-air combat, a number of measurements are simply too complex to be measured subjectively. An example of this type of measure is “controls intercept geometry”. To calculate this measurement, illustrated in Figure 1, the performance measurement system needs to know the positions of the friendly aircraft (blue) and the threat aircraft (red). The system will then calculate the aspect angle of the threat to the friendly aircraft. If the aspect angle is greater than 120 degrees, then given the threat’s altitude, weapon type, and quadrant, is the range less than that of a configuration table value? If yes, then the friendly has violated the Minimum Abort Range (MAR). Figure  SEQ Figure \* ARABIC 1. "Controls Intercept Geometry" EMBED Word.Picture.8  To meet the objective measurement needs of distributed simulation networks, researchers and engineers at the Air Force Research Laboratory, Mesa, Arizona (AFRL/Mesa) have developed and implemented an objective measurements system to record and analyze data during simulated missions. The objective measurement system development had three primary goals. First, produce a software tool capable of objective measurements over multiple platforms, including live, virtual, and constructive simulators. Second, the tool must serve multiple purposes, such as human performance assessment and simulator technology validation.  Finally, the tool must operate on multiple levels, including measurements designed for individuals, groups of individuals (teams), teams of teams (packages), and multiple packages (forces). 2. Performance Effectiveness Tracking System (PETS)The initial PETS proof-of-concept project was designed to record 70-80 "core" air combat measurements in real-time in a DIS environment. These core measurements were identified and selected by researchers to specifically assess warfighter skills with relation to MECs. In addition to real-time time-stamped output files, the system provided two summary files: a shot summary and a trial summary.  The shot and trial summary files were designed for empirical studies—the time-stamped output files were designed for diagnoses, operator feedback, and performance modeling efforts.  Because the proof-of-concept was designed for empirical uses, some simple rules of engagement for the DIS DMO environment needed to be established, a discussion we will later elaborate on.2.1 Early SuccessesThe initial proof-of-concept was successfully integrated into the DIS network at AFRL/Mesa in January, 2002. The proof-of-concept allowed accurate measurements of many outcome measures such as shots and kills and time spent within critical range boundaries, such as the controls intercept example described earlier.  Outcome measurements ranged from simple external state data to complex calculations with the intent of quantifying human performance on specific tasks.  A simple example of a successful proof-of-concept measurement is PETS’s shot tracking mechanism. On the surface, measuring shots and kills in a DMO environment may appear to be a simplistic task, however this is not the case.  To illustrate, DIS standards allow for multiple missiles to detonate on a single target, and instructor operator station (IOS) functionalities allow for the deletion of entities and missiles.  To correctly track shots and assign kills required extensive logic routines to account for DIS anomalies and for the numerous ways in which the IOS operator can perform “God-like” functions.  The PETS shot and kill tracking routine alone required over 5000 lines of code. It is this level of accuracy and attention to detail that has generated a significant interest in PETS throughout the distributed mission operations community.A more complex example involves reporting of kill results. It is allowed within the DIS protocol to have multiple DetonateEntityImpact (kill) results on a single entity. If multiple weapons were bearing down on a single target, occasionally more than one missile would report a kill result, even though once the first missile struck the entity it was removed from the simulation space. This particular case made tracking shot results very difficult; accurate shot tracking requires that only a single entity be credited for the kill shot. The solution developed in the proof-of-concept PETS was to track all detonation events, as well as all entity removal events. If an entity was detonated upon with a detonate code of DetonateEntityImpact, a kill shot by DIS standards, the shooter would be given credit for the kill. When the entity was removed from the simulation, PETS would check for incoming munitions having the recently removed entity as a target. All incoming shots on that entity were then assigned an AFRL-specific detonation code of “Detonate Other Missile Hit First”. When the detonation events for the other incoming shots were received, PETS would ignore them, regardless of the detonation result. Another more complicated measurement example is the method PETS uses to measure bombing accuracy. When a friendly aircraft goes into a bombing run and deploys ordinance against a stationary target, PETS will, assuming a user has input a bombing target list, analyze the results of that aircraft’s bombing accuracy by marking the bomb detonation point and comparing that with the actual target location. When the weapon is pickled, an imaginary line is drawn in the direction of the aircraft’s heading (see Figure 2). This line is then moved over the target location and another line is drawn through the target location, perpendicular to the first. The bomb accuracy is determined by measuring the amount of long/short error and left/right error, as seen below.Figure  SEQ Figure \* ARABIC 2. Bombing accuracy assessmentThe level of detail presented in the shot tracking and bombing run scenarios is necessary to accurately assess how the warfighter is performing during DMO exercises. If we were to allow multiple kills on a single entity, this would distort a) the number of kill shots per pilot, and b) the percentage of missiles fired that resulted in a kill, either of which would lead to incorrect aggregate information in a week-ending debrief. Additionally, if these errors were not caught, subsequent studies based on aggregate summary data would also reflect the erroneous detonation values, and would present an incorrect picture of the warfighters’ actual training experience in the simulated exercises.Immediately after the successful initial integration of the PETS proof-of-concept, the potential applications for research and engineering became readily apparent.  Given the vast amount of data produced by PETS, performance on hundreds of trials (e.g., individual scenarios) could easily be aggregated and analyzed to evaluate DMO training effectiveness.  Researchers at AFRL/Mesa could track outcome measures in pre- and post-test studies to determine the return on investment (ROI) of their simulation systems.  Furthermore, if additional measurements were created, effectiveness evaluations could be extended to individual MEC skills, thereby illuminating which warfighter skills are best being trained and by how much.Subsequently, the development of additional objective measurements took top priority.  The proof-of-concept architecture rapidly expanded to over 1,000 time-stamped outputs, 150 shot summary outputs, and 500+ trial summary outputs.  Additionally, the PETS proof-of concept architecture also has the ability to collect the same data from pre-recorded missions, allowing standardization of data output. Schreiber, Watz, & Bennett [2] aggregated and analyzed a preliminary data set (collected using the PETS proof-of-concept) on 19 teams (76 pilots) of F-16 pilots performing air combat scenarios.  The F-16 pilots, after just 3.5 days of DMO air combat training, prevented 63% fewer enemy strikers from reaching their targets, killed 24% more enemy fighter aircraft, while simultaneously allowing 68% fewer F-16 mortalities. These results were collected on pre- and post-test mirror-image point defense scenarios [2]. These aggregated training effectiveness results were profound, and clearly demonstrate the power of empirically collected research data.  Additional skill and munitions measurements were collected but distribution is limited.Analyses for datasets of the size reported in Schreiber et. al [2], if combined with subsequent transfer of training data, provide strong evidence for the ROI of a DMO training system.  Because of this, and combined with the potential to expand this capability to a number of detailed skill measurements, a number of other DMO users/developers have expressed a desire to obtain and use the PETS proof-of-concept assessment tool, including Shaw AFB, Nellis AFB, the F/A-22 and JSF programs, the United Kingdom, and the Navy Aviation Simulation Master Plan. For successful transition from proof of concept to multi-DMO implementation, however, two steps are desired—a re-architecture of the PETS proof-of-concept and additions to the DIS/HLA network protocol standards.  We will discuss each in turn.2.2 Lessons learnedThe PETS proof-of-concept system was initially developed to serve the needs of the DMO network at AFRL/Mesa, which consists of four high-fidelity F-16 simulators, several IOS consoles, an Airborne Warning and Control System (AWACS) station, and various threat generators, running over a DIS network. The first version of PETS provided a number of previously unavailable objective measures, however due to its origin as a proof-of-concept application; some limitations to its extensibility exist. The original application design could no longer be expanded to meet the growing requirements of the project. As a result of the variability in different IOS stations and network environments, PETS lacked the flexibility and configurability needed for use at other sites, making any potential distribution of the software require a customized patch. Inevitably, limitations in the original design led to the first PETS system not being able to reach its full potential. 3.	The Need for a New ArchitectureA revised PETS architecture, unofficially called “PETS2”, is currently being driven by several factors, including a number of additional requirements, the need for higher-level measurement capabilities, and potential to support new operating environments. PETS2 will need to be more flexible in several areas than its predecessor version.  Theoretically, PETS2 will collect data from any DIS/HLA network, without requiring special operating procedures or ties into the existing IOS software.  The new software will also be designed to have a minimal impact on the simulations and federates on the network.In addition to tracking measurements at the entity level, PETS2 will be required to support measurements designed for teams, packages, and forces. It is important to note that the proof-of-concept implementation of PETS had a very limited concept of a team, and both packages and forces were left undefined. The long-term vision for the PETS2 system is to quantify learning objectives at all levels of competency: individual, team, inter-team, and finally at the Command and Control (C2) level. Research at AFRL/Mesa is underway to develop measurements to assess these higher order competencies.3.1 PETS2 Architecture OverviewWe will begin with an illustration of the PETS2 architecture, followed by examples of how this design will address areas where the PETS proof-of-concept was lacking. A high-level view of the PETS2 design is illustrated in Figure 3. The PETS2 project is being designed as a modular, multithreaded application capable of robustly handling high volumes of network entities. We anticipate the system to be capable of operating in a DMO environment with as many as several hundred entities. It will provide a number of user interfaces that can be used to customize default entity properties. The system will be more extensible; it shall be able to handle custom-developed entity types and measurements, all of which will be configurable through User Interface (UI) interactions. PETS2 will also include several additional output files, which will allow a greater flexibility in distributing and using the PETS2 data. PETS2 will continue to support recording trial data, although it will no longer be restricted to operating solely during a trial. Previous restrictions on console and pilot operations have been removed in PETS2. The new system will also employ a multi-level lookup system for additional internal state data that may be unavailable on the network, thus making it more interoperable to any network by removing the dependency on custom data requests.  Figure 3. Overview of PETS23.2 PETS2 ExtensibilityA primary issue to address with the PETS2 architecture is the extensibility of the new system to allow for external software development. The new system design will allow it to be configured to accept externally developed measurements for a single entity or group of entities. The PETS2 system will provide extensibility at the entity level through its ability to consume and execute external measurements. An external measurement is defined as a measurement capability that has been developed outside of the (main) PETS2 code base and resides outside of the main application in a dynamic link library (DLL), and is configured at startup through user configuration screens. External measurements can be associated with a particular type of entity such as an F-16C, or a group of entity types, as in all types of F-16 entities. The intention is to have external measurements that can be developed at the team, package, and force levels and incorporated into PETS2 in the same manner. It is possible, for example, to externally develop a team-level measurement and apply it to any number of teams (red, blue, or white), or to apply it only to a specific team.For example, we will assume that a research study requires measuring a component of A-10 pilot performance that is applicable only to A-10 fighter aircraft. A custom library (DLL) that contains the specific measurements can then be created for an A-10 entity. When the application is started, a user will configure the current Session to attach this custom measurement to any A-10 that enters the simulation space. All A-10 entities will be assigned the default properties and measurements of an Aircraft entity (see Figure 4), and will also include the additional measurements specified in the A-10 DLL. The PETS2 external measurement API limits each DLL to contain a single exported measurement, however, it is possible to configure multiple custom measurements per entity type by using multiple DLLs. Another example where a custom measurement could be applied is in calculating the ranges for an entity’s threat rings, or MAR. PETS2 will apply a default MAR calculation for all aircraft entities. However, due to differences in aircraft performance and weapons capabilities, the MAR ranges will undoubtedly be different, for example, for an A-10 vs. an F-15 Eagle aircraft. A custom MAR calculation can be implemented for the F-15 entities to more accurately reflect their Minimum Out Range (MOR)/MAR threat ranges. External measurements will be implemented using an external Application Programming Interface (API). This external API will allow software developers to define external measurement capabilities without knowledge of the internal entity representation. In addition, the API will allow full access to all entity properties. PETS2 can also be configured to consume externally developed entity types. To ensure that the new system can handle externally developed entity types, the PETS2 core classes have been designed using a hierarchical entity object model, as seen in Figure 4. PETS2 will provide a default implementation for the base Entity object (green), as well as Aircraft and Munition objects (purple). Figure 4. Entity Object ModelAn external entity is an entity class that requires additional processing functions above and beyond the measurements provided by the PETS “core” entity classes. For example, we are developing an extension to the Aircraft object, the Aircraft_F-16, to represent the F-16 Block 30 simulators currently in use at AFRL/Mesa. The external class would set its weapons load to a typical weapons load used in the F-16 Block 30 simulators. Additionally, the Aircraft_F-16 object would be capable of processing measurements specific to an F-16 entity, such as its missile flyout model and radar functions. This customization allows AFRL researchers to collect significant amounts of additional data related to pilot performance in AFRL/Mesa’s DMO environment. The extensibility model in PETS2 can be developed even further to include additional development that is based on an existing external entity. In this example, the Aircraft_F-16 entity described above will be used as the “base” external object. We will assume the system is to be implemented at another site that uses F-16 Block 50 simulators. We also assume that the newer F-16’s have an upgraded flight control system, which includes a different weapons flyout model. However, the Block 50 F-16 may still retain many backwards-compatible measurements that exist in the “generic” Aircraft_F-16 object. As illustrated in Figure 5, a new external class object, the Aircraft_F-16_Block50, could be developed. This new object would implement only the upgraded weapons model and would retain all properties of the parent entity object. Here, we have shown how the PETS2 system can theoretically be extended into other environments and, through the use of external libraries, can continue to provide accurate measurement capabilities.Figure 5. Extending the external development modelPETS2 custom development may be extended to the team, package, and force levels. The system will have the ability to calculate measurements at the team, intra-team, and teams-of-teams levels, which can theoretically extend the potential measurement capabilities of PETS2 up to, and including, the C2 level. The PETS2 core will provide a default implementation of team, package and force objects, however, the capability exists to build upon the base implementation as measurements are developed to assess higher-level competencies. 3.3 Network requirements and operationThe simulated environment training at Mesa is done through “missions”, where each mission consists of a number of “trials”. A typical 1-hour mission will contain approximately 7 – 8 trials. Each trial has a global start and a global stop. These events are sent by the IOS station to launch all entities or freeze all entities, respectively. The PETS proof-of-concept project was built around the trial concept, where every trial is started using a Global Start event and is stopped using a Global Stop event. Since every trial is encapsulated between a global start / global stop pair, the system was made to listen specifically for these events, and only these events, as the beginning and end of a trial. At the start of each trial, PETS interacts with the AFRL/Mesa IOS software and performs a “handshake sequence”. The purpose of this handshake is for the IOS software to ask PETS if it is ready to begin data collection. If PETS were to respond to the handshake with a “not ready” event, mission execution would be suspended until any PETS issues were resolved. PETS would stop recording when it detected a global stop PDU that was sent from the IOS.  This extra protocol was necessary to maintain an experimental control of what a trial was, and to only collect data during periods of a trial. In addition, the protocol also served to maintain quality control of the research data being collected.This limited definition of how PETS was to be started and stopped meant that PETS would operate only when a trial was running and at no other times. Due to these restrictions, the proof-of-concept PETS does not include the ability to collect data without a trial running. Additionally, the PETS system lacks manual control over trial start and stop events. The PETS proof-of-concept also enforced a set of rules on the console operators and simulator pilots, by forcing them to operate under a set of guidelines known as the PETS Rules of Engagement (ROE). As a result of the PETS ROE, console operators were restricted from performing a number of their “usual” activities both outside of and during an engagement. These measures were necessary to maintain experimental control of the simulation environment. Examples of the restrictions imposed by the ROE included no freezing of an engagement for any reason, and no altering of an entity for any reason once the engagement had started. Pilots became more restricted in they could no longer have their weapons reloaded or have their entity reconstituted once they had been killed. Again, the ROE was designed to place controls on the simulation environment to ensure the validity of the data. Had we allowed entities to be reconstituted during a trial, this would have required significantly more shot tracking logic as well as additional work for a number of other measurements. When AFRL/Mesa began to consider the transition of this technology to other DMO users, we recognized the limitations imposed by the PETS ROE approach.  Some users would be unable to integrate PETS into their operations based on the ROE. For example, at the Nellis AFB range it would not be practical to restrict range operations to the ROE imposed by PETS. Additionally, the lack of a formal definition of engagements, as demonstrated by the continuous battlespace seen on the Nellis range, would have rendered the tool ineffective at collecting data. These strict operating requirements were one of the limiting factors as to where the proof of concept system was usable. PETS2 will not require that a trial be running for it to operate. The system will be able to run on any network, at any time. PETS2 will continue to support trials, and will be able to simultaneously record a large battle as well as individual trials within the battlespace. An example of this would be an air-to-air refueling task during a joint DMO engagement. In addition to recording and analyzing the overall DMO engagement, a user could manually begin a PETS2 trial immediately before the task begins. The trial would then be manually ended when the task has been completed. The system will also support recording multiple trials, in the event of multiple air-to-air refueling tasks.3.4 Custom data requirementsThe PETS proof-of-concept level of integration with the simulation environment was also a hindrance to transitioning PETS to other sites. Some of the detailed measures required data that were not available using standard DIS entity state protocol data units (PDU).  For the PETS proof-of-concept, we modified our simulators to respond to Data Query PDUs and to output Data PDUs to provide PETS the additional or “custom” data.This “custom” data is any data that can be consumed by PETS that is made available outside of (in addition to) the DIS standard. A particular example is weapons load information. All Mesa F-16 simulators are initialized with a standard weapons load. Periodically, squadrons training at Mesa have requested that they receive a different weapons load. At Mesa, our systems provided the modified weapons load information and all measurements were calculated correctly. However, if PETS did not receive weapons load information from a specific F-16 before the timeout period expired, that F-16 would have its weapons load set to the standard defaults for the engagement. This caused problems in the clear avenue of fire and Escape G measurements, both of which are composite measurements based on entity positions and specific weapon types. Both measurements are no longer calculated after the F-16 runs out of a particular weapon. For example, if PETS did not receive the correct weapons load, the system could potentially think that an F-16 was out of Advanced Medium Range Air-to-Air Missiles (AMRAAMs) before the last actual AMRAAM was fired, and it would stop calculating that F-16’s clear avenue of fire and Escape G measurements for the AMRAAM. Additionally, if the pilot was to deploy the only available Aim-9 missile, and PETS thought there were more Aim-9’s remaining, it would continue to calculate measurements for that weapon. With the proof-of-concept PETS, collecting data on another site would have resulted in a significant reduction in the number of available measurements, due to the lack of the additional PETS information. PETS2 will resolve this issue by employing a multi-tiered approach to looking for non-standard network data. A new feature in the PETS2 application will be a set of user configurability screens, designed to allow user input for any non-standard data the system will use.To obtain non-standard data, PETS2 will first attempt to find it through the normal network traffic. If the data is not present on the network, PETS2 will search for a user-entered value for each particular variable. If the user has not entered any information for a particular variable, PETS2 will then fall back on the default value for that measure. Finally, if a default is not available for a particular value, the measure will not be calculated and a period will be output in its place. 3.5 Data needed from simulation communityA number of the currently available measures under DIS and HLA are outcome-oriented, with only a limited number of available data being process- or skill-oriented. While having outcome-oriented measures allows assessment of a handful of competencies, a large number of the 37 skills identified for air-to-air MECs cannot be measured with the data that is currently available throughout the standards.  If more data addressing skills were available, it would allow a more thorough assessment of MEC abilities. To allow this expanded measurement capability, simulation entities will need to expose more internal state data. The data output should also be required for all entities throughout the simulation community.Examples of additional state data include, but are not limited to, weapons load, fuel burn, cockpit switch positions, entity mass, radar modes, and throttle position, all of which are not currently output in the DIS/HLA standards.  Although many simulation environments already support additional internal state data, there are no standards that enforce how internal state data is put on to the network and represented in the DIS/HLA data structures.  From a distributed network standpoint, this makes it extremely difficult for wide-scale data acquisition and analysis software such as PETS2 to interoperate.  We propose to extend the amount of standardized data by using existing and new capabilities of the DIS/HLA standards. When an entity is created, it will send an initial update of all the properties it will expose. During the simulation, any entity exposing additional properties would be required to send an update when its internal state data changes. Discrete data, such as radar mode and throttle and cockpit switch positions, would be updated when the state changes. Continuous data, such as fuel burn, would be updated at periodic intervals and would contain the amount of existing fuel and the current burn rate. The interval at which continuous data is updated will be determined by a timeout value and/or when the rate of change exceeds a certain threshold value.In a DIS simulation, the additional data could be passed as a DIS_DATA_RECORD object.  Although this is a common and accepted practice, it does not require the simulation community to adhere to a standard data format, thus making it very difficult to collect measurements and perform analysis at a distributed level.  We propose that that a new set of PDUs be added to the current DIS protocol suite to support internal state data for use in data acquisition systems. In an HLA simulation, the data would be exposed as additional properties of the federates, and would be made available using standard HLA requests. Data would be updated according to the protocol set by DIS updates.As the complexity of objective measurements increases, the data needs from simulation entities may increase as well. At AFRL/Mesa, we have implemented several additional data measurements, including radar modes, additional weapon state information, and weapons loads using the techniques described here. These data have enabled a number of additional measurements of training effectiveness in our DMO environment. We advocate that future simulation network standards will someday include the additional entity data required for performance measurements purposes. 4. Challenges in other environmentsAs shown in Figure 6, a Mesa-developed Network Interface Unit (NIU) provides a layer of abstraction between PETS and any specific network protocol. The NIU translates incoming data into an object-oriented, Common Data Format (CDF) representation of the underlying network protocol.   Our current NIU supports both DIS and HLA environments.Figure 6. Network abstraction through NIUUnder the High Level Architecture (HLA) environment, AFRL/Mesa initially operated using the Tasmanian Devil Federation Object Model (FOM) and investigated the potential of an agile FOM interface to increase our interoperability.  Given the lack of a standard Air Force DMO FOM, we made the decision to implement the RPR FOM. As we implement PETS2 within other HLA environments, a key interoperability challenge will be the extent to which our NIU will require modifications to support a specific FOM.Another challenge to implementing PETS2 in an HLA environment is ensuring that PETS2 will be getting the data it needs without adverse impact on the environment. The DMO applications being considered by AFRL/Mesa have high fidelity performance requirements, with some data requiring updates at the 60 Hz rate. Additionally, as discussed above, PETS2 requires extensive data from each federate.  The combination of these performance requirements and the additional data requirements may raise data delivery issues in some scenarios. Finally, in a DMO environment consisting of bridged federations, a single PETS application may not be adequate to collect and analyze all the data required by the users.  We are considering potential approaches to expanding the architecture for cooperative performance measurement between PETS applications.We also see challenges to integrating PETS2 into a live/virtual environment such as the Nellis range. We expect the Nellis range to migrate to the Test and Training and Enabling Architecture (TENA) protocol. The TENA architecture provides a fully functional TENA/HLA gateway module through which PETS2 may interface with a TENA network. This gateway would allow PETS2 to collect data on the TENA network. Because TENA is a superset of HLA, data collection issues on a TENA network are expected to be minimal, given that PETS2 can support the HLA FOM produced by the TENA gateway. However, existing range instrumentation systems may not provide all the data required to measure performance across the MECs and skills or they may provide data in different formats or fidelity. These data translation issues may impact our integration with live ranges. 5. Benefits to the M&S CommunityIn the introduction to this paper, we established the need for human performance assessment in distributed simulation environments. We can no longer simply operate a DMO installation without considering the warfighter and the training effects they receive as a result of time spent in the simulated environment. Assessment of human performance goes far beyond just counting shots and giving credit for kills; we need to focus on higher order competencies and skills (MECs) in order to evaluate the effectiveness of a distributed learning environment.  We have also identified a need for objective measurement capabilities within the entire training simulation community. More thought needs to be given as to why new technologies are being implemented, and ultimately, how their use will increase the training opportunities available to the warfighter. As previously stated, a great deal of time, money, and development effort goes into building highly sophisticated DMO networks whose sole purpose is to train the warfighter. There are many DMO installations throughout the Modeling and Simulation (M&S) community, and currently these sites have no method of objectively assessing the degree or amount of knowledge transfer that takes place when warfighters train in these virtual environments.The PETS proof-of-concept system showed that such an application could be developed, and that it was capable of assessing human performance in a distributed simulation environment. PETS has been able to record measurements on an unprecedented scale, recording over 1 million data points per minute. In the section discussing the initial successes achieved using the proof-of-concept PETS, we illustrated several examples of the power of objectively collected data. As a result of the success of the proof-of-concept PETS project, other organizations within the simulation community have expressed interest in the PETS project. The collective DMO community has been voicing its need for such an objective measurement tool.Due to the growing interest from within the simulation community, researchers and engineers at AFRL/Mesa are taking steps to fill the need for an objective performance measurement tool in developing the PETS2 project. PETS2 will be a flexible, extensible measurements system, capable of operating in virtually all DMO installations.In Section 3 of this paper, we identified how the capabilities of the PETS2 system can be extended to perform measurements on virtually any type of entity or group of entities by using additional software libraries. The extended capabilities will include custom-developed measurements, when the existing entity model needs to be extended slightly; or entirely new entity types, designed to provide complete support for a specific type of simulation entity. Any installation that uses the PETS2 software will be able to develop their own, unique libraries to collect additional measurement data. In addition, PETS2 will be able to calculate measurements for any team, package, or force, meaning that in addition to blue (friendly) teams, PETS2 can also evaluate teams of Computer Generated Forces (CGF). This opens up an entirely new area of measurement capabilities for researchers and for those who develop CGF generators. Teams of threats can now be evaluated to measure their mission performance relative to their team assignments. PETS2 also affords the ability to quantify the effect of adding a new technology or a new system at other sites. Program managers considering a technology upgrade to their DMO installation will now have the ability to evaluate the training effectiveness of the new technology against the existing system. An example of this would be the use of repeated tasks, such as an air-to-air refueling scenario, to evaluate the effectiveness of a new image generator. If scenario results show a significant improvement during the tasks, these results could be quantified and used as additional justification to go ahead with the new product.Finally, while PETS2 is being designed to support performance measurements supporting individual, team, and inter-team training, the concepts and lessons learned from this activity are equally valid for developing data collection and analysis capabilities to support other M&S domains such as Analysis; Research, Development, and Engineering; and Test and Evaluation.REFERENCES[1] Colegrove, C.M. & Alliger, G.M. (2002). Mission Essential Competencies: Defining Combat Mission Requirements in a Novel Way. Paper presented at the NATO SAS-038 Working Group Meeting, Brussels, Belgium.[2] Schreiber, B.T., Watz, E.A, & Bennett, W. (in press). Objective Human Performance Measurement in a Distributed Environment: Tomorrow’s Needs. Paper is accepted to Interservice/Industry Training, Simulation and Education Conference (I/ITSEC 2003).[3] Schreiber, B.T., Watz, E.A, Bennett, W. & Portrey, A. (2003), Development of a Distributed Mission Training Automated Performance Tracking System.  In Proceedings of the 12th Conference on Behavior Representation in Modeling and Simulation. Scottsdale, AZ.Author BiographiesERIC WATZ is a Software Engineer with Lockheed Martin at the Air Force Research Laboratory, Warfighter Training Research Division, in Mesa, AZ.  He is currently pursuing an M.S. in Computer Information Systems.  He completed a B.S. in Computer Science from Arizona State University in 2002 and a B.S. in Human Biology from the University of Wisconsin in 1997.BRIAN SCHREIBER is a Staff Scientist with Lockheed Martin at the Air Force Research Laboratory, Warfighter Training Research Division, in Mesa, AZ.  He completed his M.S. in Human Factors Engineering at the University of Illinois at Champaign-Urbana in 1995.JAMES McCALL is a Senior Analyst with Simulation Technologies Inc at the Air Force Research Laboratory, Warfighter Training Research Division in Mesa, AZ.  He supports training research across several operational domains and emphasizes the extension of distributed simulation technology to meet the expanding requirements of high fidelity training.  He completed his M.S. in Teleprocessing Science at the University of Southern Mississippi in 1989. He is the Secretary of the SIW Research, Development, and Engineering Planning and Review Panel and the newly elected secretary of the SISO Standards Activities Committee.LOREN KECK is a Lead Software Engineer with Lockheed Martin at the Air Force Research Laboratory, Warfighter Training Research Division in Mesa, Arizona.  He holds a B.S. in Computer Science from Arizona State University, and has several years of experience in the real-time data acquisition & analysis, voice communications and human operational training fields.Winston Bennett, Jr. is a senior research psychologist and senior scientist for training systems technology and performance assessment at the Air Force Research Laboratory, Human Effectiveness Directorate, Warfighter Training Research Division in Mesa, AZ.  He received his PhD in Industrial/Organizational Psychology from Texas A&M University in 1995.  