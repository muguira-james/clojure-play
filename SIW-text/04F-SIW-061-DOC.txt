Improving Vendor Feedback in the RTI Verification ProcessJohn SchlomanJonathan LabinShon VickThe Johns Hopkins University/Applied Physics Lab11100 Johns Hopkins Rd.Laurel, MD 20723{john.schloman, jonathan.labin, shon.vick}@jhuapl.eduKeywords:RTI, Verification, XMLABSTRACT: The DMSO sanctioned RTI Verification Process occurs in four major steps.  After receiving a new RTI from a vendor, an up to date version of the Verification software is built and installed. Next the Verification Tool must be configured to run the candidate RTI.  Under this configuration, a set of scenarios will be run testing the RTI against the HLA 1.3 or 1516 specification.  The results of this run are then analyzed and a text-only report is returned to the RTI Vendor documenting any failures. This is the current status of the RTI Verification suite of tools.  This implementation has areas for improvement. First the text-only deliverables returned to the vendor could be more content rich and informative.  Additionally the workflow of the system could be improved to decrease the turn around time for the Vendor to see results. A prototype has been created to evaluate the viability of providing these improvements.The main thrust of the Johns Hopkins University Applied Physics Laboratory’s (JHU/APL) prototype effort is in two major areas.  In the revised prototypical system, the Vendor will receive a set of expanded results.  In addition to textual analysis and HLA specification references that are currently returned, the system will produce actual testing results in a schematized XML document along with a tool to replay the results of the test execution.  This should aid the Vendor in visualizing and replicating any errors.  1.  Possible ImprovementsThe RTI verification process builds upon a system that was originally developed by the MITRE Corporation [6].  The focus of this paper is proposed and designed extensions to the original RTI vendor feedback.  The text report of an RTI verification run is composed of three Adobe Systems Portable Document Format (PDF) files with four sections for each phase of the verification that contains errors (totaling 15 files).  Contained is any verifier analysis, a summary and description of the test and a log of the test run.  In cases where the RTI fails upon a fundamental HLA feature, the generation of these reports is sometimes replaced with a single message.  In these cases generating all reports would prove redundant and time-intensive when a small set of root causes can be found.The output from current RTI verification process is information rich, but relies on a detailed understanding of how the verifier software works.  To discern the events that lead to a test failure, vendors must learn to decipher the generated error messages and log files.  While seasoned RTI vendors develop this understanding, it provides a barrier to new entrants into the RTI market.  The results of the verification tests are supplied in proprietary and unwieldy Adobe PDF format form. While this allows the results to be human readable it largely precludes further processing.  In the present system, if the vendors wanted to keep a database of past failures to help diagnose future ones that would not be possible without a great deal of work.  A more versatile alternative would be a single format combining all descriptive content of the above output while structuring the data in some way.  One possible option is an extension of the Extensible Markup Language (XML) [1].  Defining the verification XML files using the XML Schema [2] standard, would be a human and machine readable solution.  The long test descriptions and content of the current deliverables would be retained.  The major difference would be the structure of the test failure same data.  Additional elements could then be inserted into each test failure.  Unlike the previous PDF representation the analysis, test description and log could all be organized under a single XML element instead of using multiple files.The structured results can be compacted to the minimal data rich analysis or expanded to reveal all components.  Each “event” that occurs during an individual test may have a specific federate number, timestamp, log entry, exception thrown, return type, return value, and associated error message.  All of these are explicit unique elements and attributes instead of a compacted single formatted string.XML and XML Schemas are components of the wider interoperable technologies maintained by the World Wide Web Consortium (W3C). With an XML format in place, the verification processes of both RTI vendors and the verification team could be automated.  For vendors, this format fosters more comprehensive post-processing of error results.  Tools could be written to allow for visualization of the results both between federates and over time to provide a clearer view of what occurred during the test.  A large body of tools and supplementary technologies exist for the handling and manipulation of XML (The Extensible Stylesheet Language Family (XSL) [3]).Comparison between verifications runs of an RTI would be a simplified, providing answers to questions such as “What changed between these builds?” or “Which tests failed when testing on operating system A compared to B?”2.  The XML SchemaAs any improvements to the deliverables hinge upon the definition of our XML schema (which we have termed the Verification Error Markup Language [VEML]). The current output was analyzed for data elements that are shared across test failures. XML elements were defined for each. Of these, the initial thrust was to replicate the content currently sent to vendors.  Modifications were then planned to the HLA verification suite to generate the correct XML objects to represent all “HLA Events”:  federate calls, callbacks, exceptions, did-not-receives and RTI errors.  Table 2.1 lists all HLA event types and gives a short description of each.To validate our XML schema, a small Java application was written using the free Xerces-J [5] an implementation of W3C’s Document Object Model (DOM) [4].  The current schema is included in Appendix A.Next the verification testing suite was modified in the prototype to automatically build an XML text segment representing the execution of the test.  A series of ‘Event’ XML elements is created and, if the test fails, they are collated under a single ‘Error’ element and stored to the database. When verification run is being analyzed, these snippets are then collated into a single document which verification engineers attach analysis and descriptive explanations.HLA Event TypeDescriptionSuccess, ExceptionAn exception is returned in response to a call, in compliance with the test.Success, CallbackAn expected or acceptable callback is received.Success, BaseA successful call made by the federate.Error, ExceptionAn unexpected exception is returned or an expected exception is not received.Error, CallbackAn unexpected callback is received from the RTI.Error, Did Not ReceiveAn expected callback is not received.Error, RTIAny error within the RTI.Error, BaseAny error that does not fall into one of the above HLA event types.Non-eventAn event that is not integral to the success or failure of a test (e.g. log text message generated by the Verifier suite)Table 2. SEQ Table \* ARABIC 1For the initial XML generation, events are ordered using their index in place of timestamps.  Future versions could capture system time and use this for time stamping instead, providing an accurate history of the test.  When the suite consults the database for all failures and the appropriate test descriptions, a more rigorous search could be performed.  Results from previous builds by the same vendor could be located and referenced in the new result set.  RTI vendors would then have a history not limited to a single test but spanning the verification history of their application.Iterations to the XML schema (e.g. a new timestamp type or verification history elements) would be pushed out to the public version of the schema and be available to all vendors.3. Replay Tool As previously mentioned, the XML structure of the test result data provides the ability for software tools to be constructed that can read, display, perform analysis and comparison on the data.  One such tool (built by this group) has been designed to read and display the results in text and graphics.  This Java application, The Vendor Replay Tool, is extensible, extendable and modifiable.  The Java language allows a vendor to use the tool as is, augment the tool with their own classes, or even write their own tool that relies on the XML parser and storage structures provided by the tool.  This last option is best for a vendor wishing to abstract away from the XML completely and engineer a Java application which deals with Java objects representing the test results.  The Vendor Replay Tool (See Figure 3.1) accepts a VEML file as input.  The failed tests listed in the file are then displayed in a tree hierarchy.  Each test is listed in the hierarchy according to the test scenario and series in which it was executed.  Selecting a test loads the Description window with text explaining the purpose of the selected test.  Additionally, this action loads the Analysis window with an explanation of the failure provided by the Verification Engineer.  After selecting a test, the user may view a replay of the test.  As time moves forward through the test, events are displayed in the Timeline as an icon representing the type of event.  Concurrently, log text is displayed in the Log Window for each event.  The log text is similar to the logs of the current PDF deliverables except some format changes.  The control panel allows for the test to be paused, rewound or stepped to a specific event.  The last event in the test will be the error causing event.  Playback stops when this event is reached.  For vendors who do not wish to see the timeline progression of events, the tool supports a Compact Mode in which the timeline is hidden and the Log Window is combined with the Test Overview window (See Figure 3.2).  By selecting Generate Classic Report from the menu, and selecting an output file, the tool will generate a text file in similar layout and content as the classic Vendor Report.4. Replay Tool with Live PlaybackThe current replay tool is the first step to a more ambitious, later project:  the real-time replay of a test against a Vendor RTI.  In this version, the replay tool would connect to the RTI as a set of federates.  Then, using the VEML as a script, execute actual HLA calls and report returned values to the GUI.  The functionality of the real-time replay tool will be identical to the tool described above with the user being able to pause and restart the test using the control panel.  The tool would then allow the Vendor to debug against the actual test results returned from verification.Of course there are many challenges remaining to be scoped before actual development can take place.  The real-time replay tool would need to imitate both C++ and Java federates in all HLA specifications.  Actual federate code could be generated from the tests for each member federate.  But constructs, either inside the federate code or above it, executing the federates, would need to exist to synchronize the federates so they would perform consistently with the test’s events.  Once its design is finalized, the real-time replay tool is just one of the possible applications that could be built from the baseline of the VEML format.5.  ConclusionAs the RTI vendor community expands, the results of the RTI verification process should become less qualitative and require less interpretation.  Optimally results and associated tools can be provided to vendors that allow them to understand the nature of the verification problems. These analysis tools will be used to isolate the part of their implementation that results in test failure and fix the problem in a timely fashion.  A set of tools that provides these facilities has been described in this paper. The mechanism for generating this structured text is not only integral to our system architecture but allows vendors to incorporate verification results in the development of their own product.  6. ReferencesWorld Wide Web Consortium (W3C) “Extensible Markup Language (XML)”, http://www.w3.org/XML/, July 13, 2004.World Wide Web Consortium (W3C) “XML Schema”, http://www.w3.org/XML/Schema, June 30, 2004.World Wide Web Consortium (W3C) “The Extensible Stylesheet Language Family (XSL)”, http://www.w3.org/Style/XSL/, July 8, 2004.World Wide Web Consortium (W3C) “Document Object Model (DOM)”, http://www.w3.org/DOM/, July 7, 2004.Apache Software Federation “Xerces Java Parser”,  HYPERLINK "http://xml.apache.org/xerces-j/" http://xml.apache.org/xerces-j/.Simulation Interoperability Standards Organization (SISO) “Verification System Enhancements for an Expanding Mission”,  HYPERLINK "http://www.sisostds.org/doclib/doclib.cfm?SISO_FID_9076" http://www.sisostds.org/doclib/doclib.cfm?SISO_FID_9076, Sep 23, 2003.Author Biographies JOHN SCHLOMAN is a Software Engineer at the Johns Hopkins University Applied Physics Laboratory.  In 2003 he completed his Master's in Computer Science at Michigan State University.  In 2001 he received a degree Cum Laude in Systems Analysis from Miami University in Oxford, Ohio.JONATHAN LABIN is a Software Engineer at the Johns Hopkins University Applied Physics Laboratory.  In 2003 he received a degree Magna Cum Laude in Computer Science at the University of Maryland Baltimore County.SHON VICK is a Senior Professional Staff member at the Johns Hopkins University Applied Physics Laboratory (JHU/APL), Mr. Vick's current research interests include modeling and distributed simulation, agent based computing as well as automating model composition. In addition to his position as Computer Scientist at JHU/APL he is an Adjunct Faculty Member in the Department of Computer Science and Electrical Engineering at the University of Maryland Baltimore County. Mr. Vick has an MS from Johns Hopkins University in Computer Science and a BA in Economics/Mathematics from Rutgers University.Appendix A.	XML Schema for VEML<?xml version="1.0"?><xsd:schema xmlns:xsd="http://www.w3.org/2001/XMLSchema">	<xsd:annotation>		<xsd:documentation>RTI Verification Error XML (VEML) Schema.  Designed for the HLA RTI developed by JHU/APL NSAD.  When a vendor RTI is verified		and a set of errors occur, this file represents the totality of failures for that single RTI binary.  It is the 		history of all error output that occurred during VV&A.</xsd:documentation>	</xsd:annotation>	<xsd:element name="History">		<xsd:complexType>			<xsd:sequence>				<xsd:element name="Test" type="errorType" minOccurs="1" maxOccurs="unbounded"/>			</xsd:sequence>			<!--Global Attributes.  Metadata to inform about the VV&A process involved for this specific			error set-->			<xsd:attribute name="Date" type="xsd:date" use="required"/>			<xsd:attribute	name="POC" type="xsd:string" use="required"/>			<xsd:attribute name="RTI" type="xsd:string" use="required"/>			<xsd:attribute name="Build" type="xsd:string" use="required"/>		</xsd:complexType>	</xsd:element>	<xsd:complexType name="errorType">		<xsd:annotation>			<xsd:documentation>				An individual error occurrence.  This is at the test level so it contains all events in a test up				to where the verification suite exited.  Each may contain multiple failures.			</xsd:documentation>		</xsd:annotation>		<xsd:sequence>			<!--Boilerplate from HLA Spec that this error involves-->			<xsd:element name="TestDescription" type="xsd:string"/>			<!--Textual analysis from the Verification Engineer-->			<xsd:element name="Analysis" type="xsd:string" minOccurs="0"/>			<!--the error message.  "Failed with reason, unexpected RTI service indication".-->			<xsd:element name="Message" type="xsd:string" minOccurs="0"/>			<xsd:element name="Event" type="eventType" maxOccurs="unbounded"/>		</xsd:sequence>		<!-- HLA SPEC Context attributes.  These attributes represent the location in the spec that was		being tested when this error occurred.-->		<xsd:attribute name="Scenario" type="xsd:string" use="required"/>		<xsd:attribute name="Series" type="xsd:string" use="required"/>		<xsd:attribute name="Test" type="xsd:string" use="required"/>	</xsd:complexType>	<xsd:complexType name="eventType">		<xsd:annotation>			<xsd:documentation>				An event, good or bad, in an error sequence.			</xsd:documentation>		</xsd:annotation>		<xsd:sequence>			<!--the appropriate HLA code in this event-->			<xsd:element name="Code" type="xsd:string"/>			<!--the appropriate HLA exception.  May or may not exist-->			<xsd:element name="Exception" type="xsd:string" minOccurs="0"/>			<!--the appropriate HLA return value.  May or may not exist.  Probably			won't exist if an Exception is thrown-->			<xsd:element name="ReturnValue" type="xsd:string" minOccurs="0"/>			<!--the human readable text that describes this event-->			<xsd:element name="Text" type="xsd:string" minOccurs="0"/>		</xsd:sequence>		<xsd:attribute name="Time" type="xsd:positiveInteger" use="required"/>		<xsd:attribute name="ID" type="xsd:ID" use="required"/>		<xsd:attribute name="Federate" use="required">			<xsd:annotation>				<xsd:documentation>					Which federate on which this event occurred				</xsd:documentation>			</xsd:annotation>			<xsd:simpleType>				<xsd:restriction base="xsd:positiveInteger">					<xsd:minInclusive value="1"/>					<xsd:maxInclusive value="5"/>				</xsd:restriction>			</xsd:simpleType>		</xsd:attribute>		<xsd:attribute name="Type" use="required">			<xsd:annotation>				<xsd:documentation>					What sort of error is this?				</xsd:documentation>			</xsd:annotation>			<xsd:simpleType>				<xsd:restriction base="xsd:string">					<xsd:enumeration value="error-exception"/>					<xsd:enumeration value="error-callback"/>					<xsd:enumeration value="error-dnr"/>					<xsd:enumeration value="error-base"/>					<xsd:enumeration value="error-rti"/>					<xsd:enumeration value="success-exception"/>					<xsd:enumeration value="success-callback"/>					<xsd:enumeration value="success-base"/>					<xsd:enumeration value="non-event"/>				</xsd:restriction>			</xsd:simpleType>		</xsd:attribute>	</xsd:complexType></xsd:schema>Figure 3.1Log text of all events to current timeFigure 3.2Log text of all events to current timeTimeline of all events to current timeDescription of the current testList of failed tests