Validation of Human Behavioral ModelsAvelino J. GonzalezMaureen MurilloElectrical and Computer Engineering DepartmentUniversity of Central FloridaOrlando, FL 32816-2450 HYPERLINK mailto:ajg@ece.engr.ucf.edu ajg@ece.engr.ucf.eduKeywords:Validation and verificationHuman behavior modelsABSTRACT Validation of human behavioral models, such as those used to represent hostile and/or friendly forces in training simulations is an issue that is gaining importance, as the military depends on such training methods more and more.  However, this introduces new difficulties because of the dynamic nature of these models and the need to use experts to judge their validity.  As a result, this paper discusses some conceptual approaches to carry out this task.  These are based on comparing the behavior of the model to that of an expert, while the latter behaves normally in a simulated environment, under the same conditions as perceived by the model.1.  IntroductionThe field of intelligent systems has matured to the point where significant research is now being focused on modeling human behavior.  Earlier research work, mostly in the form of expert systems, concentrated on developing means of representing and manipulating deep but narrow and specialized knowledge efficiently and effectively.  Their objective was to provide expert advice in the process of solving difficult and specialized problems.  This objective has generally been met successfully, with research in expert systems currently having shifted to more efficient means of knowledge acquisition, and system validation and verification. Expert systems, however, have three significant limitations in our quest for a truly intelligent system: 1) they are, by definition, generally quite limited in breadth, 2) they typically do not directly control anything, and 3) they do not learn easily.  Truly intelligent systems must be able to interact with their real world environment in all of its breadth and scope, as well as learn from it on a continuous or semi-continuous basis.  Additionally, they must be able to directly control their own actions if they are to survive in the real world.  Much of the current research effort has centered on developing intelligent systems that can do these things, either as robots in the physical world, or as computer generated entities in a simulation of the real world.  The latter are most often used to assist in simulation-based training, but have applications in entertainment and control.To be successful in this endeavor, researchers must look to the human mind and attempt to replicate its mode of operation. Doing this at the biological level has been nearly impossible to do, as the inner workings of the human brain and how that translates into intelligence are not well understood.  Furthermore, it is beyond the realm of modern science to be able to reproduce brain tissue artificially.  Simulating the neural function in a digital computer using connectionist approaches has shown significant success when applied to certain classes of problems.  They enjoy the distinct advantage of being able to learn when presented with examples.  However, neural networks have not shown success in demonstrating general intelligence by themselves.  Furthermore, it is clear even to the most naïve that the human brain does not employ the complex mathematical algorithms which neural networks use to learn.  Modeling human behavior at the procedural level has shown significant promise.  Through introspection, humans can and have been able to identify several high level techniques used to solve some problems, especially that of interacting with our environment in order to live, thrive and survive in it.  Certainly, solving problems in the same high level way as humans do is a step in the right direction.The field of human behavior representation research is basically divided in two parts: those working in robotics as the embodiment of these models, and those working in intelligent simulated entities for training.Intelligent robotic research has generally focused on giving robots the ability to handle fairly low level tasks, such as route planning, obstacle avoidance, and searching for specific objects in a closed environment.  A second aspect of this work has been in reinforcement learning – learning by experiencing the environment in much the same way that a child learns not to touch a hot stove after she gets burned once.Research in simulated intelligent entities, on the other hand, has focused on modeling a higher level of behavior, such as that used in tactical behavior modeling.  This different focus is largely a result of the need for displaying tactically correct behavior in a wartime simulation.  While route planning and obstacle avoidance have also been areas of intensive work, learning has not been a major issue up until recently.  More of interest has been to develop efficient and effective models that truly represent tactical behavior at a minimum of cost to develop as well as to execute.Human behavioral models, known in the military simulation and training community as Computer Generated Forces (CGF), have been successfully incorporated in several significant simulation training systems.  However, there is a serious need on the part of these users to be able to validate the behaviors demonstrated in order to ensure a sound training process.  But such validations are not easy.  This paper provides an insight into the type of procedures that would be required to adequately validate these human behavioral models.  However, prior to that discussion, a review of the most common and popular means of representing CGF systems would be appropriate as a way to set up the discussion on their validation. CGF Implementation TechniquesWhen a human tactical expert is asked what he would do under certain circumstances, the response is typically framed as a conditional.  “If this was green and that was blue, then I would turn left”.  Thus, the most intuitive as well as popular means of representing tactical human behavior is through the use of rules.  However, rules have the drawback that they are quite myopic in scope.  To develop a system with any kind of tactical realism, a large number of rules need to be developed and executed, as the numerous conditions resulting from the many variations can translate into an explosive number of rules for even relatively simple tasks.  This is not efficient.  Furthermore, gaps in the behavior can be easily left uncovered.  Whereas these gaps can be easily filled with new rules, it is a makeshift process that does not have natural closure.  Experts systems suffer from the same deficiency.  But the domain of expert systems, being more limited and the inputs more predictable, can easily tolerate the situation.Because of its intuition and popularity, most of the early systems that implemented CGF’s were based on rules.  Golovcsenko [1] discusses some Air Force prototype systems that exhibit certain amount of autonomy in the simulated agents.  One in particular, a special version of the Air Force's TEMPO force planning war game system, uses rule-based techniques to replace one of the human teams involved in the game. One notable CGF system is the State Operator and Results system (SOAR) [2][3].  SOAR takes a goal-oriented approach in which goals and sub-goals are generated and plans to reach them are formulated and executed.  These plans are in effect until the goals are reached, at which point they are replaced by new goals that address the situation.  However, SOAR is based on the rule-based paradigm, which, as mentioned before, has many disadvantages.Another popular technique used in CGF systems has been Finite State Machines (FSMs).  FSM’s have been used to implement goals or desired states in the behavior of the CGF [4].  These states are represented as C-Language functions.  Three major FSM-based CGF systems are the Close Combat Tactical Trainer (CCTT) [5], ModSAF [6], and IST-SAF [7].  These systems all employ FSMs as the representational paradigm.  The knowledge found on FSM’s does not necessarily aggregate all the related tasks, actions, and things to look out for in a self-contained module.  This makes their formalization somewhat difficult from the conceptual standpoint.  Some FSM-based systems allow for the control of one entity by more than one FSM at the same time.  This can be dangerous in that an incorrect behavior can be easily displayed.Other, less popular, alternative representation and reasoning paradigms such as model-based, constraint-based or case-based reasoning, although promising in some respects, (see [8][9][10]) are not "natural" for this form of knowledge since they do not easily capture the heuristics involved in tactical behavior representation.  Another common approach to the CGF problem has been to use blackboard architectures to represent and organize the knowledge.  One implementation [11] uses separate knowledge sources to carry out tasks such as situation assessment, planning, formulation of objectives, and execution of the plan.  This system also implements adaptive training so that the CGF can modify its action according to the skills on which the trainee needs to concentrate.  The system uses a modified version of Petri Nets to represent instructional knowledge.  Work carried out in the form of maneuver decision aids at the Naval Undersea Warfare Center [12] has also employed a blackboard architecture.  The objective of this research is to assist the submarine approach officer in determining the most appropriate maneuver to carry out to counter an existing threat, or to accomplish a mission.Another approach has come from cognitive science researchers [13] [14] [15].  These efforts do not directly address CGF's, but rather, the closely related problem of cognitive modeling of the human decision-making process.  Their efforts also make use of a blackboard architecture.  The COGNET representation language [16] uses a task-based approach based on the GOMS concept [17] [18], in an opportunistic reasoning system.  In this approach, all actions are defined as tasks to be performed by the CGF entity.  The definition of each task includes a trigger condition that indicates the situation that must be present for that task to compete for activation with other similarly triggered tasks.  The use of blackboard system, however, introduces a high overhead and much added complexity.As can be seen, there are several means of representing the knowledge required to model human behavior as it applies to tactics.  The problem remains how to validate the behavior models in a way that makes all the different representational paradigms transparent.  The next section discusses some conceptual approaches to the problem. State of the Art in Validation Techniques for Human Behavior ModelsBirta and Özmizrak [19] propose an approach for behavioral validation of simulation models, which is intended to perform automatic validation.  The key element of their software environment is a validation knowledge base (VKB), whose objective is to comprehensively identify the expected behavior of the simulation model.  Based on the information in the VKB, a set of experiments are then defined, to be performed by the simulation model. Finally, the results of the execution are compared against the expected behavior in the VKB to determine the validity of the model [19].The global architecture of the software environment is made up of four components [19]:Simulation Model: this is the computer program that consists of the human model under investigation.Validation Knowledge Base (VKB): this is the base of knowledge used for the definition of experiments and for the evaluation of the model.  It contains the information regarding the expected behavior of the model, which is acquired either from an expert or from data obtained from an existing system and/or previously validated models.Experiment Generator: this is the module that designs the experiments to be executed by the model simulation, based on the specifications of the VKB.Evaluator: this takes the results of the experiments and compares the output of the model simulation against the expected behavior characterized of the specifications within the VKB.  Important inconsistencies would invalidate the model.The framework for developing the VKB is formulated around the concept of dynamic behavior and it is represented by means of an abstraction called dynamic object.  A dynamic object O is an ordered pair of vectors X and Y, i.e., O = (X, Y), where X corresponds to the input of the dynamic object and Y corresponds to the output of the object.  The fundamental property of the dynamic object is “its ability to generate (exhibit) behavior over some prescribed time interval [19, p.81].  It’s assumed a causal relationship between the vectors X and Y.The VKB should contain all possible instances of a dynamic object.  These instances or elements are described in terms of their attributes, i.e., their input and output vectors.  This characterization is presented as the union of three disjoint sets of relationships:Formal Specifications: “these are known relationships among the components of the input and output vectors that must always hold” [19, p.84]. As an example, the authors mention that in an automobile simulation, vertical displacement of the automobile must never be negative.  These specifications are non-causal relationships.Qualitative Specifications: these are known causal relationships between the components of the input and output vectors.  Again, in an automobile simulation, the engine torque has to reduce to zero if the fuel level has reached zero.Observational Specifications: these specifications refer to the behavior acquired either from a real system or from another previously validated simulation model.The proponents of this approach developed an example problem showing the different relationships contained in the VKB. The simulation is about a computer model, where a single processor computer is connected to n terminals. This example can be found in  [19].Caughlin [20] denotes the lack of efficient techniques to compare a simulation with the real situation that is being modeled, or to compare two different models of the same situation.  His proposal is directed to both verification and validation of models and simulations.  However, only the validation process is addressed in this paper.His idea is to abstract the original model and to reduce its complexity by aggregating the details into a more general module.  Caughlin proposes to analyze the model as a whole and to determine if it meets the expected behavior according to the real phenomenon that is under study, instead of analyzing the different parts of the model and then trying to integrate the results.His approach consists in reducing the order (to abstract) of the model representation by means of what he calls reduced order metamodels.  These metamodels would be used as black boxes that approximate the causal time dependent behavior represented by the simulation [20].  As he states, “as an abstraction, a metamodel is a projection of the model onto a subspace defined by new constraints or regions of interest” [20, p.1408].Caughlin presents two basic techniques for reduced order metamodels::Direct methods: these techniques consist of applying general principles to the original model in order to obtain a more abstract model.  The idea is to take each component of the model and to apply to them a process called “zooming”, and then to interconnect them again to build the physical representation of the model.Inverse methods: these methods are based on the input data and outputs generated by the original model.  This kind of techniques produces a reduced order metamodel through a mathematical transformation between a set of inputs and outputs.In order to carry out the validation process using reduced order metamodels, it is necessary to have data available from the real situation that was modeled.  This data should include all the aspects concerning the behavior simulated in the computer system.  Having this information, the validation process consists in developing a reduced order model of both the real situation and the model or simulation and then comparing the reduced order model coefficients.According to Caughlin, this approach is cost effective, timely and objective [20, p. 1412].Page and Canova [21] discuss in their paper some mechanisms and research directions that need to be applied to confront the differences that the verification and validation processes for advanced distributed simulation (ADS) present.ADSs make reference to efforts directed to build an environment that provides the integration of three different types of simulations: live, virtual and constructive.  “A live simulation involves real people operating real systems in realistic operational conditions, a virtual simulation involves real people operating in simulated systems, while constructive simulation refers to the more commonly recognized computer simulations (simulated people operating in simulated systems)” [21, p.394].One of the most important ADS is the Aggregate Level Simulation Protocol (ALSP) Joint Training Confederation (JTC).  JCT is a collection of training simulations that supports joint training at the command and battle staff levels during major [21, 1997].  The authors describe in their paper the mechanisms used for the verification, validation and accreditation processes of ADS.  They present the development process of the JTC and they emphasize the importance of validation in each stage of this process as well as a general validation of the whole system.The paper is extensively technical, descriptive and specific.  Therefore, any additional information about the approach can be found in [21, 1997].The approach proposed by Hone and Moulding [22] consists of improving the understanding of the application domain in which the model (called by the authors Synthetic Environment) is to be employed.   This is done instead of focussing only in the problem domain being modeled as most of the validation and development techniques for simulations do.Their objectives are to complement existing development approaches for synthetic environments (SE) with application domain modeling, and to define a suitable set of applications-oriented models which are relevant to the military training and to the military equipment procurement [22].  The authors argue that these models could be used as the basis for the verification and validation processes.This validation perspective is still in an early stage of research, but the authors have proposed some modifications to various explicit representations that traditionally have been used in the design process.  Their proposal is based on two existing architectures [22]:At a first abstraction level from the problem domain, via the Conceptual Models of the Mission Space (CMMS).At the simulation level by High Level Architecture (HLA) Federation Object Models and Simulation Object Models (FOMs and SOMs).According to Hone and Moulding, this new approach would provide stronger validation capabilities to the development process of synthetic environments.  At the same time, they suggest carrying out the validation process as early as possible in the development process in order to minimize possible costs and risks.Validation Through Automated Observation of Agent BehaviorSince by definition these models are designed to simulate human behavior, it becomes clear that they must be compared to actual human behavior.  Validation of the more traditional expert systems is really no different, as these attempt to model the problem solving ability of human experts.  Expert systems have traditionally been validated using a suite of test cases whose solution by human domain experts is known ahead of time.  However, these tests are generally static in nature – they provide the system with a set of inputs and obtain its response, then check it against the expert’s response to the same inputs.  Time is typically not part of the equation, unless it is already implicitly incorporated into the inputs (i.e., one input could represent a compilation of the history of one input variable). The techniques suggested by Abel [23] provide effective and efficient means of generating good test cases based on the validation criteria specified for the intelligent system. The Turing Test approach proposed by Knauf [24] is a promising way to incorporate the expert’s opinion in a methodical fashion for time-independent problems and their time-independent solutions.Validating human behavioral models, on the other hand, requires that time be explicitly included in the expression of the tactical behavior.  Such behavior not only has to be correct, but also timely.  Reacting to inputs correctly, but belatedly can result in the decision-maker’s destruction in a battlefield.  Furthermore, tactical behavior is usually composed of a sequence of decisions that are made as the situation develops interactively.  Such developments are generally unpredictable and, therefore, it becomes nearly impossible to provide test inputs for them dynamically.Certainly the test scenarios could be “discretized” by de-composing them into highly limited situations that would take the time out of the equation.  However, several of these would have to be strung together in sequence in order to make the entire test scenario meaningful.  This would be artificial, and the expert may have difficulty in visualizing the actual situation when presented thusly.  Furthermore, interaction would not be possible.  Therefore, I do not believe this would be acceptable as a mainstream solution.One alternative would be to observe the expert or expert team while he/they display tactical behavior, either in the real world, or in a simulation specially instrumented to obtain behavioral data.  Certainly, using a simulation would make the task of data collection and interpretation much easier, at the cost of having to build a simulation.  However, since the models are to be used in a simulation, it is likely that such an environment already exists.Conceptually speaking, validation can be executed for these types of models by comparing the performance of the expert with that of the system while being subjected to the same initial inputs.  The performance of each (the intelligent entity and the expert) can be represented as a sequence of data points for the observable variables over a period of time.  Overlaying one on top of the other may provide some indication of validity for the system’s performance.  A complete match between the expert’s performance and the model’s behavior would certainly justify validation of the model.  Realistically, however, a significant amount of deviation may exist between the two performance records.  While some deviations may be indicative of a serious discrepancy in behaviors, others may simply be a different and equally appropriate way of achieving the same goal.  Thus, expert input may be necessary to determine what is correct and what is not correct.  Alternatively, an intelligent system could be developed to perform this task of determining what is an acceptable deviation and what is not, but it would also ultimately have to be validated itself, and that would ultimately require human expertise.Furthermore, due to the interactive nature of the model and the domain, the system being validated may make a different decision from what was made by the validating expert which, although correct, progresses into a different scenario.  Consequently, the two performance records could no longer be adequately compared, as their situations may have diverged significantly enough to make them not relevant to each other.In reality, none of the above techniques provide us with an effective and efficient means to validate the performance of human behavioral models.  This leaves us in a quandary, until we take into consideration how the model was built in the first place.  Building the model in the traditional way – interviewing the subject matter experts (SME’s) and building the model by hand from their response to the numerous queries made in these interviews, would in fact place us in this quandary.  There would be little relationship between the means of model development and that of validation.  However, by tying the means of building the model with the validation process, some of the obstacles described above may be overcome.  This is described in the next section.Learning by Observation of Expert Performance in a SimulationHumans have the uncanny ability to learn certain tasks through mere observation of the task being performed by others.  While physical tasks that involve motor skills do not fit under this definition (e.g., riding a bicycle, hitting a golf ball), cognitively intensive, or procedural tasks can be relatively easily learned in this way.  Very often we hear people asking for examples of how to perform a task so they can see how it is done.  If such is the case for humans, certainly machines can be made to do the same type of learning.This idea was seized by Sidani [25], who developed a system that learns how to drive an automobile by simply observing expert drivers operate a simulated automobile.  The system observed the behavior of an expert when faced with a traffic light transition from red to green.  It also observed the behavior when a pedestrian attempted to cross the street.  Furthermore, it was able to correctly infer a behavior it had not previously seen when faced with both, a traffic light and a pedestrian on the road.  Sidani compartmentalized the behaviors by training a set of neural network, each  of which was called to control the system under specific circumstances.  A symbolic reasoning system was used to determine which neural network was the one applicable for the specific situation.Further work in the area is currently being carried out by Gonzalez, DeMara and Georgioupoulos [26, 27] in the tank warfare domain.  Using a simulation as the observation environment, behavior is observed and modeled using Context-based reasoning (CxBR).  For an explanation of CxBR, see [28].  Supported by the U. S. Army under the Inter-Vehicle Embedded Simulation for Training (INVEST) Science and Technology Objective, this project also extends the concept of learning through observation by including an on-line refinement option.  See Bahr and DeMara [29] for further information on the nature of the INVEST project.5.1 On-Line Refinement and ValidationThe concept of refinement involves improvement of an intelligent system during or after initial validation.  The model being developed by Gonzalez, DeMara and Georgioupoulos (which was learned through observation) is intended to predict the behavior of a human combatant in a federated simulation.  This is rather different from conventional CGFs that attempt to simply emulate actual entities in a general way.  Prediction also carries a much heavier burden when it is regularly and continuously compared to actual behavior, as is the case in the application of the resulting model.  However, it provides the opportunity to implement validation relatively easily.A predictive model is required because an accurate prediction of actual human behavior would reduce the need for real-time on-air communications of the position of an actual vehicle in the field to all others in the simulation.  Each live vehicle in the simulated exercise must be aware of the position of all other live, simulated and virtual vehicles in the simulated environment, which may or may not be the same as the physical environment where the vehicle is physically located.  Rather than communicating its whereabouts constantly (which is expensive due to the small bandwidth available), each live vehicle has in its on-board computer a model of all other live and simulated vehicles.  If the other vehicles all behave as modeled, the personnel in that vehicle can confidently predict its location and would need no update on its position.  However, it is unrealistic that a perfect model could be found, in spite of the latest modeling and learning techniques.  Each vehicle carries a model of itself in it on-board computer.  It compares the actual position, speed and other observable actions with those predicted by the model.  If in agreement, no action is necessary.  However, if a discrepancy arises, all other models of this vehicle resident in all the other vehicles (called “clone” models) are not correctly predicting its behavior.  Thus, some corrective action must be initiated.  Such action can be: 1) discarding the model and providing constant updates through communication, 2) modifying context in which the model is on an on-line basis, or 3) permanently changing the  model to reflect reality.  This last step can be referred to as refinement.Therefore, a means to detect deviations becomes necessary.  As such, an arbiter system must be developed to determine when the model no longer correctly predicts the behavior so that a correction is initiated.  This arbiter, called the Difference Analysis Engine (DAE), basically serves to compare the behavior of the human with that of the model. 5.2 The DAE as a Validation EngineThe DAE is designed to ascertain when deviations between the human and the model are significant enough to warrant initiation of corrective action (i.e., communication sequence).  However, it would be relatively easy to convert the DAE into a validation engine instead.  The model and the human expert could be reacting to the simulated environment simultaneously, with the DAE continuously monitoring them for discrepancies.  Upon discovering a serious enough discrepancy, the DAE could note it and either continue, or modify the model so that it agrees with the human’s behavior.  Through the use of contexts as the basic behavioral control paradigm, the requisite action would merely be to suggest a context change in the model.  This new suggested context would agree with the expert’s action and the validation exercise could continue in a synchronized fashion.  While this would represent external manipulation of the  model, a team of experts could afterwards, in an after action review, determine whether the model should be permanently modified to reflect that change.  Alternatively, the change could simply be noted, recorded and presented to the panel of experts at an after action review.  The drawback to this is that unless rectified, a discrepant decision by the model could serve to make the rest of the validation exercise irrelevant, as the model may be faced with situations that are different from that of the human.Summary and ConclusionIt is clear that validation of human behavioral models introduce a new level of difficulty in the validation of intelligent systems.  Conventional validation techniques, such as the ones used for expert systems, may not be effective for such a task.  A promising alternative exists with the concept of learning and refining a model through observation of expert behavior.  While this concept is relatively immature and requires significant further investigation, I feel that it represents a very viable approach to this very difficult problem of validating human behavior models.References[1] Golovcsenko, I. V., "Applications of Artificial Intelligence in Military Training Simulations", Master's Degree Research Report, University of Central Florida, 1987.[2] Laird, J. E., Newell, A. and Rosenbloom, P. S., "Soar: An Architecture for General Intelligence", Artificial Intelligence, 33(1), 1987, pp. 1-64. [3] Tambe, M., Johnson, W. L., Jones, R. M., Koss, F., Laird, J. E. and Rosenbloom, P. S., "Intelligent Agents for Interactive Simulation Environments", AI Magazine, Spring, 1995, pp. 15-39. [4] Dean, C. J., "Semantic Correlation of Behavior for the Interoperability of Heterogeneous Simulations", Master's Thesis, Department of Electrical and Computer Engineering, University of Central Florida, May 1996.[5] Ourston, D., Blanchard, D., Chandler, E. and Loh, E., "From CIS to Software", Proceedings of the Fifth Conference on Computer Generated Forces and Behavioral Representation, Orlando, FL, May 1995, pp. 275-285. [6] Calder, R., Smith, J., Coutemanche, A., Mar, J. M. F. and Ceranowicz, A., "ModSAF Behavior Simulation and Control", Proceedings of the Third Conference on Computer Generated Forces and Behavioral Representation, Orlando, FL, March 1993, pp. 347-356.[7] Smith, S. and Petty, M., "Controlling Autonomous Behavior in Real-Time Simulation," Proceedings of the Southeastern Simulation Conference, Pensacola, FL, 1992, pp. 27-40.[8] Borning, A., "ThingLab - An Object-oriented System for Building Simulations Using Constraints," ACM Transactions on Programming, Languages and Systems, 3 (4) October, 1977, pp. 353 - 387.[9] Castillo, D., "Toward a Paradigm for Modelling and Simulating Intelligent Agents," Doctoral Dissertation, University of Central Florida, December, 1991.[10] Catsimpoolas, N. and Marti, J., "Scripting Highly Autonomous Behavior Using Case-based Reasoning", Proceedings of the 25th Annual Simulation Symposium, Orlando, FL, April 1992, pp. 13-19.[11] Chu, Y. Y. and Shane, D., "Intelligent Opponent Simulation for Tactical Decision Making," Report PFTR-1120-11/86, sponsored by the Office of Naval Research and the Naval Training Systems Center, 1986.[12] Benjamin, M., Viana, T., Corbett, K. and Silva, A., "The Maneuver Decision Aid: A Knowledge Based Object Oriented Decision Aid", Proceedings of the Sixth Florida Artificial Intelligence Research Symposium, April, 1993, pp. 57-61.[13] Weiland, M. Z., Cooke, B., and Peterson, B., "Designing and Implementing Decision Aids for a Complex Environment Using Goal Hierarchies," Proceedings of the Human Factors Society 36th Annual Meeting, 1992.[14] Zubritsky, M. C., Zachary, W. W. and Ryder, J. M., "Constructing and Applying Cognitive Models to Mission Management Problems in Air Anti-Submarine Warfare," Proceedings of the Human Factors Society 33rd Annual Meeting, 1989, pp. 129-133.[15] Zachary, W. W., "A Context-based Model of Attention Switching in Computer Human Interaction Domain," Proceedings of the Human Factors Society 33rd Annual Meeting, 1989, pp. 286-290.[16] Zachary, W., Ryder, J. and Ross, L., "Intelligent Human-Computer Interaction in Real Time, Multi-tasking Process Control and Monitoring Systems", in M. Helander and M. Nagamachi (Eds.) Human Factors in Design for Manufacturability, New York: Taylor and Francis, 1992, pp. 377-402.[17] Card, S., Moran, T. and Newell, A., The Psychology of Human-Computer Interaction, Hillsdale, NJ: Lawrence Erlbaum Associates, 1983.[18] Olsen, J. R. and Olsen, G. M., "The Growth of Cognitive Modeling in Human-Computer Interaction since GOMS", Human Computer Interaction, 5(2&3), 1990, pp. 221-265.[19] Birta, L. G. and Özmizrak, F. N.,  “A Knowledge-Based Approach for the Validation of Simulation Models: The Foundation”, ACM Transactions on Modeling and Computer Simulation, Volume 6, Number 1, January 1996.[20] Caughlin, D., “Verification, Validation, and Accreditation (VV&A) of Models and Simulations through Reduced Order Metamodels”, Proceedings of the 1995 Winter Simulation Conference,1995.[21] Page, E. H. and Canova, B. S.  “A Case Study of Verification, Validation, and Accreditation for Advanced Distributed Simulation”, ACM Transactions on Modeling and Computer Simulation, Volume 7, Number 3, July, 1997.[22] Hone, G. and Moulding, M., “Application Domain Modeling to Support the Verification and Validation of Synthetic Environments”, Spring Simulation Interoperability Workshop,  Orlando, FL, March, 1998.[23] Abel, T. and Gonzalez, A. J., “Enlarging a Second Bottleneck: A Criteria-based Approach to Manage Expert System Validation Based on Test Cases”, Proceedings of the 1997 Florida Artificial Intelligence Research Symposium, Daytona Beach, FL, May 1997.[24] Knauf, R,. Jantke, K. P., Gonzalez, A. J., and Philippow, I., “Fundamental Considerations for Competence Assessment for Validation”, Proceedings of the 11th International Florida Artificial Intelligence Research Society Conference, Sanibel Island, FL, May 1998, pp. 457-461.[25] Sidani, T. A. and Gonzalez, A. J.: “IASKNOT: A Simulation-based, Object-oriented Framework for the Acquisition of Implicit Expert Knowledge” Proceedings of the IEEE International Conference on Systems, Man and Cybernetics, Vancouver, Canada, October 1995.[26] Gonzalez, A. J., DeMara, R. F. and Georgiopoulos, M. N., “Vehicle Model Generation and Optimization for Embedded Simulation”, Proceedings of the Spring 1998 Simulation Interoperability Workshop, Orlando, FL March 1998.[27] Gonzalez, A. J., DeMara, R. F. and Georgiopoulos, M. N., “Automating the CGF Model Development and Refinement Process by Observing Expert Behavior in a Simulation”, Proceedings of the Computer Generated Forces and Behavioral Representation Conference, Orlando, FL, May 1998.[28] Gonzalez, A. J., and Ahlers, R. H.: “Context-based Representation of Intelligent Behavior in Simulated Opponents” Proceedings of the 5th Conference in Computer Generated Forces and Behavior Representation, Orlando, FL, May 1995. [29] Bahr, H. A. and DeMara, R. F.: “A Concurrent Model Approach to Reduced Communication in Distributed Simulation” Proceedings of 15th Annual Workshop on Distributed Interactive Simulation, Orlando, FL, Sept. 1996.  