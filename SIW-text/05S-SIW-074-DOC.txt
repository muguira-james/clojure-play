DTE 4 : Test Execution in a Distributed EnvironmentMichael J. O'ConnorITT Industries6767 Old Madison Pike, Suite 160Huntsville, AL 35806256.964.1470 HYPERLINK "mailto:michael.oconnor@itt.com" michael.oconnor@itt.comDavid J. RibailMateriel Test DirectorateWhite Sands Missile Range, NM 88002505-678-8715dribail@wsmr.army.milPeter ValentineElectronic Proving GroundFt Huachuca, AZJim ChaseAmtec Corporation500 Wynn Drive, Suite 314Huntsville, AL 35811256.319.6145HYPERLINK "mailto:jchase@amtec-corp.com"jchase@amtec-corp.comKeywords:Test Execution, Test IntegrationABSTRACT: Conducting any test requires careful coordination and planning to ensure smooth execution.  Distributed testing complicates this process and magnifies the problems usually associated with planning and coordination.  Through initiatives such as Distributed Test Events, the US Army Developmental Test Command  has developed  tools, processes, and people  that support the execution of distributed testing.  DTE-4 was the fourth in a series of DTC sponsored events designed to integrate geographically disparate live, virtual, and constructive elements into one cohesive and integrated synthetic battlespace; a goal established as part of the Virtual Proving Ground (VPG) Synthetic Environment Integrated Testbed (SEIT) program.  This paper describes DTE 4 test preparation, integration, and execution.  Processes for site and range coordination were developed to support this distributed test event.  Additionally, tools were also introduced to help with the integration process and during test execution.  The use of these tools and processes helped to insure success for the event.  Drawing from  experience garnered during  previous distributed test events, VPG created two specialized tools to support test execution.  One, the Distributed Capabilities Integrated Toolbox (DCIT) Connectivity Matrix Utility (CMU) tool,  was used during test preparation.  The DCIT CMU supports the test integrator during the integration phase.  The second tool,  TestTalk,  provides a distribute test clock and event list.  Both of these web based tools and their use is described in this paper.    Additionally lessons learned and future improvements to the tools and processes will be presented.1. Introduction The successful conduct of a large distributed test involves large numbers of skilled people, processes, and tools during the actual execution.  Additional skilled people, processes, and tools are required to support planning, scenario generation, network connectivity, data collection, analysis, and other aspects of distributed testing.  This paper focuses on the people, processes, and tools involved in the execution of distributed test events and focuses specifically on the execution of the US Army Developmental Test Command (DTC) Virtual Proving Ground (VPG) Distributed Test Event 4 (DTE 4).Generally, there are two major phases involved in the distributed test process.   The first phase is the simulation integration phase.  During DTE 4, this phase was conducted over 6 months in a series of one week long spirals.  Each spiral had specific goals.  To insure success, a lead Test Integrator was designated for spiral integration.  The Test Integrator was responsible for all activities leading up to test execution, from establishing the simulation federation, to insuring that network connectivity between participant test centers was established and maintained.  The second major phase was the test execution phase.  The test execution phase lasted one week.  During the test execution phase, a Test Director was responsible for event synchronization, mission control, and time ordered events.Many specialized tools, developed under the auspices of  DTC’s VPG program, were utilized during integration and execution.  Of note were the Distributed Capabilities Integrated Toolbox (DCIT) Connectivity Matrix Utility (CMU), which was utilized during the simulation integration phase, and TestTalk, which was utilized during the execution phase.1.1 DTE 3DTE 3 was the third in a series of VPG distributed test events.  This event was conducted in December of 2003.  DTE 3 was primarily concerned with linking ATEC test center capabilities, not with conducting a test.  The lessons learned from DTE 3 were applied to DTE 4.  A number of papers describing DTE 3 have been written. [1] [2]1.2 DTE 4DTE 4 was the first of VPG’s distributed test events to include some element of testing.  An analysis plan was developed to support specific test objectives for DTE 4.  The lessons learned from DTE 3 were applied to DTE 4.  These lessons included the need for improvements to the execution process and the need for additional specialized tools to support the integration and execution processes.DTE 4 included more sites and a larger number of entities than DTE 3.  DTE 4 used a large tactically correct battlefield scenario.  This allowed for test items to be immersed in a larger environment.  Data was collected during execution of the event  and used for analysis of the test.  2. Simulation Integration PhaseThe simulation integration phase is required to ensure that all participant test center capabilities required for test can be successfully integrated to meet the needs of the test.  Establishment of the test support network, as well as the establishment of the simulation federation, consisting of Live, Virtual, and Constructive representations of battlefield entities, occurs during this phase,  The duration of this phase depends on the number of sites and federates required for the test.   2.1 Test IntegratorThe Test Integrator leads the simulation integration phase. The Test Integrator schedules the spirals and oversees  integration of all test center capabilities. The Test Integrator is responsible for the technical integration of all the simulation federates and the simulation network.  The Test Integrator, working with the Test Sponsor, Test Director, and participant test centers, creates a plan for each of the spirals. 2.1 Integration SpiralsA series of spirals events were scheduled based on the simulation integration requirements.  Spiral events each month help the integration process by establishing key integration objectives during each spiral that eventually lead up to a fully integrated test bed.  Initial spiral activities focus on establishment of the network and connectivity checks between participant test centers. Follow-on spirals focus on integration of specific test center entity representations, terrain databases, and integration of the instrumentation layer.  Final spiral activities focus on integration of Semi-Automated Force Simulations and the Time Ordered Events List. The final spiral was a dress rehearsal for the test week.  A total of eight spirals were conducted.  An integration plan that included specific objectives and schedules was prepared for each spiral.  The schedule was updated each day based on the success of each day’s integration activities.  All spiral activities were fully documented by the Test Integrator.  At the end of each spiral event, the Test Integrator created a spiral report which was provided to all of the participants.2.2 Distributed Capabilities Integrated Toolbox (DCIT) The DCIT is a collection of web-based tools under continuous development by the VPG Tools Focus Group. This toolbox supports the pre-test integration phase for distributed testing.  The Connectivity Matrix Utility (CMU), the first component of the DCIT to be developed, was used during DTE-4.   The CMU is browser-based application that facilitates the connectivity check procedure for a test network connecting multiple nodes with multiple simulation components.  The application tabulates in matrix form the connectivity verification input from operators at each remote site.  Checklist categories and network nodes for display are user-configurable.The CMU provides for significant time reduction by simplifying the verification of each test step between the test integrator and each DTCC.  The CMU allows each test location to input test results into a centrally located table or matrix that is viewable by all DTCCs.   Menus on the CMU show all functions, although some functions are available only to users with Event Administrator privileges.  Standard Users have the ability to input or change status and add notes on every matrix cell corresponding to their pre-assigned location.  Standard Users may view and hide matrix notes, view a POC list, view a help file and refresh the data.  Event Administrators may select any valid location or DTCC to represent, and may input or change status and add notes to matrix cells relative to that location.  Event Administrators may create, edit and delete test matrices within a defined event.   This user may also lock, unlock and reset test matrices.  Event Administrators may also add or delete locations within a defined event, add instruction notes for each matrix, and set/reset any individual row or column.  Like the Standard User, the Event Administrator may also view and hide matrix notes, view a POC list, view a help file and refresh the data.2.3 DCIT UsageThe CMU was created in response to a process issue identified in DTE-4 Spiral 1.  During that spiral, the Test Integrator undertook a time-consuming effort to verify bidirectional connectivity by voice and paper notation over a teleconference between several test sites.  For example, if the DTCC at Dugway Proving Ground were to publish Role Player WorkStation (RPWS) entities to the distributed test network, each test site participating in that test would be called upon to verify the receipt of that data or broadcast message.  The test integrator would then query each DTCC sequentially and log their results in a notebook.  This process would be repeated until each DTCC has acted as both the producer and as a client for the other participating DTCCs for the RPWS connectivity test, which may be only one among many required connectivity tasks.  These steps would have to be repeated for each task or test category being investigated, resulting in hours of wasted time that can be reclaimed with the use of the CMU and its user-configurable matrix feature.Figure 2.3-1 is an example where DPG is the information producer.  In this figure OTC and WSMR have indicated that they did not receive the information.Figure 2.3-1 – DCIT CMU Example 1The CMU is browser-based, written in ASP.NET and utilizing Microsoft Visual Studio .NET 2003, Windows Server 2003 and SQL Server 2000.  The relatively small size of this program and of the required data transfer size over the network allows for efficient use of data resources with a negligible effect on bandwidth.  Access to the DCIT requires a user account, created and controlled by the DCIT Master Administrator.  The user account defines a username, password, location ID, and permissions level.  The test integrator has Event Administrator privileges for the DCIT.  The test integrator may edit the “node” list for any matrix, adding or deleting the locations listed in a matrix as necessary.  A new matrix may also be created, the size of which is dependent upon the number of locations selected to participate in the subtest.  All matrices are NxN (i.e., 3x3, 6x6, 10x10) in dimension.  Location information is stored in the database.  Event Administrators can choose which location to represent; Standard Users are locked to a particular location as defined by their user account information, which is also stored in the database.Figure 2.3-2 – DCIT CMU Example 2When adding or editing a matrix, the Event Administrator may add, remove or change the display order of “node” locations; provide a Matrix Name; fill in a Matrix Description; add instructions or other information to the Matrix Notes text field; define a Matrix Column Label and a Matrix Row Label; and set the matrix to Unlocked or Locked mode.  Clicking the Submit button sets the matrix parameters and creates the matrix. Figure 2.3-2 illustrates the Edit Matrix capability provided to the Event Administrator.  This example shows the setup for the OTC Connectivity Check from DTE-4 Spiral 5.Once created, users can view the matrix.  If more than one matrix has been created for an event, the list of matrices will be displayed in a box to the right of the active matrix.  Clicking on any of these matrix hyperlinks will make the selected matrix active in the browser window.  Matrix cells are defaulted grey when created.  Each matrix cell can be set to the appropriate status definition as indicated by the Key in the bottom right corner of the browser window.  The browser window automatically refreshes once per minute.  The auto-refresh option may be turned on or off by clicking the Auto Refresh link in the upper right corner of the matrix window. The matrix may be manually refreshed by clicking on the Refresh link.Each node location is able to update the information applicable to their node location only.  The applicable row is highlighted in yellow on all matrices, and each cell within that row may be updated by the user in accordance with subtest procedures defined by the test integrator.Figure 2.3-3 – DCIT CMU Example 3The test integrator coordinates the subtest by instructing locations to update their status to indicate that they can (or cannot) see entities or data produced by a particular location. Users then click the matrix cell corresponding to the intersection of their highlighted row and the producing location’s column.  Currently, this opens a page within the same browser window showing a list of radio buttons and a text box. Users select the button corresponding to their response:  Affirmative (green), No Response (Gray – default setting), Negative (red) or N/A (black).  Selecting a choice updates the database with the entered status response and updates the matrix to show the updated color of that intersection matrix cell.  (The diagonal cannot be modified, as there is currently no defined requirement mandating that a node location needs to verify that it can “see” the same entities or data it generates).  The text box allows the user to provide information related to the status response for a particular cell.  In the matrix view, all users can mouse over a particular matrix cell and view the notes for that cell.  A matrix cell containing user notes is identified by a small white square in the upper right corner of the cell.Figure 2.3-3 is an example of the view notes capability.  A mouse-over of the EPG-FHu/ATC cell displays the notes entered for that particular check.Figure 2.3-4 illustrates the matrix cell status control window, brought up when the user clicks a matrix cell.  If a user attempts to click on a cell that is outside of their available row (i.e., clicks on another location’s cell), there is no action; this prevents a location from accidentally changing another location’s information.  In addition, when the mouse pointer is over an intersection box the description (Producer -> Viewer -> Status) of that matrix cell appears, along with any notes the user may have entered.Figure 2.3-4 – DCIT CMU Example 4The Event Administrator may preset or reset individual rows and columns.  By clicking on any node location label, the Event Administrator can preset or reset that row or column to gray (default), green (Affirmative), red (Negative), or black (N/A).  This is handled by a script which prompts the Event Administrator to enter the desired action in a text field, which is parsed into a command which initiates the updates to the matrix view and the databaseThe Auto-Refresh switch and the Add Notes capabilities of the CMU were not added until shortly after DTE-4.  The next iteration of the CMU will add the ability to generate reports based on matrix notes and other input.Figure 2.3-5 is an example where no site received information from YPG and YPG did not receive information from any other site.  Figure 2.3-5 – DCIT CMU Example 5The pre-test integration phase of distributed testing is a critical phase in which numerous systems and capabilities across multiple test sites are brought together to meet the requirements of distributed test events such as DTE-4.  Any tool or utility that can simplify or streamline this process provides a value-added contribution that lends itself to the overall success of the test event.  3. Test PhaseThe conduct of DTE 4 requires personnel at all ten test centers.  Coordination is conducted via a teleconference.  The exercise is conducted using the Time Ordered Event List (TOEL).  The TOEL is generated from the Test Conduct and Reporting System (TCRS).3.1 Test DirectorThe nature of distributed events dictates that a centralized controlling element be established for distributed operations.  This is evident in the private sector when task forces are set up to handle catastrophic events that require bringing together large numbers of personnel and resources from geographical disparate locations in the greater community.  The task force commander (or director) is responsible for assuring that these disparate personnel and resources are brought together in a seamless fashion and work collectively for the task at hand.  For distributed testing, each participant facility or range is responsible for their own internal operations, but the Event Test Director (or Pit Boss) is responsible for ensuring that all participant facility operations and range operations are synchronized and work together seamlessly during test execution.   The Test Director monitors integration efforts leading up to test execution and monitors status of participant’s facilities.  When necessary, the Test Director makes GO/NO-GO decisions based on pre-established criteria.  The Test Director is also responsible for insuring that events scheduled in the TOEL are conducted in a timely and synchronized fashion. 3.2 Test ControlTest Control is a key facet of any test but it takes on a new meaning in the distributive test world.  Distributed Test Control consists of maintaining a white cell (controlling cell) view of the full operation.  This means having a status monitor function that enables the white cell to gage each participant test centers status.  Visualization tools enable the white cell to monitor and control both the operational test scenario and the technical aspects that go into building and maintaining that test scenario.  Of the utmost importance is the need for verification that the proper data elements are being collected for post test analysis and reporting.  To enable these key facets, a centralized test control cell must be established.  Test Control includes visualization tools for monitoring the progress of the test event as well as test monitoring tools for maintaining test synchronization and timing.  The test tools also provide status on network operations which are crucial to the success of the test event.  Centralized communications between all participant test centers are established and controlled via the white cell as well.  For DTE-4, VTCs were established with most participant test centers that allowed personnel interaction between the white cell and each test center.3.3 Time Ordered Event ListThe TCRS is a requirements-driven system that attains efficiencies through detailed research, analysis, and coordination during test planning; focused and quality assured data collection during test conduct; and predefined reporting formats.  DTE 4 uses the TCRS to decompose tactical missions into tasks related to system requirements, which drives the development of a Task Ordered Event List (TOEL).  The TOEL in turn drives distributed test execution, data collection, and reporting.  The complete TOEL for DTE 4 was over 140 pages with approximately 700 steps.3.3 Test TalkTestTalk is a simple web-based tool that evolved out of the requirement for a common “Test Clock” and a need to share the operator oriented view of the progress of a distributed exercise.  It provides a number of valuable services that assist in the conduct of a large scale distributed simulation or exercise.  The requirements for TestTalk were discovered during the SEIT IOC and DTE exercises: 1.	A common, synchronized notion of current time, both current (real) time and “Test Time”, is needed during the course of a distributed exercise.  2.	Any tool used must be easily accessible to participants without requiring additional software to be loaded on their test/simulation platforms and must operate equally well on the diverse mixture of hardware and operating systems in use during the exercise. 3.	Need the ability to record/annotate what has occurred as the test/exercise progresses, specifically allowing operators who have left the room to know what has happened while they were out. 4.	As the Task Organized Event List (TOEL) continues to grow from a couple of pages to literally hundreds, we needed a mechanism which could show the operator a customized subset of the overall TOEL and indicate to the operator the time priority of upcoming events they needed to perform.  5.	Operators needed to be able to quickly and conveniently confirm that they had performed a given event and annotate any peculiarities with the execution of that event.  6.	The ability to have visibility of who was “on-line” and monitoring the event was required. 7.	A mechanism to distribute the data logged by TestTalk after the event was finished, so that interested individuals could download the logs. 8.	A general mechanism for manual data entry was added.   This allowed the entered data to be automatically time-tagged with both real-time and test-time.Figure 3.3-1:  TestTalk Client ScreenshotThe solution to these requirements was the TestTalk software which evolved over the course of several IOC and DTE events.  Its capabilities include: A common TimeService that allows independent track of both UTC/Zulu Time and Test Time (T-Time, i.e. T+03:34 or T-05:00 for example).  Administrators can Start, Pause and Reset T-Time to any given T-Zero date/time they want. Figure 3.3-2:  TestTalk Time DisplayA custom Web-Service that does not rely upon any commercial web-server that is both light-weight and inherently secure due to its inability to execute external programs (CGI, Scripts, etc).  This web server implements basic HTTP and basic authentication, and manages its own independent list of users with assigned “Roles”. A custom Server-Side-Include mechanism which allows HTML pages to include custom HTML tags <SSI/> that are replaced with dynamic HTML content generate by the TestTalk content server. An Event Service which can display and edit time-ordered events logs.  This service can be used to create dynamic XML Logs associated with HTML “templates” providing users of TestTalk with the ability to create and manage any number of custom “databases” of tabular data. The Event service was used to create several very useful tools:  The MessageLog which is basically a time-tagged “chat-room” which allowed participants during the course of the exercise to post messages for everyone to see… these messages in turned formed a very important time-tagged log of events that occurred during the course of the exercise.Figure 3.3-3:  TestTalk Message logThe EventLog which provided each user with a list of their current and upcoming events, ordered by time and color-coded with the time-criticality of the upcoming event allowing operators at a glance to identify how soon an event was going to occur based upon its color.Figure 3.3-4:  TestTalk Event LogAn On-Line Users list which allowed participants (and specifically the Test Controller) to know who was on-line and monitoring the progress of the test. Figure 3.3-5:  TestTalk On-line Users ListBasic Web Services that allow HTML and other documents to be posted to the web-root for access by members such as the daily message logs in both XML and ASCII form.  Additional pages of information such as links to members web-sites and to other web-based tools within the exercise can also be posted as simple HTML pages.  TestTalk was architected to operate both stand-alone or as an embeddable .NET control which can be dropped into any .NET application to provide any or all of the benefits of TestTalk and additionally allow that application to host/provide its own dynamic content through the SSI mechanism.  4. Lessons Learned (All)An after action review meeting was held to review the data collected for DTE 4 and to review the conduct of the test.  This included developing a set of lessons learned.  Two of the lessons related to tools.For future events a recommendation was made to use a broader methodology for conducting tool evaluations and assessments, including:  1) Issue Tracking – collecting data or problems reports during pre-test spirals and other activities, for the purpose of analysis, troubleshooting, and problem resolution; 2) Requirements or Baseline Testing – develop process for introducing new tools into the distributed test environment, and schedule activities designed to prove out a tool’s ability to meet its defined requirements; 3) Comparison Evaluation – schedule activities designed to compare a tool against a pre-defined baseline or comparable competitor; 4) Suggestion “box” capability to capture feedback and/or, enhancement requests.  Based on direct observation and feedback, most tools operators indicated that the learning curve was steep.  While advanced training on tools and software is always desirable, the reality is that such opportunities are going to remain limited without a semi-permanent DTCC staff or group of test operators.  The creation of tutorials, FAQs, and detailed supporting documentation could be helpful to this end.  With the playback features of some of these tools, perhaps the DTE-4 scenarios can be saved and used as training devices for operators during down times between distributed tests or spirals.Additional lessons learned where documented relating to the process used to conduct the distributed test events.The logistics of planning, scheduling and coordinating all the elements required across multiple test centers for the purpose of conducting distributed testing is highly complex and time consuming.  Much of the coordination took place via teleconferences and email communications.  While always necessary, there are limitations to these channels, limitations that could perhaps be reduced by re-examining the process and exploring opportunities for process-related tools that can assist in and simply the logistics of preparing for and conducting a distributed test. An effort has been initiated to revisit the DTCC Concept of Operations from a business-enterprise or process perspective for the purpose of identifying potential tools or tools-needs that would benefit the process of performing a distributed test across multiple test centers or locations.  In additionally  a meeting was held with the DTE-4 test integrator to discuss recommendations for additional tools or capabilities that could be added to the DCIT tool ahead of the next DTE test event.  VPG will pursue two avenues:  short term features/capabilities that support the upcoming DTE-5 planning process and spiral preparation; and a broader look at the whole of the ATEC distributed test capability to identify, procure and/or develop tools that simply the business process of distributed testing.5. SummaryThe processes and tools used for execution made DTE 4 successful.  These tools and processes will be improved and used in future VPG DTE test events.  Even with tools, a distributed test event requires substantial coordination among participants.  It is particularly important for coordination between the Test Integrator and Test Director.Planning is currently underway for DTE 5.  Both TestTalk and DCIT will be used to support the Test Director and the Test Integrator.  Both tools have been modified to add new capabilities.6. References[1]	Michael J. O'Connor, Tim Clardy: “SEIT: A Mixed Network Architecture to Support Distributed Testing” 04S-SIW-071 Simulation Interoperability Workshop, Spring 2004.[2]	Ralph Liebert, Tim Clardy, Hal Meyer, Daniel Bissel, David J. Ribail, “Synthetic Environments Integrated Test Bed (SEIT): Building a Simulation-based Distributed Test Capability” 04S-SIW-082 Simulation Interoperability Workshop, Spring 2004.Author BiographiesMichael J. O’Connor is a Principal Simulation Architect with ITT Industries Advanced Engineering & Sciences Division.  He received a bachelor of computer engineering from Auburn University in 1987 and a masters in computer science from the University of Alabama in Huntsville in 1991.  Mr. O’Connor has been involved in the distributed simulation community for over ten years.  In that time he as served as the Editor of the Real-time Platform Reference Federation Object Model (RPR FOM), Chair of the SISO Standards Activity Committee, and currently serves as the SISO Executive Committee Vice Chair.  Mr. O’Connor is currently applying the standards developed by SISO in his role as simulation integration lead for the US Army Test and Evaluation Command’s Virtual Proving Ground Distributed Test Event initiative.David J. Ribail is the chief of the Future Force Office at White Sands Missile Range.  He received a bachelor of science degree in Electrical Engineering from New Mexico State University in 1984 and has done coursework for a masters degree in Engineering Management at University of Southern California. Mr. Ribail has been involved in the distributed test and training community for over ten years.  During that time he has been instrumental in developing concepts for distributed testing, specifically in the Joint community.  He was instrumental in developing the range linkages for test and training ranges participating in Millennium Challenge 02.  He also developed concepts for the ATEC Test Integration Network, as well as the Inter Range Control Center at White Sands.  Recently he has been named as the lead for the Future Combat Systems Range Support Team. JIM CHASE is an electrical engineer and project manager with Amtec Corporation.  He received a Bachelor’s of Electrical Engineering from Auburn University in 1994.  Mr. Chase began his career as an embedded software engineer for telecommunications equipment providers Motorola and Verilink, providing leadership to engineering teams and the implementation of SEI/CMM, ISO 9000 and TL-9000 quality management systems (QMS).  Currently, Mr. Chase serves as the Assistant Project Manager (APM) for the Distributed Operations Integrated Tools (DOIT) task sponsored by the Virtual Proving Ground’s (VPG) Tools Focus Group, which supports tools development/acquisition and process development for the Developmental Test Command’s distributed test capability.