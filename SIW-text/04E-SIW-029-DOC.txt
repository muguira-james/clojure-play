A Formal Approach to Modeling Teamwork Utilizing Context Based ReasoningGilbert C. BarrettAvelino GonzalezIntelligent Systems LaboratoryUniversity of Central Florida4000 Central Florida Blvd.Orlando, FL USA 32816407-823-5027 HYPERLINK "mailto:gilbarrett@isl.ucf.edu" gilbarrett@isl.ucf.edu HYPERLINK "mailto:Gonzalez@mail.ucf.edu" gonzalez@mail.ucf.edu Keywords:Collaborative Behavior, Teamwork, Context Based ReasoningABSTRACT:  The research presented focuses on expanding Context-based Reasoning (CxBR) to facilitate modeling collaborative behaviors and employing Joint Intentions Theory (JIT) in CxBR.  Collaborative behaviors are a fundamental necessity of teamwork.  To allow for high-fidelity models of teamwork to be effectively and efficiently built using CxBR, there is a need to formalize and extend within CxBR the concepts that are consistent with the existing theories on modeling collaborative behaviors.  There is also a need for these formalized concepts to be incorporated into the legacy CxBR Framework.  At a minimum, the extension of the theories on collaborative behavior needs to encompass the contextual modeling paradigm.  Currently, commonly accepted theories focus on Belief-Desire-Intention (BDI) models.   While CxBR includes the functionality of BDI models, the CxBR paradigm offers a different perspective towards modeling than does BDI.  Therefore, the first task of this investigation was to formalize the connection between CxBR and existing theories on collaborative behaviors.  Once the theory of collaborative behaviors was extended to CxBR, the focus of this research was to extend CxBR to allow for effective and efficient models of highly-structured tactical-teams whose members act collaboratively.  This paper discusses the ongoing research involved with the advantages and disadvantages of adding a TeamConstruct Class to the CxBR paradigm.  It also discusses the relationship between CxBR and BDI models, with specific emphasis on Joint Intentions Theory.  Most importantly, a formalism for modeling teamwork using CxBR is presented and justified.1. IntroductionIn order to effectively model collaborative agents utilizing Context Based Reasoning (CxBR), it is first necessary to formalize modeling collaborative behaviors for the paradigm.  The formalization discussed in this paper relates CxBR to the more popular Belief-Desire-Intention (BDI) [1] models.  This relation serves two purposes.  First, developers unfamiliar with CxBR are provided a familiar basis of reference.  Moreover, the more significant reason is that the current theories on collaboration [2], [3], [4], [5], and [6] are primarily concerned with reasoning as accepted from a BDI perspective. The formalization of collaborative behaviors within CxBR discussed in this paper is founded on the most widely accepted of these collaborative theories, Joint Intentions Theory (JIT) [2], [4], [5], and [6]. This paper is organized as follows:  Section two provides a background of CxBR, BDI, and JIT.  Section three formalizes the relationship between CxBR and BDI.  Section 4 formalizes the relationship between CxBR and JIT thus defining teamwork within CxBR.  Section 5 discusses modeling teamwork in CxBR.  Finally, a summary is provided as section 6.2. BackgroundThe formalization of modeling teamwork in Context Based Reasoning (CxBR) is founded on the Joint Intentions Theory (JIT) of Cohen and Levesque [2].  Much of the definitions and theorems involved with JIT deal with terms and definitions common to Belief Desire Intention (BDI) models.  To better understand the formalizations discussed in Sections 3 and 4 of this paper, an overview of JIT, BDI, and CxBR is provided in this section.2.1 Belief Desire Intention ModelPerhaps the most common paradigm for modeling intelligent agents is the belief-desire-intention model  [1].  Georgeff loosely defines BDI in AI terms as follows:Beliefs - represent knowledge the agent possesses of the world.Desires - correspond to goals of the agent.Intentions - are plans to which an agent is committed.These definitions are critical in defining terms related to collaboration and teamwork.  2.2 Joint Intentions TheoryThe following definitions provide the foundation upon which Joint Intentions Theory [2] is built.  The numbering is added to facilitate later discussions in this paper.Definition 1	An agent has a persistent goal relative to q to achieve p iff:The agent believes that p is currently falseThe agent wants p to be true eventuallyIt is true (and the agent knows it) that (2) will continue to hold until the agent comes to believe either that p is true, or that it will never be true, or that q is false.Definition 2	An agent intends relative to some condition to do an action just in case the agent has a persistent goal (relative to that condition) of having done the action and, moreover, having done it, believing throughout that the agent is doing it.Definition 3	An agent has a weak achievement goal relative to q and with respect to a team to bring about p if either of these conditions holds:The agent has a normal achievement goal to bring about p, that is, the agent does not yet believe that p is true and has p eventually being true as a goal.The agent believes that p is true, will never be true, or is irrelevant (thus q is false), but has a goal that the status of p be mutually believed by all the team members.Definition 4	A team of agents have a joint persistent goal relative to q to achieve p just in caseThey mutually believe that p is currently falseThey mutually know they all want p to eventually be trueIt is true (and mutually known) that until they come to mutually believe either that p is true, that p will never be true, or that q is false, they will continue to mutually believe that they each have p as a weak achievement goal relative to q and with respect to the team.Definition 5	A team of agents jointly intends, relative to some escape condition, to do an action iff the members have a joint persistent goal relative to that condition of their having done the action and, moreover, having done it, mutually believing throughout that they were doing it.2.3 Context Based ReasoningStensrud succinctly summarizes CxBR in [7] as follows:  CxBR is a reasoning paradigm by which an autonomous agent can be modeled to execute a specifically defined task in either a simulated or real-world environment.  The task assigned to the agent is encapsulated within a CxBR mission.  This mission provides for the agent both a set of goals, which represent the criterion for completing the task, and a set of constraints specific to that task.  Also present within a mission is a list of contexts that serve to partition the agent’s task-related knowledge by the situations under which it applies.A context represents a situation, based on environmental conditions and agent stimuli, which induces a certain agent behavior specific to that mission.  When an agent is executing a mission within CxBR, its behavior is primarily controlled by the current applicable context, (known as the active context) a determination made by context-transition logic.  At each time step, this transition logic examines the current stimuli on the agent and makes a determination of the active context for the subsequent time step.  This logic is often in the form of sentinel rules that contain the conditions for a specific context-to-context transition; however the transition logic is not required to be rule-based.The assignment of a certain CxBR behavior onto an autonomous agent (Autonomous Intelligent Platform, AIP) is done through the creation of a CxBR model.  Encoded within a CxBR model is the mission assigned to the agent, a set of contexts applicable to that mission, and transition logic that defines the conditions under which each context is pertinent.  Also present within the model is an interface connecting it to the physical or simulated agent responsible for executing the behavior.  Because of this, the interface used by the CxBR model is representative of the agent it connects to.  Low-level functions (such as moving, scanning, etc) are all encoded within this interface, as is the data representing the agent’s current state (position, velocity, eye angle, etc).  When the model determines a course of action, therefore, it does so in terms of the functions and representations within this interface.A model is executed by assigning a mission to the agent interface.  The first context listed by the mission is denoted as the default context, and serves as both the initial context and the context used if no situational determination can be made by the transition logic of the model.  When executed, the initial context is either selected by the transition logic or assigned by the mission, and the behavior defined by the model begins.  The model executes the logic encapsulated within the current active context, consults the transition logic for the appropriate next context (whether or not a transition is necessary), and repeats until the goal criteria are reached. [7]3. BDI and CxBR FormalismsFor the purpose of discussing collaborative behaviors and teamwork the most noteworthy aspect relating CxBR to BDI is that CxBR fully encompasses the BDI structure in the follow way.Beliefs - represent knowledge the agent possesses of the world.  CxBR provides a paradigm for representing an agent’s knowledge into Missions, Contexts, and Sub-Contexts.  However, an agent’s environmental knowledge is largely independent of the CxBR structure.  Environmental knowledge could be stored in any suitable data structure.  It is also important to note that although CxBR provides for tactical knowledge representation in terms of Missions, Contexts, and Sub-Contexts – the exact AI representation of this knowledge is not restricted by CxBR.  For example, the knowledge required for transitioning to any given Context could be captured in a rule based system or could be contained in a neural network.  CxBR is only concerned with the existence of the knowledge not the specific AI implementation.Desires - correspond to goals of the agent.  A CxBR Mission includes the highest level goal for an agent.  Sub-goals, primarily concerned with accomplishing the Mission’s goal, are either implicitly or explicitly contained within Contexts and Sub-Contexts.Intentions - are plans to which an agent is committed.  A CxBR plan consists of a sequence of Contexts.  During a scenario the plan is composed of the series of sequential Active Contexts.  An agent’s past Active Contexts compose the plan that was executed.  Likewise, an agent’s current Active Context determines the present plan.  Thus, an agent’s intentions are determined by the agent’s Active Contexts.  The agent’s currently Active Context determines the agent’s current intentions towards carrying out a plan.Corollary 1:  High level desires and goals are captured in CxBR Missions.  Lower relative sub-goals and desires are captured in the Missions associated Contexts and Sub-Contexts.To allow for more specific definitions and theorems of collaboration in CxBR, the formalism for transition logic must be expanded.  “In order for a Context to be Active certain prerequisites must be met (with the exception of choosing a default Active Context).  The prerequisites for choosing an Active Context are specified as the Transition Criteria and captured as part of the Transition Logic.” [7] Transition Logic should be considered as a set of criterion.  From this set, it is possible that a criterion element is sufficient for causing a Context transition, yet this element may not be strictly required.  In other words, the criterion will cause a Context transition but its absence does not prohibit the transition.  This element belongs to a disjunctive set of transition criteria.  In contrast, there may be certain criteria that are necessary for Context transition.  In other words, in order for the Context transition to occur certain criterion are absolutely required.  These elements belong to a conjunctive set, which will be termed Transition Requirements.  Hence, Context c with Transition Requirement q can be Active iff q is believed true.Corollary 2:  Given fact q is a transition requirement of Context c, if c is the Active Context then q is believed true.Corollary 3:  Given Context c is part of the context set associated with Mission m and m contains goal g, if c is the Active Context for an agent with Mission m, then the agent has Mission goal g and any goals (Mission sub-goals) of c. 4. Collaboration in CxBRA major obstacle for agents seeking collaboration is in verifying the intentions and beliefs of prospective collaborators.  It is expected that the complexity of this verification can be reduced through communicating in Contexts.  For a correctly specified CxBR model, in order for a Context to be Active there are certain conditions that must be true and other conditions that are implied.  Knowledge of these conditions provides insight as to an agents beliefs, desires, and intentions.  For simulated agents, each team member could be provided with knowledge of the other team members’ Context and Mission specification.  This knowledge includes transition requirements and goals of the Contexts and Mission.  Therefore, once a prospective collaborating agent’s Active Context is recognized or communicated the recipient can infer the prospects beliefs, desires and intentions.  This inference can then be used to help determine whether or not collaboration exists between the agents.Theorem 1:  A CxBR agent has a persistent goal (individual commitment) relative to q to achieve p iff:  The Active Context c has transition requirement q and goal p or if the Active Context c has transition requirement q and the Mission m has goal p.This is justified from the JIT Definition 1 of a persistent goal and CxBR corollaries 2 and 3.Theorem 2:  A CxBR agent intends relative to some condition, consisting at a minimum of some transition criterion, to do an action determined by the agent’s Active Context for the sake of a persistent goal. This theorem is adapted from the JIT Definition 2 for intentions and the fact that actions in CxBR are determined by the agent’s Active Context.Theorem 3:  A CxBR agent has a weak achievement goal relative to q and with respect to a team to bring about p if: The agent’s Active Context is part of Mission m’s related Contexts intended to accomplish p, or the agent has a goal that the status of p be mutually believed by all other teammates, regardless of the current state of p (true, false, or irrelevant).  This is justified by the JIT definition for weak achievement goal and the fact that the sequence of Contexts forms a plan that allows the accomplishment of the Mission goal.Theorem 4: A team of CxBR agents has a joint persistent goal relative to q to achieve p if each agent shares the same team Mission with goal p.	This is justified by the JIT Definition 4 for joint persistent goal and Corollary 1.Theorem 5:  A team of CxBR agents jointly intends to do some action iff they share the same team Mission and believe they are accomplishing their Mission by following a plan of Mission related Contexts.Theorem 6:  Theorem 4 and theorem 5 could both be extended to include Context rather than Mission as a matter of scale.This is justified since Mission is essentially a special form of Context.  When considering a Context hierarchy including multiple Missions, the Missions themselves become the Context-set of some higher Mission.Based on JIT definitions and CxBR corollaries, the theorems detailed here provide a foundation for modeling teams in CxBR.  The next section will outline the building of such models.5. Modeling Teams with CxBRThe assignment of Missions and their associated Contexts through the CxBR paradigm provides a feasible method of modeling abstract team entities as well as the team members themselves.  Consider a real world analogy.  A business might have an associated mission statement, which is distinct from any missions that the business employees might themselves have.  Thus, a CxBR model of such a small business could represent the business itself as an entity that is composed of some small number of employees.  A slightly larger business might represent the business as being composed of a number of departments, which are in turn composed of a number of employees.  In this case, the business, each department, and the employees could each be represented as a distinct entity including a specific Mission (and associated Contexts) for each entity.5.1 Simple CxBR ModelTo better facilitate understanding hierarchical team modeling in CxBR, first, consider a simple CxBR model.  Such a model is diagramed in Figure 1.  As shown, each agent is assigned a Mission, which includes a specified mission goal.  The Mission itself specifies a set of associated Contexts that the agent will likely encounter while attempting to accomplish the mission goal.  Each Context includes a set of Transition Criteria and Transition Requirements.  These Criteria and Requirements specify the conditions that will cause the associated Context to become Active.  Contexts may have a set of associated Sub-Contexts.  Sub-Contexts are essentially Contexts at a lower level of resolution.  For example, a driver agent may have a Mission of drive-home-from-work.  The Mission may include Contexts such as highway-driving, city-driving, emergency-road-service.  The highway-driving Context could have Sub-Contexts of passing-other-driver, being-passed-by-other-driver, exit-highway.  A Transition Requirement for the exit-highway Sub-Context might be approaching-highway-exit, while a Transition Criterion for the same Sub-Context might be need-to-refuel.  The knowledge base is used to store facts that the agent is aware of, including knowledge, perceptions, beliefs, and inferences about the environment and other agents.Figure 1: Simplified CxBR Model (UML class diagram - diamond denotes composition)Teams not requiring a structured chain-of-command hierarchy can be modeled following the structure outlined in this subsection.  However, teams requiring a structured chain-of-command are better modeled by implementing a Team Construct Class, as outlined in the next subsection.Figure 2:  Class Diagram of Team Hierarchy Model (UML class diagram - triangle denotes inheritance, diamond denotes composition)5.2 Modeling Team HierarchiesThe complexity of representing complex team structures is simplified by adding a layer of abstraction to the simplified model discussed in the previous subsection.  Teams and team members alike can be represented as intelligent entities.  This allows for clear representation of teams’ missions, goals, and objectives in addition to individual team members’ missions, goals, and objectives.  The addition of a Team Construct Class, as illustrated in Figure 2, allows for nesting of team hierarchies.  An example realization of such a nesting, a platoon of tanks, is illustrated in Figure 3.  As shown in Figure 3, a tank platoon is composed of two sections, each of which is composed of two tanks.  The platoon itself represents a team of sections, yet each section is a team of two tanks.  At such a small scale such distinctions might seem trivial.  However, the hierarchy provides a means of scalability for larger formations, such as companies, divisions, battalions, brigades, etc.  Although there are undoubtedly limits to scalability, these limits are not due to representation.Figure 3: Sample Model of Tank Platoon (UML class diagram - triangle denotes inheritance, diamond denotes composition)Modeling abstract entities such as a team, platoon, business, etc., which represent group formations, present certain challenges compared to modeling concrete entities such as a team member, tank, or sales manager.  For example, the concept of applying a mission to an abstract entity such as a business or team differs in some respects to applying a mission to an agent such as a salesman or a ball player.  These team formations present emergent properties making them greater than the sum of their individual members, which is of course a result of collaboration.  “Collaborative behavior – coordinated activity in which the participants work jointly with each other to satisfy a shared goal – is more than the sum of individual acts [8] and [9] and may be distinguished from both interaction and simple coordination in terms of the commitments agents make to each other [10], [11], and [12].” [3] Thus, the mission of a team is also more than the combination of individual team member missions.  Worthy of consideration though is the method for modeling both a team and its individual team members, to allow for simultaneous representation of each within the same model.  At the lowest level of representation for entities, the bottom layer of the team hierarchy, the agents’ physical behaviors should be modeled, while the highest layer provides for representation of abstract team formations – as intelligent entities.Consider the tank platoon illustrated in Figure 3. As illustrated the lowest level of this team hierarchy represents the tanks.  The second level of the hierarchy represents the tanks sections.  And, the top level of the hierarchy is the tank platoon.  Characteristics of a tank section should emerge from the behavior of the tanks within the section.  Likewise, the characteristics of the platoon emerge from the sections behaviors, which emerge from the tanks behavior.  In this regard, the team construct is an abstraction that acts as a container who’s behave represents the emergent behavior of the individual team members.In contrast to the above example, consider a model that does not require as fine of a resolution.  The specification of the model may be concerned with the behavior of a tank platoon but may not require/desire modeling to the level of individual tanks.  In this case, the tank platoon is modeled as a single entity versus a team of sections composed of tanks.  6. SummaryThis paper presents two major concepts regarding modeling collaborative behaviors in Context Based Reasoning (CxBR): formalizing teamwork in CxBR and teamwork representation using CxBR.  First, to formalize teamwork in CxBR it is necessary to define collaboration in CxBR.  This definition is based on the widely accepted Joint Intentions Theory (JIT).  Formalisms are presented that explain how Belief-Desire-Intention (BDI) models are encompassed within the CxBR paradigm.  From a base of corollaries on CxBR, theorems are then developed to justify CxBR model behaviors in terms of JIT definitions.  These corollaries and theorems are used as a means of justifying communicating agents’ intent through the use of Contexts.  It is shown that by knowing which of an agent’s Contexts is the currently Active Context, and having some knowledge of that Context’s specification, much can be inferred about the agents intent.  From this, agents are able to make decisions regarding collaboration of other agents.  Following the formalism of teamwork within CxBR, a few simple CxBR models of teams are outlined.7. References[1]	Georgeff, M. Pell B., Pollack M., Tambe M., and Wooldridge M. The belief-desire-intention model of agency. In Proceedings of Agents, Theories, Architectures and Languages (ATAL), 1999.[2]	Cohen, P. R., Levesque, H. J., 1991 “Teamwork.” Nous, 35 1991[3]	Grosz, B. and Kraus, S. 1999. The evolution of SharedPlans. In Rao, A. and Woolridge, M., editors, Foundations and Theories of Rational Agency, pages 227--262. Kluwer.[4]	Tambe, M., 1997, “Towards Flexible Teamwork.”  Journal of Artificial Intelligence Research 7, pp. 83-124.[5]	Jennings, N. R., 1993, “Specification and Implementation of a Belief-Desire-Joint-Intention Architecture for Collaborative Problem Solving.”  International Journal of Intelligent and Cooperative Information Systems, 1993[6]	Jennings, N. R., 1995, “Controlling Cooperative Problem Solving in Industrial Multi-Agent Systems using Joint Intentions.”  Journal of Artificial Intelligence, 74(2), 1995.[7]	Stensrud, B., Barrett, G., Trinh, V., and Gonzalez, A., “Context Based Reasoning: A Revised Specification.” In the proceedings of Florida Artificial Intelligence Research Society 2004.[8]	Searle, J. R., 1990.  “Collective Intentions and Actions.”  In Intentions in Communications, chapter 19. The MIT Press. The evolution of SharedPlans. In Rao, A. and Woolridge, M., editors, Foundations and Theories of Rational Agency, pages 227--262. Kluwer.[9]	Grosz, B. and Sidner, C.,  1990, “Plans for Discourse.” In Cohen, P.< Morgan< Press, Cambridge, MA.  In Grosz, B. and Kraus, S. (1999). The evolution of SharedPlans. In Rao, A. and Woolridge, M., editors, Foundations and Theories of Rational Agency, pages 227--262. Kluwer.[10]	Bratman, M. E., 1992,  “Shared Cooperative Activity.”  The Philosophical Review, 101(2):327-341. In Grosz, B. and Kraus, S. (1999). The evolution of SharedPlans. In Rao, A. and Woolridge, M., editors, Foundations and Theories of Rational Agency, pages 227--262. Kluwer.[11]	Grosz, B. and Kraus, S. 1996 “Collaborative plans for complex group action.” Artificial Intelligence, 86(2):269:357 In Grosz, B. and Kraus, S. (1999). The evolution of SharedPlans. In Rao, A. and Woolridge, M., editors, Foundations and Theories of Rational Agency, pages 227--262. Kluwer.[12]	Grosz, B., 1996, “Collaborative Systems: 1994 AAAI Presidential Address.”  AI Magazine, 2(17):67-85.  In Grosz, B. and Kraus, S. (1999). The evolution of SharedPlans. In Rao, A. and Woolridge, M., editors, Foundations and Theories of Rational Agency, pages 227--262. Kluwer.Author BiographiesGilbert Barrett is a PhD candidate and instructor at the University of Central Florida.  His current research interests include modeling teamwork, affective reasoning, and evolutionary computing/ genetic algorithms.  Gilbert is expected to defend and receive his doctorate in late 2004.Avelino Gonzalez is a Professor of Computer Engineering at the University of Central Florida.  He earned his Ph.D. in Electrical Engineering at the University of Pittsburgh in 1979, M.S. in Electrical Engineering at the University of Miami in 1974 and B.S. in Electrical Engineering at the University of Miami in 1973. Research Interests include: Intelligent systems, human behavior representation, robotics control, automated diagnosis, Context-based reasoning, validation and verification of intelligent systems, automated knowledge acquisition and machine learning.  Avelino currently heads the Intelligent Systems Laboratory at UCF, where he actively pursues research in these areas of interest. 