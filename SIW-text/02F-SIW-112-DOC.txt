CRISIS:  A Framework for Modeling Infrastructure DefenseKatherine L. Morse, Ph.D.SAIC10260 Campus Point DriveSan Diego, CA  92121(858)826-6728 HYPERLINK "mailto:kmorse@epsilonsystems.com" morsek@saic.comDavid L. DrakeSAIC10260 Campus Point DriveSan Diego, CA  92121(619)985-8651 HYPERLINK "mailto:ddrake@san.rr.com" ddrake@san.rr.comKeywords:Vulnerability modeling, security ABSTRACT: The President’s Commission on Critical Infrastructure Protection report summary states “... the Commission recognizes that we are not now able to deploy several capabilities that we need.  We have, therefore, recommended a program of research and development focused on those future capabilities.  Among them are new capabilities for ... improved simulation and modeling capability to understand the effects of interconnected and fully interdependent infrastructures.” Models of several information infrastructure elements already exist.  However, they are not currently integrated, nor is there an analytical model for evaluating their vulnerability relationships.  What is needed are both a framework which implements a model of security incorporating interrelationship, and a physical testbed for applying the framework to evaluating the vulnerability of the national information infrastructure and focusing recommendations for strengthening it.  In the current atmosphere of hightened concern about homeland security, this need is greater than ever.Critical Information Systems and Infrastructure Simulation (CRISIS) is such a framework.  CRISIS is designed to integrate existing infrastructure models within an HLA-compliant framework that is based on a model of security derived for risk assessment.  Unlike commonly used actuarial models, the eight-stage model of security recognizes that security breaches involving intelligent adversaries are chains of events which can be broken at several points with countermeasures, reducing final losses.Two types of analysis can be performed with CRISIS.  Experiments can simulate attack initiation at points in the infrastructure and analyze ultimate harm results across the entire infrastructure, indicating systems with the highest vulnerability.  With these results, additional experiments can be performed which evaluate the levels of countermeasure effectiveness required to reduce resulting harm to acceptable levels.  These results can drive trade-off analyses between cost of loss and cost of countermeasures effective enough to prevent it.IntroductionThe President’s Commission on Critical Infrastructure Protection [5] report summary states “... the Commission recognizes that we are not now able to deploy several capabilities that we need.  We have, therefore, recommended a program of research and development focused on those future capabilities.  Among them are new capabilities for ... improved simulation and modeling capability to understand the effects of interconnected and fully interdependent infrastructures.” Models of several information infrastructure elements already exist [3].  However, they are not currently integrated, nor is there an analytical model for evaluating their vulnerability relationships.  What is needed are both a framework which implements a security model incorporating interrelationship, and a physical  testbed for applying the framework to evaluating the vulnerability of the national information infrastructure and focusing recommendations for strengthening it.The Eight-Stage Security ModelWhile the High Level Architecture (HLA) [6] provides a physical framework for integrating infrastructure element models, it does not provide the semantic relationship between the models.  The semantic integration requires a valid security model. This eight-stage security model [1] specifies mathematical relationships between events in the chain of events initiated by an attack and resulting in harm.Relationships between connected and interdependent infrastructure elements can be modeled with connected attack chains.  For example, a successful attack on the power grid triggers a loss-of-availability attack on computers.  This scenario is modeled as the resultant harm of the power grid attack chain triggering the initiation of loss-of-availability attack chain.  A simple interdependence model can be built by connecting attack chains using simple statistical measures for countermeasure effectiveness.  More powerful and accurate interdependence models can be built by integrating infrastructure element models.  These models will replace the simple statistical countermeasures.In  REF _Ref414760562 \h  \* MERGEFORMAT Figure 21, time flows from left to right.  The internal influences are depicted as squares.  The external influence (a security-related attack) to the system is depicted as a triangle.  The consequences are depicted as circles. The objective of the security system is to prevent unwanted consequences of the security attack by employing the activities represented in the squares. The consequences, represented by circles, will occur if these activities are insufficient. One of the major principles of the model is that a system under attack has three opportunities to reduce the resultant harm: before the attack occurs, after the attack occurs but before a security breach occurs, and after a security breach occurs but before the resultant harm occurs.Figure  STYLEREF 1 \s 2 SEQ Figure \* ARABIC \s 1 1.  8-Stage Security ModelWhen performing an assessment, we assess more than the physical system itself.  We include both the automated security mechanisms of the physical system and the procedural requirements levied on the users and administrators.  We refer to these collectively without ambiguity as the “system.”  The eight-stage model is used to evaluate this system.Performing an assessment using the eight-stage methodology involves three major steps:data gatheringconstruction of eight-stage chains of security-relevant eventsperforming the quantitative analysis.These steps are described in the subsections below.Gathering DataThe steps to gather the data for the assessment are:Obtain the definition of the security boundary and the interfaces that will be defended by the system, both automatically and procedurally.  The definition should be provided in the security policy.  The boundary may be more extensive than just the security perimeter due the environment being assessed.  There may be security boundaries within the perimeter designed to deal with internal threats.Obtain the list of system assets to be protected, what constitutes a security breach, the associated harm that could befall the assets, and a quantitative loss per asset if it were compromised, modified by an unauthorized agent, or its availability were lost.  This list should also be provided in the security policy.Delineate the attack scenarios that will (and will not) be defended against, and the likelihood of occurrence of each.  When we performed this analysis for firewall assessments [2], we collected a long list of attack scenarios that cover all pertinent insider and outsider attacks.Delineate each of the system's countermeasures that protect it against attack. A determination is made for each countermeasure if it is used to obstruct, detect or recover from an attack, or to detect or recover from a security breach. This distinction is used to support the quantitative assessment of each countermeasure's effectiveness.Constructing the Chains The lists resulting from the data gathering phase are used to construct eight-stage event chains.  One eight-stage chain is constructed for each attack scenario.  In the appropriate stages, all applicable countermeasures, breaches, and are listed.For each of the attack scenarios, the system's ability to defend against it is calculated based on the quantitative measures collected in section  REF _Ref422993724 \r 2.1. There are eight data points that are collected during the data gathering phase for the eight-stage event chains: the effectiveness of the attack obstruction (CEAO), the likelihood of an attack within the time period of interest (PRA), the effectiveness of the attack detection (CEAD), the effectiveness of the attack recovery (CEAR), the loss in dollars if a security breach occurs (PLB), the effectiveness of the breach detection (CEBD), the effectiveness of the breach recovery (CEBR), and the total value in dollars of the assets at risk in the attack scenario (PLH).  The likelihood of an attack is stated as the average number of attacks that will occur within a the time period of interest.  This is a departure from our 1994 paper [1] where we had assumed that the number of attacks would be less one per year.  Anyone who reads the newspaper knows that this is no longer the case.  Given this, time period of interest should be selected to be short enough that the probability of an attack isn’t one.  All effectiveness measures are stated as probabilities ranging from 0.0 to 1.0.  In our years of using this methodology, we found that the actual loss in dollars related to the security breach, PLB, was always so low that it was not worth tracking.  For example, if a hacker were breaking into a system, most security policies state that a security breach has occurred as soon as the hacker, as an unauthorized user, has somehow logged into the system.  But at that point, no real dollar loss has occurred.  There are exceptions, but to simplify the equations in this paper, we will assume that PLB is always zero.The likelihood that an attack will happen in the time period of interest and successfully result in a security breach (ERB) is ERB = PRA ( (1 - CEAO ) ( (1 - CEAD ( CEAR)The likelihood that an attack will happen and successfully result in a harm (ERH) in the selected time period isERH = ERB ( (1 - CEBD ( CEBR)  The potential dollar loss per time period (ELT) for the one attack scenario being analyzed isELT = PLH ( ERTIt is important to keep in mind that the effectiveness measure for the attack obstruction (CEAO) is the combined effectiveness of all of the mechanisms being used for attack obstruction against the one attack scenario being analyzed.  Additional analysis may need to be performed to determine how all of these attack obstruction mechanisms interplay.  The same is true for the analysis of the effectiveness for all detection and recovery mechanisms.  In cases where mechanisms have different reactions based on situations, it may be necessary to decompose the analysis into more specific attack scenarios, resulting in more eight-stage event chains to analyze.  See our earlier work [1, 2] for the underlying formulation of all calculations.Performing the AnalysisThe quantitative analysis is performed using automated means with the CRISIS Framework.  We describe the framework in the next section and its application the analytical task in section  REF _Ref423773690 \r 4.The CRISIS FrameworkThe primary objective of the CRISIS Framework effort is to build a testbed for integrating infrastructure elements models and performing vulnerability analysis. The final testbed will integrate existing simulations with the 8-Stage Security Model via the HLA. The objectives for this project are:Physically integrate existing infrastructure modelsLogically connect infrastructure models with a valid security modelAnalyze risk relationships between infrastructure elements and identify key risk areasPerform countermeasure tradeoff analysisTwo types of analysis can be performed with the testbed.  Experiments can simulate attack initiation at points in the infrastructure and analyze ultimate harm results across the entire infrastructure, indicating systems with the highest vulnerability.  With these results, additional experiments can be performed which evaluate the levels of countermeasure effectiveness required to reduce resulting harm to acceptable levels.  These results can drive trade-off analyses between cost of loss and cost of countermeasures effective enough to prevent it.The Critical Information Systems and Infrastructure Simulation (CRISIS) framework will abstract out complexities and unused HLA services.  This approach has met with general success as several domain-specific, HLA-compliant middleware frameworks have been built for other applications. Assuming a single, simple execution process model reduces the degree of modification required for existing simulations, and APIs can be tailored to fit the general object model and to mask unneeded HLA services.   REF _Ref481299838 \h Figure 31 provides a conceptual model of the CRISIS architecture.The repository stores models and attack chains which operate on the models.  The user selects attacks from the repository via the controller.  Based on the attack chains selected, the controller pulls the associated models from the repository and launches them.  As the results of the attacks are updated via the RTI, the controller reflects the results and provides appropriate visualization for the user.  The other responsibility of the controller is to search the repository for other attack chains which are triggered by the results of previous attacks and launch other models as appropriate.Figure  STYLEREF 1 \s 3 SEQ Figure \* ARABIC \s 1 1.  CRISIS ArchitectureIn the simple, initial implementation it’s reasonable for the controller to expect to launch models.  For large, complex models which require dedicated platforms which may not move, the controller will have to be able to differentiate and know that it only needs to invoke the model rather than launch it.For the sake of simplicity, we have represented the simulations as encapsulating their models of infrastructure components as well as models of countermeasures and recovery mechanisms.  This may be unrealistic for legacy models.  These additional mechanisms may have to be encapsulated in separate physical federates, in which case the infrastructure models would become composable component federates [4].Attack Ontologies and FOMsIn order to coordinate instantiation of appropriate models, the controller requires the specification of attack ontologies, i.e. meaningful descriptions of the types of attacks and the infrastructure components they impact.  The FOM captures the federation relevant portions of the ontology as:Attack type – enumeratedMagnitude – floatUnits – enumeratedMagnitude and units are specific to the type of attack.  In addition, the attack chains are tagged with triggering relationships.  These relationships specify which attack chains can trigger other attack chains given a threshold of resultant loss output from the first chain.  The FOM captures these thresholds as:Loss – floatUnits – enumeratedIf the result of a particular attack is a loss of a specified threshold, the attack should trigger initiation of other attack chains tagged for the original attack and at least the specified threshold.Performing AnalysisThe thresholded relationships between attack chains are the key to performing vulnerability analysis and countermeasure return on investment (ROI) analysis.  These types of analysis are somewhat analogous to forward chaining and backward chaining in rule-based systems.  Vulnerability analysis is analogous to forward chaining as it allows the analyst to launch an attack and observe the resultant impact on the infrastructure as a whole.  Countermeasure ROI is somewhat analogous to backward chaining.  The analyst is effectively answering the question, “how effective does a countermeasure have to be to reduce the resultant loss to an acceptable level, and is the cost of the countermeasure worth the reduction in loss?”Future WorkWith the preliminary design of CRISIS complete and development underway for the controller, our focus is shifting to the task of making the framework a productive tool for the analyst.  Our next two tasks will be:Identify authoritative simulations for integration.Develop a GUI to support construction and connection of attack chains.References[1]	David L. Drake and Katherine L. Morse, “The Eight Stage Security Specific Risk Assessment Methodology,” Proceedings of the 17th National Computer Security Conference, Baltimore, MD, Oct. 11-14, 1994, pg. 441-450.[2]	David L. Drake and Katherine L. Morse,  “Applying the Eight Stage Risk Assessment Methodology to Firewalls," Proceedings of the 19th National Information Systems Security Conference, 1996, Baltimore, MD.[3]	Personal communication with Maj. Perry Luzwick, USAF, Pentagon Joint Staff, J6, Information Warfare Division.[4]	Katherine L. Morse, Maximo Lorenzo, William Riggs, and Peter Rizik, “Sensor  Simulation Scalability Using Composable Component Federates,” Proceedings of the Fall 2000 Simulation Interoperability Workshop, September 2000.[5] President’s Commission on Critical Infrastructure Protection,  HYPERLINK "http://amc.citi.net/amc/ci/pccip/ index.htm" http://amc.citi.net/amc/ci/pccip/ index.htm[6]	Defense Modeling and Simulation Office, High Level Architecture for Modeling and Simulation, Version 1.3, December 1997, available at http://www.dmso.mil.Author BiographiesKATHERINE L. MORSE is a chief scientist with Science Applications International Corporation in San Diego, CA.  For the last six years she has been a member of the Defense Modeling and Simulation Office’s (DMSO) technical support team for the High Level Architecture (HLA) in which capacity she was responsible for the design of the HLA Data Distribution Management services and has been a member of the HLA benchmark team.  Her survey work in the area of interest management is broadly known as is her work in dynamic multicast grouping for interest management.  She is vice chair of the IEEE Simulation Interoperability Standards Committee and served as the vice chair of the IEEE 1516 standards working group for the HLA.  Dr. Morse received a B.S. in mathematics (Cum Laude, 1982), B.A. in Russian (Cum Laude, 1983), M.S. in computer science (1986) from the University of Arizona, and M.S. (1996) and Ph.D. (2000) in information and computer science from the University of California, Irvine. Her dissertation topic was multicast grouping algorithms for dynamic Data Distribution Management.  Dr. Morse has worked in industry for over 20 years in the areas of compilers, operating systems, neural networks, simulation, speech recognition, image processing, computer security, and engineering process development.DAVID L. DRAKE is a consulting employee to SAIC and the former Vice President of Software Development at Sequentis, Inc. Mr. Drake where he has been responsible for the design, development, and deployment of Sequentis’ security. Mr. Drake has 23 years as a computer security professional in computer security design, implementation and evaluation at companies including Science Applications International Corporation (SAIC), Computer Sciences Corporation, and the MITRE Corporation. At SAIC, he was a senior computer scientist and a manager for the Commercial Security Products Research and Development Division. While at the MITRE Corporation, he was lead developer for the Practical Verification System, a formal specification and automated verification.  Mr. Drake received a Bachelors degree in mathematics from State University of New York at Buffalo and did graduate studies there in artificial intelligence. Mr. Drake received additional computer science training at Stanford Research Institute in California, Northeastern University, and Wang Institute in Massachusetts. He has published articles and given speeches on security and risk assessment topics and has a patent pending on the process for enterprise-wide intrusion detection. Emphasis added. Emphasis added.