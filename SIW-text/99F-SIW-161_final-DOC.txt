Predictive Contract Methodology and Federation Performance Bernard P. Zeigler Hyup J. ChoJong S. LeeYoung K. ChoHessam S. Sarjoughian AI and Simulation Group Department of Electrical and Computer EngineeringUniversity of Arizona,Tucson, Arizona{zeigler,hyup,jslee,ykcho,hessam}@ece.arizona.edu Keywords:Predictive contracts, message reduction, scalability, fidelity/performance tradeoffABSTRACT: Large distributed simulations such as JSIMS must operate over wide area networks to connect  enormous numbers of entities, of many diverse types, residing on geographically separated hosts.  Predictive Contract mechanisms, such as dead reckoning, work in conjunction with data management interest-based services, to reduce the message traffic between federates required to supply the volume of state updates needed to make such simulations work.   A paper presented at the Spring, 1999 SIW confirmed earlier developed theory,  and  presented  some very promising empirical results obtained for tradeoff between message bandwidth utilization versus error incurred in predictive quantization, a form of predictive filtering.  In this paper, we will broaden the family of predictive contract methods considered. The  scalability of  the various methods will be characterized in terms of required  computation overhead and related to their ability to  reduce bandwidth requirements for distributed simulation. While the DEVS formalism is employed to formally characterize predictive contract mechanisms in a generic manner,  the application of the methods and the results of  performance tests are generally applicable in distributed simulation environments, whether HLA-compliant or not.IntroductionPredictive Contract mechanisms, such as dead reckoning, work in conjunction with data management interest-based services, to reduce the message traffic between federates in distributed simulations. Our primary concern here is with message traffic reduction as is important in logical discrete event simulation.  So we are not addressing the use of local models such as in dead reckoning for overcoming latency in real time simulation.The methods can be represented in the framework of Figure 1.  A message processor intercepts the messages that a federate would otherwise be publishing for other subscribers. Such processing may involve filtering so that only a fraction of the messages are actually sent out. The processing may require modification of the federate itself at both the sender and receiver sides. Furthermore, in some methods, there may be global processor that is required to coordinate the message flow.  We refer to the computation required by the message processing, whether at a federate or at the coordinator, as the local computation overhead incurred by a scheme. EMBED PowerPoint.Slide.8  Figure  SEQ Figure \* ARABIC 1 Framework for Message Reduction SchemesSome message reduction schemes under consideration are characterized in Table 1 in terms of the processing required at the sending, receiving or coordinating site. SchemeSending ProcessorReceiving ProcessorCoordinator Non-predictive QuantizationQuantizer for each published attributePersist subscribed attributenonePredictive QuantizationMultiplexedModify federate modelCollecting, encodingPersist subscribed attributeDecoding,distributionnoneLocal ModelsMaintain own model and reflect to remote replicatesUpdate local models of remote federatesNone Spatial Encounter PredictionReflect location to coordinatorUpdate local models of remote federatesLog federate locations; determine communication possibility; transmit messagesTable  SEQ Table \* ARABIC 1 Message Reduction SchemesTwo major concerns are 1) the error incurred by a scheme caused by the reduced information flow and 2) the scalability of a scheme.  The latter can be visualized in Figure 2 where schemes can be qualitatively assigned to locations in the space spanned by degree of  message traffic reduction and associated local computation (both execution and memory) overhead incurred. The scalability of scheme can be assessed in terms of the growth of the local computation overhead with the federation size as measured, for example, by the number of federates in a federation times the number of entities in each federate.  All other things being equal (such as the error incurred), a scheme in which local computational complexity is constant is preferred to one in which it grows linearly or quadratically with federation size.  Thus schemes that fall in the upper left of the tradeoff space (high message reduction, low overhead) are most preferred.  However,  it  may not be possible  to achieve a desired level of message reduction without incurring high overhead, and therefore potentially non-scalable schemes must also be considered.  To justify the qualitative placement of schemes in Figure 2, consider that non-predictive quantization (see Appendix for a brief review) requires a quantizer for each published attribute of each entity enclosed within a federate. Thus, the overhead grows linearly with number of entities within a federate.  Predictive quantization comes in two forms to be discussed later.  For the same error level, it can produce significantly greater message reduction at lower overhead than non-predictive quantization  ADDIN ENRfu [1].  Spatial encounter prediction employs a model of the spatial environment in which entities interact to reduce the message exchanges to those that are consistent with their current trajectories and detection/reception fields. Described more fully in  ADDIN ENRfu [2], it includes the routing space mechanism of HLA, and requires a global agent which logs entity positions, intercepts their emissions, and forwards them to potential receivers, accounting for the effects of noise and attenuation.  The overhead includes additional federate logging updates and the emissions sent to the coordinator, as well as its processing and retransmission of emissions.  The latter can grow quadratically with  total number of entities since each pair may have to be examined for potential coupling. Use of local models, or endomorphic models ADDIN ENRfu [2], generalizes DIS dead reckoning.  In its basic form, each federate requires a predictive model for each of the others leading to linear growth in local computation. More efficient schemes in which clustered federates are managed by dead reckoning servers have been investigated  ADDIN ENRfu [3]. It should be noted that schemes are not necessarily exclusive of each other. For example, it is possible to use the quantization schemes in combination with spatial encounter prediction for distance-based quantization ADDIN ENRfu [1].Also spatial encounter prediction can be used to limit the number of active local models that need to maintained at a federate to those of federates that are in current communication with it. In general, a mixture of schemes may provide the best solution to the performance demands of a particular application. EMBED PowerPoint.Slide.8  Figure  SEQ Figure \* ARABIC 2  Traffic Reduction/Local Overhead Tradeoff  Space and Hypothesized Locations of Message Reduction SchemesPredictive Quantization  for Large Scale SimulationWe now examine predictive quantization in more detail. Predictive QuantizationDEVS/HLA enables federation developers to choose among many structure-based mappings of model components into federates  ADDIN ENRfu [4]. Since HLA supports execution of more than one federate on a network node, the result is a large space of assignments of components to nodes. Execution time may vary widely among the assignments in such a space and subject to constraints imposed by other factors (such as fixed physical locations of components) optimal or near optimal assignments are of interest.  Figure 3 illustrates a coupled model with three assignments to federates: a) all component in one federate, b) a two/one split, and c) each component assigned to its own federate.  When components within a federate are split into distinct federates, their message exchanges now cross the RTI (e.g, the bold arrows in the figure).  To implement this change, DEVS/HLA allows the developer to appropriately modify the newly exposed couplings. Consider an internal coupling from “fireOut” to “fireIn” in d).  To expose the coupling, an instance of the class HLA port, “fire”, is declared and the existing ports are recoupled to it.  The HLA port “fire” invokes a corresponding interaction class to send the send messages across the network. EMBED PowerPoint.Slide.7  Figure  SEQ Figure \* ARABIC 3  Alternative assignments of components to federatesExecution Time (secs.)  for case                         (Quantum_size (a)b)c)0.015,2374,8566,6840.051,050942 *1,0290.1757845997Table  SEQ Table \* ARABIC 2  Execution times for quantum size/federate  assignment pairs ( * indicates best choice).Table 2 provides execution results for an example model with the structure of Figure 3 in which each component is a predictive-quantization integrator and the composition simulates the Roesler differential equation system. The execution times are for the same length of simulated time, with different quantum sizes for each of the component-to-federate mappings. In line with quantization theory, the execution times  decrease significantly as quantum size increases. The largest decrease of approx. 5 fold occurs with the 5 fold increase of quantum from 0.01 to .0.05. Since the error increase (not shown)  is within tolerance, the best choice of quantum size is 0.05.  With this choice, the best federate assignment is the two/one split, case b). Consider a situation in which different sites each own a component  and  the participants are unwilling or unable to redistribute their models.  This corresponds to the case c) in Table 2.  We note that it happens that this case exhibits the largest reduction in execution time (6.5) afforded by the quantum increase from .01 to .05. This is consistent with the fact that  this case presents the largest external message traffic to the network (some 40,000 messages, not shown) where the quantization-based message filtering is expected to yield the largest effect. Multiplexed Predictive QuantizationAs mentioned, predictive quantization makes it possible to reduce the information sent about a boundary crossing  to one bit – indicating whether the boundary crossed is one above or one below the current threshold.  But, given the overhead bits that must be included in a packet, reducing the payload from 64 bits to 1 may not produce a significant overall reduction. However, when large numbers of entities reside on each federate, it is possible to package their reduced outputs together into a single packet and exploit the message size benefits of predictive quantization. For comparison with the baseline case in Figure 4, we have developed such a multiplexing approach. As illustrated in Figure 5, in this scheme, two bits are employed for each entity (model component in DEVS/HLA), the first indicates whether, at this global  event time, the component is active (crossed a boundary); the second carries the boundary crossing direction (+1/-1).  Encoding involves packing these bit pairs in a fixed order into a single packet. Decoding entails extracting the  two-bit payload for each component and if it is active, sending  the boundary crossing data to the subscribers. (As usual, the subscriber must know the quantum size D, and add or subtract according to the +1/-1 information.) The additional encoding and decoding processes constitute the local computational overhead of this method, which is linear in the number of entities (components) contained in a federate.EMBED PowerPoint.Slide.8Figure  SEQ Figure \* ARABIC 4  Non-Predictive Quantization BaselineProvided a federate has a large enough population of  components,  close to 100 percent efficiency can be achieved for multiplexing relative to non-predictive quantization.  This is shown by considering the  ratio of the  message size needed for multiplexed predictive quantization  to the number of bits needed for  non-predictive quantization with the same number of components. This ratio is: EMBED Equation.3   EMBED Equation.3   	as Ncomp >>  EMBED Equation.3  where SOH  is the number of overhead bits, Sdata the bit size of a single attribute (often, a double sized real number), and Ncomp is the number of components. For example,  the ratio becomes less than 1% when we reflect a double size attribute across the network using the RTI  with SOH =20 bytes, Sdata = 8 bytes (double size), Ncomp = 1000.  EMBED PowerPoint.Slide.8Figure  SEQ Figure \* ARABIC 5 Multiplexed Predictive QuantizationThe multiplexed scheme was implemented and experimentally compared with the baseline for the pursuer/evader federation in Figure 6 (see  ADDIN ENRfu [1] for a description of this test example). EMBED PowerPoint.Slide.8Figure  SEQ Figure \* ARABIC 6 Pursuer Evader ExampleIn the implementation in DEVS/HLA, the boundary crossing data are encoded into bit sequences and sent across the network using HLA interactions.  For convenience, integer sized blocks (4 bytes) were used. Thus, a one-integer (32 bits) payload can accommodate 16 components (2 bits per component), a two-integer payload can accommodate 32 components, and so on. We experimented with payloads ranging from 1 to 10,000 integers, representing from 16 to 160,000 pairs of components, respectively. We ran experiments consisting of 100 DEVS simulator cycles. In the baseline case, this represents transmission of 100* Ncomp messages, each with an 8 byte payload (double real), where Ncomp is the number of components employed in the experiment.  In the comparable multiplexed case every run requires 100 messages (independent of Ncomp), however each message has a payload size which is linear in Ncomp.Figure  SEQ Figure \* ARABIC 7 Multiplexer Pursuer Evader Results REF _Ref439386029  \* MERGEFORMAT Figure 7 shows the results obtained using Unix workstations connected via an Ethernet network.  Clearly, as Ncomp increases, the execution time of the non-predictive quantization baseline increases in an exponential manner (due to saturation of the network). In contrast, the execution time of the multiplexed case is flat until 16000 components and then starts to increase. However, the execution time of the latter is uniformly less than the baseline, indicating an extremely  positive benefit/cost ratio for the additional local computation incurred. It should be noted that all components were active in the experiments – the best case scenario for multiplexing, but the large improvement in execution time observed indicates that a robust advantage for the multiplexer. Let a be the average percent activity (relative number of active components) of a block over a run. Then for Ncomp  components, the multiplexer must work with Ncomp  inputs. However, the effective number of components for the  baseline case is a*Ncom in terms of execution time. The flat nature of the multiplexer indicates that, even though the baseline execution time is reduced, it still exceeds that of the multiplexer for a as small as 0.1%.Conclusions and Further ResearchPredictive Contract mechanisms work in conjunction with data management interest-based services, to reduce the message traffic between federates required to make large scale distributed simulations work. In this paper, we provided a simple framework in which to characterize the scalability of  the various methods in terms of required  local computation overhead and related it to their ability to reduce bandwidth requirements for distributed simulation. Quantization, especially, predictive quantization was shown to provide a scalable mechanism with good message filtering properties. We presented a new method to exploit the ability of predictive-quantization to reduce the message size as well as message traffic. An implementation showed how multiplexing can exploit the message size reduction of predictive-quantization for large numbers of entities, providing high communication reduction with low computation overhead.  Quantization  methods can be combined with schemes such as local models and spatial encounter prediction that potentially provide greater load reduction at the cost of increased local computation.  Approaches to developing the best mix of schemes for a given application context remain for further investigation.AppendixWe review the basic concepts of predictive quantization and non-predictive quantization. See  ADDIN ENRfu [1, 5, 6] for details.In quantization-based filtering, rather than represent a continuous curve by points sampled at regular time intervals, the curve is represented by the crossings of an equal spaced set of boundaries, separated by a quantum size.   The baseline mechanism for quantization, called non-predictive quantization is summarized by:Sender federate generates fixed (or variable) time step outputs.Quantizer demon is applied to sender output.This reduces the  number of messages sent (although not their size).The quantizer incurs some computation at the sender’s federate.The sender’s  model computation is unaffected.A more efficient form of quantization is predictive quantization, summarized byThe sender employs a model to predict  successive boundary crossings.It sends a one-bit message at crossings ( whether the next higher or next lower boundary has been reached.The main advantage over non-predictive quantization  is that both number of messages and their size can be reducedA second advantage, is that if simple predictive models are used,  discrete event prediction can also greatly reduce the sender’s state transition computation execution time and frequency.The model employed for predictive quantization can be very simple. An ordinary differential equation system (ODE) consists of a finite number of integrators connected by instantaneous derivative  functions to each other and to the external interface.  A straightforward mapping of such a network on to a distributed equivalent is such that each derivative function is mapped to a persistent function element and each integrator is mapped to a predictive quantization equivalent while preserving the interconnection topologyReferences ADDIN ENBbu 1.	Zeigler, B.P., et al. Bandwidth Utilization/Fidelity Tradeoffs in Predictive Filtering. in SIW. 1999. Orlando, FL.2.	Hall, S.B. and B.P. Zeigler. Joint Measure: Distributed Simulation Issues In a Mission Effectiveness Analytic Simulator. in SIW. 1999. Orlando, FL.3.	Bassiouni, M.A., et al., Performance and Reliability Analysis of Relevance Filtering for Scalable Distributed Interactive Simulation. ACM Trans. on Model. and Comp. Sim. (TOMACS), 1997. 7(3): p. 293-331.4.	Zeigler, B.P., Mapping Hierarchical Discrete Event Models to Multiprocessors: Concepts, Algorithm, Simulation. J. Par. & Dist. Comp., 1990. 9(3): p. 271-281.5.	Zeigler, B.P., DEVS Theory of Quantization, . 1998, DARPA Contract N6133997K-0007: ECE Dept., UA, Tucson, AZ.6.	Zeigler, B.P. and J.S. Lee. Theory of Quantized Systems: Formal Basis for DEVS/HLA Distributed Simulation Environment. in Enabling Technology for Simulation Science(II), SPIE AeoroSense 98. 1998. Orlando, FL.BERNARD P. ZEIGLER is Professor or Electrical and Computer Engineering at the University of Arizona, Tucson. He has written several foundational books on modeling and simulation theory and methodology. He is currently leading a DARPA sponsored project on DEVS framework for HLA and predictive contracts. He is a Fellow of the IEEE. This work was supported by Advance Simulation Technology Thrust (ASTT)DARPA Contract N6133997K-0007 The spirit of the update concept of HLA would be better served by employing attribute reflections rather than interactions for the multiplexed data. In this initial test of utility however, it was much easier to implement the concept using HLA interactions.  This indicates an efficient implementation of the multiplexer. The initial implementation did not show this advantage. The difference is due to the use of an array in to which each active component  deposits its information directly rather than through coupling.