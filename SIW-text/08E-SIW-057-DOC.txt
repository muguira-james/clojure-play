Evolving the Validation Process Maturity Model (VPMM)S.Y. HarmonZetetixP.O. Box 705, Myrtle Creek, OR  97457(541) 863-4639 HYPERLINK "mailto:harmon@zetetix.com" harmon@zetetix.comSimone YoungbloodJohns Hopkins University Applied Physics Laboratory11100 Johns Hopkins Way, Laurel, MD  20723(240) 228-7958 HYPERLINK "mailto:Simone.Youngblood@jhuapl.edu" Simone.Youngblood@jhuapl.eduKeywords:validation, process maturity, fidelityABSTRACT:  This paper proposes modifications to the validation process maturity model (VPMM) that were derived from the lessons learned from several use cases and community feedback.  Like the original VPMM, this model begins where users demand absolutely no information about the validity of the models they desire to apply.  The first layer of maturity, where the validation process plays any role at all, is supported by subject matter experts (SMEs) opinions, an assessment that depends entirely upon subjective sources of requirements, referent and validity judgments.  The next layer describes a process that primarily improves the specification of the required functional inventory.  The next two layers progressively improve the objectivity and detail of the validation process, primarily by leveraging rigorous descriptions of simulation fidelity.  The final maturity layer posits a validation process that does not currently exist.  The process at this level of maturity applies rigorous mathematical techniques to transform informal user needs statements into formal acceptability criteria then continue to apply formal techniques to prove conceptual model and simulation results validity and, ultimately, to develop the acceptance recommendations.  At this final state of maturity, only the user introduces subjectivity into the validation process, an inescapable error source for simulation validity.1.  IntroductionThe validation process maturity model (VPMM) grew from the ideal of a parallel to the Software Engineering Institute (SEI) Sofware Engineering Capability Maturity Model (SW-CMM) or the System Engineering Capability Maturity Model (SE-CMM) [1].  The original motivation for the VPMM coalesced in 1999 when the International Test Operations Procedure (ITOP) Committee on Verification and Validation (V&V) Procedures tried to develop levels of credibility and asked the U.S. Defense Modeling and Simulation Office (DMSO) for assistance.  The DMSO Verification, Validation and Accreditation (VV&A) Technical Support Team (TST) spent considerable time discussing the notion of simulation credibility and the ways in which VV&A could contribute to the credibility of a simulation.  This team began by examining several definitions for credibility, e.g.,“The quality of being trusted and believed in” [2]“The quality of being convincing or believable” [2]“The believability of a statement, action, or source, and the propensity of the observer to believe that statement” [3]“The quality, capability, or power to elicit belief” [4]“A capacity for belief” [4, 5]“The quality or power of inspiring belief” [5]These definitions all have several implications in common that complicate the objective assignment of any measure of credibility to a simulation or, even, a VV&A product.  Credibility is nearly synonymous with trustworthiness.  Credibility is a property of the information being presented that involves the belief of the observer or recipient of that information.  Therefore, the perception of credibility is inherently subjective and cannot be meaningfully measured.  Further, credibility is only loosely coupled to the process for deriving the presented information.  Therefore, the integrity of that process can only contribute to credibility if the observer understands that process and appreciates its limitations (e.g. mathematical logic).  Finally, in order for the observer to trust the credibility of the process for producing information they must also trust that the people who applied that process did so correctly.These observations about credibility caused the VV&A TST to steer around the notions of simulation credibility and concentrate upon trying to understand the characteristics of a validation process that could increase a simulation user’s confidence in a simulation that was validated through that process.  This thinking eventually led to the first formulation of the VPMM in 2002.  Three technical papers document the progress to advance the VPMM concepts [6-8].  The details in these papers will not be reviewed here.  This paper documents the results of the recent efforts to apply the VPMM in several use cases and evolve it as a result of the feedback produced by those use cases.2.  VPMM Development and Application HistoryThe VPMM concept emerged from a series of discussions on simulation credibility in late 2002 and continued to evolve to the present.  Table 1 summarizes the major steps in the development and application of the VPMM.The VPMM has matured through feedback from the simulation and VV&A technical communities and through its application in various use cases.  The next section in this paper discusses these use cases.  Intensive outreach to the relevant technical communities has been key to the VPMM’s evolution.  The first concepts were exposed to the community in early 2003 followed by presentations in 2004, 2006 and 2007.  Each of these exposures generated community response that shaped the VPMM as it exists today.3.  Use Case ResultsA questionnaire was developed for the use case applications of the VPMM that served as a vehicle to elicit information about the validation processes that different organizations applied.  This questionnaire consisted of thirty-six questions and was validated using a classical survey validation process [9].  This involved a series of structured interviews where questions were posed and responses received.  The interviewer then explored the respondent’s interpretations of the questions and their thinking underlying their answers in subsequent discussion.  This validation process resulted in small changes to the questions and the addition of a short glossary that defined the meanings of terms with possible multiple interpretations.  This validated questionnaire was then applied to the VV&A staff of ten additional simulation programs.  Table 2 presents the results from these use cases of the VPMM.The numbers in the leftmost cells of Table 2 are the maturity levels of each of the components of the validation process.  The rightmost column gives the integrated maturity levels derived from the maturity levels of the individual validation process components.  This integrated maturity level was determined by the lowest maturity level of any of the components of the validation process with some minor exceptions that are discussed below.Table 2 shows the results from eleven use cases of the VPMM.  The questionnaire was validated from the responses to the Trial 1 case and is included in Table 2 for completeness.  JMRM is the Joint Multi-Resolution Model and is a federation that uses JTLS and JCATS as its core models [10].  JCATS is the Joint Conflict and Tactical Simulation and it represents the dynamics of individual soldiers, platforms and weapons [11].  The Fed 1 and 2 simulations were special purpose federations that the Joint Forces Command (JFCOM) employed for training and analysis.  JLVC is the Joint Live, Virtual, Constructive Architectural Environment that supplies the training architecture and infrastructure for joint service, multinational, agency and combatant command training and mission rehearsals [12].  These use cases were executed in a cooperative research program between the Defense Modeling and Simulation Office (DMSO) and the Joint War Fighting Center (JWFC) at JFCOM.The remaining use cases in Table 2 were simulations that were not associated with JFCOM and were performed in cooperation with the individual programs.  STORM is the Synthetic Theater Operations Research Model and it is a simulation of campaign-level military operations used to analyze air and space power [13].  JEM is the Joint Effects Model and it represents the time-varying characteristics of plumes from the releases of chemical, biological, radiological and nuclear agents [14].  JAnT is the JOEF Analytical Toolset.  It is a confederation of simulations that support the Joint Operational Effects Federation (JOEF) and that represent chemical/biological environment effects on personnel, equipment and operations [15].  SERPENT is the Simulation Environment and Response Program Execution Nesting Tool and it is a confederation of simulations that represent elements of the effects of attacks upon chemical and biological targets [16].  The VPMM was also applied in formulating the content of the proposed IEEE Standard for the VV&A Overlay to the FEDEP [17].4.  Lessons LearnedThe community feedback, shown in Table 1, and use cases results, shown in Table 2, provided invaluable lessons about the structure, function and application of the VPMM.  These lessons are summarized below.Despite performing these evaluations in an interview setting (i.e., where the meaning of each term could be carefully explained to the respondent), several important terms seemed to be consistently misunderstood even within the VV&A community (e.g., validation criteria, referent, conceptual model and subject matter expert).  These misunderstandings persisted in some cases despite providing a glossary that explained how each term was used in the VPMM questionnaire and an introductory briefing that discussed the lexicon used in the VPMM.  This lesson emphasizes the importance of clarifying terms and their definitions and using definitions that are consistent with common usage in the VV&A community.The version of the VPMM that was applied in the use cases does not handle evaluation of efforts that do not use the information from a simulation conceptual model.  In many of the use cases, no or a poor conceptual model was available.  However, the VV&A teams for these simulations performed well despite this deficiency.  It did not seem fair to penalize the maturity of their efforts because of something over which they had no control since the developer should provide the conceptual model.  As a result, if no conceptual model was available, the contribution of conceptual model validation to the integrated maturity score was ignored.  Some have criticized this strategy [18] and, perhaps, justifiably so.  The contributions of conceptual model validation to overall validation process maturity need to be carefully reconsidered and the VPMM modified to reflect the outcome of this consideration.Most VV&A practitioners seem to feel that verification improves the validation process but they did not respond affirmatively to the questions that elicited their application of verification information to their validation processes.  They did not verify any development products against a validated conceptual model.  This step would link verification to the acceptability criteria, at least at the level of abstraction in the conceptual model, and provide further evidence to support the simulation’s validity.  They did not use verification products to guide output sampling and, thus, improve the efficiency, coverage and confidence of the results validation.  They did not use the verification products to contribute to confidence estimation in the validation results.  These appear to be all of the contributions that verification could make to validation.  Clearly, the contributions of verification to validation need to be reconsidered to better understand the role that they play in the validation process.Some simulation programs built sophisticated referents but appeared not to use them to estimate the simulation’s error.  It was not clear why the program should make the considerable investment in referent construction and not use the products from that investment.  This will require further exploration of the role of referents in simulations and their validation.Information from conceptual model validation and verification can improve the results of face validation but this VPMM version does not resolve that improvement.  This may require modification of the VPMM to better gauge the relationships between conceptual model validation, verification and results validation even if that results validation is performed at a maturity level of one (i.e., subject matter expert validation).Use and V&V histories can contribute valuable information to the validation process but this VPMM version does not apply that information.  It is possible to perform a complete accreditation based upon only historical information.  The VPMM needs to be extended to account for the application of historical information to the validation process.This version of the VPMM does not directly associate risk reduction with higher levels of maturity and account for an improved ability to estimate validation effort costs.  The authors feel strongly that increasing the validation process maturity level should directly affect the risk that users take when applying a simulation.  However, the linkage between validation maturity level and use risk is not clear.  Further work is needed to clarify this dependency.5.  VPMM EvolvedThe VPMM maturity matrix and questionnaire have been modified to reflect the lessons learned from the community feedback and use cases presented in this paper.  Overall, these modifications are minor with one exception.  The glossary section of the questionnaire has been revised to bring the definitions of common terms into agreement with common usage within the VV&A and simulation communities.  The importance of independent referents has been re-emphasized in order to avoid the calibration/validation conflict that Logan and Nitta describe [19].  The use of referents for simulation calibration may explain the investment in referent construction without their application to the validation process.  The contributions of historical information, the conceptual model, verification products and results validation to the validation process are now all captured by a single column that represents the validation evidence and its handling.  The VPMM now judges the use of any single source of evidence or any combination of sources in the same way.  This addresses the lessons learned about the conceptual model, verification products and use and V&V histories in a single major modification of the VPMM maturity matrix.  While this modification may address the specific lessons learned from the use cases, it creates another problem by raising the abstraction of the validation process maturity matrix to a level that may reduce its value as a communications tool.  This problem is still under consideration.Table 3 presents the current modified validation process maturity matrix.  The questionnaire for assessing validation process maturity has also been modified from the lessons learned.  Together, the validation process maturity matrix and the validation process maturity assessment questionnaire compose the VPMM evolved from the lessons learned.The VPMM Evolved still does not address the lesson learned about how use risk changes with the validation process maturity.  Considerable thought has been applied to this problem.  Solving it will involve expanding the scope of the VPMM to encompass the accreditation process since that process produces the acceptability criteria and other products that characterize the use, including the tolerable use risks.  Linking use risk to process maturity will require considering the VV&A process as a whole and cannot be done based upon the validation process alone.  This direction will be the subject of future research effort.6.  SummaryThe VPMM has gradually matured since 2002 through generous feedback from the technical community.  The VPMM concepts have been vetted by the U.S. DoD M&S community through the DoD VV&A Technical Working Group & DoD standards process,VV&A community through Foundations ‘04), and M&S community in SISO through SIWs & in SCS through JDAMS article.Proof-of-concept studies are complete and the lessons learned from these studies have been applied to further mature the VPMM concepts.  Activities aimed toward creating a SISO standard have been started.  These activities may integrate with a larger study group exploring the fundamentals of V&V or risk-based tailoring of VV&A processes.  Activities aimed toward creating an interim DoD standard have been started and draft documentation will be completed in July 2008.  Work is currently underway to expand the VPMM concepts to encompass risk-based VV&A.7.  References[1]	M.C. Paulk, C.V. Weber, W. Curtis & M.B. Chrissis (eds.), The Capability Maturity Model: Guidelines for Improving the Software, Addison-Wesley Publishing Company, Reading, MA, 1995.[2]	Credibility, Apple Dictionary, Version 1.01, nd.[3]	Credibility, Wikipedia, March 2008, at <http://en.wikipedia.org/wiki/Credibility>.[4]	Credibility, The Free Dictionary, 2003, at <http://www.thefreedictionary.com/credibility>.[5]	Credibility, Merriam-Webster Online Dictionary, nd, at <http://www.merriam-webster.com/dictionary/credibility>.[6]	S.Y. Harmon & S.M. Youngblood, “A Proposed Model for Simulation Validation Process Maturity,” Paper No. 03S-SIW-127, Proc. SISO Spring 2003 Simulation Interoperability Workshop, Orlando, FL, April 2003, np.[7]	S.Y. Harmon & S.M. Youngblood, “Simulation Validation Quality and Validation Process Maturity,” Paper No. 04S-SIW-125, Proc. SISO Spring 2004 Simulation Interoperability Workshop, Orlando, FL, April 2004, np.[8]	S.Y. Harmon & Simone M. Youngblood, “A Proposed Model for Simulation Validation Process Maturity,” Jour. of Defense Modeling and Simulation, 2 (4), October 2005, pp179-190.[9]	F.J. Fowler, Jr., Improving Survey Questions, Design and Evaluation, Sage Publications, Thousand Oaks, CA, 1995.[10]	Modeling and Simulation, U.S. Joint Forces Command (USJFCOM), Norfolk, VA, nd, at <http://www.jfcom.mil/about/fact_modsim.htm>.[11]	Joint Conflict and Tactical Simulation (JCATS), U.S. Joint Forces Command (USJFCOM), Norfolk, VA, nd, at <http://www.jfcom.mil/about/fact_jcats.htm>.[12]	Joint Live, Virtual, Constructive Architectural Environment, U.S. Joint Forces Command (USJFCOM), Norfolk, VA, nd, at <http://www.jfcom.mil/about/fact_jlvc.htm >.[13]	Modeling and Simulation, Systems Planning and Analysis, Inc., Alexandria, VA, 2007, at < http://www.spa-inc.net/spamodandsim.htm >[14]	Joint Effects Model, Joint Program Executive Office for Chemical and Biological Defense, Falls Church, VA, April 2007, at <http://www.jointeffectsmodel.org/>.[15]	Joint Operational Effects Federation (JOEF), Joint Program Executive Office for Chemical and Biological Defense, Falls Church, VA, 2007, at <http://www.jpeocbd.osd.mil/page_manager.asp?pg=2&sub=26>.[16]	ITT Industries, Inc., SERPENT User’s Manual, Version 2.1, Report No. A-04-57U (R), ITT Industries, Inc., Advanced Engineering and Sciences Division, Colorado Springs, CO, June 2005.[17]	IEEE Computer Society, IEEE Recommended Practice for Verification, Validation, and Accreditation of a Federation – An Overlay to the High Level Architecture Federation Development and Execution Process, IEEE Std 1516.4-2007, Institute for Electrical and Electronic Engineers, Inc., New York, NY, 20 December 2007.[18]	W. Oberkampf, Sandia National Laboratories, Albuquerque, NM, Personal Communication, May 2006.[19]	R.W. Logan & C.K. Nitta, Verification and Validation (V&V) Guidelines and Quantitative Reliability Confidence (QRC): Basis for an Investment Strategy, UCRL-2002x0266, University of California, Lawrence Livermore National Laboratory, Livermore, CA, August 2002.8.  AcknowledgementsThe authors would like to thank the U.S. DoD Modeling and Simulation Coordination Office for its support for conducting this research and preparing this paper.Author BiographiesSCOTT HARMON is president of Zetetix, a small business specializing in modeling complex information systems.  Mr. Harmon has been developing rigorous techniques for the validation of simulation federations and human behavior representations.  He recently contributed as editor of the IEEE Std 1516.4-2007.SIMONE YOUNGBLOOD is a member of the Principal Professional Staff at the Johns Hopkins University Applied Physics Laboratory (JHU/APL).  For the past ten years, Ms. Youngblood has served as the DoD VV&A focal point at the Defense Modeling and Simulation Office's VV&A Technical Director and is currently providing VV&A technical expertise to the Modeling and Simulation Coordination Office. Leveraging an extensive background in simulation development, modification and application, Ms. Youngblood has been active in the VV&A community for the past fifteen years.  She currently serves as the chair of the VV&A forum at the Simulation Interoperability Workshops.  In this role, she recently shepherded the development of IEEE Std 1516.4-2007.  Ms Youngblood has a Master of Science in Computer Science from The Johns Hopkins University and a Bachelor of Arts in Mathematics and a Bachelor of Science in Computer Science, both from Fitchburg State College.Table 1.  VPMM Development & Application HistoryDateEventOct. 2002Presented initial VPMM concepts to DMSOMar. 2003Detailed the VPMM concept & presented it at the Spring 2003 SIWMar. 2004Revised the VPMM to account for levels of simulation validation & presented it at the Spring 2004 SIWOct. 2004Revised the VPMM to account for uncertainty, applied it to evolving a VV&A technology roadmap & presented these at the Foundations of VV&A ‘04 WorkshopMay 2005Began a proof of concept study with JWFC to use the VPMM to improve their V&V processes for training simulations; developed the validation process maturity level questionnaireNov. 2005Applied the VPMM to JEM V&V processesJan. 2006Prepared & submitted a draft DoD VPMM standard for reviewMar. 2006Revised the validation process maturity level questionnaire & began applying it to additional ongoing simulation V&V effortsApr. 2006Proposed a SISO study group for an IEEE VPMM standardMay 2006Began developing a maturity model for VV&A processes that incorporates simulation use risk management as a keystone element & accounts for additional information sourcesJun. 2006Completed the JWFC proof of concept & its final report; revised the validation process maturity level questionnaireNov. 2006Applied the VPMM to V&V of the JOEF simulation componentDec. 2006Applied the VPMM to the SERPENT V&V processesSep. 2007Detailed the foundation concepts for VPMM revisions & presented them at the Fall 2007 SIWMar. 2008Completed a preliminary revised VPMM & circulated for commentsTable 2.  Results of VPMM Use CasesSimulationValidation CriteriaReferentConceptual ModelVerification ProductsSimulation ResultsIntegrated Maturity LevelTrial 1122111JMRM222422JTLS223422JCATS231121Fed 1222111Fed 2243422JLVC11-111STORM11-111JEM332113JAnT222222SERPENT33-222Table 3.  VPMM Maturity Matrix Evolved from the Lessons LearnedLevelAcceptability CriteriaValidation ReferentValidation Evidence0none existnone chosennone produced1represented by SME opinionrepresented by SME opinionjudgments of SMEs observing simulation results2described in terms of the required functional inventoryrespresented by SME opinioncompleteness judged from the functional inventory & correctness judged by SMEs3described in terms of required functional inventory & maximum tolerable errorsderived from single sources with estimates of errorscompleteness & correctness judged from functional inventory & error properties4described in terms of required functional inventory, maximum tolerable errors & minimum acceptable confidencessampled from multiple independent sources & correlated statistically with estimates of confidence intervalsLevel 3 validation evidence plus estimates of confidence in the evidence correctness5mathematically derived from user input using causality arguments & stated in Level 4 termsmathematically derived from multiple independent sources & characterized statistically with estimates of confidence intervalsLevel 4 evidence derived from mathematical analysis of simulation characterization, validation referent & acceptability criteria