DRAFTParallel and Distributed Modeling & Simulation Standing Study Group(PDMS-SSG)FINAL REPORTVolume 1: PDMS TechnologySubmitted to:The SISO Standards Activities Committee (SAC)23 March 2009SISO-REF-###-2009TABLE OF CONTENTS TOC \o "1-3" 1.0	Introduction	 PAGEREF _Toc99461048 \h 32.0	Force Modeling and Simulation	 PAGEREF _Toc99461049 \h 33.0	Multicore Computing Revolution	 PAGEREF _Toc99461050 \h 44.0	Processor Architectures	 PAGEREF _Toc99461051 \h 65.0	Introduction to Parallel and Distributed Computing	 PAGEREF _Toc99461052 \h 66.0	History and Evolution of Parallel and Distributed Computing	 PAGEREF _Toc99461053 \h 137.0	Threads, Processes, and Shared Memory on Multicore Computers	 PAGEREF _Toc99461054 \h 138.0	Parallel and Distributed M&S Communications Services	 PAGEREF _Toc99461055 \h 158.1	Startup and Shutdown	 PAGEREF _Toc99461056 \h 168.2	Miscellaneous Services	 PAGEREF _Toc99461057 \h 168.3	Global Synchronization and Reductions	 PAGEREF _Toc99461058 \h 168.4	Global Reductions	 PAGEREF _Toc99461059 \h 168.5	Synchronized Data Distribution	 PAGEREF _Toc99461060 \h 178.6	Asynchronous Message Passing	 PAGEREF _Toc99461061 \h 178.7	Coordinated Message Passing	 PAGEREF _Toc99461062 \h 178.8	Object Request Broker (ORB) services	 PAGEREF _Toc99461063 \h 179.0	Logical and Real Time Scheduling	 PAGEREF _Toc99461064 \h 179.1	Challenge of Parallel Discrete Event Simulation	 PAGEREF _Toc99461065 \h 189.2	Representation of Time	 PAGEREF _Toc99461066 \h 199.3	Conservative – Time Stepping	 PAGEREF _Toc99461067 \h 209.4	Conservative – Time Windows and Lookahead	 PAGEREF _Toc99461068 \h 219.5	Conservative – Topology	 PAGEREF _Toc99461069 \h 239.6	Optimistic – Time Warp	 PAGEREF _Toc99461070 \h 269.7	Optimistic – Risk Free and the Event Horizon	 PAGEREF _Toc99461071 \h 309.8	Optimistic – Flow Control	 PAGEREF _Toc99461072 \h 349.9	Real Time Scheduling	 PAGEREF _Toc99461073 \h 359.10	Emerging Technology – Five Dimensional Simulation	 PAGEREF _Toc99461074 \h 3610.0	Scalable Publish and Subscribe Data Distribution	 PAGEREF _Toc99461075 \h 3611.0	Plug-and-Play Model Components	 PAGEREF _Toc99461076 \h 4212.0	Establishing a Common Unified Technical Framework	 PAGEREF _Toc99461077 \h 4213.0	Summary and Conclusions	 PAGEREF _Toc99461078 \h 4514.0	Recommendations	 PAGEREF _Toc99461079 \h 4515.0	References	 PAGEREF _Toc99461080 \h 45IntroductionForce Modeling and SimulationForce modeling and simulation (FMS) [ REF _Ref94427756 \r \h 47] is a branch of Modeling and Simulation (M&S) that enables the United States Department of Defense (DoD) to better understand the complex behavior of battlefield entities engaging in conflict-oriented scenarios. FMS can address capabilities such as: (a) analysis of simulated battlefield excursions, (b) warfighter training exercises, (c) the testing needs of various defense systems, (d) Dynamic Situation Assessment and Prediction (DSAP) [ REF _Ref76618046 \r \h 4, REF _Ref76621279 \r \h 8, REF _Ref54272796 \r \h 12, REF _Ref76619272 \r \h 61, REF _Ref54184575 \r \h 99], (e) decision support in live data-rich tactical environments, (f) the pre-planning and statistical analysis of operational scenarios in preparation for combat, (g) assessing logistics requirements, and (h) acquisition decision-making for the procurement of future combat systems [ REF _Ref76614189 \r \h 7, REF _Ref22196254 \r \h 24, REF _Ref94430450 \r \h 36, REF _Ref76618077 \r \h 124].The United States DoD uses several other types of M&S systems that would not directly fall under the category of FMS. Examples might include: (a) Computational Fluid Dynamics (CFD), (b) electromagnetic wave and signal propagation, (c) underwater sonar detection and tracking systems, (d) kinematics of store separation for the release of onboard aircraft missile systems, (e) Transport and Dispersion (T&D) models of hazardous materials released in the atmosphere, (f) environment representations, weather, terrain, roads, and various other structures such as buildings and bridges, (g) miscellaneous assortment of system-specific testbeds, and (h) logic emulation of electronic circuit boards. These kinds of DoD M&S systems are often integrated with FMS applications to produce credible results. A common high performance software framework [ REF _Ref95189795 \r \h 68, REF _Ref76622892 \r \h 77, REF _Ref76619348 \r \h 84, REF _Ref76619554 \r \h 85, REF _Ref54184520 \r \h 98] can universally support FMS systems, other types of model collections, and even real-world services operating in a scalable manner on emerging multicore computing environments [ REF _Ref94431155 \r \h 104]. However, such a universal framework must be widely embraced by multiple diverse communities [ REF _Ref95190113 \r \h 54, REF _Ref22197436 \r \h 56, REF _Ref22197438 \r \h 57]. The formation of new high performance interoperability standards [ REF _Ref54185048 \r \h 83] that simplify software development and integration without sacrificing scalability will become an essential priority as multicore computing technology moves forward [ REF _Ref76618138 \r \h 96, REF _Ref54183090 \r \h 100].FMS applications are typically composed of battlefield entities that in the real world are able to freely interact with other battlefield entities at arbitrary times. Such entities can represent (a) individual soldiers, (b) mobile land, air, sea, and space vehicles that are hierarchically composed of multiple systems and/or subsystems, (c) communication networks that collectively form the Global Information Grid (GIG) [ REF _Ref76613413 \r \h 1, REF _Ref22196153 \r \h 19, REF _Ref76613511 \r \h 26, REF _Ref22197093 \r \h 39], (d) surveillance systems, (e) Global Command and Control Systems (GCCS) [ REF _Ref76611704 \r \h 114], Net-Enabled Command Capability (NECC) [ REF _Ref76613994 \r \h 20], and Net Centric Enterprise Services (NCES) [ REF _Ref76553184 \r \h 21], (f) operational centers, etc.Oftentimes, several physical entities are combined into aggregate representations to simplify model development efforts and to generally reduce computations. The level of aggregation normally depends on the degree of granularity and fidelity required by the application during its operational usage. Entities can be thought of as being composed of model components, which in turn can be further composed of other model components. Hierarchical composition is a key enabling concept in supporting (a) aggregate model representations, and (b) plug-and-play component-based model interoperability. Abstraction rules, readily available open system frameworks, suitable software interoperability methodologies, and ontologies that describe (i) object relationships, (ii) exchanged data, and (iii) interaction patterns will be required to support model reuse in a cost-effective manner [ REF _Ref76626973 \r \h 6, REF _Ref76623757 \r \h 11, REF _Ref54183136 \r \h 18, REF _Ref77669121 \r \h 30, REF _Ref22197207 \r \h 40, REF _Ref22197238 \r \h 42, REF _Ref95190492 \r \h 53, REF _Ref76616169 \r \h 79, REF _Ref76617555 \r \h 108].A universal discrete-event scheduling approach provides an ideal framework for supporting FMS applications because arbitrary interactions between entities in the physical world can occur at any time. Discrete-event scheduling can be contrasted with other approaches that either (a) restrict which entities are permitted to interact with which other entities [ REF _Ref94432049 \r \h 15], (b) introduce arbitrary minimum interaction time delays between interacting entities [ REF _Ref94432145 \r \h 31, REF _Ref94432265 \r \h 63], and/or (c) force interactions and state updates to occur in a synchronized lock step manner. These unnatural restrictions can significantly impact the fidelity of high-resolution models and should be avoided when designing complex FMS systems.Besides providing the most flexible means for processing events, the discrete-event approach can also universally support both real-time and logical-time modes of operation transparently. Therefore, discrete-event provides an ideal methodology for implementing highly reusable plug-and-play model components that are able to freely operate in multiple execution modes. Entity-to-entity interactions might include (a) various sensor types detecting nearby entities, (b) exchange of weapon fire occurring during combat engagements, and (c) wireless radio transmission and reception.One of the most difficult problems in FMS is to efficiently determine which entities are able to interact with which other entities. Often, interactions are based on proximity [ REF _Ref76620244 \r \h 89] between large numbers of moving entities distributed across the simulated battlefield. The distance between entities universally affects sensor detections, weapon fire exchanges, and wireless communication. Other factors such as radar cross section, temperature, emitting frequencies, etc., can help determine which entities require state information about which other entities. Scalable Interest Management (IM) [ REF _Ref22197601 \r \h 66, REF _Ref22198171 \r \h 93, REF _Ref22198308 \r \h 95] operating within a high performance publish/subscribe data distribution framework [ REF _Ref94435232 \r \h 60] is essential in facilitating the exchange of state information in parallel and distributed FMS. This non-trivial problem will be discussed in more detail later in this paper.The High Level Architecture (HLA) [ REF _Ref22197266 \r \h 43, REF _Ref22197268 \r \h 44, REF _Ref22197270 \r \h 45, REF _Ref22197273 \r \h 46, REF _Ref76614409 \r \h 59, REF _Ref22199595 \r \h 125], Distributed Interactive Simulation (DIS) [ REF _Ref76614503 \r \h 22, REF _Ref76614506 \r \h 23, REF _Ref22196699 \r \h 34], and Test and Training Enabling Architecture (TENA) [ REF _Ref76615214 \r \h 106] standards were primarily designed to support interoperability between multiple FMS applications [ REF _Ref95226112 \r \h 49]. The primary focus of these standards is to support the distributed operation of FMS applications operating as an integrated federation within a networked computing environment. While modern HLA, DIS, and TENA implementations could be optimized to more efficiently support multicore shared-memory computing environments [ REF _Ref76625202 \r \h 10, REF _Ref76625215 \r \h 97], these network-based standards were not designed to utilize the advanced M&S technologies that have been developed over the past 20 years [ REF _Ref76619761 \r \h 33]. For example, communications through shared memory on multicore computing systems [ REF _Ref76620915 \r \h 101] is fundamentally reliable, which makes heartbeat strategies that pump out regular entity state update Protocol Data Units (PDUs) unnecessary. Reliable scalable communication between processors necessitates a fundamentally different mechanism for communicating state information between entities. In addition, HLA, DIS, and TENA do not provide an actual programming framework for hosting true plug-and-play model components, which is necessary to fully accomplish the elusive goals of model interoperability and reuse [ REF _Ref76623883 \r \h 27]. FMS on emerging multicore computers should embrace the new technologies that will best enable both (a) scalable performance and (b) model component interoperability.Composable FMS systems require a modeling framework that can coordinate multiple concurrent behaviors within each entity. For example, an aircraft in combat must simultaneously model the behavior of its (a) airborne radar system, (b) various weapon systems, (c) communications links, (d) motion, (e) fuel consumption, and (f) the human decision-making process of the pilot trying to accomplish the mission goals. These behaviors operate asynchronously and at their own natural time scales. All of these modeled behaviors must be permitted to interrupt each other and mutually affect behaviors as needed through standard modeling constructs and interfaces. Without such standards, models will never be able to directly integrate with other models operating in a common high-performance execution framework.In general, unconstrained parallel and distributed FMS is extremely difficult to support and requires a significant investment in technology, interoperability standards, and open system architecture frameworks. This paper describes these issues, along with various known solutions and open source implementations. The strengths and weaknesses of each diverse approach are analyzed to help guide the future direction of FMS applications in anticipation of their required operation on emerging multicore computers.Multicore Computing RevolutionThe multicore computing revolution has begun and will soon demand a paradigm shift in how software is developed [ REF _Ref76616905 \r \h 103]. Chip manufacturers have hit the performance wall. Three primary technical factors limit the ability to make software running on single CPU microprocessor architectures execute faster [ REF _Ref94436721 \r \h 64].Power and overheating considerations make it difficult to increase CPU clock speeds beyond current capabilities. Power consumption goes up by a factor of four when doubling the clock speed.Memory access is a bottleneck and cannot be easily optimized any further. Multilevel caching techniques have been used to reduce the overhead of memory access bottlenecks, but there is little room for further improvement.Instruction-level parallelism (e.g., pipelining, branch prediction, hyper-threading, etc.) cannot be easily optimized beyond the current state of the art. Performance gains from single CPU chip designs have apparently reached their limit.Even if one or two of these performance walls could be improved, all three must advance together to minimize bottlenecks and achieve actual performance gains for typical software applications. There is only one foreseeable way to improve computational performance. The universally accepted path forward is to pack multiple CPUs, known as cores, onto a single chip. Because chips can be manufactured with multiple layers, there is virtually no limit to the number of cores that can be provided on a single chip. Thus, the multicore computing revolution has begun. Hardware, operating system, and commercial software vendors alike are all competing to rapidly bring these technologies to market.The commercial entertainment and gaming markets are driving much of the new multicore computing technology, with the supercomputing community benefiting as a secondary result. The need for faster and more realistic graphics in video games has provided the economic stimulus for investing in new affordable multicore Graphics Processing Units (GPUs) [ REF _Ref94437004 \r \h 41] that reside on graphics cards. Meanwhile, the majority of the software industry is either unaware of where the new computing technology is heading, or they are frantically trying to figure out how to produce next-generation applications that can benefit from parallel processing.“New Generation of Processors Presents Big Problems, Potential Payoffs for Software Industry.”… Experts predict dire consequences if the software for more complicated applications isn't brought up to speed soon. They warn that programs could suddenly stop getting faster as chips with eight or more cores make their way into PCs. The software as it's currently designed can't take advantage of that level of complexity.“We'd be in uncharted territory,” Patterson said. “We need to get some Manhattan Projects going here – somebody could solve this problem, and whoever solves this problem could have this gigantic advantage on everybody else.”Jordan Robertson, Associated Press, July 22, 2007.A large portion of the commercial software industry is focused on moving towards 64-bit memory architectures. Other commercial products may argue that their applications currently run fast enough, and would not benefit from further performance gains. This view, in many cases, appears rather shortsighted. Performance improvements almost always benefit software applications. The implications of multicore computing have not yet been fully understood by the software industry. Yet, like a tsunami, the multicore computing revolution is approaching quickly and will forever change the landscape of how software is developed.Designing general-purpose software to run efficiently in parallel on multicore computing architectures is highly challenging. Existing software must either be rewritten or significantly refactored to scale on emerging multicore computer architectures. For most applications, refactoring existing software proves to be too difficult. New software designs and implementations are required. Reluctance to embrace multicore computing might save initial software development costs, but at the risk of making existing software applications obsolete in a competitive world. Computationally intensive, time-critical, DoD information-processing systems must maintain their performance edge over adversarial counter-part systems. Despite potential resistance from the software industry, the multicore computing software transformation must eventually occur, so it is prudent to begin planning for it right now.Time-critical computationally intensive applications, such as large-scale FMS systems and other military M&S applications, should receive the highest priority in making the transition to multicore computing. Perhaps, the most critical applications are those used during live battlefield operations, where decision-making based on the analysis of predicted outcomes must be performed much faster than real time. The lives of allied warfighters and innocent civilians could be at stake.Experts uniformly agree that parallel programming is much too difficult for the average software developer and that a new suite of powerful tools must be provided to abstract away the associated programming difficulties. A suite of parallel software development tools might include (a) programming frameworks, (b) new languages or extensions to existing languages that automate parallelism through embedded services and/or programming constructs, (c) new optimized compilers that automatically parallelize code whenever possible, (d) transactional memory mechanisms that automatically coordinate reading and writing between threads or processes, (e) optimally parallelized math libraries for supporting massive number crunching, and (f) improved communication libraries that simplify communication between processors through shared memory.Hardware and software vendors alike are now feverishly working to bring these capabilities to market [ REF _Ref94431155 \r \h 104]. Fortunately, an advanced, freely available, open source, parallel and distributed M&S framework [ REF _Ref94438033 \r \h 105] exists today and is readily available for use by the United States DoD.It is critical for the United States to maintain its information processing edge over its adversaries, especially in its fight against global terrorism. The United States Defense Department must begin utilizing parallel processing techniques to (a) speed up calculations, (b) provide higher fidelity results, (c) perform more computations, (d) analyze more data, and (e) simulate larger scenarios. The world of software is changing rapidly and will soon demand new programming techniques to embrace the coming revolution.Processor ArchitecturesIntroduction to Parallel and Distributed ComputingDistributed computing has many forms, ranging from (a) old-fashioned centralized mainframe computers with distributed users connecting through remote terminals, (b) Internet applications based on standardized web services [ REF _Ref76615842 \r \h 17, REF _Ref22196441 \r \h 28, REF _Ref76616053 \r \h 112, REF _Ref76615262 \r \h 115, REF _Ref22198543 \r \h 116, REF _Ref22198626 \r \h 117, REF _Ref22199117 \r \h 118, REF _Ref22199153 \r \h 119, REF _Ref22199216 \r \h 120, REF _Ref22198674 \r \h 121, REF _Ref76616538 \r \h 122], (c) network-based grid computing systems [ REF _Ref77668519 \r \h 71] that farm work to available machines that are distributed across a wide area networks, (d) distributed client/server applications [ REF _Ref76617091 \r \h 110], (e) distributed databases, and (f) distributed Live, Virtual and Constructive (LVC) M&S based on standards such as HLA, DIS, and the Test and Training Enabling Architecture (TENA) [ REF _Ref76624949 \r \h 9]. However, these types of distributed systems are fundamentally different from traditional parallel computing applications in several important ways.Parallel computing is different from distributed computing in how systems are decomposed to processors, and in how they communicate. Furthermore, parallel computing is not the same as concurrent computing. For example, handling mouse clicks in a Graphical User Interface (GUI), while performing processing in the background, can be supported effectively using interrupts and concurrent threads. The goal of parallel computing is to achieve scalable high-performance execution. Running fast enough to meet a responsiveness criteria is not the same as running as fast as possible. The idiosyncrasies of parallel computing become more evident as the number of processing cores becomes large. This is precisely where the multicore computing technology is heading.Parallel computing is normally performed on multicore and/or other scalable computing architectures that provide either (a) shared memory access between all processors, or (b) extremely fast node-to-node communication through high speed channels. Parallel computing architectures rarely use standard message-passing network protocols with large packet transmission overheads such as UDP or TCP. Special backplane harnesses with highly optimized proprietary communication protocols are usually provided on large supercomputers to support extremely fast communications between processors and/or their hierarchical shared memory hardware architectures. Hypercube, 2D, and 3D meshes (see  REF _Ref92965641 \h Figure 1) are the most common backplane communication topologies used for interconnecting processing nodes on massively parallel supercomputers.Figure  SEQ Figure \* ARABIC 1: Common parallel computing node interconnection topologies. Hypercube topologies provide a very rich scalable interconnection topology by adding a communications link to each node whenever doubling the number of nodes. 2D meshes provide four neighbor links per node, while 3D meshes provide 6 links per node. Meshes are often connected into toroids to connect edges. Toroids can cut the worst-case number of hops to send a message from one node to another node by a factor of two.Scalability becomes a critical concept in designing massively parallel applications. Scalability applies to (a) computations performed, (b) the number of messages passed between nodes, (c) total message-passing bandwidth consumed by the application, and (d) the amount of memory utilized.These four scalability metrics are typically a function of (a) the amount of input data processed, (b) resolution of the input data, and (c) the number of processing nodes applied to the problem. For FMS, this translates to the size of the scenario in terms of entity counts, aggregate model representation if applicable, and the number of processing nodes used during execution. Time resolution (or step size), while directly affecting the amount of computations performed, is generally not considered when evaluating the scalability of an M&S application because it equally impacts sequential and parallel performance.Algorithms that maintain global information about every entity on every node, or even worse, replicated within every entity, do not scale in terms of memory consumption. In addition, techniques that broadcast entity state information across all nodes do not scale in terms of messages. More efficient means must be used to distribute entity state data between processors. Scalability considerations demand the use of sophisticated publish and subscribe multi-resolution data distribution mechanisms that are integrated with parallelized interest management to determine which nodes, and subscribing entities on those nodes, should receive published data. This requires scalable techniques to: (a) determine which entities require state information about which other entities, and (b) efficiently distribute and represent published state information across processing nodes to subscribing entities.Various techniques have been successfully used to provide scalable data distribution in parallel and distributed FMS applications. Examples of such techniques include: (a) distributing dead reckoning or other types of motion algorithms to reduce time between updates, (b) predictive contracts that specify worst-case tolerances due to stale state updates, (c) attribute throttling that establishes a minimum time between distributing updates to eliminate update jittering, (d) optimized range-finding algorithms that determine when entities enter and exit specified ranges while avoiding computational thrashing as entities change their motion, and (e) multi-level physically correct filtering that efficiently coordinates data distribution between sending and receiving nodes and entities [ REF _Ref76620689 \r \h 14, REF _Ref22197433 \r \h 55, REF _Ref94435232 \r \h 60, REF _Ref22197601 \r \h 66, REF _Ref76620244 \r \h 89].An application is said to be scalable if its (a) run-time performance, (b) total number of messages passed, (c) bandwidth consumed, and (d) memory consumption is a linear (or nearly linear) function of the size of the data set and the number of processing nodes. All four quantities must scale for the application to be considered scalable. For example, an FMS application perfectly scales if doubling the number of entities and the number of processing nodes results in virtually the same processing time, number of messages passed and received per node, total message bandwidth utilized per node, and memory consumed per node.Parallelized software applications normally operate as a homogeneous execution whose data is decomposed across processors that are able to communicate at very high speeds. For FMS, entities and their internal model components are distributed across processors where their behaviors are executed in parallel. Dynamic connectivity of software applications commonly found in distributed client/server and/or Service Oriented Architecture (SOA) applications is not permitted because that would impact scalability and performance. An exception to this rule is grid computing, where work is farmed out to available processors across a network. Grid computing is a popular way to support large-scale embarrassingly parallel computing applications in cluster and network environments that do not require high-speed communications between processors as they execute. Multicore computing, in contrast to cluster computing, offers extremely fast communication operations through shared or common memory between processors. High-speed communications between processors within multicore computing systems enables true scalable parallelism of next-generation applications.The fundamental goal in parallel processing is to achieve computational speedup by distributing and processing data in a concurrent manner. In a sense, each node operates on a different part of the problem. No matter how many processors are used to execute the application, the final results should be exactly the same.  Speedup is defined as follows: EMBED Equation.3 Here, T1 is the time to execute the program on a single processor and TN is the time required to execute the same program obtaining the same results on N processors. Parallel speedup is obtained when T1 is larger than TN.As an analogy, one can think of parallel processing as managing a team of workers with the goal of being N-times more productive than what could be accomplished with a single worker. Tasks are divided among the workers, who each contribute their part to the total effort. The coordination and interaction of team members working on a project is similar to the regular data exchanges that are necessary between processing nodes while executing a parallel software application. For FMS systems, communication is in the form of scheduled time-tagged events that occur between entities, which are distributed across processing nodes.In principle, speedup should never be more than a factor of N. However, there are situations when superlinear speedup can become possible. This is normally due to information losses that are accentuated by unoptimized data structures and/or algorithms that poorly scale [ REF _Ref76619554 \r \h 85]. Superlinear speedup can also occur with fixed-sized problems when their data is distributed across processors, resulting in more efficient memory caching due to smaller amounts of memory being consumed per node.Speedup values less than one actually indicate a slowdown. This is often due to spending excessive amounts of time communicating between nodes in comparison to the actual time spent doing useful computations. Sometimes, slowdown is caused by a poor decomposition of data to processing nodes, resulting in large wasteful idle times between synchronization and communication. A good parallel application keeps nodes busy performing useful work most of the time.When measuring speedup, it is important to compare parallel performance with an optimized sequential implementation. It would be unfair to claim speedup for software made significantly slower than its sequential counterpart due to noticeable parallelization overheads. Total execution speed must always remain the primary objective, and not just parallel speedup.Parallel applications usually reach a plateau where the maximum speedup for a given problem is achieved on a certain number of nodes. Adding further nodes without changing the size of the problem causes the application to actually slow down. This usually occurs because adding more processing nodes reduces the amount of work performed by each node while communication overheads remain the same or even slightly increase. To achieve speedup, each node must generally perform a significant amount of work relative to all associated communication and serial processing overheads. Load balancing becomes more important as the number of nodes increases. This is because most parallel applications process synchronously and do not progress faster than the slowest node.Another common metric used to measure parallel processing performance is Efficiency, which is defined as:Ignoring the potential for superlinear speedup, efficiency normally ranges from 0 to 1. An application achieving an efficiency of 1 is maximally efficient because it achieves perfect speedup. In other words, N processors are able to make the application run N times faster. Efficiency values greater than 1 indicate superlinear speedup, which is rarely observed for efficient FMS discrete-event systems. Efficiency values less than 1/N indicate a slowdown. Hopefully, this never occurs.Related to efficiency is Relative Efficiency, which is the speedup efficiency when increasing the number of processing nodes from M to N. Normally when characterizing relative efficiency for an application, the N to M ratio is a factor of two. Going from one node to two nodes often achieves little speedup and sometimes even slows the application down due to the introduction of parallel processing overheads. However, going from two to four nodes (or beyond) can become very efficient. Relative efficiency indicates the ability for an algorithm to continue obtaining speedup as more processors are added. Relative efficiency is a useful metric in understanding the initial parallelization overheads and the full scalability of an algorithm.Various factors and processing/communication strategies come into play when designing truly scalable and highly efficient parallel applications. Most parallel applications operate synchronously in that the data is distributed across processors, where computations are performed in a synchronized manner by the same program independently replicated across processors. This approach is known as data parallelism.At regular intervals, the processors operating in parallel synchronously communicate their results with other processors as needed before continuing with the next processing step. This processing loop repeats until the desired work is completed. Parallel processing speedup is achieved by: (a) decomposing the problem uniformly so that each node has approximately the same amount of computations to perform between communications, and (b) organizing the decomposed data in such a manner that minimizes communication overheads between nodes. Depending on the problem, a tradeoff may be required to reduce communication overheads at the expense of impacting load balancing considerations.An extremely simple data-parallel application might be of the following form, shown in  REF _Ref92973730 \h Code Segment 1.Code Segment  SEQ Code_Segment \* ARABIC 1: Basic processing loop for synchronous data-parallel applications.int main() {// Launch program across multiple processors and// establish communications  Startup(NumNodes);// Initialize the program and distribute// data across processors  Initialize();// Main processing loop performs computations// and then synchronously communicates results// between nodes before starting the next cycle.  for (cycle=0; cycle<NumCycles; cycle++) {    Compute();    Communicate();  }// Store results  StoreResults();// Shut down communications and exit the// application  Terminate();  Return 0;}The basic processing sequence for the simple data-parallel application shown in  REF _Ref92973730 \h Code Segment 1 is depicted in  REF _Ref91575150 \h Figure 2.Figure  SEQ Figure \* ARABIC 2: Simple parallel application.During the Startup function, the program is either replicated into (a) separate processes, or (b) multiple threads are spawned for each processor to achieve concurrency. The Startup function also creates any shared memory segments that might be required by the application. Node (or thread) Identifiers are numbered from zero to the total number of nodes (or threads) minus one. Each node is able to obtain its NodeId to assist in the decomposition of data during Initialize and then later while processing the data. After Startup, all nodes or threads concurrently and independently execute the same program. Speedup is obtained as each node simultaneously performs the Compute operation on its local data. Computed results that must be shared are then synchronously delivered to other nodes during the Communicate function before starting the next cycle. The new cycle does not begin until all nodes have completed their communication. After all cycles have been computed, which for FMS systems would indicate that the simulation has reached its end time, the StoreResults function is called to output results to the screen or to a file. Finally, when the application is done, each node calls the Terminate function to tear down the communications infrastructure that was setup. Each node then exits the program by returning the value 0 to indicate successful completion of the program.One file per node can be created while storing results, or the results can be combined into a single stream before writing a consolidated/merged output file. Oftentimes, inputting and outputting results can be so data intensive that it becomes the overwhelming bottleneck of a parallel application. Special Parallel I/O capabilities are provided on some supercomputers to reduce I/O bottlenecks. Parallel I/O techniques include providing multiple paths between processors and disks, and special striped file formats that allow multiple readers and writers to simultaneously perform file operations.It is essential for parallel processing algorithms to be robust on each node because the data is distributed in a scalable manner across processors. Data is generally not duplicated across processors because that would introduce memory scalability problems. If a node goes down for any reason, whether it is due to a hardware failure or a software crash, there is no practical way to recover because the data assigned to that node is forever lost. In addition, most synchronous communication services either hang or crash while waiting for messages that will never be received from nodes that have unexpectedly stopped functioning due to software or hardware failures.If a single node goes down unexpectedly, the operating system or supporting framework should cleanly terminate the entire parallel application. Otherwise, users would have to manually bring down each node and then clean up all of the temporary shared memory segments that were created during execution. This can become tedious, especially if the number of nodes is large. To the user, a parallel application should be thought of as a single indivisible program. If any part of it crashes, then the entire parallel application itself crashes as a unit.The most practical way to offer fault tolerance for parallel applications is to implement periodic checkpointing that can be used to restart the application from a checkpoint file if a node ever crashes [ REF _Ref22197433 \r \h 55, REF _Ref94444952 \r \h 94]. Of course, if a software bug caused the crash, then there is a good chance that after a restart, the program will crash again at exactly the same point in its execution. Normally, each node performs its own checkpoint, which means that the application must restart using the exact same number of processors. This can be a problem when a CPU is no longer functional. Big and Little Endian data representations must also be considered when storing checkpoints. An application running on a Little Endian machine will probably not restart correctly on a Big Endian machine unless special care is taken to handle both formats in a universal manner.Most parallel applications are not fault tolerant because of the complexity and overheads in supporting checkpoints and restarts. Fault tolerance can be an important consideration for applications that operate for long periods of time on very large parallel supercomputers, where hardware failures occur more frequently. Machines with multicore chips are much more reliable because all of the processors reside on a single chip and are not prone to such frequent hardware failures.Algorithms are often difficult to completely parallelize and almost always contain at least some sequential portions of code. This introduces Amdahl’s law [ REF _Ref94445311 \r \h 3] that predicts the performance limits for mixed sequential and parallel algorithms. Amdahl’s law is straightforward to derive by identifying the time to perform the computation on one processor and then again on N processors. Assuming that the sequential time to complete the computation is normalized to 1,In Amdahl’s law, P is the portion of computations during sequential processing that can be efficiently parallelized, (1-P) represents the serial portion of the computation, and N is the number of processing nodes. In the limit as N gets very large, Amdahl’s law can be used to indicate the limit of parallelism for an application containing some serial code. The maximum speedup is simply:Another popular approach to parallelism is to decompose applications by function [ REF _Ref94445592 \r \h 35] instead of distributing data across processors. Functional decomposition can sometimes be easier to implement when refactoring existing legacy applications. In some ways, the concept of functional decomposition is similar to the HLA approach of federating an arbitrary collection of interacting simulations. In such federated systems, each simulation provides a unique function within the overall federation. Applications can sometimes be thought of as a series of black box functions that receive input data and produce output results that are consumed by other functions. A topological graph of these functions is constructed to define the data flow. Each function can then be assigned to a processor to achieve parallelism. A scheduler is required to activate functions when more than one function is assigned to a processor. The scheduler can be the host operating system that wakes up individual processes when their function is requested, or the scheduler can be implemented within a more elaborate execution framework.Several problems arise with functional decomposition that can severely limit scalable performance. First, applications typically have a limited number of functions. This restricts the amount of possible parallelism in the application. This may not be a problem for small-scale parallel systems. However, this limitation can become a serious factor as the number of processors in a multicore computing environment becomes large. Second, it is difficult to achieve significant parallelism because computationally intensive functions almost always emerge within the application as crippling performance bottlenecks. The parallel application can only execute as fast as the slowest function, which usually dominates performance. This is regularly experienced within time managed HLA federations. The slowest simulation within the federation almost always dominates execution speed. It is not unusual for the communications infrastructure to introduce such large overheads to the distributed system that the application runs slower than it would on a single processor. This is frequently observed in time managed HLA federations, especially when zero or small values of lookahead are used.In general, data parallelism is the preferred approach for supporting scalable multicore processing. In terms of FMS, this means that evenly decomposing battlefield entities across processors will likely perform and scale better than functionally distributing simulations within federations. Distributing entities across processors within a common high performance computing parallel M&S framework is a paradigm shift away from the HLA functional decomposition approach.In summary, data parallelism for FMS distributes entities across processors and does not easily support fault tolerance or dynamic processor reconfiguration during execution. Checkpoints, while challenging to fully implement, are generally required to provide some fault tolerance. Memory management technologies such as object persistence can be used to automate checkpoint and restart. The challenge is to support this transparently and with minimal overheads.History and Evolution of Parallel and Distributed ComputingThreads, Processes, and Shared Memory on Multicore ComputersThreads and processes are related concepts in computer science [ REF _Ref95215443 \r \h 107]. Processes are independent programs running under the control of a host operating system such as Windows, Linux, Mac OS X, Solaris, HPUX, AIX, etc. Threads go one level deeper in that they potentially provide concurrent processing within the same executing process. Threads operating within a process can mutually access and modify all of the common memory within the application, which requires careful coordination.Parent processes in UNIX can create or launch child processes using the fork operation. Windows does not support the fork operation, but does allow applications to launch other applications using the system command. In either case, the Startup operation normally performs fork or system calls to create new processes. Threads are different in that they are created or spawned by the parent process and given a user-provided function name as their starting entry point. Spawning processes and threads is normally automated within a framework Startup function.Modern programmers are generally familiar with threads and related concurrency issues. However, as previously mentioned, developing concurrent software is not the same as developing high performance scalable software. While many applications today utilize threads to simultaneously perform multiple tasks or functions, this is not the same as using threads to speed up internal computations. These differences will become more important over time as the number of cores per chip steadily increases.Each process has its own memory space allocated by the operating system and its virtual memory management framework to prevent different processes from overwriting each other’s memory during their execution. This keeps applications from adversely affecting other independently executing applications, including the operating system itself. The scheduler within the operating system kernel coordinates the execution of processes using time slicing or other priority-based mechanisms when there are more processes than CPUs available. Often, many idle processes are actually running in the background on a machine. However, these processes do not consume CPU resources. Idle processes may unexpectedly wake up under certain conditions, which can affect the performance of other executing applications. Some operating systems allow processes to be locked down to specific CPUs to avoid this problem. Locking down processes to CPUs can be critical in hard real-time or embedded applications where predictable performance is mandatory.Parallel applications running on multicore computers can execute as multiple independent processes that simply communicate through specially created shared memory segments. Data transmitted between nodes is simply written into and then out of shared memory. Parallel applications built from such processes can perform very well when the processes are assigned to dedicated CPUs. Performance of process-based parallel applications can become poor when the operating system frequently swaps in and out processes that require regular synchronization.Threaded applications do not suffer as much from processes being swapped in and out, but must deal with other extremely challenging problems that will be discussed later in this paper. One solution to the process-swapping problem is to always run fewer processes than the total number of processors on the machine. This technique can be extremely effective when the number of available processors is large. For example, running 120 processes on a machine with 128 processors can still produce nearly 94% computational efficiency, while managing a handful of other background processes.Most supercomputing systems provide a batch mode job submission framework that guarantees execution on a dedicated set of nodes. Such batch-mode execution frameworks will probably not be provided on emerging multicore desktop or laptop machines, where it is assumed that a single user operates the machine.Unlike the use of separate processes, threads share the internal memory space of an individual process. There is no need to create special shared memory segments for threads because all of the memory used by the application is shared by the threads.Reading and writing to memory by active threads can lead to severe problems if not handled correctly. POSIX threads (Pthreads) and other threads packages [ REF _Ref95215443 \r \h 107] provide locking mechanisms to help programmers serialize the critical sections of their code that cannot be safely performed concurrently by multiple threads. Without such mechanisms, (a) data structures can become corrupted, (b) variables may take on unpredictable values, and (c) applications exhibit unpredictable pathological behaviors that are very difficult, if not impossible, to troubleshoot. The problems caused by these kinds of race-conditions are not repeatable. While locking mechanisms are necessary to resolve such problems, they can be very difficult to use in practice. Locking mechanisms may have significant overheads that can introduce bottlenecks, which severely serialize the application. In addition, overly clever attempts to minimize critical sections of code can significantly make software more complex and difficult to maintain. If locking mechanisms are used too frequently, this can also introduce more overheads, leading to serious performance degradation.Most affordable multicore chips today support true shared-memory. In other words, all of the cores on the chip have common access to a single physical memory that is shared by the processors. A common bus shared by all processors from the chip to the shared memory can be a significant bottleneck. Because of this bottleneck, true shared-memory cannot scale as the number of cores per chip becomes large. This problem will likely become significant over the next several years. This motivates the need for multilevel distributed hierarchical shared-memory chip designs that utilize cache coherency techniques and internal high-speed communications between shared memory segments to still provide a consistent shared memory abstraction for applications.Cache coherency enforces the ordering of reads and writes into shared memory. This is almost always essential in developing robust parallel applications. For example, one node might first write a record into shared memory, and then set a flag in another shared memory location indicating that the data has been written. If ordering across the hierarchical shared memory system is not preserved, another node may read the flag incorrectly thinking that the data has already been written and is now safe to read. For performance reasons, some chip designs do not provide automatic cache coherency and rely on the programmer, operating system, or compiler to manually perform memory and instruction synchronizations that preserve the ordering of data being distributed across hierarchical shared memory. Forcing software developers to perform this manually is extremely costly and error-prone. Again, development frameworks and tools should be provided to hide this level of detail from application programmers.Another problem with threads is data localization. Chips may organize memory in such a manner that groups of processors utilize true shared-memory, but then employ hierarchical shared memory and cache coherency to communicate with other processors. If threads do not make an attempt to localize state data, a tremendous amount of thrashing will almost certainly occur as threads freely access and update memory that is not local to their processor. This is similar to message passing systems that cannot achieve significant speedup because of large message passing overheads. There is always a delicate balance between computations and communications, even when using threads.While the use of threads can provide tremendous processing capabilities, they can also introduce serious problems and performance bottlenecks. The optimal parallel computing approach probably combines (a) multiple processes communicating through hierarchical shared memory, and (b) threads within a process using true shared-memory that resides on the chip. This approach requires the ability to map processes and threads to specific processors. Because of the complexity involved, parallel-processing frameworks should be used to hide these low-level details from normal application programmers. For FMS, such frameworks are sometimes known as Parallel and Distributed Simulation Engines.Parallel and Distributed M&S Communications ServicesOne of the benefits of launching independent processes to support parallelism is that both shared memory and network message passing services can be unified into a common integrated communications framework that allows one or more multicore computers to operate as a single parallel computing platform. Achieving parallelism through message passing can support mixed multicore and network computing configurations, but at the expense of slower communications between nodes on different machines. Sometimes, these extra communication overheads are negligible, especially when message passing between machines is infrequent. Careful decomposition becomes important when operating in mixed parallel and distributed configurations. Applications that are communications intensive should probably operate within a single multicore computer.Several high-performance communications libraries for parallel computing have been developed over the years. The two most commonly used libraries are the (a) older Parallel Virtual Machine (PVM) [ REF _Ref94447941 \r \h 75], and (b) newer Message Passing Interface (MPI) [ REF _Ref94448080 \r \h 67]. These communications libraries were primarily designed for synchronous high-performance physics-based applications. They were not designed for FMS applications that require asynchronous discrete-event scheduling capabilities.The Open Modeling and Simulation Architecture (OpenMSA) defines a more optimized High Speed Communications (HSC) library [ REF _Ref76620915 \r \h 101, REF _Ref76620919 \r \h 111] for FMS systems. It was specifically designed to support advanced parallel discrete-event simulation requirements, which are significantly more challenging than the requirements of typical physics-based parallel applications. A reference implementation of this communications library is freely available to United States citizens with source code [ REF _Ref76619063 \r \h 72,  REF _Ref76619066 \r \h 78] for non-commercial use [ REF _Ref94438033 \r \h 105]. It currently supports parallel and distributed communications services on Windows (both XP, and Vista), Mac OS X, Linux, and all other versions of UNIX. Mixed machine configurations are supported.The HSC provides eight categories of services that are necessary to support general-purpose parallel computing applications, and specifically parallel discrete-event simulation for FMS.Startup and shutdown servicesMiscellaneous servicesGlobal synchronizationsGlobal reductionsSynchronized data distributionAsynchronous message passingCoordinated message passingObject Request Broker (ORB) servicesSoftware developers using the OpenMSA do not directly invoke these services. However, they are critical in supporting the underlying event scheduling and time synchronization services provided by the OpenMSA that are required to perform scalable parallel discrete-event simulation. These services are briefly described below.Startup and ShutdownApplications automatically begin parallel execution when they initiate the HSC through its startup interface. The startup service automatically launches all of the necessary processes to support parallel processing [ REF _Ref94448519 \r \h 62]. Shared memory segments are created within the communications infrastructure to coordinate all of the inter-process communications services. Network connections, when executing in a network or cluster environment, are also established during the startup process. When applications have completed their work, the shared memory segments are deleted by the HSC through its terminate interface.Miscellaneous ServicesThe HSC defines a number of miscellaneous services that allow applications to determine their node identifier and the total number of nodes executing in the application. Node identifiers range from zero to the number of nodes minus one. Several other services are provided that allow applications to tune the size of shared memory segments and how they are partitioned into mailboxes for supporting asynchronous and coordinated message delivery. Default values are provided by the HSC and offer excellent performance for most applications.Global Synchronization and ReductionsThe simplest communications operations are the basic synchronization services. Two flavors of these services are supported: (a) barrier synchronization provides blocking synchronization between all processes, and (b) fuzzy barrier synchronization allows applications to enter a fuzzy barrier without blocking. When all processes have entered the fuzzy barrier, the exit criterion is satisfied. Fuzzy barriers are extremely useful for applications that wish to indicate a readiness to synchronize, but would like to continue processing while waiting for the rest of the nodes to synchronize. This unique service is not provided by PVM or MPI and is essential for optimistic event processing algorithms such as Time Warp and others.Global ReductionsGlobal reductions provide a second category of synchronized blocking operations. These operations allow applications to synchronously obtain the sum, maximum, minimum, etc. of integer and double precision values passed as arguments by each node. A set of commonly used operations for integers and doubles are directly provided. A more generic global reduction capability is also provided that allows users to define their own abstract reduction operations on larger classes or structures of data. This is useful when collecting multiple statistics from each node during global time updates.Synchronized Data DistributionThe HSC supports five synchronized blocking services that are commonly used to distribute data between nodes: (1) the scatter operation allows a node to partition and distribute a block of data to receiving nodes, (2) the gather operation is the inverse of scatter and allows a node to collect partitioned data from the other nodes, (3) the form vector operation concatenates buffers from each node to construct a replicated vector on all of the nodes, (4) the form matrix operation allows each node to input a vector of data that is used to construct a replicated matrix on all of the nodes, and (5) the broadcast service broadcasts a buffer from one node to all of the other nodes. These services require the synchronous participation of all nodes in the application. These services are commonly used in parallelizing computationally intensive matrix operations.Asynchronous Message PassingAsynchronous message passing is supported with up to 256 user-defined queued message types. Three types of services are implemented: (a) unicast that sends a message from one node to another node, (b) destination-based multicast that sends a message to multiple nodes specified by a bitfield where each bit represents a node in the application, and (c) broadcast that sends a message to all other nodes. Asynchronous messages do not require pre-allocating buffers. The HSC creates the memory for messages received. As an optional optimization, users can specify how to perform memory allocation for incoming messages. This can significantly improve performance and is automated by the OpenMSA for implementing user-defined event messages.Coordinated Message PassingCoordinated message passing [ REF _Ref94951886 \r \h 2] allows applications to exchange multiple messages between nodes in a synchronized manner. Each node may have an arbitrary set of messages that are sent to other nodes. After each node finishes sending all of its coordinated messages, each node then begins reading incoming messages that were potentially sent by other nodes. A NULL pointer for the message is eventually returned to each node when it can be guaranteed that there are no more coordinated messages to be received by the node. All nodes must participate in this synchronized operation. Coordinated message passing is used in parallel simulation to ensure that each node has received all event-scheduling messages before starting its next event-processing cycle.Object Request Broker (ORB) servicesAn Object Request Broker capability allows applications to asynchronously invoke methods on objects residing on other nodes. The ORB supports user-defined methods with up to twenty strongly type-checked arguments, along with variable-length data that may be appended to the message. The ORB services are layered on top of the asynchronous message passing services. By abstracting away message passing interfaces, the ORB services provide a very user-friendly high-performance communication mechanism. The ORB automatically uses free lists to reuse messages.Logical and Real Time SchedulingFMS applications execute in either logical or real time. Logical time simulations operate as fast as possible, which can be faster or slower than the wall clock. Real time simulations synchronize their event processing to the wall clock. Such synchronization can occur before each individual event is processed or at the boundary of each processing cycle. A logical time simulation with valid models is said to be incorrect if its events are not processed in a causal and repeatable manner. On the other hand, a real time simulation with valid models is said to be incorrect if it cannot sufficiently keep up with the wall clock. Sometimes, real-time simulations operate in scaled time to smoothly speed up or slow down their execution. A scale factor of two, for example, would execute the simulation twice as fast as real time.Logical time simulations require a sophisticated parallel discrete-event simulation engine to obtain both speedup and unconstrained entity interactions. Logical time simulations can be synchronized to the wall clock to support real-time execution. Real time simulations do not require the same level of sophistication in their simulation engine infrastructure, and cannot easily be adapted to support as-fast-as-possible execution. Sometimes a scale factor greater than one is used to speed up real-time simulations, but this still does not achieve as-fast-as possible execution. This approach can also introduce model artifacts when a real-time application begins to reach its performance limits. Models should be able to transparently operate in either logical or real-time execution modes. Model reuse is only possible within a common discrete-event simulation engine that provides a modeling framework capable of transparently supporting all modes of execution.Challenge of Parallel Discrete Event SimulationProviding scalable parallel and distributed logical-time event processing for FMS applications, where any entity can freely interact with any other entity at any simulated time, is extremely challenging. In the worst case, the entity with the earliest time tagged event could arbitrarily schedule an event for any other entity in the simulation at its current event time. This means that no other entity within the simulation would be able to safely process its next event until the current entity finishes processing its event. At first glance, this situation appears to completely serialize event processing, with little or no hope of achieving parallel speedup on multicore computers. Fortunately, several innovative techniques have been developed by the research community to solve this extremely challenging problem.Researchers have aggressively explored two philosophically different approaches over the past twenty years [ REF _Ref76619761 \r \h 33]. These approaches have matured over time. They introduce unique strengths and weaknesses that are now well understood.The Conservative Approach only processes events for entities when it can be guaranteed that no other event with an earlier time tag will be received by the entity. To guarantee this, conservative approaches impose one or more limitations on how entities may interact. Combinations of the following three assumptions are required: (a) entities are only able to interact with specified entities that are identified by a connected graph that is established during the initialization of the simulation and then never changed, (b) events scheduled from one entity to another entity must always be monotonically increasing in time, and/or (c) events scheduled between entities must never occur tighter in time than a specified value known as Lookahead. These three assumptions are almost never valid in FMS applications. They can also introduce significant software complexities and/or fidelity impacts. Most of the time, only assumption (c) can be used, and even this often creates model validation problems. This leads to the second approach.The Optimistic Approach uses a fundamentally different philosophy for obtaining parallelism by aggressively processing events without regard for causal correctness instead of cautiously waiting until it is safe. Rollback techniques are utilized to undo events that might have been processed out of order whenever straggler event messages are received from other nodes. In this manner, events are processed optimistically with the hope that they will not be rolled back. While the optimistic approach places no restrictions on how entities can interact, the biggest drawback is that models must be developed in a manner that supports rollbacks. However, optimistic approaches are able to approach critical path parallelism and sometimes even more when communication and rollback overheads are negligible when compared to event processing times.The OpenMSA and OSAMS define a highly optimized rollback framework that is designed to support a wide variety of data types, container classes, random number generators, I/O, and other miscellaneous operations. Incremental state saving techniques rapidly undo event-processing operations in the reverse order that they were performed. Refactoring existing models to run optimistically in a common parallel discrete event simulation engine can be a challenge. However, new models that are developed should be designed to support rollbacks and optimistic processing. Rollback overheads are automatically removed when running conservatively.Causal and repeatable event processing for each entity in logical-time FMS applications must be preserved. This should be independent of the number of processing nodes are used during the execution of the FMS application. The next section discusses how to represent time in a manner that guarantees repeatable results, even when operating in a parallel and/or distributed manner on arbitrary numbers of nodes. Then, an overview of the most commonly used time synchronization techniques is provided along with a discussion of their relative strengths and weaknesses.Representation of TimeSimulations evolve entity models over time, so the notion of simulated time is fundamental. Events are scheduled to occur at specific time values, sometimes known as the event’s time tag. Repeatability problems can occur when multiple events are scheduled for the same entity at the same time. If special care is not taken, the events can be processed in arbitrary order, resulting in non-repeatable results. As stated earlier, the results of parallel applications must be repeatable and independent of the number of nodes used during execution.Some might argue that the event-processing order should not matter if multiple events are scheduled to occur at the same time for the same entity. Even if this were true from a pure modeling perspective, the arbitrary ordering of event processing for simultaneous events can affect repeatability. For example, generating random numbers while processing each event would likely result in different outcomes. As a general rule, time managed simulations must produce the same exact results independent of the number of processors used. Otherwise, it would be impossible to truly verify, validate, and accredit models. So, the repeatability problem must be carefully addressed.One approach commonly used to preserve repeatability is to place the burden of time ordering on the model developer. Tiny epsilon increments are added or subtracted from time during event scheduling to manually produce unique ordering. This fudge-factor approach breaks down quickly as models and interactions become complex, especially when many events can potentially be scheduled for the same entity at the same time. From a modeling perspective, it is fundamentally wrong to tweak calculated event times in this manner. Events should occur at their calculated times. Such approaches can lead to difficult-to-debug software errors, especially when exact event times are computed for physical events such as entering a grid cell, determining when two entities are within interaction range, etc. Another common use of epsilon is to support event scheduling in parallel and distributed logical time systems where true zero-delay event scheduling  is not actually supported. Fortunately, the use of epsilon is not required here either.The best way to guarantee repeatability and to support zero-delay event scheduling [ REF _Ref94952533 \r \h 32] is to provide a standard abstract representation of time that guarantees all scheduled events have unique time tags. Special integer tie-breaking fields guarantee repeatable and causally correct ordering for event time tags with the same logical time value. This is accomplished by the following abstraction, summarized in  REF _Ref91657944 \h Table 1.Table  SEQ Table \* ARABIC 1: Abstract representation of time.FieldNameTypeDescription1TimedoubleLogical time of the event2PriorityintegerUser settable priority3CounterintegerEntity event counter 4UniqueIdintegerUnique Id for each entityFirst, all entities are assigned a unique identifier (UniqueId) as they are constructed. Entities automatically save their UniqueId in time tags, as events are scheduled. Including the UniqueId in the abstract time representation guarantees that all events scheduled by different entities within the simulation have unique time tags.Second, all entities manage a special Counter that is incremented as events are scheduled. Entities automatically save their Counter in the time tag as new events are scheduled. Including the Counter in the abstract time guarantees that all events scheduled by an entity will be unique. Together, the UniqueId and Counter tie-breaking fields guarantee that all event time tags within the simulation are unique.Third, users must be able to manually set priorities for events scheduled at the same time. While this is not required for uniqueness, often event sequences between entities must occur at the same logical time, but in a coordinated order established by the modeler.Finally, the logical time of the actual event must be included. This is normally represented as a double precision value with units of seconds past midnight of some start date. Usually, Greenwich Mean Time (GMT) provides a global time reference.One further situation must be handled to support correct and repeatable event processing. Suppose EntityA has a Counter value of 50, and is currently processing an event that was scheduled by EntityB whose Counter value was 100. If the event schedules a new event for the same Time and Priority, the Counter must jump to 101 instead of incrementing by one to 51. Otherwise, time would actually go backwards for events scheduled with zero delay, which is not allowed. All time ordering and causality problems are completely solved by this abstract representation of time.In C++, the abstract time can be implemented as a class containing these values. Tie breaking for event ordering is performed in the following steps: (1) logical Times are compared first. If the logical times are exactly the same, then (2) user-defined Priorities are compared next. If the Priorities are the same, then (3) the Counters are compared. If the Counters are the same, then (4) the UniqueIds are compared to provide final tie breaking, which is guaranteed to be unique.Because C++ supports operator overloading and default conversion operators, the abstract time class can behave as a double precision value when used in mathematical calculations, but with unique event-ordering properties. Unless users care about setting their own priority fields, the abstract time class simply behaves as a double precision value. The internal machinery that guarantees event-processing repeatability is completely hidden from the user within the OpenMSA simulation engine because it automatically sets the UniqueId and Counter tie-breaking fields after events are processed.Conservative – Time SteppingThe simplest conservative parallel event processing strategy is Time Stepping [ REF _Ref76619761 \r \h 33]. This is sometimes also referred to as the Lock Step approach to parallel simulation. All event processing occurs in lock-stepped update cycles. The event actually updates all entities residing on the node. The basic processing loop performed by each node in the simulation has the following form.Code Segment  SEQ Code_Segment \* ARABIC 2: Basic processing loop for time stepped simulations.for (time=0; time<EndTime; time+Step) {  UpdateState();  Communicate();}The Update function updates the state of each entity on each node. After the update is completed, relevant information about each entity is communicated to the other nodes. The communication step can be simple or complex depending on the sophistication of the data distribution and remote entity representation mechanisms.Time stepping has several fundamental flaws. First, time cannot take on any value, but instead is locked into discrete time steps. This unnatural representation of time makes it difficult to manage physical events that occur at arbitrary points in simulated time. An example might be a weapon detonation that logically occurs between time steps. Models must provide a way to handle such physical events that occur at arbitrary times.Second, it can be extremely inefficient to update all entities every time step. Oftentimes, entities do not need to be updated. Other times, behaviors within entities need to be updated more frequently and asynchronously, not in lock step. A way around this problem is to add special logic to determine which entities actually need to be updated during each step. However, this approach can cause extremely poor load balancing problems when executing in parallel. For example, imagine a simulation running on 100 nodes with 100 entities per node. At one step, node 53 might have the most work to perform with 47 entities to update. The next step, node 53 only needs to update 6 entities, while node 39 now has 64 entities to update. It is easy to see how this approach is able to reduce processing time, but can become very inefficient in terms of parallelism. In a sense, time stepping with variable entity updates is really trying to become discrete event. It makes more sense to simply use a true parallel and distributed discrete-event simulation engine to support general event processing.Two basic strategies are used to synchronize time-stepped simulations to the wall clock for real-time support. The simplest approach is to measure how long it takes in real time to perform each time step and then increment simulation time for the next cycle by that amount. This approach uses a variable time step that does its best to update entity states as often as possible in real time. Large unacceptable errors can arise if update times become too large. Computer performance problems can bias results. A second approach is to fix the time step and either (a) hope that all entities can be updated in the allotted time step, or (b) simply quit updating entities when the allotted time is up. Updating entities is prioritized by the last update time to ensure all entities are regularly updated.Conservative – Time Windows and LookaheadRather than updating in lock step, a much better conservative strategy is to allow events to be scheduled and processed asynchronously at arbitrary times, but to enforce a simple constraint. Events scheduled by an entity for another entity must never occur tighter in time than a specified value known as the global Lookahead, L, of the simulation. In other words, if EntityA is at time T, then the earliest it can schedule an event for EntityB is T+L. Processing events in parallel using Lookahead-based Time Windows is sometimes also called Fixed Time Bucket (FTB) synchronization [ REF _Ref76619557 \r \h 92], shown in  REF _Ref92188330 \h Figure 3.Like the time-stepped approach, event processing is performed in cycles. However, events are now able to occur asynchronously at any point in time. As mentioned previously, the lookahead constraint for event scheduling between entities is actually no more limiting than the time-stepped approach, but it offers a much more efficient mechanism for updating entities. In addition, self-scheduled events are not required to use lookahead. This allows entities to update their state whenever needed without regard for time steps or lookahead constraints.Figure  SEQ Figure \* ARABIC 3: Fixed Time Buckets use lookahead to process events concurrently in synchronized time windows.To better understand why lookahead-based time windows work, imagine that an event at the start of a cycle schedules an event for another entity. In the worst case, the time tag of the new event must be scheduled at least L units of time into the future. Therefore, it is safe for each node to process events L units of time from the start of the cycle into the future before globally synchronizing and communicating messages between nodes. If after synchronizing, it turns out that the earliest global event time in the simulation is further out into the future, the start time of the next cycle can jump forward to that time. The jumping forward trick completely solves the time creeping problem, which can be significant for small Lookahead values that approach zero.The biggest problem with time windows is that Lookahead still constrains how tightly entities can interact. There are circumstances when model validity demands immediate interactions between entities. For example, imagine a sensor/tracking system forming detections of moving entities. If an entity changes its motion, the sensor will have the wrong position for L units of time before it snaps back to the correct value. This can be unacceptable for testbeds studying advanced tracking algorithms such as Kalman Filters in a simulated environment. One might argue that the solution is to simply reduce the lookahead value. However, the opportunity for parallel speedup significantly diminishes when lookahead gets small. In the worst case, the simulation completely serializes if lookahead becomes zero. A tradeoff must be made to choose a reasonable value for L that minimizes modeling artifacts, while making L large enough to obtain parallel speedup. Sometimes this tradeoff cannot be reasonably achieved, leaving the user with no workable solution.Time Windows can also suffer from load balancing problems. Event processing performed in each cycle takes as long as the slowest node, which can vary from cycle to cycle. Like the time-stepped approach, global time can be synchronized to the wall clock to support real-time operation.Conservative – TopologyTopology-based synchronization [ REF _Ref94432049 \r \h 15] can be useful for simulations involving queuing systems with rigid interaction topologies having sparse interconnections. Examples might include (a) emulating the logical operation of complex circuit boards, (b) simulating communications of local and/or wide area networks with fixed interconnections, (c) modeling production-lines in manufacturing factories, and (d) analyzing the processing and communication performance of parallel computers.Conservative topology-based event processing is not very useful for FMS applications because physical battlefield entities are generally able to interact with all other battlefield entities. In other words, the topology for FMS applications is not a sparse graph, but instead a fully interconnected graph. Awkward attempts have been made to apply topology-based synchronization for FMS problems using geographical grids to manage battlefield entities that migrate from grid to grid. However, such attempts significantly complicate and limit the ability to represent truly complex entities that are hierarchically composed of model components. Previous work has resulted in producing only toy demonstrations that cannot be easily extended to efficiently and intuitively model the actual behaviors of complex battlefield entities. In addition, parallel performance has not been notable.Topology-based synchronization uses the basic notion that each entity (sometimes called Logical Processes, or LPs) has a set of input queues with events that were scheduled by other entities. Entities are always allowed to schedule events for themselves. An example of a topology-based simulation involving seven entities is shown in  REF _Ref92193815 \h Figure 4.Figure  SEQ Figure \* ARABIC 4: An example of a topology-based simulation of seven entities. In this example, EntityA is a pure event-scheduling source, while EntityG is a pure event-receiving sync. The other entities schedule and receive events from other entities and themselves as defined by the graph topology.A First In First Out (FIFO) scheduling constraint is normally required to preserve monotonically increasing logical times for events scheduled by one entity for another entity. In other words, the events scheduled by EntityA for EntityB must always increase in time. It would be incorrect for EntityA at time 10 to schedule an event for EntityB at time 20, and then later EntityA at time 12 schedules a subsequent event for EntityB at time 18. Event crossing between entities is not permitted. The FIFO constraint also assumes that the communication framework preserves message ordering for delivery between nodes, which imposes an additional requirement on the message-passing framework. The input and output queues for EntityD in  REF _Ref92193815 \h Figure 4 is shown in  REF _Ref92194119 \h Figure 5.In addition to the topology and FIFO constraint, lookahead can be applied on each link to help entities advance time for other entities. As an important improvement over time windowing, lookahead values are allowed to be different for each link between entities. Rather than impose a global lookahead value defined as the minimum lookahead across all links, each link itself employs its own lookahead value. This can significantly improve performance when lookahead values vary from link to link, but potentially at the expense of larger synchronization overheads.Figure  SEQ Figure \* ARABIC 5: Input and output queues for EntityD. Note that all entities are permitted to self-schedule events for themselves.In the simplest case, an entity knows it is safe to process its next event if all of its input queues have at least one pending event. The entity simply processes the event in its set of input queues with the earliest time tag. As events are processed and new events are scheduled, other entity input queues may become populated enabling further event processing. However, it is possible for situations to arise where no entities are able to process their next event because one or more of their input queues are empty. Deadlocks can occur if this situation is not handled correctly.Various solutions have been proposed using special NULL time-synchronization messages to update time between entities. A simple NULL message approach transmits the new time of an entity plus lookahead to its links whenever an event is processed. The NULL message indicates the earliest time a receiving entity’s input queue might receive an event from the first entity. If the earliest actual event for an entity receiving the NULL message is earlier than the minimum time of all empty input queues, then it is safe for the entity to process its next event. While this approach provides a more aggressive strategy for updating time, it introduces more communication overhead and still does not completely solve the deadlock problem.NULL messages can generate other NULL messages when an entity is not able to process an event, but able to update the earliest time it might schedule an event for other entities. In this manner, NULL messages are able to propagate between entities to break deadlocks and eventually allow entities to process their next event. Further deadlock or time creeping complexities arise when lookahead values are zero. Feedback loops also present interesting problems. NULL synchronization message explosions have been observed, making the topology-based approach very inefficient and potentially unstable.A further problem with topology-based synchronization is that entities can process events at widely varying times. Event processing is not performed in strict time order within a node, but instead based on whichever entity is able to process its next event. Entities can be widely out of sync in terms of simulated time from each other. This situation makes it difficult to exchange information with external systems such as graphical displays, hardware in the loop, and humans being trained.One of the benefits of topology-based synchronization is that event queues are FIFO, which eliminates the need to sort events within each queue. This performance benefit, unfortunately, does not outweigh all of the other problems. It can also be insignificant when event computations are significant, which is often the case for FMS applications. Because of its many problems, topology-based synchronization does not provide a good solution for FMS applications.Optimistic – Time WarpConservative synchronization techniques have been discussed up to this point that cautiously process events only when it is safe to do so. Although there are several conservative synchronization techniques, they all basically employ the same principles (fixed time steps, exploitation of global and/or link-based lookahead, fixed entity interaction topologies, NULL synchronization messages, deadlock handling, etc.). As previously discussed, none of the conservative techniques fully provide unconstrained scalable run-time performance without adversely impacting model validity. This paper now explores optimistic strategies, starting with the original Time Warp algorithm [ REF _Ref54183639 \r \h 50].In Time Warp, events are processed optimistically with the hope that they will be executed in their correct time order. Rollback techniques correct out-of-order event processing when straggler messages are received. A good Time Warp event management framework automatically provides an efficient rollback infrastructure for each entity. The simulation time of each entity is defined as the time tag of its last processed event. When an entity receives a straggler event in its past, it rolls the entity back to its last correctly processed event before processing the new event. Events that were rolled back are either reprocessed, or rolled forward whenever possible using lazy cancellation and/or lazy reevaluation techniques [ REF _Ref94955940 \r \h 88, REF _Ref94956533 \r \h 113]. An example of an entity on a node scheduling an event in the past of another node is shown in  REF _Ref91670124 \h Figure 6.Figure  SEQ Figure \* ARABIC 6: Optimistic event processing. An event on node 0 sends an event-scheduling message to node 1 in its past. Rollbacks are required to undo those events that were incorrectly processed out of order.A good optimistic simulation framework does not rollback the entire node when an entity receives a straggler message (see  REF _Ref92440504 \h Figure 7). Instead, only the affected entity is rolled back. Of course, during the rollback, all events scheduled by those events that were rolled back must also retracted, potentially causing secondary rollbacks. Each event must therefore keep track of its generated events until the event itself is committed. Retraction messages, used to withdraw incorrectly scheduled event messages in Time Warp, are called Antimessages. Special optimizations are normally provided for locally scheduled events to directly retract those events that were scheduled for entities residing on the same node. This is discussed in more detail later in this section.A state saving mechanism must be provided to support rollbacks as straggler messages are received. Rolling back an event returns the entity to the state it had before the event was processed. One state-saving technique that is commonly used for rollbacks is to save a full copy of the entity’s entire state before the event is processed. This approach is called full state saving [ REF _Ref94956086 \r \h 52]. Rollbacks due to the arrival of a straggler message is accomplished by returning the state of the entity back to its earlier value just before the time tag of the straggler. This approach, however, does not scale well when entity states are large or fragmented across many memory segments, which is common for FMS applications. Full state saving can also become extremely complicated with excessive overheads when dynamic memory and data structures like linked lists or trees are used. It is computationally wasteful to save large amounts of state data for events that only modify small fractions of the state. The full state saving method also has one other fatal drawback. It can very quickly consume all of the available memory on a processor and thereby force either page faulting, or even program crashes as disk swap space becomes filled. This can seriously impact the performance of optimistic military simulations and should therefore not be used.Figure  SEQ Figure \* ARABIC 7: Rollbacks are performed on individual entity objects, not on the entire node. In this example, only two events for Obj-7 are actually rolled back on Node 1. This figure illustrates the rule of thumb that more objects per node produces less rollbacks. For example, if there were 1000 objects per node, then a node could be 1000 events ahead of another node, receive a zero-delay event, and on the average only experience one rollback. Secondary rollbacks may occur, however.A much more efficient approach to support rollbacks for FMS applications is incremental state saving [ REF _Ref94956257 \r \h 80, REF _Ref94955940 \r \h 88]. With incremental state saving, state-modifying operations performed by events transparently generate rollback items that are automatically collected in a queue managed by the event’s rollback manager. These rollback items keep track of affected state and allow each operation to be rolled back in the reverse order that they were performed.To support incremental state saving, entity states must be composed of rollbackable data types, container classes, memory allocations, etc. Most of this can be made transparent in C++ using rollbackable data types to represent primitive variables such as integers, doubles, Booleans, strings, etc. Operator overloading allows these data types to operate as their primitive counterparts, except that they save rollbackable state information whenever modified. Other operations are easily made rollbackable through reversible algorithms. For example, if an object were inserted into a rollbackable tree, a rollback would simply remove the object from the tree. A rollforward would then reinsert the object back into the tree. Reversible operations nicely fit the incremental state saving paradigm and are generally much more efficient than full state saving strategies.Global Virtual Time (GVT) [ REF _Ref54183639 \r \h 50, REF _Ref95218135 \r \h 91] is defined as the time tag of the earliest unprocessed event or message within the simulation that is still in transit. Because events are normally never allowed to schedule new events in the past, events that are processed with time tags less than GVT can be committed. In other words, those events with time tags less than GVT have been correctly processed and will never be rolled back. Therefore, it is safe to reclaim memory, etc., for all events with time tags less than GVT. The goal is to update GVT across all nodes as often as possible without bogging down the processing of the simulation with excessive synchronization overheads.Besides reclaiming memory resources, there is another important reason for updating GVT as often as efficiently possible. Because humans in the loop, hardware in the loop, external modules, visualization tools, etc., do not participate in rollbacks, messages sent outside the core of the parallel simulation to the external world must always be valid. Valid messages from an event can only be safely released when the event is committed, or if the event’s time tag is actually GVT. If GVT is computed every 50 ms in a real time interactive simulation, then interactive response times at a human level can be supported very tightly. On the other hand, if GVT is only updated every 5 seconds, then interactive response times will be sluggish [ REF _Ref76619911 \r \h 109]. It is therefore important for optimistic simulations to use an efficient and scalable GVT algorithm. This is also an important motivating factor for why parallel simulation should be hosted on true parallel machines with shared memory, ultra-high-speed communications, or at least on a local area network where latencies are manageable.People often mistakenly claim that optimistic simulations cannot support real-time interactive simulations. Nothing could be further from the truth. In fact, in most circumstances, optimistic simulations are superior to conservative simulations in supporting real-time interactive execution [ REF _Ref76619554 \r \h 85, REF _Ref76619906 \r \h 87]. Assuming that the simulation can run faster than real time, GVT is simply throttled to the wall clock. Messages are safely released to the outside world, as events are committed.By processing events optimistically, expensive computations can be amortized over time. In conservative window-based simulations, all events within the window time step must be processed in real time. An expensive computation might violate this constraint, causing the simulation to fall behind in real time. Optimistic simulations handle this situation better that conservative simulations because expensive computations can be performed ahead of time. In addition, messages received from the outside world can be time tagged to GVT when they are received, again, leading to tighter interactions between the simulation and external systems.Figure  SEQ Figure \* ARABIC 8: Rolling back an event in Time Warp. Here, a straggler message arrives for an object in the past of several erroneously processed events. Time Warp rolls back each erroneously processed event and then processes the straggler. As each event is rolled back, antimessages may be generated, which can cause further rollbacks.When an event is processed, it might generate new events. These new events are scheduled by sending messages. Events scheduled for entities residing on the same node bypass the communications framework and are directly handled by the event management framework. When an event is rolled back, the entity must be restored to its original state. Antimessages are generated to retract any messages that were sent to other nodes. This means that Time Warp entities must maintain retraction information concerning the messages that were generated during the processing of each event. The steps in rolling back an event are shown in  REF _Ref91670176 \h Figure 8. REF _Ref91670218 \h Figure 9 shows the steps involved when an antimessage is received for an event that has already been processed. The entity is rolled back to the point of the retracted event. Then, the retracted event is removed from the event list of the entity. Of course, as events are rolled back due to the arrival of antimessages, they too might have incorrectly generated messages that must be canceled by releasing yet further antimessages. Secondary antimessage generation is known as cascading antimessages.Figure  SEQ Figure \* ARABIC 9: Handling antimessages in Time Warp. Here, an antimessage cancels an event that has already been processed by the simulation object. Several events might have to be rolled back in order to cancel this event which can cause further antimessages to be released.Unbridled optimistic event processing can lead to instabilities that are sometimes observed in Time Warp. Rollback and cascading antimessage explosions can occur for simulations executing on large numbers of nodes. For example, in one simulation study, Time Warp operating on 32 nodes generated approximately 250,000 messages that were later retracted by 230,000 antimessages. In other words, almost a total of 500,000 unnecessary messages were transmitted during the execution of the simulation. The simulation only committed approximately 30,000 events. The result of this Time Warp execution was extremely poor performance.While this gross behavior may not always be exhibited, Time Warp without flow control can become unstable when the load balancing is not perfect, when scheduling many events in the near future, or when fan-in and/or fan-out event scheduling transactions are required. Without providing effective flow control mechanisms, Time Warp in its basic form does not satisfy all of the performance goals of FMS on multicore computers. It can become highly unstable when excessive rollbacks and antimessages are generated, and therefore may not scale well under adverse conditions. This motivated the development of the Breathing Time Buckets (BTB) and Breathing Time Warp (BTW) algorithms that are discussed next.Optimistic – Risk Free and the Event HorizonAn algorithm that completely solves the instability problems sometimes exhibited in Time Warp is the Breathing Time Buckets [ REF _Ref76619554 \r \h 85] algorithm. Breathing Time Buckets is a cross between the Fixed Time Buckets algorithm and Time Warp. Like Time Warp, Breathing Time Buckets processes events optimistically. However, unlike Time Warp, messages generated while processing events are never actually released until it is known that the event generating the messages will never be rolled back. This means that bad messages are never released so that antimessages, along with the overhead required for their support, are not required. While messages sent by Time Warp have maximal risk of not being valid, messages released by Breathing Time Buckets are sent in a risk-free manner. Breathing Time Buckets is Time Warp without antimessages.Breathing Time Buckets is also like Fixed Time Buckets in the sense that events are processed in cycles. However, unlike Fixed Time Buckets, Breathing Time Buckets does not require knowledge of a global lookahead value. Instead, it determines the optimal time window in an adaptive manner as events are optimistically processed during each cycle. Thus, cycles are not fixed, but instead fluctuate, or "breathe”.The fundamental concept behind Breathing Time Buckets is the Event Horizon [ REF _Ref77669959 \r \h 90]. The event horizon is a concept that has meaning even for sequential simulations executing on a single processor. Consider the sequential simulation shown in  REF _Ref91670303 \h Figure 10. As events are processed, they may generate new events. If those new events were collected in a separate list, the simulation would eventually get to the point where its next event to be processed is in the separate list. The time tag of this next event is called the event horizon. The event horizon is the point in time where events generated by the simulation fold back into the simulation. This is similar to the notion of the event horizon for black holes where gravity pulls back any particles trying to escape [ REF _Ref95219233 \r \h 48]. At the event horizon, those new events could be sorted and merged back into the main event queue.Figure  SEQ Figure \* ARABIC 10: The event horizon. Events are processed in cycles, with the next cycle boundary defined as the earliest time tag of a newly generated event in a current cycle.The key concept exploited by Breathing Time Buckets is that the events processed in each event horizon cycle could have been processed in parallel since, by definition, no straggler messages arrive with time tags earlier than the event horizon in each cycle. The trick, then, is to adaptively determine the global event horizon as events are processed. Optimistic event processing and fuzzy barriers provided by the communications framework are used to adaptively determine the event horizon on the fly. The net result is the Breathing Time Buckets algorithm.Determining the event horizon on a single processor is not very difficult. It is much more challenging to find the event horizon when executing in parallel. For now, assume that each node is allowed to process events until its local event horizon is crossed. At this point, all nodes will have processed events up to their local event horizon and are ready to synchronize.The next step is to compute the global event horizon as the minimum local event horizon across all nodes. This is accomplished using the GlobalMinimum reduction operation that is provided by the HSC. In other words, the earliest time stamp of a message waiting to be released is determined. By definition, global event horizon is GVT. Once GVT is determined, all events with time stamps less than or equal to GVT are committed. This means that event messages that were generated by events having time tags less than or equal to GVT are transmitted using the coordinated message service to the appropriate node containing the receiving entity. When messages arrive at their destination nodes, they create their corresponding events and are queued up for processing. The basic event processing cycle is depicted in  REF _Ref92446012 \h Figure 11.There is an obvious problem with what has been described so far. Some of the nodes may have processed events that went beyond the true event horizon. An event processed by an entity must be rolled back when a newly generated events is received in its past. Rollback involves discarding unsent messages that were generated by the event and then restoring state variables. Rollback overheads in Breathing Time Buckets remain small. Antimessages are never required because bad messages that would create bad events are never released.Figure  SEQ Figure \* ARABIC 11: The Breathing Time Buckets event-processing cycle. Each node processes events without releasing newly generated event messages. The global event horizon, which is also GVT, is defined as the minimum local event horizon across all nodes. Events with time tags less than GVT are then committed, which releases their messages. Events processed beyond GVT may be rolled back as straggler messages are received, causing unsent messages to be discarded. Breathing Time Buckets processes events optimistically, but with risk-free message passing that provides stability through flow control. Cascading antimessage and rollback explosions never occur.The Breathing Time Buckets algorithm does not end here. Pathological situations could still arise if the algorithm were not modified. The problem with Breathing Time Buckets, as presented so far, is that all nodes wait for the slowest node to determine its local event horizon. A modification to the basic algorithm is needed to circumvent this problem.An asynchronous broadcast mechanism rapidly informs all nodes when a local event horizon is crossed. When the first node reaches its local event horizon, it asynchronously broadcasts its local event horizon to all other nodes. When a node receives an asynchronous broadcast message, it may determine that it has gone beyond the point in time of the other node’s event horizon; thus, it should stop processing. On the other hand, the node may not have reached that time yet, so processing will continue. It is possible for the first node to cross its local event horizon to have a greater value for the event horizon than subsequent nodes. When this happens, other nodes will broadcast their time as well. Multiple broadcasts may occur within each cycle. These broadcasts are staggered to avoid congestion.It is important to get a proper view of the broadcast mechanism. Runaway nodes that process beyond the true event horizon while the rest of the nodes are waiting can ruin the performance of the Breathing Time Buckets algorithm unless something is done. The proper view of the broadcast mechanism is that it aids in speeding up determining the event horizon by quickly stopping runaway nodes. The asynchronous broadcasts are in no way required by Breathing Time Buckets to rigorously support causal event processing. The broadcasts operate in the background and only aid in enhancing performance.With the asynchronous broadcast mechanism designed to stop runaway nodes, the Breathing Time Buckets algorithm becomes a viable solution to support general-purpose discrete event simulations. However, there still is room for improvement. It is wasteful for nodes that have crossed their local event horizon to be idle while waiting for other nodes to complete their processing. Note that this problem always arises in the world of synchronous parallel computing. It is important to evenly balance the workload on each node so the time spent waiting for the slowest node to finish its job is minimized.The Breathing Time Buckets algorithm, as described so far, suffers from this same “waiting” problem. An observant simulation developer might ask, “Why do you insist on stopping just because the event horizon has been crossed?” In fact, there really is no reason to stop processing events until all nodes have crossed the horizon! Erroneously processed events can always be rolled back without much overhead (because no risky communications are involved). Therefore, it does not hinder performance to continue processing events beyond the event horizon. It might pay off to be optimistic with the possibility that processed events with time stamps greater than the event horizon will not be rolled back. The trick then is to efficiently determine when all the nodes have finished.This is where fuzzy barriers provided by the HSC are used. Each node enters the fuzzy barrier when it determines that it has crossed the event horizon. However, each node keeps processing its events until the last node enters the fuzzy barrier. This triggers the exit fuzzy barrier criteria. All nodes then proceed to determine the true event horizon, which becomes the next value for GVT.One final improvement is made to the Breathing Time Buckets algorithm. Events that are generated for entities residing on the same node do not participate in the event horizon calculation. Rather, they are inserted into the event list and possibly processed within the same cycle. This capability is extremely important for efficiently supporting self-scheduled events at arbitrary time scales.The one drawback to Breathing Time Buckets is the possibility that not enough events will be processed on the average during each cycle to remain efficient. Keep in mind, however, that Breathing Time Buckets will always process at least as many events per cycle as the Fixed Time Buckets algorithm. Like Time Warp, Breathing Time Buckets places no event scheduling constraints such as lookahead or topology limitations on the programmer, but its performance will be poor if very few events are processed on the average per cycle. This problem is exacerbated by simulations that frequently interact with objects on other nodes at very tight time scales.Because of this problem, Breathing Time Buckets does not scale well for many times of FMS applications that frequently require tight interactions between entities. In other words, for simulations that do not process enough events per cycle, too many synchronizations would be required, which would impact scalability and performance. Fortunately, there is a solution to this problem, known as Breathing Time Warp.Optimistic – Flow ControlThe main problem exhibited by Time Warp is that messages are released with very high risk, which can cause rollback and cascading antimessage explosions. Breathing Time Buckets, being completely stable due to its risk-free message sending strategy, introduces a different problem of synchronizing too often. These two algorithms can be combined into a new algorithm, known as Breathing Time Warp (BTW) [ REF _Ref76619710 \r \h 86], which solves all of the instability problems without requiring excessive synchronizations. Some general considerations are provided, before discussing the BTW algorithmCascading antimessage explosions can occur when runaway nodes get too far ahead of the rest of the simulation. As a general rule, events processed far ahead of the rest of the simulation tend to have a higher probability of being rolled back than events close to the current GVT. Because events processed far ahead of the rest of the simulation will likely be rolled back, it might be better for those runaway events to not immediately release their messages. That way, if they are rolled back, the network does not become flooded with large numbers of antimessages that are used to retract large numbers of erroneously sent messages.Rollbacks on runaway nodes usually do not affect the critical path of the simulation since the runaway node is far ahead of the rest of the simulation and can afford the luxury of extra rollback overheads. However, bad messages that are later canceled by antimessages can affect the critical path of the simulation because (a) the slower nodes must still process messages and antimessages as they are received, and (b) the transmission of unnecessary messages and antimessages can waste precious bandwidth in the communication network, which can potentially delay the receive time of messages that are on the critical path.As a final flow control mechanism, when a runaway node gets too far ahead of the rest of the simulation (in terms of numbers of events processed beyond GVT), it probably makes sense to temporarily stop event processing altogether on the runaway node until the rest of the simulation catches up. Without going into too much detail, the main ideas behind the Breathing Time Warp algorithm are summarized below.The first Nrisk events processed locally on each node beyond GVT release their messages right away as in the Time Warp algorithm. After that, messages are held back and the Breathing Time Buckets algorithm kicks in. When Ngvt events are processed, or when the event horizon is determined, each node requests a GVT update by entering the fuzzy barrier. The actual GVT update occurs when all of the nodes have entered the fuzzy barrier, which triggers the exit fuzzy barrier notification. If a node ever processes Nopt events beyond GVT, it temporarily stops processing events until the next GVT cycle begins. Nrisk, Ngvt, and Nopt are user-settable flow control parameters. Default values of 500, 1000, and 2000 usually provide excellent scalable performance. Adaptive techniques can be used to optimize these values while the simulation is running. An example of a typical processing cycle for a four-node execution is provided in  REF _Ref91671050 \h Figure 12.Figure  SEQ Figure \* ARABIC 12: A typical processing cycle in the Breathing Time Warp algorithm. Each node processes events with message-sending risk in the Time Warp mode until Nrisk events are locally processed beyond GVT. Then events are processed in a risk-free manner using the Breathing Time Buckets algorithm until either the event horizon is crossed or Ngvt events have been locally processed beyond GVT. At this point, GVT is updated and events are committed. All messages from committed events that were previously held back are released before the next cycle begins.Breathing Time Warp provides all of the necessary flow control for both controlling message-passing risk and optimism. Performance is only limited by the amount of parallelism in the simulation itself, and not by the underlying synchronization algorithm.Real Time SchedulingAs discussed in previous sections, both conservative and logical time simulations can be synchronized to the wall clock to support real-time or scaled-time execution. A very different strategy is to completely abandon the notion of logical time and repeatability altogether. Instead, an event-scheduling framework can be used to manage best-effort event processing in real time. This interesting approach extends parallel simulation technology into the domain of hard and/or soft real-time scheduling [ REF _Ref95218647 \r \h 13] where non-simulation applications ranging from tightly synchronized embedded systems to loosely synchronized interactive systems operate.This simple extension could provide a common Unified Technical Framework (UTF) [ REF _Ref94431155 \r \h 104] for hosting both real-world services and models that can operate in logical and/or real-time. The benefits of a common architecture include (a) full interoperability between models and real-world services, (b) scalable high performance computing using state-of-the-art parallel and distributed event-scheduling technologies, (c) lower costs for software maintenance, (d) automatic interoperability with other mainstream standards such as HLA, DIS, and TENA, (e) integration with Service Oriented Architecture (SOA) web technologies, and (f) reduced verification and validation costs [ REF _Ref22196322 \r \h 25, REF _Ref76624055 \r \h 73, REF _Ref76624086 \r \h 81].Emerging Technology – Five Dimensional SimulationThis section on scheduling algorithms would not be complete without discussing emerging technologies. A new simulation technology, known as HyperWarpSpeed [ REF _Ref75509448 \r \h 102], has been recently developed that allows applications to model the behavior of complex systems within a five dimensional simulated universe. The ramifications of this new predictive technology for decision making is staggering.Normally, computer simulations are able to represent modeled systems in three spatial dimensions plus time. This is known as four-dimensional simulation. HyperWarpSpeed is able to internally spawn multiple behavior timelines at key decision or branch points during a single execution of the simulation. Coordinating how alternative timelines mutually interact within the five-dimensional world is fully automated through new multidimensional state management and timeline splitting/merging algorithms. Models transparently operate and interact in a five dimensional overlapping parallel universe as they explore alternative timelines.Perhaps a good way to understand this technology is to consider a chess game, where instead of two players taking turns exploring possible moves, you had, instead, an unlimited number of players potentially exploring possible moves whenever necessary. HyperWarpSpeed is able to support this kind of capability for some FMS applications with processing speeds that are orders of magnitude faster than any other known approach.Instead of running hundreds or thousands of independent simulation excursions to explore command decisions or uncontrollable stochastic outcomes, HyperWarpSpeed can obtain faster and more accurate results within a single execution. Because independent computations that are common between parallel timelines are shared whenever possible, the execution time can be orders of magnitude faster than traditional methods.HyperWarpSpeed was designed to run on emerging multicore computing architectures. All computations are reversible, which means that this technology can also support real-time estimation and prediction [ REF _Ref76621457 \r \h 37, REF _Ref54183907 \r \h 123, REF _Ref54183595 \r \h 58] with live data feeds that continually rewind and calibrate the modeled system [ REF _Ref54184575 \r \h 99]. Predictions are always based on best estimates of the dynamically changing current world state. HyperWarpSpeed may be an ideal strategy for supporting Dynamic Situation Assessment and Prediction (DSAP) for command centers of the future [ REF _Ref76619272 \r \h 61].United States National Intelligence requires revolutionary technology to rapidly and continually predict uncertain future outcomes based on live intelligence net-centric [ REF _Ref22197697 \r \h 70] data feeds. It is critical for the U.S. to provide this capability faster than its adversaries. HyperWarpSpeed enables warfighting decision makers to more rapidly outthink their enemies.Scalable Publish and Subscribe Data DistributionOne of the most challenging computational problems in FMS is determining which entities are able to interact with which other entities. In most cases, entities are only able to interact when they are close enough to each other to (a) perform sensor detections, (b) mutually communicate, and/or (c) exchange weapon fire. Other additional factors, such as emitting radio frequencies, radar cross-section, temperature, weather, time of day/night, etc., can also dictate which entities are able to interact. However, proximity detection usually dominates interaction computations and must be carefully addressed in virtually all large-scale FMS applications. This section discusses various approaches to parallel proximity detection [ REF _Ref76620244 \r \h 89] in publish and subscribe systems, with extensions to support other kinds of data filtering and data transmission.To get a perspective of this problem, imagine a simulation involving 10,000 entities with sensors, wireless radios, and various weapon systems that freely move about in the battlefield with arbitrary motion. In reality, some entities move fast, while others move more slowly or do not move at all. Similarly, some sensors have large detection ranges, while others can only detect entities that are very close. Communications can also have different range-based properties that depend on transmitted power, antenna gains, environmental effects, and jamming. To simplify the problem, imagine that a single consolidated interaction range is used for each entity.The simplest approach for an entity to determine which other entities are within range is to simply iterate through the list of all entities at regular time intervals, compute the range, and then interact with those entities that are within range. For example, whenever a sensor performs a scan, it could iterate through the list of all other entities to determine which of them are detectable. This n2 brute force approach completely breaks down in all aspects of sequential, parallel, and distributed computing for large numbers of entities. A more efficient approach is needed to support simulations involving 10,000 or more entities.Figure  SEQ Figure \* ARABIC 13: Range between two entities plotted over simulated time. In this example, an entity enters another entity’s detection range in approximately 8 minutes, where it is then added to the in-range list. It subsequently exits the detection range in approximately 27 minutes where it is then removed from the in-range list. Sensors simply access its list to determine which entities are within range.Another way to determine which entities are within a specified range of other entities is to compute exactly when each entity enters and/or exits the range of each other entity (see  REF _Ref92792867 \h Figure 13). At first glance, this appears to be better than time stepping and computing the range between every entity pair. Events only occur when entities enter and exit each other’s range instead of at regular time steps. Each entity would continually maintain and update a list of which other entities are in its interaction range. Sensor scans, for example, would only iterate through the list of entities that are within the sensor’s detection range. Of course, other considerations besides range might affect detections, such as transmitted power, radar cross-section, temperature, antenna gain, jamming, etc.This discrete-event approach may sound attractive at first until it is realized that the simulation would have to continually manage 100,000,000 events at all times. Memory consumption, message passing-overheads, and event processing factors completely break down with this popular approach. Besides not scaling, a serious problem arises when entities frequently change their motion. This pathological situation requires the continual recomputing of enter and exit range times for entity pairs, which has been observed in some scenarios to waste more than 90% of computations. Computing when two entities come within a specified range is known as range finding and usually requires iterative techniques. This can be quite expensive for complex motion algorithms operating in a World Geological Survey 1984 (WGS84) [ REF _Ref95219481 \r \h 69] ellipsoidal coordinate system. Because of its lack of scalability and wasteful thrashing of computations when motion changes, the purist discrete-event simulation approach is not acceptable for supporting large FMS applications.One approach to eliminate the problem of thrashing, due to entity motion changes, is to defer exact range finding computations until entities are close enough to warrant such computations. By knowing the maximum speed of each entity and their current range, the earliest possible time for two entities to come within a specified detection range can be computed.(T is the earliest time two entities can come within a specified range, S1, S2 are the maximum speeds of each entity, CurrentRange is the current distance between the two entities, and DetectionRange is the maximum distance that an entity can detect the other entity.Using this approach, events can be repeatedly scheduled between each entity pair using (T to determine when two entities might be close enough to be considered within range. Most of the time, (T is very large, which means that entities rarely perform expensive range finding computations. Tolerances on the range can be used to place entities in the in-range list, or exact computations can be performed when an entity is within tolerance of being in range.This approach has two very nice features. First, it does not waste expensive computations for entities that never come within range. Second, it does not thrash when entities change their motion. However, this approach still suffers from scalability because it requires the management and processing of n2 events.A much better approach uses a grid-based data structure to organize which subscribing entity regions overlap which publishing entity locations (see  REF _Ref92885781 \h Figure 14) [ REF _Ref22197601 \r \h 66]. At any given point in time, a subscribing sensor’s coverage overlaps one or more grid cells. Similarly, a publishing point-sized entity is always located within a single grid cell. Grid-based publish and subscribe data distribution systems simply deliver publisher entity information to subscribers based on overlapping grid cells. All entities published in grids that overlap subscriber grid cells are provided to the subscriber as candidates for being within range. Subscribers then perform additional filtering on the list to determine which entities are actually within range.Figure  SEQ Figure \* ARABIC 14: Using grids cells and regions to determine publish and subscribe overlaps. In this example, the subscription region for EntityC overlaps the publication region for EntityA, so EntityA delivers its FO to EntityC. The subscription region of EntityB does not overlap any publication regions so it does not discover any FOs.Moving entities add further complexity to publish and subscribe systems because entities must periodically update their set of overlapping grids (see  REF _Ref92885825 \h Figure 15). This means that both publishers and subscribers regularly communicate with grid cells to register enter and exit overlap information for their regions. Inefficient publish and subscribe filtering occurs if this operation is not performed often enough. On the other hand, inefficient grid overlap computations are performed if this update is performed too often. An appropriate update time between grid overlap computations for both publishers and subscribers must be achieved. The optimal update time is sensitive to entity speed and detection range.Because such updates are performed at discrete points in time, extensions must be made to publish and subscribe regions to accommodate additional grid cells that might be overlapped between updates. So, instead of being a point located within a single grid, publishers are now regions that can overlap several grids. Sensor regions are similarly expanded. Assuming that an entity might move with maximal speed S, between updates occurring at time intervals (t, the region expansion (R can be calculated as follows:The relationship between range expansion, maximum speed, and update time can be used to (a) fix the amount of expansion by allowing update times to vary, (b) fix the update time by allowing the range expansion to vary, and/or (c) apply other heuristics that prevent update times from being less than a specified quantity.Figure  SEQ Figure \* ARABIC 15: Expansion of subscription and publication regions due to moving entities. In this example, the sensor does not detect the target. As the sensor and target move over time, the sensor receives the target’s FO when their regions overlap. The sensor must still determine if the target is actually in range and detectable based on other criteria.One of the challenges when using grids for publish and subscribe systems is to determine the size of the grid. Fixed size grids can break down very quickly for FMS systems with largely varying (a) sensor sizes and/or (b) moving entity speeds. Grids that are too large provide inefficient filtering for small publish and subscribe regions. On the other hand, grids that are too small introduce large overheads because each publisher and subscriber must coordinate its regions in many grids.Various techniques have been used to solve the problem of fixed sized grids. One popular approach is to use quad-trees [ REF _Ref94957531 \r \h 29] for two-dimensional spatial representations and oct-trees for three-dimensional spatial representations. These data structures bisect in two and three dimensions as needed to support desired publish or subscribe region resolutions.A more recent data structure, known as Hierarchical Grids (HiGrids) [ REF _Ref94435232 \r \h 60] was developed that can support multiple user-defined resolutions for publisher and subscriber regions. The HiGrid automatically inserts regions at appropriate resolutions. A pyramid resolution scheme is used to determine which publisher and subscriber regions overlap. In the same way that hash tables can be faster than binary trees for lookups, HiGrids can be faster than oct-trees and quad-trees. HiGrids have also been extended to support other kinds of filtering dimensions beyond geographical space. Arbitrary multidimensional filter spaces can be constructed from these dimensions. Dimension types supported include: 2D and 3D spatial regions, double precision valued intervals (see  REF _Ref94947111 \h Figure 16), integers, strings, bitfields, flags, enumerations, object identifiers, etc. Furthermore, HiGrids themselves can be distributed across processors to achieve parallel scalability.Figure  SEQ Figure \* ARABIC 16: An example of a HiGrid interval dimension with multiple resolutions. Publication and subscription regions are inserted into the dimension using the resolution that is most consistent with the region’s interval width. HiGrids are composed of multiple independent dimensions. Subscription region c overlaps publication region g, while subscription regions h, b, and a overlap publication region e.Parallel and distributed publish and subscribe systems allow entities to publish Federation Objects (FOs) [ REF _Ref22198308 \r \h 95] that contain exportable state attributes. Subscribers discover these FOs based on a subscription criteria such as FO type, location in space, and/or other more complex filters. Subscribers receive updates whenever state attributes in FOs are modified. When a FO no longer satisfies an entity’s subscription criteria, it is removed from the subscribing entity.HLA, DIS, and TENA all provide mechanisms to (a) create and discover entities, and (b) distribute their state attributes between multiple applications operating within a network. While DIS and TENA operate only in real-time, HLA is also able to support both real time and logical time for small scenarios. Best effort message passing protocols such as UDP and/or IP Multicast are used to transmit data in real-time. Messages can be lost or corrupted with these best-effort data transmission strategies. Heartbeat updates are normally required to maintain distributed state information between simulations. Heartbeats are not necessary for parallel simulations operating on multicore computers because data can be transmitted in a scalable manner reliably. Instead, FO discovery/removal and attribute update/reflection can be coordinated in simulated time using reliable data distribution through shared memory. This is a completely different data distribution paradigm from distributed simulation. Tremendous optimization is possible by eliminating heartbeat updates.In addition to better performance, a more elegant and automated data distribution mechanism can be supported within a common simulation framework. Federation Objects are composed of exportable attributes that are represented as C++ classes. Operator overloading allows these classes to behave as primitive data types such as integers, doubles, strings, Booleans, etc. Whenever assigning new values to the attribute, operator overloading automatically supports (a) rollbacks when running optimistically, (b) coordinating overlap regions with publish and subscribe interest management mechanisms, and (c) automating the distribution of modified attributes to subscribing entities. The following exportable attribute data types are defined within the OpenMSA and OSAMS.Integer – normal integer values.Double – normal double precision values.Logical – normal Boolean values.String – normal string values.Fixed Position – (X, Y, Z) or (Lat, Lon, Alt) in various coordinate systems such as Earth Central Rotating (ECR), Earth Central Inertial (ECI), Round Earth (EARTH), and World Geographical Survey 1984 (WGS84).Generic Buffer – arbitrary structures, classes, variable-length buffers, etc.Dynamic Integer – time-based sequence of one or more integer values. Each value in the sequence has a start time and end time.Dynamic Double – time-based sequence of one or more double precision valued functions of time. Each function in the sequence has a start time and end time. This is used to construct complex continuous functions of time.Dynamic Logical – time-based sequence of one or more true/false values. Each value in the sequence has a start time and end time. This can be used to construct exportable complex on/off states.Dynamic Position – time-based sequence of one or more motion algorithms used to represent the location and orientation of an entity over time. Motion algorithms include (a) great circle, (b) rhumbline, (c) extrapolated motion, (d) elliptical orbit, (d) loiter, (e) polynomial-based motion, (f) splines, and (g) fixed location. Each motion algorithm in the sequence must have a start time and end time.These attributes are normally embedded within model component states and then mapped into FOs during construction. Operation with FOs is then performed automatically.Plug-and-Play Model ComponentsEstablishing a Common Unified Technical FrameworkSeveral requirements and emerging high performance computing technologies are simultaneously converging that argue for migrating to a common Unified Technical Framework (UTF) for hosting both operational and M&S applications. Such a framework would address the following areas:Support scalable high performance simulation on emerging networks of multicore computers operating within a net-centric operational battlefield environment [ REF _Ref22197697 \r \h 70, REF _Ref54184193 \r \h 76]. Scalable run time performance should embrace emerging multicore, net-centric, and software technologies, along with a flexible model composition strategy.Provide interoperability with existing simulations, models, test articles, and humans in Live, Virtual, and Constructive (LVC) execution environments. Both real-time and logical time execution modes must be supported. Mainstream LVC standards include the High Level Architecture (HLA), Distributed Interactive Simulation (DIS), Test and Training Enabling Architecture (TENA), and Synthetic Environment Data Representation and Interchange Specification (SEDRIS).Automate the ability for simulations developed within a standards-based framework to provide net-centric Service Oriented Architecture (SOA) web-based interfaces. These include XML-based standards such as Web Service Description Language (WSDL), Simple Object Access Protocol (SOAP), and Universal Description Discovery and Integration (UDDI) web standards.Provide cost-effective methodologies and modeling constructs to support the development and integration of highly interoperable and reusable model-components. Such standards will foster the creation of maintainable and validated plug-and-play model-component libraries. Configurable simulations can be configured from these model-components to efficiently satisfy specific M&S needs and requirements.Provide a flexible user-definable way to map individual models, collections of composite models, and legacy simulation tools to processors, machines, and supercomputers connected across local and wide area networks. The goal is to support scalable parallel and distributed simulations executing on machines ranging from laptops to multicore supercomputers connected across the Global Information Grid (GIG).Provide a cognitive architecture framework able to represent complex human behaviors without resorting to rigid entity scripting or complex decision logic. The cognitive architecture must support human behavior representation, cognitive thought triggering, goal optimization, and decision support in live operational settings.Provide the ability to rapidly develop scenarios, model composition tools, data visualization tools, and statistical analysis utilities that operate in a common manner independent of any specific application or implementation. Common tools and utilities will greatly simplify the operation of DoD simulations.The community took its first step in establishing a Unified Technical Framework in 2007 with the formation of the Open Source Initiative for Parallel and Distributed Modeling and Simulation (OSI-PDMS) study group. The OSI-PDMS met three times at Simulation Interoperability Workshops (SIWs) that are conducted twice a year by the Simulation Interoperability Standards Organization (SISO) [ REF _Ref54184375 \r \h 82]. Participants include members from industry, academia, and government.Figure  SEQ Figure \* ARABIC 17: Unified Technical Framework combines the OpenMSA, OSAMS, and OpenCAF architectures into a common execution framework that can universally support high performance M&S and operational net-centric operational systems within a common standard infrastructure.During its initial year of operation, the OSI-PDMS study group investigated two primary architectures, known as the Open Modeling and Simulation Architecture (OpenMSA) and the Open System Architecture for Modeling and Simulation (OSAMS). The OpenMSA focuses on the technology of high performance simulation and standards-based interoperability with legacy M&S systems, while OSAMS focuses on reusable plug-and-play component-based interoperability methodologies and programming constructs. A third architecture, known as the Open Cognitive Architecture Framework (OpenCAF), is being designed to support cognitive reasoning of complex human behaviors in a natural way. OpenCAF uses automatic rule triggering, management of goals with state machines, and five dimensional simulation techniques to rapidly explore multiple future outcomes in multicore computing environments. These three architectures that make up the UTF are conceptually shown in  REF _Ref92453819 \h Figure 17.One can think of the combination of OpenMSA, OSAMS, and OpenCAF as a SOA-based approach to M&S, where models and behaviors are provided as composable services that are distributed across arbitrary networks of single or multicore computers. A goal of the OSI-PDMS study group was to refine these architectures, investigate and explore other architectures, identify existing open source reference implementations, and potentially develop them into standards. The OSI-PDMS will probably evolve into an established Users Group, with participation from industry, academia, and government guiding the evolution of the OpenMSA, OSAMS, and OpenCAF as de facto standards based on a readily accessible common open source implementation. Without a readily available open source PDMS framework, developing the technology to achieve scalable performance and model-component interoperability becomes prohibitively expensive. An open source reference implementation provides the only practical strategy for bringing next-generation high-performance architectures such as OpenMSA, OSAMS, and OpenCAF to the FMS community in a cost-effective manner.Summary and ConclusionsModeling and Simulation (M&S) is vital in supporting current operations, improving current capabilities, and designing and building for the future. Due to prohibitive costs and environmental safety issues, M&S is often the only way to analyze the performance of complex systems, study concepts of operations with simulated scenarios, training warfighters how to respond to a variety of weapons attacks, and to measure the performance of embedded software, hardware, and human test articles within a simulation test bed.The ability to simulate complex systems through verified, validated and accredited models provides the critical foundation necessary to improve the types of systems deployed, perform cost/benefit tradeoffs, study operational concepts, and develop successful rules of engagement. M&S also allows new ideas to be safely explored, tested, and refined in a cost effective manner, with the purpose of designing and building capabilities for the future. All of these things help protect United States Warfighters as they willingly and selflessly protect their country.To provide these vital capabilities, M&S itself must sustain its current set of highly used tools that are in operation today, make enhancements to these tools as necessary to improve their accuracy, execution speed, and interoperability with other tools, while continually looking to the future as new technologies in model design and emerging computer technologies advance.In particular, future M&S systems must support net-centric operational warfare across the Global Information Grid (GIG) on emerging multicore computing platforms. Models must be developed in a manner that promotes interoperability and reuse within a common technical framework. New Science and Technology breakthroughs will play a major role in the transformation of M&S in the 21st century.This paper addresses an urgent community-wide educational need [ REF _Ref22197541 \r \h 65] that has been identified by the Parallel and Distributed Modeling and Simulation Standing Study Group, sponsored by SISO. The multicore revolution will demand a new software paradigm for Force Modeling and Simulation. Future M&S must obtain computational speedup through parallel processing technologies. Lives of warfighters and civilians are at stake. Next generation frameworks that automate parallelism, such as the UTF, and open architectures such as OpenMSA, OSAMS, and OpenCAF, will play a key role as the number of processing cores on a chip significantly increases over the next decade. The M&S community must be prepared for this transformation.RecommendationsReferencesAlberts, David S., 2002. “Information Age Transformation, Getting to a 21st Century Military.” ISBN 1-893723-06-2 (pbk.)Aloisio, G., Veneziani, N., Kim, J. S., and Fox, G. C., 1988. “The prime factor non-binary discrete Fourier transform and use of Crystal_Router as a general purpose communication routine,'' in G. C. Fox, editor, The Third Conference on Hypercube Concurrent Computers and Applications, Volume 2, pages 1322-1327. ACM Press, New York, January 1988. Caltech Report C3P-523.Amdahl’s Law, Wikipedia. http://en.wikipedia.org/wiki/Amdahl%27s_law.Ancker COL Clinton, Burke LTC Michael, 2003. “Doctrine for Asymmetric Warfare.” Military Review July-August 2003.Anderson Mark, and Whitcomb Patrick, 2000. “DOE Simplified, Practical Tools for Effective Experimentation.”Base Object Model (BOM) Template Specification, SISO-STD-003-2006.Bassett LTC, Dave, and Emery, David. 2005. “SOSCOE – The Glue That Holds FCS Together.” ARMY AL&T, September-October, 2005.Bell B., Santos E. Jr., and Brown S., 2002. “Making Adversary Decision Modeling Tractable with Intent Inference and Information Fusion.” In proceedings of the 11th Conference on Computer Generated Forces and Behavioral Representation.Bizub Warren, Cutts Dannie, 2007 “Live Virtual Constructive (LVC) Architecture Interoperability Assessment.” In proceedings of the 2007 Interservice/Industry Training, Simulation & Education Conference (I/ITSEC).Blank Gary, Steinman Jeff, Shupe Scott, Wallace Jeff, 2000. “Design and Implementation of the HPC-RTI for the High Level Architecture in SPEEDES 0.81.” In the proceedings of the 2000 Spring Simulation Interoperability Workshop. Paper 00S-SIW-153.Boehm, et. al. Software Cost Estimation with COCOMO II. http://sunset.usc.edu/research/COCOMOIIBusch T., "Modeling of Air Operations for Course of Action Determination" in Enabling Technologies for Simulation Science VI, Alex Sisti and Dawn Trevisani Editors, Proceedings of SPIE Vol. 4716, pp 35-40, (2002).Buttazzo, Giorgio C., 2005. “Hard Real-Time Computing Systems.” Predictable Scheduling Algorithms and Applications Series, Vol. 23.CASS Motion Library Version 1.0. Functional Description of the CASS Algorithms and their Applications in Military Simulations.Chandy, K., and Misra, J. 1979. “Distributed Simulation:  A Case Study in Design and Verification of Distributed Programs.”  IEEE Transactions on Software Engineering.  Vol. SE 5, No. 5, pages 440–452.Chen D., et. al., 2004. “Incremental HLA-Based Distributed Simulation Cloning.” In proceedings of the 2004 Winter Simulation Conference, pages 386-394.Curbera, Francisco, Ehnebuske, David, and Rogers, Dan, 2002. “Using WSDL in a UDDI Registry Version 1.07, UDDI Best Practice” May 21, 2002. http://www.uddi.org/pubs/wsdlbestpractices.pdf.Davis P., and Anderson R., 2004. “Improving the Composability of Department of Defense Models and Simulations.” Prepared for the Defense Modeling and Simulation Office (DMSO). RAND National Defense Research Institute.Defense Acquisitions, The Global Information Grid and Challenges Facing Its Implementation. United States Government Accountability Office, Report to Subcommittee on Terrorism, Unconventional Threats, and Capabilities, Committee on Armed Services, House of Representatives, July 2004.Defense Information Systems Agency (DISA), “Net-Enabled Command Capability (NECC).” http://www.disa.mil/necc/index.html.Defense Information Systems Agency (DISA), 2008. “Net-centric Enterprise Services (NCES) User Guide.” Version 1.2, 12 March 2008.Distributed Interactive Simulation - Application protocols, IEEE 1278.1A-1998.Distributed Interactive Simulation - Communication Services and Profiles, IEEE 1278.2-1995.DoD Architecture Framework (DoDAF) Version 1.0. http://www.dod.mil/nii/global_Info_grid.html.DoD Modeling and Simulation Office (DMSO), Department of Defense Verification, Validation and Accreditation Recommended Practices Guide, November 1996.DoD Directive Number 8100.1, September 19, 2002. Global Information Grid (GIG) Overarching Policy.Ewing Mary, 2001. “The Economic Effects of Reusability on Distributed Simulations.” In proceedings of the 2001 Winter Simulation Conference.Extensible Modeling and Simulation Framework (XMSF) Challenges for Web-Based Modeling and Simulation. Findings and Recommendations Report: Technical Challenges Workshop, Strategic Opportunities Symposium, October 22, 2002.Finkel R. and Bentley J., 1974. "Quad Trees: A Data Structure for Retrieval on Composite Keys." Acta Informatica 4 (1): 1–9.Fowler Martin, 2004. “UML Distilled.” Pearson Education, Inc., Rights and Contracts Department, 75 Arlington Street, Suite 300, Boston, MA  02116.Fujimoto, R. 1988. “Lookahead in Parallel Discrete Event Simulation.” International Conference on Parallel Processing.  Vol. 3, pages 34–41.Fujimoto, R. 1997. “Zero Lookahead and Repeatability in the High Level Architecture.” In Proceedings of the 1997 Spring Simulation Interoperability Workshop (SIW).Fujimoto R., 2000. “Parallel and Distributed Simulation Systems.” John Wiley & Sons, Inc., 605 Third Avenue, New York, NY 10158-0012.Fullford Deb and Lubetsky Ben. “Transitioning Your DIS Simulation to HLA.” http://www.mak.com.Functional Decomposition, Wikipedia. http://en.wikipedia.org/wiki/Functional_decomposition.Future Combat Systems, https://www.fcs.army.mil/.Gelb A., 1974. “Applied Optimal Estimation.” MIT Press.Gilmer, J. B. Jr. and Sullivan F. J., (2005). “Issues in Event Analysis for Recursive Simulation.” In proceedings of the 2005 Winter Simulation Conference: 8.Global Information Grid Enterprise Services (GIG-ES): Core Enterprise Services (CES) Implementation. http://www.dod.mil/nii/global_Info_grid.html.Granowetter Len. “Solving the FOM-Independence Problem.” http://www.mak.com.Graphics Processing Unit (GPU). http://en.wikipedia.org/wiki/Graphics_processing_unit.Guide for Base Object Model (BOM) Use and Implementation, SISO-STD-003.1-2006.HLA Framework and Rules, Version IEEE 1516-2000.HLA Interface Specification, Version IEEE 1516.1-2000.HLA Object Model Template, Version IEEE 1516.2-2000.HLA Federation Development and Execution Process, Version IEEE 1516.3.High Performance Computing Modernization Program (HPCMP), http://www.hpcmo.hpc.mil/community/SAS/Portfolio/cta_description.php#FMSHawking S. 1988. "A Brief History of Time." Bantam Books, New York, New York.Hurt T, McDonnell J, Mckelvy T, 2006. “The Modeling Architecture for Technology, Research, and Experimentation.” In proceedings of the 2006 Winter Simulation Conference (WSC). Pages 1261-1265.Hybinette, M. and R. M. Fujimoto. 2001. Cloning parallel simulations. ACM Transactions on Modeling and Computer Simulation, 11: 378-407.Jefferson D., 1985. “Virtual Time.” ACM Transactions on Programming Languages and Systems, Vol. 7, No. 3, pages 404-425.Jefferson D., 1990. “Virtual time II: storage management in conservative and optimistic systems.” In proceedings of the ninth annual ACM symposium on Principles of distributed computing, p.75-89, August 22-24, 1990, Quebec City, Quebec, Canada.Joint Consultation Command and Control Information Exchange Data Model. http://www.mip-site.org/publicsite/04-Baseline_3.0/JC3IEDM-Joint_C3_Information_Exchange_Data_Model.Joint Program Executive Office (JPEO) Chemical and Biological Defense (CBD) Modeling and Simulation Strategic Plan.Joint Simulation System (JSIMS) Common Component Simulation Engine (CCSE) Software Design Document (SDD).Joint Simulation System (JSIMS) Common Component Simulation Engine (CCSE) Software User Manual (SUM).Joint Simulation System (JSIMS) Common Component Simulation Engine (CCSE) Interface Requirement Specification (SDD).Katsuhiko O., 2005. “Discrete-Time Control Systems (2nd Edition).” ISBN/UPC: 0130342815. Pearson Education.Kuhl Frederick, Weatherly Richard, and Dahmann Judith, 2000. “Creating Computer Simulation Systems, An Introduction to the High Level Architecture.” Prentice Hall PTR, Upper Saddle River, NJ 07458.Lammers Craig, Steinman Jeff, 2004, “JSIMS CCSE Performance Analysis.” In proceedings of the Fall 2004 Simulation Interoperability Workshop, paper 04F-SIW-34.Lammers C., et. al., 2005. “Applying a multi-replication framework to support dynamic situation assessment and predictive capabilities.” Proc. SPIE Vol. 5805, p. 257-268, Enabling Technologies for Simulation Science IX; Dawn A. Trevisani, Alex F. Sisti; Editors.Lammers C., Valinski M., Steinman J., 2009. “Multiplatform Support for the OpenMSA/OSAMS Reference Implementation.” In proceedings of the Spring 2009 Simulation Interoperability Workshop (SIW).Lin, Y., and Lazowska, E. 1990. “Exploiting Lookahead in Parallel Simulation.” IEEE Transactions on Parallel and Distributed Systems. Vol. 1, No. 4, pages 457–469.Manycore Computing Workshop, 2007. Sponsored by Microsoft. Participation was by invitation only. http://science.officeisp.net/ManycoreComputingWorkshop07/Program.aspxModeling and Simulation Professional Certification Commission (M&SPCC). http://www.simprofessional.org/.Morse K., Steinman J., 1997. “Data Distribution Management in the HLA: Multidimensional Regions and Physically Correct Filtering.” In proceedings of the Spring Simulation Interoperability Workshop. Paper 97S-SIW-052.MPI: Message Passing Interface Standard Version 2.1. Message Passing Interface Forum. June 23, 2008.Mutschler David, 2005. “Parallelization of the Joint Integrated Mission Model (JIMM) Using Cautious Optimistic Control (COC).” In proceedings of the Summer Computer Simulation Conference (SCSC), 2005.National Geospatial-Intelligence Agency. “EGM2008 – WGS 84 Version.” http://earth-info.nga.mil/GandG/wgs84/gravitymod/egm2008/egm08_wgs84.html.Network Centric Warfare, Department of Defense Report to Congress, July 27, 2001. http://www.dod.mil/nii/NCW/.Open Grid Forum. http://www.ogf.org.Open Source, from Wikipedia. http://en.wikipedia.org/wiki/Open_source.Pace, D. K., “Verification, Validation, and Accreditation (VV&A),” In Applied Modeling and Simulation: An Integrated Approach to Development and Operations, D. J. Cloud and L. B. Rainey, editors. Pp. 369-410, McGraw-Hill, New York (1998).Parallel and Distributed Modeling and Simulation Standing Study Group (PDMS-SSG) Terms Of Reference (TOR).Parallel Virtual Machine (PVM). http://www.csm.ornl.gov/pvm/.Phister P., Busch T., and Plonisch I., 2003. “Joint synthetic battlespace: Cornerstone for predictive battlespace awareness.” In Proceedings of the Eighth International Command and Control Research and Technology Symposium.Rao Dhananjai et. al., 2007. “Design and Implementation of Virtual and Constructive Simulations Using OpenEAAGLES.” Wright State Research Institute, Joshi Research Center, Wright State University, 3640 Colonel Glenn Highway, Dayton, Ohio – 45435.Raymond Eric, 1999. “The Cathedral and the Bazaar.” O’reilly. ISBN 1-56592-724-9.Reilly Sean and Briggs Keith (Editors), 1999. “Guidance, Rationale, and Interoperability Modalities for the Real-Time Reference Federation Object Model (RPR-FOM) Version 1.0.” Simulation Interoperability Standards Organization.Ronngren R., et. al., 1996. “Transparent Incremental State Saving in Time Warp Parallel Discrete Event Simulation.” ACM SIGSIM Simulation Digest, Volume 26, Issue 1 (July 1996), pages 70-77.Rothenberg, J., Rand. “A Discussion of Data Quality for Verification, Validation, and Certification (VV&C) of Data to be used in Modeling.” Rand Project Memorandum PM-709-DMSO, Rand, August 1997.Simulation Interoperability Standards Organization (SISO), http://www.sisostds.org.SISO, Open Source Initiative for Parallel and Distributed Modeling and Simulation (OSI-PDMS) study group. http://www.sisostds.org/index.php?tg=articles&idx=More&topics=120&article=471.Sperber Joan, 2001. “Up to SPEEDES.” Military Training Technology, MT2, Volume 6, Issue 1, 2001.Steinman Jeff, 1992. "SPEEDES: A Multiple-Synchronization Environment for Parallel Discrete-Event Simulation." International Journal in Computer Simulation. Vol. 2, Pages 251-286.Steinman Jeff, 1993. "Breathing Time Warp." In proceedings of the 7th Workshop on Parallel and Distributed Simulation (PADS93). Vol. 23, No. 1, Pages 109-118.Steinman Jeff, 1993. "Synchronization of Parallel and Distributed Interactive Military Simulations Using SPEEDES." In proceedings of the 1993 Summer Computer Simulation Conference. Pages 701-710.Steinman Jeff, Incremental state saving in SPEEDES using C++, Proceedings of the 25th conference on Winter simulation, p.687-696, December 12-15, 1993, Los Angeles, California, United States.Steinman Jeff and Wieland Fred, 1994. "Parallel Proximity Detection and the Distribution List Algorithm." In proceedings of the 1994 Parallel And Distributed Simulation Conference. Pages 3-11.Steinman Jeff, 1994. "Discrete-Event Simulation and the Event Horizon." In proceedings of the 1994 Parallel And Distributed Simulation Conference. Pages 39-49.Steinman Jeff, Nicol David, Wilson Linda, and Lee Craig, 1995. "Global Virtual Time and Distributed Synchronization." In proceedings of the 1995 Parallel And Distributed Simulation Conference. Pages 139-148.Steinman Jeff, 1996, “Scalable Synchronization of Distributed Military Simulations.” In proceedings of the 1996 Object Oriented Simulation Conference. Pages 247-252.Steinman Jeff, Tran Tuan, Burckhardt Jacob, Brutocao Jim, 1999. “Logically Correct Data Distribution Management in SPEEDES”, In proceedings of the 1999 Fall Simulation Interoperability Workshop, Paper 99F-SIW-067.Steinman Jeff, 2003. “The SPEEDES Persistence Framework and the Standard Simulation Architecture.” In proceedings of the Parallel and Distributed Simulation (PADS03) conference.Steinman Jeff, 2004. “The Distributed Simulation Management Services Layer in SPEEDES.” In proceedings of the Spring 2004 Simulation Interoperability Workshop, paper 04S-SIW-98.Steinman J. and Hardy D., 2004. “Evolution of the Standard Simulation Architecture.” In proceedings of the Command and Control Research Technology Symposium, paper 067.Steinman Jeff, 2005, “WarpIV Kernel: Real Time HPC-RTI Kernel.” In proceedings of the Spring 2005 Simulation Interoperability Workshop, paper 05S-SIW-71.Steinman J, 2005. “The WarpIV Simulation Kernel.” In proceedings of the 2005 Principles of Advanced and Distributed Simulation (PADS) workshop.Steinman J. and Busch T., 2006. “Real Time Estimation and Prediction Using Optimistic Simulation and Control Theory Techniques.” In proceedings of the Spring 2006 Simulation Interoperability Workshop, 06S-SIW-017.Steinman J., et. al., 2007. “A Proposed Open System Architecture for Modeling and Simulation.” In proceedings of the Fall 2007 Simulation Interoperability Workshop, 07F-SIW-044.Steinman Jeff, 2007, “WarpIV Kernel: High Speed Communications.” In proceedings of the Fall 2007 Simulation Interoperability Workshop, 07F-SIW-045.Steinman J., et. al., 2008, “Simulating Parallel Overlapping Universes in the Fifth Dimension with HyperWarpSpeed Implemented in the WarpIV Kernel.” In proceedings of the Spring 2008 Simulation Interoperability Workshop, 08S-SIW-025.Steinman J., 2008. “The Need for Change in Modeling and Simulation.” Article to be published in Chem-Bio Defense Quarterly.Steinman J., Lammers C., Valinski M., 2008. “A Unified Technical Framework for Net-centric Systems of Systems, Test and Evaluation, Training, Modeling and Simulation, and Beyond…” In proceedings of the Fall 2008 Simulation Interoperability Workshop (SIW). Paper 08F-SIW-041.Steinman Jeff, 2008. “Open Source Licensing and the WarpIV Kernel.” In the proceedings of the Fall 2008 Simulation Interoperability Workshop, 08F-SIW-065.TENA, The Test and Training Enabling Architecture, Architecture Reference Document. Foundation Initiative 2010 Project Office.Threads (Computer Science), http://en.wikipedia.org/wiki/Thread_(computer_science).Tolk Andreas, Muguira James, 2003. “The Levels of Conceptual Interoperability Model.” In proceedings of the 2003 Fall Simulation Interoperability Workshop. Paper 03F-SIW-007.Tung Yu-Wen and Steinman Jeff, 1993. “Interactive Graphics for the Parallel and Distributed Computing Simulation.” In proceedings of the 1993 Summer Computer Simulation Conference. Pages 695-700.WarpIV Technologies, Inc., Copyright © 2007, “WarpIV Object Request Broker User’s Guide: Version 1.5.”WarpIV Technologies, Inc., Copyright © 2007, “High Speed Communications Package, Programming Guide: Version 1.5.”Web Services Description Language (WSDL) 1.1. W3C Note 15 March 2001. http://www.w3.org/TR/wsdl.West, D., 1988. “Lazy Rollback and Lazy Reevaluation.” MS thesis, University of Calgary, January 1988.Wikipedia, “Global Command and Control System”, http://en.wikipedia.org/wiki/GCCS-A.World Wide Web Consortium, Extensible Markup Language (XML) 1.0 (Fourth Edition), W3C Recommendation 16 August 2006. http://www.w3.org/TR/2006/REC-xml-20060816/.World Wide Web Consortium, XML Schema: Formal Description, W3C Working Draft, 25 September 2001. http://www.w3.org/TR/2001/WD-xmlschema-formal-20010925/.World Wide Web Consortium, SOAP Version 1.2 Part 0: Primer, http://www.w3.org/TR/2003/REC-soap12-part0-20030624/.World Wide Web Consortium, Document Object Model (DOM) Level 1 Specification Version 1.0, http://www.w3.org/TR/REC-DOM-Level-1/.World Wide Web Consortium, OWL Web Ontology Language Overview, http://www.w3.org/TR/2004/REC-owl-features-20040210/.World Wide Web Consortium, Web Services Addressing 1.0 – WSDL Binding, http://www.w3.org/TR/2006/CR-ws-addr-wsdl-20060529/.World Wide Web Consortium, Simulation Reference Markup Language, http://www.w3.org/TR/2002/NOTE-SRML-20021218/.XSL Transformations (XSLT), Version 1.0. HYPERLINK "http://www.w3.org/TR/xslt"http://www.w3.org/TR/xslt.Zarchan P., and Musoff H., 2005. “Fundamentals of Kalman Filtering: A Practical Approach.” AIAA.Wisnosky Dennis and Vogel Joseph, 2004-2005. “DoDAF Wizdom, A Practical Guide to Planning, Managing and Executing Projects to Build Enterprise Architectures using the Department of Defense Architecture Framework.” Wizdom Press.Yu Lois, Steinman Jeff, Blank Gary, 1999. “Adapting Your Simulation for HLA.” SIMULATION, Vol. 71, No. 6, December 1998. Pages 410-420.	Discrete event simulations allow events to be scheduled and processed for entities at arbitrary points in simulated time. In parallel and distributed object-oriented systems, an event is associated with a single entity. Initial events are scheduled as the simulation starts up for each entity. Then normal event processing begins. As an event is processed, it is free to modify the state of its associated entity and/or schedule new events for any entity, including itself, residing within the simulation. Causality requires the events for each entity to be processed in ascending time order. Events are never permitted to schedule new events in their past because this would violate causality and potentially introduce unstable time vortices.	Multicore chips pack multiple processors, known as cores, onto a single chip. Manycore chips generally contain eight or more cores on a single chip. The distinction between multicore and manycore will likely evolve over time as microprocessor chip designs improve.	Researchers have been investigating extending the use of multicore GPUs for applications beyond just graphics. The multicore revolution will likely catch up with GPUs in terms of cores per chip, while providing a less specialized parallel programming paradigm. 	The need for better parallel computing software development tools was universally acknowledged at the 2007 Manycore Computing Workshop that was sponsored by Microsoft. Participants at this workshop included the who’s who of parallel computing from around the globe.	Parallel compilers can untangle simple loops within regular programs, but have a very difficult time obtaining performance for irregular FMS applications, where arbitrary interactions occur asynchronously over simulated time.	Parallel math library currently exist and are continually being refined, as new number-crunching algorithms are being developed.	Standard high-performance communication libraries must be continually optimized as emerging multicore chip architectures evolve.	Hypercube communication topologies provide the most scalable massively parallel communications infrastructure, but can be expensive for large systems. Meshes are used to still provide high-speed communications between processors and can be very efficient when nodes only talk to neighboring nodes. This is often the case for Computational Fluid Dynamics (CFD) and other physics-based problems that decompose data processing across grid cells.	The number of messages passed and bandwidth consumed are different measures of scalability. Large numbers of small messages passed between nodes may consume less bandwidth than a smaller number of very large messages transmitted. Both scalability metrics must be considered when evaluating the scalability of an application.	When many mobile entities require published data from other entities based on proximity, object migration techniques may be required to continually migrate entities across nodes based on their geo-spatial location on the battlefield.	An FMS system is generally thought to be scalable if its scalability metrics perform as N log2 (N) or better, where N is either the number of nodes or the size of the scenario.	Dynamic connectivity allows applications to join and exit from the execution environment. Client/server applications allow clients to dynamically connect to servers to request a service. Once the service has been completed, clients disconnect. Servers are typically designed to handle arbitrary numbers of clients.	Embarrassingly parallel is a phrase used to describe applications that require minimal or no communications between processes to coordinate their execution. Farming Monte Carlo simulations to available processors is embarrassingly parallel when there are fewer processors than necessary replications.	Cluster computing usually involves the integration of large numbers of processors that communicate using an off-the-shelf network infrastructure. Cluster computing systems offer affordable high performance, but at the expense of relatively slow communications. Clusters are ideal for grid computing.	For example, sequential simulations normally process events in ascending time order. The output of each event could be written to disk as a trace file. When running in parallel, each node manages fewer events, making its event queue more efficient. In addition, perfect speedup might be achieved as events are processed in parallel. This ideal situation results in superlinear speedup. However, when considering the work it would take to merge parallel event processing output streams into a consolidated trace file, the superlinear speedup term is exactly cancelled. The superlinear speedup was caused by information being lost when running in parallel. Superlinear speedup is almost never observed when using highly efficient and scalable event queue data structures such as trees, heaps, and calendar queues. However, if simple O(n2) data structures, such as linked lists or vectors, are used to manage events, superlinear speedup is quickly observed.	Smaller amounts of memory used within a process would result in more efficient caching because the probability of accessed data already residing in the cache is higher.	Some parallel applications boast of achieving a large degree of computation speedup, yet other sequential algorithms might actually perform faster. Time stepped parallel simulations vs. discrete-event simulations are a good example of this. Discrete-event simulations only process events when necessary, which can be much less frequent than time-stepped simulations.	Big Endian machines store integers and doubles in reverse order from Little Endian machines. So, variables saved as binary to files will not be readable between different Endian machines. Until recently, both data formats were commonly used. However, newer systems have almost all shifted to Little Endian processors.	For parallel discrete-event simulations, the event-processing critical path dictates the maximum speedup possible.	HLA can also be used to distribute the entities of a single FMS application that is replicated across machines. An example of this is the One Semi Automated Forces (OneSAF) program used to support real-time training exercises.	Lookahead in HLA defines how tight a federate can interact with other federates in simulated time. Large lookahead values improve performance, but at the expense of not supporting tight interactions between models. Lookahead improves run-time performance but adversely affects validity.	Persistence tracks dynamic memory allocations and pointers within an embedded database. Persistence allows complex objects and pointers to be written to disk and/or packed into messages that can be sent to other nodes. The objects and their related pointers can be reconstructed from the packed data. During reconstruction, objects states are restored and all relevant pointers are set to the new memory locations.	Persistence for supporting checkpoint/restart was proven on the Joint Simulation System (JSIMS), where all of the initial performance problems were eventually resolved before the program was cancelled.	An example might be a thread that wakes up to handle a mouse click over a button. Another example might be a thread that wakes up whenever a message is received.	Most people can still remember when buggy applications used to crash PC operating systems, requiring a reboot of the entire machine. Newer operating systems isolate processes, and the memory they use, from each other to provide a more stable computing environment.	For example, imagine a four-node application that synchronizes and communicates every 10 ms as it executes on a four-processor machine. If a node gets swapped out for 50 milliseconds due to other programs running on the machine, the rest of the nodes in the parallel application must wait for it to be reactivated before continuing with the next cycle. If this happens regularly, the performance of the entire application can slow down to much worse than serial execution.	Each process does not return from the barrier synchronization call until all processes have made the call. Then all processes simultaneously return from the call and continue processing.	This does not limit the number of events types in the OpenMSA to 256. All events are combined into a single message type. Special event headers differentiate user-defined event types within an application.	Events can do two things: (a) modify the state if their associated entity, and (b) schedule new events. Therefore, rollbacks must restore the state of the entity and retract any new events that were scheduled	The critical path of a simulation is like the critical path on a project involving multiple people with workflow interdependencies. The critical path limits the overall progress. Optimistic simulations can operate faster than the critical event-processing path when lazy cancellation or lazy reevaluation techniques are used to rollforward events that are unaffected by stragglers.	Incremental state saving can provide undo and redo operations, much like commercial software systems. Another popular approach is known as full state saving, which saves the entire state of the entity before each event is processed. Full state saving becomes impractical and wasteful for entities having large and complex fragmented states.	It is actually better to multiply time during event scheduling by a factor (1 + () or (1 - (), where ( = 10-12 to avoid round-off errors.	In HLA, zero-delay event scheduling is sometimes called the zero-lookahead problem.	To maintain 8-byte alignment, a second integer priority field can be defined. This can offer more flexibility for users to coordinate event ordering.	The current simulation time is often used in calculating equation-based state values such as the current position of an entity. The time for scheduling future events is often mathematically calculated. Examples include (a) the impact time of a dropped bomb, (b) the receive time of a wireless message packet, and (c) the reflection time of an electromagnetic radar pulse from a target.	Two engineers frequently engaged in flight simulator combat during their lunch breaks. One machine was older and significantly slower than the other one. It was observed that the player on the newer and faster machine always beat the player on the slower machine. This situation is sometimes known as an unfair fight. Machine performance should not affect FMS outcomes.	A self-scheduled event is an event scheduled by an entity for itself. Self-scheduled events usually provide the bulk of the modeling work in FMS applications.	Technically, lookahead is not required for entities scheduling events for other entities residing on the same node. However, simulations should never exploit zero lookahead event-scheduling for battlefield entities residing on the same node because that could lead to non-repeatable results when executing on different numbers of processors.	Time creeping was observed in early HLA Run Time Infrastructure (RTI) implementations and became a serious performance problem for time-managed federations that used zero lookahead. Time creeping is easily solved by time jumping, performed at the start of each new cycle.	For example, entities can be destroyed by other entities.	The full state of complex moving entities would have to migrate between grids. Furthermore, grids would provide models of entities, which is highly unnatural from a software perspective.	NULL messages are purely used to help advance time between entities. They are not to be confused with event messages.	Lazy techniques include (a) determining if state variables accessed by the event are identical before reprocessing – if so, the event will produce the same result and is therefore rolled forward, and (b) determining if newly generated event messages after reprocessing the event are identical to the earlier set of messages – if so, then the original messages do not have to be retracted.	Events are committed when it can be guaranteed that they will not be rolled back. This is normally performed after the Global Virtual Time is updated.	Event retraction using antimessages is different from user-defined event cancellation that allows applications to manually retract events previously scheduled. User-defined cancellation itself must be rollbackable.	Even though there is a one-to-one correspondence between events and messages, messages are not the same thing as events. Messages are flat buffers that provide all of the information about the event, such as its time tag, a handle describing which entity the event is scheduled for, and other miscellaneous information. In addition, messages contain the parameters that are provided when scheduling an event. An event is implemented as an object that contains exactly one input message and potentially several output messages that are created as new events are scheduled during event processing. A good modeling framework automatically constructs event messages during event scheduling and processing. To model developers, scheduling an event looks like calling a method on an object that gets activated at the scheduled event time. Events simply activate methods with user-defined arguments on user-specified objects that are executed at the scheduled time of the event.	Traditional methods used to explore large numbers of branches include Monte Carlo and Design Of Experiments (DOE) [ REF _Ref76621928 \r \h 5], Simulation Cloning [ REF _Ref54182980 \r \h 16], Object Cloning [ REF _Ref76622025 \r \h 50], and Recursive Simulation [ REF _Ref54184832 \r \h 38].	This approach eventually replaced the pure range-finding algorithm originally used in the Joint Simulation System (JSIMS) program, resulting in orders of magnitude speedup.	HiGrids implement a multidimensional filtering space that can be decomposed across processors. Each HiGrid then represents its portion of the full multidimensional space.	Because Data Distribution Management (DDM) services do not coordinate with logical Time Management (TM) services, HLA also breaks down for large-scale logical time operation.	HLA provides interest management and data distribution management using multidimensional regions that are defined independently of the actual attributes provided by the published object. One can think of HLA publication and subscription regions as discovery metadata associated with a published object. A problem with this approach is that the metadata is not provided to subscribing federates indicating why the object was discovered. A better approach is to use the published attributes within the object itself for defining interest management regions. This allows publish and subscribe interest management systems to fully automate their functionality, thereby greatly simplifying operation for users.	This study group is transitioning into the PDMS Standing Study Group [ REF _Ref95222378 \r \h 74], where its goal is to produce a comprehensive report on parallel computing and M&S. The goal of this report is to influence decision makers concerning the future of FMS applications on emerging multicore computing platforms. Information in this paper will be incorporated in the final PDMS SSG report.PDMS-SSG Final Report – Volume 1: PDMS TechnologyPAGE  Page  PAGE 1 of  NUMPAGES 1Copyright © 2009 SISO. All rights reserved.09S-SIW-021	WarpIV Technologies, Inc.	 DATE \@ "M/d/yy" 3/23/09PAGE  PAGE  4