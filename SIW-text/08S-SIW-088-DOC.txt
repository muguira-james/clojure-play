M&S Repositories – Lessons ObservedDr. Jerry M. FeinbergGary L. MischAlion Science and Technology1901 N. Beauregard St., Suite 400Alexandria, VA 22311709-933-3360, 703-933-3327 HYPERLINK "mailto:jfeinberg@alionscience.com" jfeinberg@alionscience.com,  HYPERLINK "mailto:gmisch@alionscience.com" gmisch@alionscience.comLaurie H. TalbotAlion Science and Technology12350 Research ParkwayOrlando, FL 32826407-208-5753ltalbot@alionscience.comABSTRACT:  Although there is a plethora of repositories in general, and many M&S repositories in specific, their advantages to members of the M&S community are not clear.  There is little hard evidence on repositories’ usefulness. There is less documentation on their management.  And there is almost no information on their costs.The Modeling and Simulation Information Analysis Center (MSIAC) has been involved in operating, managing, maintaining, and reviewing repositories since its inception in 1999.  This paper summarizes some lessons observed over this time period on the usefulness of M&S repositories; what makes them successful (or not); and their ability to offer a potential cost, schedule, and risk advantage to an M&S developer or user.The MSIAC has noted that a small, controlled repository is more likely to succeed, that a user-populated repository requires extreme discipline in the community or else political clout to succeed, and that a large and broadly-based M&S repository will underperform due to policy and business model issues.  These observations, and others, are presented in an evaluation framework for a repository that includes the following fields:  What is its justification?  What policy supports its use?  What is its scope?  Who will be allowed access to it?  How is it implemented, namely who populates it, who validates the information, who manages it, and who pays for it?The paper concludes with a section discussing the future of M&S repositories.  One special topic is the desirability of implementing an M&S repository with Wiki software.Keywords:Repositories, Cost Effectiveness, Reuse 1.   IntroductionM&S practitioners have used, and continue to use, M&S repositories to store and find simulation components and data.  However, the specific utility of these repositories to members of the M&S community is not clear.  There is little hard evidence on repositories’ usefulness, their management, or their costs.The Modeling and Simulation Information Analysis Center (MSIAC), the Department of Defense (DoD) IAC dedicated to supporting M&S, has been involved in operating, managing, maintaining, and reviewing repositories since its inception in 1999.  This paper analyzes information collected by the MSIAC on the usefulness of M&S repositories.  The paper also summarizes lessons observed over this time period about what makes a repository successful.This paper also presents these observations in an evaluation framework for a repository that includes fields for repository justification, policy support, scope, access, and implementation (namely population, validation, management, and funding).The paper also discusses the future of M&S repositories and, in particular, the desirability of implementing an M&S repository with Wiki software.We note that this paper presents only the lessons observed by the authors.  We suspect that there are also many additional lessons either not yet learned or not yet observed.2.  Background on RepositoriesQuoting the current default repository of knowledge, Wikipedia [1], a repository is:“a place where data are stored and maintained. A repository can be ... a place where data are stored, ... a place where anything is stored for probable reuse, ... a place to store digital data.”  This is distinguished from a depository, a place just for storing things (e.g., the United States Bullion Depository, also known as Fort Knox), by the desire for “probable reuse.”Repositories are desirable because they allow permitted members of a community to discover, obtain, and use/reuse resources.  Properly implemented repositories can then enhance efficiency, standardization, and cost savings.The downside of repositories often is the lack of a business model encouraging their population, use, and content sharing.  For example, developers can be reluctant to share their resources; data consumers are often averse to using other’s data; and owners typically want to know who will use their resources, and for what purpose, before sharing  [2].For example, if a program uses someone else’s resource, the manager is taking the chance that the resource is unsuitable for the program’s purposes, and hence might “be burned” by using it.  Similarly, from many program managers’ views, it probably costs less to build a new resource than to ensure that someone else’s resource is suitable for a particular use.One more interesting issue with repositories is why have them at all when one can just use a modern internet search engine (e.g., Google [3]) to access almost everything that is needed.  When a user employs a search engine, the user takes primary responsibility for assessing the value and suitability of the information attained.  If everyone in a community uses this approach, then the assessments are repeated multiple times resulting in gross inefficiency and extra cost.  The value of a repository is that the assessment is performed once and then others rely on it (to a greater or lesser degree, depending on the repository).  Thus the repository’s greater difficulty of individual resource discovery can be outweighed by the group savings on time and effort.3.  M&S Repository ExamplesThis section presents three important examples of M&S repositories.3.1  MSRROne well-known M&S repository is the Modeling and Simulation Resource Repository, or MSRR.  The MSRR was first conceived in 1993 as the Modeling and Simulation Data Repository. As the anticipated scope of simulation interoperability increased, it became clear that defense simulations would require, and could reuse, many more resources than just databases. The models and simulations themselves became candidates for inclusion, as well as data transformation tools, VV&A histories, use histories, etc.In a period of diminishing budgets, as perceived duplication of effort (“continually re-inventing the wheel”) was escalating costs to a politically unacceptable level, a robust reuse strategy was developed to demonstrate commitment to cost effective policies. To stave off program cancellation due to lack of funds, program managers were expected to share their resources and populate the MSRR as a matter of survival.The MSRR initially came on line in late 1995. Due to management and policy issues, the Services brought their own MSRRs on line, along with the National Ground Intelligence Center and the Missile Defense Agency.  The original concept for the MSRR was that of a true repository containing actual simulation components and datasets. However, a variety of policy factors led to providing only a catalogue of resources.  Thus the current MSRR System provides retrieval of metadata descriptions only of modeling and simulation resources.  Providers include the DoD system, Army, Navy, Air Force, and the Defense Intelligence Agency. Originally, the MSRR user base was to encompass all simulation users.  These ranged from the highest level planners to the lowest level tactical operators, and from major project systems engineers to individual programmers building simple models.  This large scope was a significant driver in both the repository’s functionality and anticipated content [4].  3.2  OAMLThe Navy’s Chief of Naval Operations (CNO) established the Oceanographic and Atmospheric Master Library (OAML) in 1984. OAML contains ocean models, electromagnetic (EM) models, acoustic models, meteorological models, and a category called other models.  OAML also contains ocean data bases, meteorological data bases, acoustic data bases, and  electromagnetic data bases.  The OAML resources support the Department of the Navy, Department of Defense, research and development laboratories and Joint and NATO activities with state-of-the-art Navy-standard products.OAML was developed to provide consistency and standardization for all oceanographic and meteorological programs used by the Navy. This was necessary since the Navy has developed and used multiple oceanographic and atmospheric models and data bases.  One way to guarantee consistency in simulation outputs was through policy requiring use of the resources in OAML, which is now the Navy standard library for meteorological and oceanographic data bases, models, and algorithms. The responsibility for maintaining the models and data bases in OAML rests with the Naval Oceanographic Office (NAVO) located at the Stennis Space Center, Mississippi.  Commander, Naval Meteorology and Oceanography Command is designated as the OAML Configuration Manager.  General descriptions of the various oceanographic and atmospheric models and data bases are provided in the “Oceanographic and Atmospheric Master Library (OAML) Summary” published by NAVO.  This documentation also delineates the applications and limitations of the OAML models and data bases. OAML started in 1984 with seven databases.  In 2004, it contained 35 models, 19 databases, and 8 algorithms, or a total of 62 resources [5], [6], [7], [8].3.3  ACAT I IDEsA valuable class of repositories is the Integrated Digital Environments (or IDEs) that have been developed for most recent ACAT I procurements (also known as Major Defense Acquisition Programs).  These are small, controlled repositories with carefully selected M&S and data components with a specific purpose in support of a single program.  Integrated Digital Environments are also called Integrated Data Environments or Integrated Development Environments.  From the Defense Acquisition Guidebook  [9], DoD policy “requires the maximum use of digital operations throughout the system life cycle.  … Program managers should establish a data management system within the IDE that allows every activity involved with the program to cost-effectively create, store, access, manipulate, and exchange digital data.  This includes, at minimum, the data management needs of … modeling and simulation activities, …”One excellent example of an IDE is that developed for the Joint Strike Fighter (JSF) program [10], [11]. 4.  Repository Evaluation FrameworkThe possibility of cost savings, re-use, and efficiency all indicate that repositories can be of value to the communities they serve.  While it is quite easy to state that one repository has been a success, and another has been a failure, it is very hard to produce a basis for such statements. We believe that declarations of success or failure without definitive measures are virtually useless.  Consequently, this section of the paper offers a framework that supports a repository evaluation.  The framework encompasses five distinct evaluation areas, or measures, each of which is described below.The top-most measure we call “justification.”  This is concerned with the fundamental community need that rationalizes the establishment of the repository.  This qualitative measure’s values could include cost savings, re-use, efficiency, or standardization.A second measure is “policy support.”  This is concerned with policies that mandate the population, use, and upkeep of the repository.  Values include existence and enforceability.A third measure is “scope.”  This involves the contents of the repository and specifically how much is included in it.  Values include the number of resource types and the overall number of resources.The fourth measure is “access.”  This is concerned with the authorized users of the repository – many or few.  If the cost of replicating certain resources is high, a repository need not have a large number of users to provide value.  A secondary issue is the difficulty of accessing the contents.The fifth measure is “implementation.”  This measure has several dimensions.  These include how the repository is populated, how the information contained in the repository is validated, how the repository is managed, and how the repository is funded.This information is summarized below in Table 4.1.Repository Evaluation FrameworkMeasureMetric ValuesJustificationCost savings, time savings, risk reduction through re-use, standardization Policy SupportExisting, enforceableScopeWide, limitedAccessOpen, restrictedImplementation:Population, Validation, Management, FundingSingle person, resource owners, programs, …Table 4.1 – Basic Framework5.  Evaluation of Selected M&S Repositories Using the FrameworkWe can test the framework proposed above by using it to evaluate the three example M&S repositories.    We stress here that these evaluations are the opinions of the authors only.5.1  MSRRTable 5.1 contains our evaluation of the MSRR in terms of the repository evaluation framework.Repository Evaluation for MSRRMeasureMetric ValuesJustificationCost savings, time savings, risk reduction through re-usePolicy SupportInconsistent – currently weak to non-existentScopeLarge (virtually unrestricted)AccessTheoretically wide but practically hindered by implementation factorsImplementation:Population, Validation, Management, FundingPopulation – originally performed by resource owners (was ineffective) and currently performed by MSIAC.  Validation – performed by resource users.  Management – MSIAC but without any clear direction.  Funding – no recent funding for maintenance or update.Table 5.1 – Evaluation for the MSRR5.2  OAMLTable 5.2 contains our evaluation of the OAML in terms of the repository evaluation framework.Repository Evaluation for OAMLMeasureMetric ValuesJustificationStandardization, cost savings, time savings, risk reduction through re-use, Policy SupportClear and enforceableScopeService-wide, limited to specific topicsAccessLimited to designated usersImplementation:Population, Validation, Management, FundingPopulation, validation, management, funding – Navy and the Program OfficeTable 5.2 – Evaluation for OAML5.3  ACAT I IDETable 5.3 contains our evaluation of an ACAT I IDE in terms of the repository evaluation framework.Repository Evaluation for an ACAT I IDEMeasureMetric ValuesJustificationCost savings, time savings, risk reduction through re-use, standardization Policy SupportClear and enforceableScopeProgram-wide, limited to specific topicsAccessLimited to designated usersImplementation:Population, Validation, Management, FundingPopulation, validation, management, funding – the ACAT I programTable 5.3 – Evaluation for an ACAT I IDE6.  Lessons Observed and Issues for M&S RepositoriesThis section summarizes some lessons observed by the authors over the course of their involvements with M&S repositories.  We find it easiest to categorize these lessons using the framework for repository evaluation.6.1  Lessons observed – JustificationApparently, it has been quite easy to justify the development of M&S repositories.  One standard method is the “if you build it, he will come” [12] approach.  This has not proven true for the MSRR, as the system seems to be very under-utilized.  The wide-scope MSRR can be justified on several grounds, primarily reuse, but the issues of return on investment or cost savings are shakier due to the low utilization.In other cases, this justification has proven out.  A shining example is OAML where the desire for standardization across the Navy M&S community largely has been attained.  Standardization and control leading to reuse also has justified the ACAT I IDEs.  6.2  Lessons Observed – Policy SupportIt is quite clear that a strong policy mandating both the use and the population of a repository is a main key to success.  Such a policy is easier to develop and implement if the scope of the repository is not too large since the number of affected users is more limited and they are then easier to control.  The MSRR could have been much more effective if there had been strong accompanying policy requiring developers to supply the metadata for their resources, however the span of management authority within the DoD makes implementation of such strong policy problematic.  This holds true for all of the nodes of the MSRR.6.3  Lessons Observed – ScopeOur review indicates that the wider the scope of the repository, the more difficult it is to make it a success.  Part of the reason, discussed above, is because it is harder to implement supporting policy.  Another part is that it is harder to maintain control over a larger number of resources and another part is the cost – the greater the number of resources, the more the cost to populate and update the repository.  Finally, attempting to serve many different types of users increases the repository requirements, consequently complicating the design and subsequent implementation, leading to higher costs.6.4  Lessons Observed – AccessWe have not seen a strong correlation between the success of a repository and the scope of allowable users.  What seems to be much more important is the ease of access to the repository.  OAML users require special approval, but seem to obtain this easily.  ACAT I IDE users are quickly authorized by their programs.  In both cases, repository effectiveness is probably improved by having remote access available with associated security procedures in place.The reported ease of use of the MSRR varies.  Many potential users are confused by the fairly archaic interface.  Yet many experienced engineers have no problems with the system.Finally, the policy/design underlying the MSRR has permitted metadata only to be stored.  Consequently, discovering the resource in the MSRR is not equivalent to attaining the resource.  Without an effective business model encouraging sharing, a metadata repository will not succeed. 6.5  Lessons Observed – ImplementationFor populating resources into the repository, the method utilized by the MSRR is totally ineffective. This method relies on the resource owner’s “good will” to take the steps required to insert the resource.  This has failed on several counts.  The first is the time and effort (hence cost) involved.  The second is that publication of a resource’s existence with a point of contact leads to frequent communications that can require significant time and effort on the contact’s part.  The third is a potential loss of control of the model’s code since the MSRR relies on the resource owner for configuration management but changes to the code could be made by any third party user.  A fourth is a perception by several owners that only they know how to execute their particular simulations properly as it is thought to be more of an “art” than a “science” to run them.OAML and the ACAT I IDEs effectively pay for the population of their repositories.  There is also strong policy (by the Navy or by the program office) requiring use of their repositories.  This has been effective.Validation and configuration management of MSRR resources is performed by the resource owners.  Validation of OAML models is performed by the OAML program.  Validation of ACAT I IDE models is performed (at least in theory) by the program office.  We have not observed any problems with these methods.  Management of OAML and of an ACAT I IDE is straightforward and executed by the program offices.  Management of the MSRR has been inconsistent, fractured, and at times chaotic.  This inconsistent management of the MSRR has led to reduced usage and has also precluded the development of effective policies.Funding of repositories is always an issue.  The management of a repository has to justify the program to maintain its budget.  Expanding the scope of a repository demands a higher budget.  OAML and an ACAT I IDE live within their fluctuating budgets by constraining the number of resources and some of the associated services provided.  In fact, there are many examples of building very effective repositories on a tight budget by choosing to constrain the number of resources being managed.6.6  Summary of Lessons ObservedSo, in summary, it first appears that a small, controlled repository is more likely to succeed.  Second, a user-populated repository requires extreme discipline in the community or else political clout to succeed.  Third, a large and broadly-based repository may underperform due to policy and business model issues.  Fourth, while incorporating a broad user base may appear to be wise, constraining the user base from which initial requirements are derived can lead to a more affordable system without necessarily impacting scalability. 7.  Future for M&S RepositoriesHow should repositories for models and simulations change to be more effective, efficient, and affordable?   In our view, the answer to this question centers on the major evaluation criteria in our framework.7.1  SuggestionsJustification:  there is a continuing need for access to modeling and simulation components for adaption or adoption, and to support standardization.  Repositories are here to stay provided that their contents are properly validated and reusable so as to compete cost effectively with internet search engines.  The initial step always is to determine who the users of a repository are going to be and what these users are going to want (requirements).  Policy support:  future repositories will succeed when supported by strong policies in two areas.  The first is to mandate the use of modeling and simulation components that are contained in the repository, whether for adoption, or adaption, or supporting new development.  The second is to require the population of these repositories with a set of M&S components satisfying important selection criteria.  These policies need to be enforceable.Scope:  future repositories have a greater likelihood of success if they are narrower in scope and controlled.  Thus we suggest a set of separate, constrained, limited-scope repositories that are tightly controlled, but linked so that discovery across the entire set is possible.Access:   future repository or repositories should allow wide access provided this does not drive the architecture/design to a difficult-to-maintain, costly-to-implement solution.  The design requirements should be derived from a smaller, cohesive set of users.  Certain resources can always be protected passwords or other means if necessary.Implementation:   future repository population should be controlled by the repository manager/owner who is also responsible for validating the information contained therein.  These individual manager/owners should also pay for development, population, and upkeep.  It does not matter if the repository is distributed across various nodes provided that it is presented to the user as one logical repository.7.2  Other ApproachesAs discussed earlier, relying on internet search engines presents a viable possibility for replacing repositories, but these engines introduce difficulties of their own.  A more interesting approach is developing a Wiki-based repository [13] in which users or third parties populate the repository with information about resources.  This approach eliminates the bottleneck of relying upon a specified entity or group to perform all the work of population and “spreads the costs around.”  However, it can lead to free-for-all battles on the information content with competing views of a resource playing out in public.  Internally, the MSIAC has been testing a Wiki-based repository for some of its own resources.  In this case, the “editors” of the resources are a limited group and the scope of the repository is controlled.  We hope to report more details of this approach in a later paper.Finally, amongst all of the hoopla and teeth gnashing concerning repositories, we suggest heeding the words of a famous British knight:“…you can’t always get what your want, but if you try sometimes, you just might find, you get what you need” [14]8.  AcronymsACAT IAcquisition Category I – Major Defense Acquisition ProgramsCNOThe Chief of Naval OperationsDoDDepartment of DefenseEMElectromagneticGIGGlobal Information GridIACInformation Analysis CenterIDEIntegrated Data EnvironmentJSFJoint Strike FighterM&SModeling and SimulationMSIACModeling and Simulation Information Analysis CenterMSRRModeling and Simulation Resource RepositoryNAVONaval Oceanographic OfficeOAMLOceanographic and Atmospheric Master LibrarySPMSmart Product ModelVV&AVerification, Validation, and Accreditation9.  References[1]    “Repository”  Accessed at http://en.wikipedia.org/wiki/Repository, 1/31/2008  [2]    Misch, Gary:  “Managing Modeling and Simulation Data”  MORS Workshop, 3/2003[3]    “About Google”  Accessed at  HYPERLINK "http://www.google.com/intl/en/about.html" http://www.google.com/intl/en/about.html, 1/31/2008[4]    Misch, Gary:  “Managing Modeling and Simulation Data”  MORS Workshop, 3/2003[5]    Northridge, Bruce: Inter-Generational Connections in Environmental Acoustics.  Commander, Naval Meteorology & Oceanography Command.  October 2004.[6]    Oceanographic and Atmospheric Master Library (OAML) Configuration Management Plan, Feb 1996[7]    “GFMPL and OAML” Accessed at   HYPERLINK "https://oceanography.navy.mil/legacy/web/nipr_2006/gfmploaml.html" https://oceanography.navy.mil/legacy/web/nipr_2006/gfmploaml.html   1/15/2008[8]    Aerographer's Mate, Module 04--Environmental Communications and Administration.    Central Edition,  1999.    NAVEDTRA No: 14272.  Accessed at  HYPERLINK "http://www.globalsecurity.org/military/library/policy/navy/nrtc/14272_ch2.pdf" http://www.globalsecurity.org/military/library/policy/navy/nrtc/14272_ch2.pdf [9]    “11.12. Integrated Digital Environments”.  Defense Acquisition Guidebook.  Accessed at   HYPERLINK "https://akss.dau.mil/dag/DoD5000.asp?view=document&rf=GuideBook\\IG_c11.12.asp" https://akss.dau.mil/dag/DoD5000.asp?view=document&rf=GuideBook\IG_c11.12.asp[10]  "Global F-35 aircraft coalition builds unique collaboration network"  from UGS Teamcenter IDE.  No Date[11]  "Lockheed Martin Aeronautics"  from Siemens PLM Software.  Accessed at http://www.plm.automation.siemens.com/en_us/about_us/success/case_study.cfm?Component=30306&ComponentTemplate=1481  Accessed 1/31/08[12]   HYPERLINK "http://en.wikipedia.org/wiki/W.P._Kinsella" \o "W.P. Kinsella" Kinsella, W.P. and Robinson,  HYPERLINK "http://en.wikipedia.org/wiki/Phil_Alden_Robinson" \o "Phil Alden Robinson" Phil Alden: “Field of Dreams” movie 1989  [13]  “Wikipedia:About”  Accessed at:   HYPERLINK "http://en.wikipedia.org/wiki/Wikipedia:About" http://en.wikipedia.org/wiki/Wikipedia:About, 1/31/2008 [14]  Jagger, Mick and Richards, Keith:  “You Can’t Always Get What You Want” Rolling Stones Lyrics.  1969Author BiographiesJERRY FEINBERG, of Alion Science and Technology, is the Chief Scientist at the Modeling and Simulation Information Analysis Center (MSIAC) in Alexandria, VA.  He has supported information analysis and repository initiatives for DoD and for the Navy.  Most recently, he led a MSIAC survey of repositories to determine how they have provided value to their users.GARY MISCH, of Alion Science and Technology, is the Knowledge Manager and Director of Technology at the MSIAC, Alexandria, VA. He was the project manager for the Defense Modeling and Simulation Resource Repository, and developed the original requirement for the Navy Battle Force Tactical Training System.LAURIE TALBOT, of Alion Science and Technology, is a Senior Military Operations Analyst and MSIAC help desk contributor, supporting the Military Modeling and Simulation Community in the Florida Simulation Center, Orlando Florida.  He was previously assigned for three years as the Air Force M&S Subject Matter Expert for the Air Force MSRR.  