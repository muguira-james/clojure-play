J9901, Federation Development for Joint ExperimentationAndy CeranowiczAB Technologies Inc.aceranowicz@abtechnologies.comMark TorpeyBill HelfinstineDave BakemanLockheed Martin Information Systemsmtorpey, bhelf, dbakeman@lads.is.lmco.com John McCarthyToyon Research Corporationjmccarthy@toyon.comLizann MesserschmidtThe MITRE Corporationlizann@mitre.orgSteve McGarryMIT Lincoln Laboratorysmcgarry@ll.mit.eduSteven MooreBMH Associates Incmoore@bmh.comKeywords:Joint Experimentation, Discovery, Attack Operations, Query Protocol, Selective ResolutionABSTRACT: J9901 is an experiment being performed by the U.S. Atlantic Command in conjunction with the Joint Advanced Warfighting Program and the Synthetic Theater of War.  J9901 is exploring approaches to Joint Experimentation in the context of future attack operations.  The experiment focuses on the use of potential future surveillance systems to defeat mobile theater ballistic missile launchers.  The virtual portion of the experiment pits a man-in-the-loop attack operations command center against four human controlled theater ballistic missile (TBM) brigades.  J9901 is a "discovery experiment", which means that the objective of the study is not to evaluate system effectiveness, but rather, to provide an environment and tools that will allow operators and analysts to discover new insights into the attack operations problem. To support J9901, a new federation was developed. Novel features of this federation include a distributed sensor model based on a query protocol, distribution of a perceived tactical picture via the RTI, selective simulation of background traffic to represent 10,000 entities, and the enhancement of analysis products to include event timelines for target entities.  This paper describes the J9901 federation and the decisions that were made in developing the federation.IntroductionJ9901 is the first experiment being conducted by the U.S. Atlantic Command (ACOM) in its new role as the executive agent for joint experimentation.  The J9 directorate of ACOM has been created for the purpose of applying joint experimentation to improve joint operations. ACOM’s executive agent for this experiment is the Joint Advanced Warfighting Project (JAWP) being conducted by the Institute for Defense Analysis (IDA).  The JAWP is investigating how experimentation can be used to discover new concepts for future military operations.J9901 looks at joint experimentation in the context of future attack operations.  It postulates that in the 2015 timeframe, it could be possible to track enemy movements well enough to successfully neutralize mobile enemy Theater Ballistic Missiles (TBM). An internetted network of multimodal radar sensors, mounted on satellites and unmanned air vehicles, together with unmanned ground sensors is used to observe and track traffic in a hostile country.  An attack operations cell uses these sensor assets to develop a common picture of ground activity.  Targets are identified using Automatic Target Recognition (ATR) and human analysis.   Standoff and unmanned attack assets are employed to destroy the targets. The J9901 experiment is composed of two overlapping phases.  The constructive phase is used to study sensor density and other parameters.  The man-in-the-loop phase is used to study what humans can do with the tracking and attack capabilities postulated. Two simulations are being used to support the experiment: Simulation of the Location and Attack of Mobile Enemy Missiles (SLAMEM) and the Synthetic Theater of War (STOW).  SLAMEM is being used to support both phases of the experiment while STOW supports the man-in-the-loop portion.  Toyon Research Corporation developed SLAMEM under DARPA and USAF sponsorship for the purpose of studying the military utility of advanced sensors in various military scenarios [1].  It is a self-contained, event based, constructive simulation capable of running faster than real time.  It runs under Windows NT on a single or multiprocessor personal computer (PC).  STOW is a simulation federation originally developed by DARPA, DMSO, STRICOM, SPAWAR, and TEC to demonstrate the applicability of entity level simulation to Joint Task force training.  STOW is an Advanced Concepts Technology Demonstration (ACTD) and its primary exercise (STOW 97) was conducted in 1997 in conjunction with the Unified Endeavor (UE) 98-1 training exercise [2].  Because JSIMS was already under development to support Joint Task Force training it was decided to investigate whether STOW could support ACOM’s new requirement of joint experimentation.Scenario and software development for the J9901 experiment started in January of 1999 and trial runs commenced in June. Seven one-week trials are being conducted through the summer and will conclude in August.  Development is proceeding during the trials and the same group of subjects is being used throughout the experiment. Thus, they are not independent trials but instead are designed to encourage the evolution of tactics and skills.This paper describes how STOW was modified to support the man-in-the-loop phase of J9901.  The changes included shifting focus from training to analysis and from force on force combat to surveillance and C4I, scaling the simulation to simulate over 11,000 entities, and the incorporation of a common tactical picture into the simulation.   This paper also points out some simulation features that are important for joint experimentation. DiscoveryA central concept of J9901 is “discovery”.  The intent of the experiment is to discover changes in technology, equipment, organization, or tactics that can revolutionize military effectiveness.  The experiment is a search for factors that generate large improvements. Unfortunately the combinatorial complexity of the world precludes an orderly evaluation of all the free variables.  Nonlinear coupling between the variables limits the validity of any sensitivity analysis to a small region.  Without some hint of where to look, chances of discovering a revolutionary change are small.  Traditional analysis, varying one or several variables at a time, is too slow for discovery.  Instead discovery has to rely on human pattern analysis as its primary mechanism.  Thus the primary role of the simulation is to provide an environment in which human participants, observers, and analysts can gain insight into the problem space and intuit which changes will provide revolutionary improvements. Although human intuition is not always reliable, it is able to draw effective conclusions where algorithmic approaches are silent. The experiment pits the blue attack operations team against a human red missile team to force the evolution of tactics and procedures.  Conventional statistical measures help to verify that conclusions are reasonable.ExperimentThe scenario for J9901 is straightforward.  The U.S. is an ally of Blueland. When Redland threatens Blueland, U.S. forces are deployed to the area.  Redland responds with mobile TBM attacks.  The U.S. counters these attacks with a special attack operations cell with dedicated sensor and attack assets.  To limit U.S. casualties, the cell uses standoff and unmanned surveillance assets and weapons to find and destroy the enemy missiles.A white cell controls the execution of the experiment.  A blue attack operations cell, supported by two blue attack cells, battles against a red TBM cell.  Analysis and blue observation cells monitor the battle and an assessment team interprets the results.  The white cell performs simulation initialization, checkpointing, verification of proper simulation operation, fault recovery procedures, and external injects into the simulation, such as Intelligence reports.  The blue attack operations cell (AOC) is subdivided into the sensor and surveillance control node (SSCN) and the attack systems control node (ASCN).  The SSCN is responsible for developing a consistent tactical picture (CTP) of ground traffic in Redland and for identifying and nominating targets.  The ASCN is responsible for destroying the nominated targets.  It is supported by an air attack cell, which maintains aircraft on station waiting for attack assignments.  A second attack cell controls the land and sea assets including a Future Army Missile System (FAMS) and a Future Navy Missile System (FNMS).  All assets used in J9901 are generic future systems.  The Blue observation cell monitors the activities of the blue players in the AOC to see how their tactics evolve over the course of the experiment. They submit their observations to the assessment team.  The analysis cell records and reduces the data to produce performance measures and event timelines for the assessment team.TerrainThe first question that came up after it was decided that STOW would support J9901 was “what terrain database should we use?”   This question was raised first because terrain development for virtual simulations (simulations that have interactive visual representations) tends to be labor intensive and requires significant lead time.  The J9901 terrain requirement was challenging because a very large area was needed to provide the OPFOR with sufficient room to hide their missile operations. The terrain also had to have sufficient resolution to represent features to hide within. To avoid the cost and time required to develop a new database it was decided to use the STOW 97 database of Southwest Asia, which was approximately 5x7 degrees in size roughly centered on Kuwait City.  It provided enough land to support OPFOR operations and the entire database was designated as the fictional Redland.Additional area was needed to support blue operations.  It was decided to expand the database by adding one degree to the north, west, and south and call that Greenland, Redland’s neutral neighbor.  The victim of Redland’s aggressions, Blueland, was represented by adding three degrees of longitude to the east of Redland resulting in a database approximately 900 by 900 kilometers.  Because STOW databases are divided into one-degree square cells, this simply entailed adding more cells rather than performing a full recompile.  The new terrain cells were created from data in a lower resolution database developed for JWARS as part of the STOW WorldWide Terrain Database (WWTDB) project [3,4]. Even though SWA 97 was selected as the best database for the experiment given the time and cost factors, it was not sufficient to support the experiment.  The road network was not dense enough in the southern areas, there were too few cities, and the buildings in the cities were too sparse to provide the masking effects desired.  Also the database had no forests to force the use of foliage penetrating radar (FOPEN).  To solve these problems without recompiling the STOW 97 database, we turned to our dynamic terrain technology.  Dynamic terrain was introduced for STOW 97 [5] to place target buildings and structures on the database and to model the effects of weapons on the terrain.  For J9901 this technology was extended to lay down roads, abstract forest canopies, and abstract city features.  Arcview, a commercial Geographic Information System, was used as the front end for this process.  Developers entered the desired features into Arcview and produced Shape format files.  The dynamic terrain simulator translated these files into environmental changes to the terrain database.  The resulting modified terrain database was saved and then distributed via compact disk to all the simulation sites used in J9901.  While these changes could have been loaded dynamically from the network at the start of each simulation run, a single offline distribution was more efficient.Joint experimentation requires the ability to generate custom terrain databases rapidly and cheaply.  STOW has solved this problem for low-resolution terrain by joining with JSIMS and JWARS to build a repository of terrain source data for the entire world and engineering an automated process to convert that data to simulation databases.  Even with real world terrain data, the ability to add more features is needed and can be supported most efficiently by adding features incrementally using a dynamic terrain capability.  Some problems can be simplified by using abstract features in conjunction with full three-dimensional objects.Communications ArchitectureAnother question that had to be answered early was which RTI to use.  Since 97, STOW has been using an early variant of the RTI known as RTI-s [6].  While a number of components of STOW had been ported to RTI 1.3, additional components still required porting.  To minimize risk,  it was decided to execute J9901 using RTI-s.  The effort to port the entire STOW federation still continues in the background.During the first trials of J9901, it became clear that we were experiencing a small amount of packet loss. We considered changing critical messages to use reliable transmission, but the reliable distribution mechanism in RTI-s is based on a TCP (Transmission Control Protocol) exploder and was only expected to handle about one packet per second.  Previous exercises had shown that this could easily be exceeded.  So a new transmission protocol for interactions was added which sends out several copies of each interaction.  On the receive side only the first copy received is forwarded to the application.  This allows us to trade off bandwidth for reliability without risking a potentially catastrophic TCP overload.The development of the J9901 federation required many extensions to the STOW FOM (Federation Object Model) and the underlying DDM (Data Distribution Management) multicast routing spaces. Some of these changes are discussed in following sections with their respective requirements.  Such extensive changes would have been almost unthinkable in the DIS (Distributed Interactive Simulation) days where the primary focus was on interoperability.  Joint experimentation must have the ability to rapidly change protocols to support new federates, new data requirements, and instrumentation.  DMSO has developed an Agile FOM Interface  (AFI) for JSAF (Joint Semi-Automated Forces) to make the STOW FOM more flexible. We plan to incorporate the AFI into STOW.Another early decision was whether to centralize the simulation at ACOM’s Joint Training and Analysis Simulation Center (JTASC) or to run it as a geographically distributed simulation.  We chose to distribute the simulation between JTASC in Suffolk VA, the WISSARD facility at Oceana NAS VA, the Space Warfare (SPAWAR) Systems Center in San Diego CA, and the Topographic Engineering Center (TEC) at Ft Belvoir VA.  Network support was provided by the DARPA DISA Joint Program Office and the Naval Research Laboratory in Washington DC. There were three reasons for running distributed. First, the STOW team is distributed around the country so the use of the development labs as simulation sites reduced exercise costs. Second, the space available at JTASC was limited.  Finally, it was desired to demonstrate that geographically distributed simulation could be used to support joint experimentation.  This is important to future joint experimentation because it can reduce the time and expense required to access remote technical expertise.Sensors and SurveillanceThe next design question was how to represent an integrated surveillance, tracking, and fusion system circa 2015.  After considering a number of implementation approaches within STOW, it was decided that it would be much easier to incorporate SLAMEM as a STOW federate.  SLAMEM already had all of the basic sensor models required and reusing them would save a lot of development time.  The SLAMEM developers brought extensive knowledge of radar sensors, tracking, and fusion to the experiment.  This factor was far more important than the development time.  Without their background it would have taken a very long time to perform the required knowledge acquisition.  Third, the results of the constructive runs, including parameter changes and sensor collection plans would directly transfer to the man-in-the-loop simulation.  Once we decided to use SLAMEM, it was a quick decision to employ a gateway to link SLAMEM to the federation. SLAMEM ran under NT and we did not have an RTI-s version for NT. SLAMEM already had an existing broadcast interface protocol, used to connect with DIS simulations, and STOW had a gateway federate to connect with DIS simulations.  So connecting SLAMEM to the gateway was fairly straightforward. The experiment uses stripmap SAR (Synthetic Aperture Radar), spotlight SAR, MTI (Moving Target Indicator), HRR (High Range Resolution) MTI, and FOPEN (Foliage Penetrating) SAR augmented with IUGS (Internetted Unmanned Ground Sensors). The radar sensors are carried by satellites and high performance UAVs (Unmanned Air Vehicles).  All of the sensors, satellites, and sensor platforms are modeled in SLAMEM. Entity state updates for the UAVs are generated by SLAMEM and transferred to the STOW network by the gateway.  Our primary problem was interfacing the radar sensors to the simulation.  Because this was a very large simulation with over 11,000 entities in Redland alone, the computational cost of the solution was important.  All 11,000 were potential radar contacts and if we fed ground truth to the radar models SLAMEM would have to sink packets from all 11,000 entities, maintain their state information, and process them for each radar sweep.  Additionally, we would have to register the terrain in SLAMEM to that in STOW, a difficult thing to do accurately since STOW and SLAMEM use different representations.  We decided that the best solution was to use a query protocol [7].  In this query protocol, SLAMEM generates sensor footprint interactions.  A footprint covers the area swept out by a sensor in about a second. Redland entity simulators (there are no Blueland ground forces in Redland) subscribe to these footprint interactions.  The simulators evaluate which vehicles are in the footprint and filter out those that are undetectable. Line of sight and velocity are checked.  For example, SAR can only see stationary entities.  Sensor blocking by forests and urban areas is also checked.  Then STOW generates “detectable” interactions listing all the entities the radar could detect and sends them back to SLAMEM via the gateway.  SLAMEM processes all these detectable interactions as if they were returns from its own internal entities.  This approach has some important advantages.  The sensor model is distributed, sharing the computational load with all the red simulators. SLAMEM does not have to process intervisibility and doesn’t have to know about entities that don’t pass the STOW filters.  The computational and network load on SLAMEM is greatly reduced.  The intervisibility calculation is done on the STOW terrain database so the correlation between the databases only has to be good enough to place the corners of the footprint accurately.  Finally the requirement for position updates from the STOW entities is relaxed because the published position data is not a factor in sensor calculations.  Thus we were able to significantly reduce the update rate for some entities.  At one point we decided to stop publishing ground truth data altogether, but it was brought back to support white cell operations and simulation verification.  SLAMEM summarizes the performance of each sensor in a data structure called a confusion matrix.  In J9901, there are 14 different track types recognized by SLAMEM sensors.  The SLAMEM Gateway maps each STOW entity into one of these types. SLAMEM then uses a random draw and a detection threshold to determine if the return was detected.  For each return detected, a second random draw is used to determine the track classification assigned by the sensor ATR.  Each row of a confusion matrix corresponds to an input ground truth track type.  Each column of the confusion matrix corresponds to a perceived track type output. The intersection of a row and a column gives the probability that the input ground truth track type will be output as the perceived track type by the ATR classification.  Radar experts at MIT Lincoln Labs provided probabilities for the J9901 confusion matrices.Individual sensor detections are combined with other detections to produce tracks that indicate the entity’s path over time. For J9901 it was not possible to build a true 2015 tracking algorithm.  Instead, a parametric model of tracking algorithm effectiveness was used. SLAMEM updates the tracks it is maintaining based on a range gate and a correct association factor.  The range gate represents how far the track could have moved since the last track update.  All new detections inside of that range gate are examined. If there is only one new detection in the range gate, it is assigned to that track.  Once all the unique track associations have been made, then a preset percentage of the remaining tracks are associated correctly using ground truth and the remainder are associated incorrectly.  The correct association factor can be changed to model trackers with different accuracies.Next SLAMEM fuses the classifications from new detections into the track classifications.  A Bayesian update algorithm is used.  Each track carries an array of 14 probabilities with it. One probability for each of the 14 track types, indicating how likely it is that the track is from an entity of that track class.  After each track association, the probability array is updated based on the probability of selecting the detection result given each possible ground truth track type.  The resulting tracks provide a consistent tactical picture (CTP) of ground vehicle traffic in Redland.The Attack Operations CellPersonnel in the attack operations cell (AOC) develop the CTP (by controlling the sensor scans), nominate target tracks, and assign assets to destroy those targets. The first two functions are assigned to the sensor and surveillance control node (SSCN) and the latter task is assigned to the attack systems control node (ASCN).  The operation of SSCN and ASCN is the focus of the experiment. How the operators use of the system to track and destroy targets is studied and their insights contribute to the discovery of new tactics, procedures, weapons, and interfaces.  Their interface to the simulation is an emulation of a future command and control system.  In the time available to create the simulated environment for J9901, it was not practical to build a future C4I system.  Rather a composite emulation was created using the STOW Plan View Display (PVD) for track manipulation, the Global Command and Control System (GCCS) for observing track history, the Internet VOice Exchange (IVOX) for voice communications, and commercial web browsers and email programs for electronic communications.  The critical design decision was how to distribute the CTP produced by SLAMEM to the AOC and white cell workstations.  We decided that the best approach would be to treat tracks as RTI objects very much like entity state objects.  This allowed us to reuse most of the entity state code to distribute and display simulation entities and provided the quickest way to generate a working system.  We had neither the expertise nor the access to modify GCCS to achieve what we wanted, so we focused on the STOW PVD as the primary interface for manipulating the tracks.  The use of RTI objects to distribute the perceived CTP is interesting because it violates one of the original laws of distributed simulation dating back to  SIMNET [8]: all information about the battlefield should be distributed as ground truth.  This law was based on the assumption that each simulator would have its own unique perception of the battlefield, which it should derive from ground truth.  With the advent of data fusion, shared perceptions of the battlespace have become possible, so it now makes sense to transmit shared perceptions directly rather than to recreate it at each simulation node.  The SLAMEM Gateway was selected as the federation owner of these track objects, which expanded its role from a gateway to the central database in the simulation.  This required adding a track state object to the STOW FOM. The next design decision was how to allow the operators in the AOC to communicate and maintain information about the tracks. When a target track is first observed it is marked as interesting, then if it continues to show suspicious behavior it is recommended for nomination. Once the SSCN director decides a track is truly a target it is nominated and given an attack priority. Then an attack planner assigns mission parameters to the track, and the attack tasker pairs the track with an attack asset.  The attack asset then confirms it is attacking the track, and marks the track as attacked once ordnance reaches the target.  It was decided that the best way to record this information and share it with other AOC personnel was to add these attributes to the track object state.  Thus the gateway not only serves as the track database but also as a communication mechanism between the AOC operators.  The unfortunate side effect of this decision is that each track state is potentially a very large object and the system is fairly sensitive to the number of track state objects in existence. The upper limit for acceptable system performance is about 8000 tracks. Thus far J9901 has seen track counts of between 2,500 and 6,000 tracks. To reduce the sensitivity to the number of tracks, the track routing space uses track state as a dimension.  Attack operators only subscribe to tracks that have already been nominated and attack assets only subscribe to tracks that have been paired. To allow track state to be manipulated by the AOC operators a track state editor was added to the PVD allowing them to inspect and modify track state.  To allow operators an alternative view of the tracks and to allow them to review past track movement the track information from the STOW federation is transmitted to GCCS via two gateways.The Attack ForcesA variety of generic future weapons were simulated for this experiment.  The focus was on standoff weapons that could be fired from aircraft, ships, and land based launchers.  They include very fast weapons such as the hypersonic missile and relatively slow weapons such as cruise missiles that are capable of loitering and being retasked in flight.  They also included unmanned weapons such as the unmanned combat air vehicle (UCAV). Response time was considered to be the critical factor for these weapons.  It was hypothesized that attack cell would have direct control over their attack platforms to minimize response time.  To model this a PVD editor was developed for the attack systems tasker in the ASCN.  This editor has a database of all the weapons currently available for tasking and their current munitions.  The database is maintained by electronic reports received directly from the attack assets.  Based on the guidance entered by the attack mission planner, a pairing algorithm selects the best munition/asset combinations to perform the attack.  The operator can accept the top match or opt for a different one. Once he selects a target/asset pairing, an attack order is automatically generated and sent to the asset.  If the asset accepts the order it notifies the asset database of its new status and changes the track state to mission executing.  With the automation of asset tasking, the role of the attack asset operators changed from controlling attacks to positioning assets so that they are available for attack tasking.  All the attack assets in J9901 are operated by a maximum of four people. Up to 70 aircraft, controlled by SOAR agents [9], orbit in attack positions waiting for attack missions.  The operator’s job is to set up the air tasking orders that bring new waves of aircraft out to their attack positions, periodically refuel them, and then return them to base. Ground and sea forces are even simpler to operate. Of course, this is partially due to the assumption that U.S. forces would have air superiority and attack operations forces would operate from safe regions. The capability to represent different command and control systems is critical for joint experimentation.  Enhanced communications and computer automation is driving the revolution in military affairs and any simulation that is going to support investigation of the future must support different models of C4I systems and interfaces.  For J9901 it was easiest for us to represent the functions of the command and control system in terms of RTI objects and interactions.  It made it very easy for us to control the distribution of the C4I data within our simulation.  It also reinforces the requirement for an easily changed federation defined FOM.OPFORA critical component in the process of discovery is an active and independent OPFOR.  By independently formulating and implementing counter tactics the Red team forces the Blue team to rapidly refine and evolve theirs.  Therefore, all the Redland entities in J9901 are represented in STOW.  However, the size of the exercise was a major challenge for the federation design.  In UE 98-1 STOW used 300 federates to simulate and control approximately 3600 entities.  The J9901 scenario called for over 11,000 with substantially less hardware and fewer operators. Fortunately there were a couple of factors on our side.  Virtually all the STOW entities in UE 98-1 were front line troops.  In J9901 only the TBM units have combat importance.  These number less than 800 vehicles.  The remainder of the vehicles whether military units or civilian vehicles are primarily background traffic to confuse the sensors and clutter the CTP.  A second factor helping us in J9901 was the availability of computers that were twice as fast and half the price of the PCs we used in 97.  We decided that much of the traffic could be completely automated.  The Redland vehicles were divided into four classes.  TBM units that would be human controlled, elements of TBM units that would be totally automated (support vehicles only), clutter vehicles that would be human controlled, and clutter vehicles that would be fully automated.  A special clutter simulation was developed by taking the STOW Joint Semi-Automated Forces (JSAF) code and stripping it down to create a simulation with only those functions required for the clutter traffic.  Primarily those were road movement, damage calculation, response to sensor footprints, and publishing entity state.  Both the road movement and the damage calculations were event driven.  Vehicles change velocity as they reach turns in the road and detonations trigger damage calculations.  Features such as collision avoidance and orientation with respect to the terrain skin were not important to the operation of the sensors and were disabled.  The clutter simulation is run on ten 200 MHz P6 computers (from STOW 97) at JTASC and simulates almost 10,000 vehicles for J9901.  It has been run with up to 20,000 vehicles in test runs. This 1000 vehicles per computer contrasts with about 18 per computer in STOW 97 (assuming 200 simulation computers). In previous attempts to increase JSAF performance with generic changes, we were only able to get a factor of two improvement and that was quickly eaten up by new features being added to the simulation.  The lesson here is that joint experimentation is going to require simulation of much more of the world than just the primary military entities.  By simulating entities at different resolutions depending on their function, it is possible to achieve vast increases in the numbers of entities that can be simulated. This can be described as selective resolution (similar to selective fidelity, a SIMNET concept).J9901 uses slightly over 1000 human controlled Redland entities simulated on 6 dual processor 450 MHz PCs. Dual processors were chosen because of the restricted space and network taps available at our Topographic Engineering Center site.  To support this the terrain access code used by JSAF was converted to use operating system memory mapping and allow sharing by multiple processes.  We felt that operation on symmetric multiprocessors was important for future applications because multiprocessors can reduce configuration and management costs. Exercise ManagementPreviously, STOW was operated by starting and stopping each simulation computer individually.  For J9901 we are simulating 24 hour trials using six hour runs over the course of four days.  Each day the simulation is restarted from the last save point.  To make the operation of the simulation more manageable, it was decided to centralize control of simulation execution. MARCI, (Multiple host Automation Remote Control and Instrumentation) is a new STOW federate which was developed to support this centralized control. Each federation computer used by J9901 runs a MARCI daemon that notifies MARCI when that computer is available for simulation.  MARCI uses data files to specify which federate and options to run on each computer. MARCI is also used to pause/resume and save/restore the federation using RTI. Saves are executed once an hour and at the end of each day’s run.  The end-of-day save files are restored the next morning.  The hourly saves are used to recover from crashes.  For J9901 STOW had to significantly improve its save/restore capabilities. SLAMEM, the SLAMEM gateway, the clutter simulation, and SOAR had to develop new save/restore capabilities.  Because of the large numbers of entities being saved, improvements in the JSAF save/restore capabilities had to be made as well. While the exercise management capability in STOW is still maturing, it has proved to be a great success. STOW is much more integrated for J9901 than it ever has been in the past.  A second and perhaps obvious lesson from J9901 is to run under the debugger.  During previous STOW exercises the goal had always been to run optimized software that would simulate the largest number of vehicles possible.  Unfortunately, when an optimized federate crashes there is no way to figure out why.  Because additional functionality is always needed, simulation software never stabilizes and we were forced to run unoptimized software in both our UE 98-1 and  STOW 98 exercises.  While this software can produce useful core dumps, the time required to dump core was so long that it was necessary to turn off the core dumps returning us to the same situation we were in with the optimized software.  So for J9901 we are running the entire exercise with unoptimized software under the debugger so that a crashed computer can be quickly queried for where the crash happened and rapidly restarted.  Another big advantage of this approach is that the software is run the same way from development through the exercise reducing the number of test variables. This has significantly reduced the number of crashes we experience.  A secondary lesson is that software for joint experimentation must be able to change continually to keep up with the rapidly evolving vision of the experimenters.Data Collection and AnalysisData collection in J9901 is accomplished by two loggers.  One logger collects analysis data in a format suitable for the analysis software.  The second logger collects data required to play back the simulation for both 2D and 3D inspection.  Data is collected in files approximately one hour long. A new file is started at each checkpoint.  Immediately after a file is complete, it is transferred to the data analysis system where it is processed and added to the analysis repository.  The analysis products, used to support the assessment process, are generated from the data stored in the repository.  These products are typically available for review on the following day.  Feedback received during the initial set of J9901 trial runs indicates that it would be useful to generate some real time analysis products to support quick-look assessments of simulation events. We are also developing some very specialized real time data analysis tools for data verification and debugging.Just as for any analysis experiment a collection of Measures of Performance/Measures of Effectiveness (MOP/MOE) were defined for J9901.  Most of them are related to tracking efficiency and attack efficiency.  These MOPs/MOEs include statistics such as average track duration and the percentage of time each TBM vehicle was tracked. While statistics can provide some insights into performance over the course of an entire run, higher resolution data is required to understand the events behind the statistics.  In previous simulations we have relied on the 2D and 3D playback to explain the statistics.  However, for a simulation as long and complicated as J9901 a more automated approach was needed.  The answer was an event timeline for each vehicle in a TBM missile unit.  The timeline depicts track histories and significant vehicle events (launches, kills, start/stop movements).  It also includes key blue cell events such as the time a track was nominated, paired with weapon systems, and attacked.  The timeline also includes a history of sensor coverage for each vehicle. A timeline provides the analyst with a detailed graphical summary of all the events affecting a vehicle.   For example, an analyst looking at the history of an entity may discover that this vehicle was never tracked because it spent the entire trial hidden in camouflage. This approach was adapted for J9901 from previous SLAMEM simulations for investigating current day counter-SCUD tactics. A lesson from this is that analysis to support joint experimentation can be augmented with intermediate representations that provide broader perspectives than can be achieved from only statistical summaries and raw data.As a training simulation, STOW was able to use wall clock time as simulation time.  With the advent of J9901 it was decided that more fidelity was required to accurately maintain simulation time across pause/saves and save/restores. The entire federation was placed on a synchronized time clock.  This proved to be a rather difficult change. However, this capability enables STOW to advance time nonlinearly and still produce data that can be processed using the analysis tools.An HLA-compatible analysis architecture [10,11,12] has provided the flexibility to easily adapt the simulation data to support tailored analysis objectives such as sensor performance and track history.  For data analysis there were two major types of FOM changes. The first involved adding new objects and interactions because the data would not otherwise be on the network.  An example of these is the contact reports that tell the analysis system when vehicles were hit by radar and what the results were. The second class of FOM changes were made to optimize the analysis process. With the simulation producing on the order of three Gbytes of data per day, searching entity state updates for events like camouflage changes is prohibitively slow.  It is more efficient to record these events as they occur by sending interactions that can be captured by the logger. For example, the FOM was extended to include a task status interaction to optimize the monitoring of key entity state changes. Depending on the radar performance parameters, Red team tempo, and track count the amount of data captured by the playback logger can exceed 500 Mbytes per hour. To store this volume of data STOW is using DVD RAM disks. During the simulation, run data is collected to an 18 Gbyte hard drive. After each day's run the files are transferred to two copies on DVD-RAM drives.  An entire day of playback data is stored on each 5.2 Gbyte disk.  One week's worth of analysis logs and processed analysis data is also stored on a single DVD disk.  A nice feature of this medium is that data can be reviewed directly without the intermediate step of transferring the data onto a hard disk (a process that takes several hours).  With this much data, efficient handling makes the difference between usable data and inaccessible data. Development ScheduleA critical point of joint experimentation is time.  The information age has accelerated technology advances to a rate that was not even dreamed of in the 70’s when computers first emerged into everyday life.  Time to market, overnight delivery, real time intelligence, and online stock trading are everyday examples of this phenomenon.  To be relevant, simulation must also respond to this contraction of time lines.  Simulation development for J9901 began with a few concept discussions in November and December, a two week design session in January and the first Trial in June. As usual both developers and simulation operators commented that if only they had a few more months the simulation could have been much better.  Yet there is a fundamental tradeoff between the time available to build the simulation and the number of alternatives it can be used to explore and the impact it can have.  A year long development cycle for a simulation could become longer than the time it takes to develop a new weapon. We must find ways to build new simulations faster and more accurately.  J9901 was radically different from any exercise previously executed by STOW, yet it was developed in 6 months.  Key factors in being able to develop the federation in this timeframe include the ability to bring new federates together, having sufficient knowledge of the domain readily available, the ability to change the FOM, and a small team with good lines of communication.  ConclusionsDiscovery experimentation may be able to support radical advances in the development of future defense concepts by combining traditional analysis with the human insight gained from experiencing the simulated environment.  The evolution of concepts and testing by opposing teams in a simulated environment is an important mechanism for discovery.  Big wins in the development of J9901 included:Integration of SLAMEM as a new federate instead of developing the surveillance capability from scratch.Using a distributed sensor model for the radar with SLAMEM and STOW components connected via a query protocolUsing a gateway to connect SLAMEM to the federationRepresenting the C4I system via RTI objects and interactionsUsing selective resolution to scale up the vehicle countCentralized simulation controlAn intermediate analysis representation between raw simulation playback and summary statisticsThe use of the High Level Architecture for developing the simulation has helped to reduce development time and improve performance.  Simulations must continue to contract their timelines to maximize their relevance to our ever-changing world.References[1]	Fennell, M. and Wishner, R., "Battlefield Awareness via Synergistic SAR and MTI Exploitation", IEEE AES Systems Magazine Vol. 13, February 1998, No. 2.[2] 	Budge, L., Strini, R., Dehncke, R., and Hunt J, “Synthetic Theater of War (STOW) 97 Overview”, 98S-SIW-086.[3]	Birkel, P. et al., “Pushing the Envelope Toward a Common Process for the Generation of Terrain Data Bases across Federations”, 99S-SIW-016.[4]	Miller D. et al., “The STOW World Wide Terrain Data Base”,  99F-SIW-042.[5]	Miller, D., Adelson, S., Janett, A., Eskew, J., Haes, S., Gibson, B., Huntley, J., Rayment, C., Stroud, B., Usher, T., and Kasputis, S. (1998), “Dynamic Terrain and Objects in the STOW 97 Advanced Concept Technology Demonstration Using the High Level Architecture”, 98S-SIW-161.[6]	McGarry, S., "An Analysis of the RTI Prototype Performance in the STOW 97 ACTD", 98S-SIW-229.[7]	McGarry, S., and Torpey, M.,  “Back to Basics: Balancing Computation and Bandwidth”, 99F-SIW-188.[8] 	Alluisi, E., “The Development of Technology for Collective Training: SIMNET, a Case History”, Human Factors 33(3), 1991, 343-362.[9]	Laird, J., Coulter, K., Jones, R., Kenny, P., Koss, F., and Nielsen, P., “Integrating Intelligent Computer Generated Forces in Distributed Simulations: TacAir-Soar in STOW-97”, 98S-SIW-212.[10]	Messerschmidt, L. and Ring, K., "Fielding an Extensible After Action Review System to Support JCOS and STOW", 1999 Spring SIW.[11]	Ring, K. and Messerschmidt, L.,  "A Federation-Neutral Interchange Specification for Logged Simulation Data" 1998 Fall SIW.[12]	Opper, J., Messerschmidt, L., and Ring, K., Van Metre, P., Gates, B.,  Bero, B.,  "A Common Object Repository For Simulation Analysis", 1997 Fall SIW. Author BiographiesANDY CERANOWICZ is the Technical Director for the STOW program. He is a Scientist at AB Technologies Inc.  Andy led the groups that developed both ModSAF and the SIMNET Semi-Automated Forces.  He has a Ph.D. from The Ohio State University.MARK TORPEY is the Lead Developer and Integrator of the Joint Semi-Automated Forces (JSAF) computer generated forces (CGF) simulation system for the STOW program, and a Staff Software Engineer at Lockheed Martin Information Systems (LMIS) Advanced Simulation Center.  Mark has worked on the STOW team for the last three years, and has been at LMIS for four.  He has his BS CS and MS CS from the University of Massachusetts at Lowell.  He recently received the prestigious LMIS 1999 Galaxy Award for his work on the STOW program. BILL HELFINSTINE is a Senior Software Engineer with Lockheed Martin Information Systems, Advanced Simulation Center.  Bill has worked on the STOW program for three years as part of the JSAF development and integration team.  He has a BS CS from the Massachusetts Institute of Technology. DAVE BAKEMAN is a Senior Software Engineer with Locheed Martin Information Systems, Advanced Simulation Center in Bellevue Washington.  Dave supports STOW with work in terrain databases, dynamic terrain, and 3D visualization.JOHN McCARTHY is a Senior Analyst with Toyon Research Corporation involved in the analysis of ISR operations against battlefield targets and is the primary architect of SLAMEM.STEVE McGARRY is a Member of the Technical Staff in the Distributed Simulation Systems group at the MIT Lincoln Laboratory. He has been integrating systems and developing software for real time simulation since 1983. He joined the SIMNET program in 1988 and has focused on distributed simulation applications since then. LIZANN MESSERSCHMIDT is a Lead Simulation Modeling Engineer at the MITRE Corporation, Reston, VA.  She is the lead technical manager responsible for providing technical oversight to MITRE's AARS efforts including JCOS and STOW. STEVEN MOORE is a Software Engineer at BMH Associates Inc.  Steven developed the MARCI simulation control federate for STOW.