Back to Basics: Balancing Computation and BandwidthStephen McGarry MIT Lincoln Laboratory244 Wood StreetLexington, MA  02420-9185smcgarry@ll.mit.eduMark TorpeyLockheed Martin Information SystemsAdvanced Simulation Center37 North AvenueBurlington, MA 01803ABSTRACT: The issues involved with wide area viewers and rapidly steerable sensors have lingered in platform federations since the inception of distributed simulation. During the SIMNET program and early DIS development much consideration was given to balancing the workload on feeble processors and squeezing data through thin communication pipes. The dead reckoning and fire/detonation protocols were developed to accommodate latency and throughput issues while accurately rendering the three-dimensional real time imaging which served as the primary sensors on the battlefield.In the post Gulf War era the Mark I eyeball is increasingly being replaced by constellations of space-based and airborne sensors with tremendous range and discrimination capabilities. It is necessary to integrate these sensors and their effects on the battlefield into distributed simulations without overburdening individual simulations and without clogging our network infrastructure. As part of the STOW program and the J9901 critical mobile target experiment, we have developed and implemented a sensor-oriented protocol that is distributed, scalable and versatile.  We will present this application-level protocol in detail along with analyses comparing it to previous approaches.1. IntroductionThe J9901 experiment postulates that continuous tracking and rapid response will allow the neutralization of critical mobile targets such as theater ballistic missile launchers and their support equipment. To effect this we have extended the capabilities of the STOW distributed simulation to include a constellation of space-based and airborne radar sensor platforms. The man-in-the-loop portion of the system has shifted from a Joint Task Force training audience to command and control cells dealing with sensor tasking, target selection, and attack prosecution. This shift from a battalion level entity simulation to a ‚Äúsensor heavy‚Äù command and control simulation has inspired us to address the issues raised by sensors with the capability to view large expanses of the terrain database at entity resolution.Traditionally a sensor with these capabilities has been forced to subscribe to entity state information within the sensor‚Äôs potential field of view. As the entity density increases, the burden on the sensor increases proportionally.  The sensor is forced to receive data at a higher resolution than it needs, more frequently than it needs, and process each entity as potentially detectable with equal weight. This issue is magnified when the focus of the simulation is to provide high enough entity density to both confuse the sensors and challenge the players.  Radar sensors are not homogeneous. They are selectively sensitive to targets with different properties. Some sensors are oriented to tracking moving targets while others are used for imaging targets in an attempt to classify them. In order to simulate the critical mobile target problem a combination of many different types of sensors must be represented.2. Sensor TypesWe take advantage of the distinguishing characteristics of the various sensor types to make the distributed sensor protocol efficient. It is useful to have a common understanding of these characteristics.Moving Target Indicator (MTI) ‚Äì This radar yields a broad picture of the target area and is sensitive to ground vehicles which are moving (have velocity greater than zero).  MTI can discriminate between tracked and non-tracked vehicles but otherwise provides little classification information. The footprint used for this type is an arc of 1 to 5 degrees and a range of hundreds of kilometers. As simulated the footprint scans a 120-degree arc composed of 3 degree segments in 60 seconds with a dwell time of 1 to 2 seconds..Figure 1 MTI sweepSynthetic Aperture Radar (SAR) ‚Äì This radar is used to image and classify stationary targets. SAR typically has a long narrow rectangular footprint in scan mode, roughly 1km wide by 10km long. In spot mode, a SAR footprint has a dwell time of 15 to 20 seconds. Figure 2. SAR StripFoliage Penetrating (FOPEN) ‚Äì This radar is still undergoing development. It is used to identify targets concealed in forest. It is most effective against targets on the boundary of the forest canopy and similar to SAR has long dwell times and is effective only against stationary targets.Internetted Unattended Ground Sensors - This is a group of sensors that cover an area as if it was one sensor.  They can sense various effects, such as movement, sound, sight, heat, etc.  The IUGS tend to cover a very small area, but have a high level of detection, and remain fairly static in their area of coverage.The differences between these sensor types can be exploited when performing trivial rejection of a footprint in a target simulator. For example, if the footprint is from a SAR sensor, a moving target cannot be detected so moving targets can be filtered out.3. BottleneckIn previous sensor implementations a sensor would receive entity state data from platform simulators by subscribing for entity state class objects. The entity state attributes contain enough information for a sensor model to determine whether the target is detectable. The sensor model would use the entity position to calculate line-of-sight to the target and entity velocity to filter according to sensor mode. Unfortunately, entity state data is typically produced at a rate chosen to satisfy a visual simulator, which is higher than necessary for a radar sensor. So the radar receives significant redundant information from the entities it is scanning. In addition, a sensor receiving entity state information must compute line-of-sight to each of the entities in its field of view, another costly computation.Figure 3. Traditional sensor model For ground entities on large terrain databases this requires loading in terrain patches for each of the entities each time the line-of-sight computation is run since the obstruction will normally be terrain. This calculation scales with the number of entities in the field of view so the higher the entity density the more expensive the calculation is for the sensor. The cumulative effect of high data input volume (from entity state updates) and expensive line-of-sight processing creates a bottleneck for the sensor application. We have attempted to segment the sensor model and balance the computation and network bandwidth impact on participants. If the goal is to distribute some of the processing associated with the sensor model it is helpful to partition the model in a useful fashion and choose a candidate to distribute. One such decomposition is into three elements ‚Äì detection, tracking, and fusion.Detection ‚Äì is it possible to see the target?Tracking ‚Äì can the target be uniquely distinguished over time?Fusion ‚Äì can the target be classified?Much like the dead reckoning protocol used to extrapolate vehicle position and reduce the number of entity state updates sent between simulators, the distributed functions should not be expensive for the participants and should require limited state information be exchanged.  Fusion, for example, would be very difficult to distribute to all participants since it requires aggregating historical state information and consistently estimating the target classification. This becomes a knotty problem when multiple sensors cooperate (multi-sensor fusion) to enhance the classification estimate.Tracking, too, is a difficult element of the model to distribute. Tracking anomalies like track merging and splitting that occur when multiple vehicles are in close proximity require knowledge of the environment that must be derived and is difficult to reproduce identically in a distributed fashion. It is much more efficient to provide the sensor with perfect association and have the sensor model these anomalies.At its root, detectability is a binary problem. A target is detectable if it falls within the footprint of the sensor field of view and otherwise it is not. This quality makes detection a good candidate for data distribution management efficiencies as well as being a simple evaluation that can be repeatably performed in a distributed fashion. The drawback is that one must know where the sensor field of view is over time and be able to tolerate the distribution of this information. By distributing detection, we parallelize the relatively expensive line-of-sight calculation. This relieves the sensor from computing line-of-sight to each entity and from having to page in the terrain associated with the entity locations. Figure 4. Distributed sensor model.Given that a sensor can produce many field of view changes over time it is important to streamline the calculations required of the participants. By providing additional information about the sensor the distributed detection algorithm can be made more efficient. One efficiency is to postpone expensive calculations until and unless they are absolutely required. If we can trivially reject the field of view information from a sensor as irrelevant before we have to run the inclusion or line-of-sight evaluation, then we can save computational resources. For example, information from sensors that can only detect stationary targets can be rejected for a moving entity or information from non-foliage penetrating sensors can be rejected for entities under a tree canopy. In this way the more expensive computations are run only when they are relevant. Additionally the sensors receive information only from detectable entities, rather than all the entities in the field of view.A protocol that distributes sensor field of view information suffers when a sensor generating field of view updates at a rapid rate. One method of accommodating this is to quantize the sensor scan into segments to reduce the number of beam prints without sacrificing model accuracy.4. ProtocolThe distributed sensor protocol is tunneled through a series of RTI interactions. Each step of the protocol utilizes a separate RTI interaction class. This allows us to maximize the use of the data distribution mechanisms in RTI-s and filter out protocol messages of no interest. The protocol has three components:The sensor platform issues a sensor_footprint   interaction indicating the sensors instantaneous field of view.Any detectable platform within the sensor_footprint answers with a sensor_detection interaction indicating that  the platform is detectable by the sensor.The sensor uses the detections to form tracks and issues track_state object class updates for use by viewers and track based weapon systems.The sensor_footprint contains enough information for any simulator receiving the interaction to determine whether its entities are within the field of view of the sensor. It also contains information to perform trivial rejection on the footprint to weed out any entities that could not possibly be detected by the sensor. Typically this data would be considered internal state of the sensor. We found that this coarse level of filtering using the internal sensor state significantly reduces nuisance detections and hence the associated network traffic. The elements of a sensor_footprint includeTag ‚Äì a tag returned in the sensor_detection used by the sensor to correlate footprints with detections.Sensor Platform Location and velocity ‚Äì used by receiving simulations to compute line-of-sight.Sensor Mode ‚Äì SAR, MTI, FOPEN, IUGS available to the receiver for mode specific filtering.Footprint vertices ‚Äì an array of vertices in geocentric coordinates used by the receiver to determine inclusion.Filter mode control ‚Äì used by the receiver to determine which filter parameters to apply (line-of-sight, velocity)Minimum and Maximum detectable target velocity ‚Äì used by the receiver to filter the detectionMaximum FOPEN penetration depth ‚Äì used by the receiver to filter the detection.Since at least one of the objectives in developing this protocol was to distribute the line-of-sight computation, it is reasonable for the sensor to provide information about its own dynamics. The simulators will receive many footprints per second from a single sensor so it is necessary to make the computation as efficient as possible for the receiver. Experience tells us that the line-of-sight computation is expensive, so it is advantageous to postpone the line-of-sight check as long as possible in the evaluation of a footprint. First the inclusion (point in polygon) is run on the vehicle position to see if it is in the sensor footprint. Next, a quick check is made to determine if the vehicle is completely concealed within an obstruction such as a building or cave. Then the radial velocity relative to the sensor is compared to the minimum and maximum thresholds provided in the sensor_footprint. If the target has passed these checks the intervisability or line-of-sight check is performed and finally the FOPEN depth if applicable. An advantage to computing the line-of-sight at the receiver is that the terrain patch associated with the entity is already loaded. Given that the sensors are airborne or space-based, it is most likely that a terrain obstruction will occur closer to the entity than the sensor so having the local terrain available for line-of-sight determination is highly useful. If a receiver has an entity in the footprint with line-of-sight and passes the coarse filters the simulator issues a sensor_detection interaction. The detection contains truth data about the entity being detected. Each detection for an entity is used to form the track. The fields of a sensor_detection interaction include:Tag ‚Äì the tag received in the footprint is returned to the sensor for correlation.Sensor location and velocity ‚Äì also returned for correlation within the sensor.Entity location and velocity ‚Äì true entity state data used for trackingEntity appearance ‚Äì includes information on Cover Concealment and Detection (CCD) state of the entity.Entity Type ‚Äì the true class of the entity, used by the fusion center processing to categorize the target.Entity ID - the unique ID of the entity used by the tracker and fusion center to associate (or miss-associate) with  existing tracks.Detection Result ‚Äì an indication of whether the entity was filtered (i.e. by line-of-sight or velocity) or is detectable.Since a sensor simulator may be simulating multiple sensor platforms it is necessary to return the tag and other sensor-specific information so that the receiving sensor simulator can correlate the return. For after action review purposes, it is also useful to emit sensor_detections for interesting targets even if the result was non-detection. A sensor receiving a detection with a negative result would discard the detection. The rest of the information in the detection is truth state of the entity. This is used by the sensor simulation for tracking and fusion of the entity in the form of a track_state class object.The sensor creates a track_state object when it has processed a sufficient amount of information about an entity via the sensor_detections to form a track. The algorithms used for tracking and fusion are beyond the scope of this paper. The protocol allows for many variants and is reasonably independent of the specific algorithms used for tracking and fusion.   A track_state object is composed of two groups of attributes. The first group is composed of attributes estimated by the sensor. These include the position, velocity and estimated type. The second group is composed of those attributes added by the attack control node and includes whether the track is targeted and the type of weapon assigned. The track_state attributes assigned by the sensor include:Track Number ‚Äì a unique ID for the track assigned by the sensor.Estimated Position and Velocity ‚Äì for the target computed by the trackerLocation Error ‚Äì a measure of the quality of the estimateEstimated type ‚Äì an array of types and associated probabilities the target is of that type.Associated entity ID ‚Äì the ID of the entity on which the track is based. This is used for AAR correlation.Number of Hits -- a count of the individual number of sensor hits (gives the players a hint of accuracy)Last update time -- when was the last time the fused track was updated by the sensor model?Other track_state attributes used to effect the simulation of the attack control node include:Nomination priority and nomination string ‚Äì applied by the ASCN.Weapon Assigned ‚Äì the chosen attack platformAttack priority ‚Äì the relative priority to use to prosecute the attackTime on target ‚Äì estimate time until the attack could be carried out, used in deciding which weapon to select.Force ID ‚Äì a user settable field indicating whether the target has been determined to be friendly, neutral, or enemy.State ‚Äì current state of the target from the players perspective (interesting, being attacked, mission ready‚Ä¶.)The tasks of the sensor model are distributed according to the following table.Sensor SimulationEntity SimulationSensor PositionFootprint InclusionSensor DynamicsLine-of-sightSensor FootprintVelocity FilterDetection ModelFOPEN Depth FilterTracking ModelConcealment FilterFusion ModelFor each entity and for each sensor footprint the entity simulation performs a portion of the sensor model.5. AnalysisA preliminary analysis shows the distributed sensor protocol to be feasible. It distributes some of the computational load without overburdening the network resources.The experiment is projected to run with 10,000 entities. This provides suitable entity density to both challenge the players and confuse the sensors. Many of the entities have been simplified, both in behavior and appearance, so they produce much less than the traditional packet per second per entity. So the network impact on a traditional sensor implementation would be:500 entities @ 1 pps/entity = 500pps9500 entities @1p/3o sec/entity = 318 ppstotal 818 pps/sensorUsing the distributed protocol a MTI scanning sensor updates it‚Äôs footprint about once per second. The driving factor for the network is the number of detection responses resulting from the footprint. The number of detections is correlated to the entity density and the size of the footprint.We distribute the 10,000 entities over roughly 18 terrain cells of 100x100km so the entity density is 10000 entities/(18*100*100) = 0.56 entities/km**2A 100km MTI scan of 3 degrees covers an area of approximately 471 km**2 so any one scan will produce on average:471km**2 * 0.56 entities/km**2 = 26 detections/second/sensorThis compares favorably with the 800pps entity state stream of packets.The sensor models have a significant computational task to model the tracking and fusion of the targets. Offloading the inclusion and intervisibility check to the many entity simulation processors is fundamentally an efficient use of available resources. 6. Future EffortsA number of refinements can be added to the protocol to further enhance its network efficiency. One current drawback is the number of interactions used to describe the sensor_ footprint. One could easily imagine simple scan modes for sensors represented as a set of initial conditions and an equation of motion. The MTI sweep and SAR strip are good candidates for such a representation because the former is sinusoidal and the latter is linear. Since the platform flies a highly predictable trajectory, dead reckoning of the platform position can be very accurate. This would significantly reduce the number of footprints requiring only updates for mode changes. This approach is complicated by the manual and automatic re-tasking of the sensors which would upset the footprint extrapolation.Most radar is sensitive to target orientation. Currently a sensor_detection is returned to the sensor regardless of the target orientation and the sensor is responsible for degrading the detection appropriately. This is another opportunity to filter the detection response at the entity simulation, which inherently knows the target orientation.Pure probabilistic models for detection and target type confusion could readily be added to the entity simulation as well. If, for example, the sensor model is dropping detections according to a statistical model, the dice roll could be run on the entity simulator as easily as on the sensor simulator, again reducing the number of detections the sensor has to process.Only a coarse level of Data Distribution Management was applied to the sensor protocol. Separate routing spaces were allocated for sensor_footprint and sensor_detection class interactions so that detections would be rejected at all but sensor simulations and footprints need only be processed by entity simulations. For all but the space-based platforms a geographic segregation is possible.6. Conclusion We have found that we can adequately distribute part of the sensor processing among the entity simulations without overburdening the network or the participating simulations. This partitioning is versatile and can be applied to a range of sensors used for tracking and imaging.7. References[1]	Ceranowicz, Andy et. al : ‚ÄúJ9901: Federation Development for Joint Experimentation‚Äù Proceeding of the Simulation Interoperability Workshop, Fall 1999, 99F-SIW-120Author BiographySTEPHEN MCGARRY is a member of the technical staff in the Distributed Systems group at the MIT Lincoln Laboratory. He has been integrating systems and developing software for real time simulation since 1983. He joined the SIMNET program in 1988 continues to focus on distributed simulation applications. MARK TORPEY is the Lead Developer and Lead Integrator of the JointSemi-Automated Forces (JSAF) computer generated forces (CGF) simulation system for the DARPA Synthetic Theater of War (STOW) program, and a Staff Software Engineer at Lockheed Martin Information Systems (LMIS) Advanced years, and has been at LMIS for four.  He has his BS CS and MS CS prestigious LMIS 1999 Galaxy Award for his work on the STOW program.  