Using Subject Matter Experts for Results Validationof a Complex Theater Warfare SimulationMichael L. MetzInnovative Management Concepts, Inc.45625 Willow Pond PlazaSterling, Virginia 20164703-318-8044 ext 210e-mail:   HYPERLINK "mailto:mmetz@imcva.com" mmetz@imcva.comS.Y. HarmonZetetix, Inc. P.O. Box 2640 Agoura, CA 91376-2640(818) 991-0480harmon@zetetix.comKeywords:    Military, Validation, Results Validation, JWARSABSTRACT:  This paper addresses the planning for the use of Subject Matter Experts (SMEs) to support the results validation of the Joint Warfare System (JWARS).  JWARS is a complex constructive simulation of theater level joint warfare under development by the Office of the Secretary of Defense (OSD) for use in analysis.  The JWARS results validation process has included limited SME predictions and comparisons to other theater level simulations.  The planning for formal results validation using SME predictions and comparing them to JWARS output is planned for the Spring of 2002.  One of the major issues in the planning is how to quantify the variations in SME-derived referents.SMEs supply much of the crucial referent information needed to validate simulations of complex phenomena.  But, SMEs also introduce special problems. The most notable of these arise from the seemingly irresolvable differences of opinions, both between the opinions from separate SMEs and between different opinions from the same SME.  This paper describes some fundamental characteristics of SME-supplied information.  We propose using well-established survey methodologies to collect referent information from SMEs with suitable redundancy.  Then, we suggest applying common statistical techniques to this information to characterize the variations naturally present in SME opinions and the subject phenomena themselves.  Finally, we will show how to use this approach to develop some of the referents for the JWARS simulation.Paper Number: 01F-SIW-0511.  IntroductionCurrently, users and developers of theater level simulations must employ subject matter experts (SMEs) for validation.  In this role, SMEs serve as sources of referent knowledge, requirements interpretation, and the ability to compare simulation capabilities against this information to make validation decisions.  This situation inextricably couples these separate functions and leads to several problems including•	Inconsistent validation assessment results,•	Lack of repeatability in validation decisions,•	Inability to identify the root causes of validity problems,•	Difficulties in reconciling the differences between SME judgements, and•	Reduced user confidence in simulation credibility.The impossibility of collecting a rich set of experimental data describing actual warfare at a theater level makes the use of SME opinions for validation a necessity for the foreseeable future.  Yet, the importance of theater level simulations and the decisions made from their results increases the emphasis upon the validity of those results.  New techniques must be developed for handling SME contributions that improve their independence, consistency, repeatability and, ultimately, credibility.The Joint Warfare System (JWARS) simulation, now under development by the Program Analysis & Evaluation (PA&E) organization in the Office of the Secretary of Defense (OSD), is a theater level simulation that currently faces the challenges of validating its results through SME opinions.  This paper addresses the planning for using SMEs to support the results validation of the JWARS simulation.  JWARS results validation encompasses two levels of SME activities, including the JWARS Verification & Validation (V&V) Team’s SMEs and the group of SMEs who are members of the JWARS User Subgroups.  Both activities are described but the emphasis is on the latter.  This paper focuses on how to quantitatively employ SME predictions of possible warfare outcomes in JWARS results validation.  The approach to these activities relies upon building separate SME opinions into a statistically consistent referent picture that describes predicted outcomes, the variances associated with those predictions, and the confidences associated with those variances.2.  The Joint Warfare System JWARS (JWARS, 2000) is a perception based constructive simulation of joint force theater level warfare intended for analysis.  JWARS will support the following analysis tasks:•	Military force assessment;•	Force planning (deliberate and crisis actions) and execution;•	Weapon system effectiveness and trade off analysis; and•	Military operations concept and doctrine development and assessment.JWARS must be a state-of-the-art, closed-form simulation of joint campaign-level warfare that represents uniquely joint functions, joint processes, and component warfare operations.  It is also required to be joint doctrine based and capable of representing future warfare forces and weapons.Future JWARS users will likely include the Joint Staff (both J-4 and J-8), the US Military Services (Army, Navy, Air Force and Marines), the US regional and functional combatant commands, the Office of the Secretary of Defense, various Joint Task Forces, and the defense industry.  Several future users are using a pre-beta installation at this time for familiarization and training. JWARS prototype development started in 1995 and the full-scale development program began in April 1997 with the addition of the concurrent V&V effort in September of that year.  The JWARS Release 1 series will culminate with the declaration by the JWARS Office of Limited Initial Operating Capability (IOC) for study execution sometime in 2002.  Both a JWARS Overview and the JWARS Operational Requirements Document (ORD) are available from the JWARS website at  HYPERLINK "https://www.jointmodels.mil" https://www.jointmodels.mil. (JWARS Office 2000)  (Joint Staff, J-8, 1998) 3.  JWARS V&V Process and PlanImmediately upon starting the V&V effort in September 1997, the JWARS Office formed the JWARS V&V Oversight Group.  This group, co-chaired by the JWARS Office and the V&V Agent, developed a tailored V&V process and an associated JWARS V&V Plan (JWARS, 1998).  The procedures for developing the V&V process were based on the Defense Modeling and Simulation Office’s (DMSO’s) Department of Defense Verification, Validation and Accreditation Recommended Practices Guide (RPG) (DMSO, 1996) (DMSO, 2000). The JWARS V&V process and plan identifies the members of the JWARS User Subgroups (a group of warfare functional experts, operations researchers, and experts in each JWARS warfare functionality area) as the JWARS Validation Authority.  The V&V Agent is named the Verification Agent and Authority as well as responsible for Results Validation in accordance with the JWARS V&V Plan.4.  Conduct of the JWARS Results Validation ProcessThe JWARS results validation process focuses on validation of the simulation results to determine if the simulation is acceptable for the intended analytic use.  Results validation involves comparing a simulation output against a referent and the purpose’s requirements.  While the ideal referent for a simulation is the real world, this referent (future joint force warfare between a US led coalition and a red force, both using future weapons and warfighting concepts) is obviously not available for JWARS validation.  Consequently, other results validation referents are required.  To date three have been identified and they include:  •	Results of operational tests and evaluations;•	Results from other similar validated simulations; and•	SME opinions.This discussion addresses using SME opinions for validation of theater level simulation results.4.1  Results Validation Process SME OpinionCurrently the JWARS V&V Team uses two methods that employ SME opinions. The first is internal to the V&V Team.  Working with the analytical output requirements, the V&V Team created Model Validation Criteria (MVC) as a surrogate for acceptability criteria (JWARS has no accreditation process and subsequently, no acceptability criteria).  Each MVC was linked to a specific output requirement and identified Derived Representation Requirements (DRRs) for the things and processes that should be represented in JWARS to produce the required simulation output.  The V&V Team then estimated what inputs could be changed to the simulation and how these input changes would affect the direction and gross quantity of the output.  The V&V Team then compared the JWARS base case results against the results with the modified inputs to assess the sensitivity of the analytic output to the input changes.  The V&V Team members have some expertise in each of the JWARS warfare functionality areas, making this process possible.The second method is more formal and involves SMEs nominated by the JWARS users from the JWARS User Subgroups.  In this process the V&V Team plans to create JWARS vignettes from the base case that are discrete enough to allow the selected SMEs to make predictions about the outcome of the warfare activity.  This may include a Blue battalion being attacked by a Red brigade or regiment sized force, Blue aircraft attacking a Red unit, Blue naval forces conducting a mine sweeping operation,  a Blue amphibious operation, or Blue strategic mobility activities.  It will be necessary to select multiple vignettes in each of the JWARS warfare functionality areas to create a sufficiently complete results validation event.  The JWARS functional areas are: •	Air combat; •	Naval combat; •	Command, control, communications, computers, intelligence, surveillance, and reconnaissance; and •	Transportation and logistics.One of the challenges for the JWARS V&V Team (with the assistance of the JWARS Office) is to carefully prepare both the results validation vignettes and the selection and use of the SMEs.5.  Evaluation of JWARS Simulation ResultsPredictions of the outcomes of theater level operations, particularly those of future joint operations, necessarily involve uncertainty.  Thus, any referent for simulations of those operations, like the JWARS simulation, should include descriptions of the uncertainty of those predictions as well as the predicted outcomes themselves.  This implies that simulations of theater level operations must portray the behavior of the uncertainties along with replicating the predicted outcomes sufficiently validly to serve a purpose.  So, any exploration of such a simulation’s validity should compare the uncertainty of the referent with that associated with the simulation results.  Traditional validation activities in which SMEs observe simulation results and informally compare those with the expectations created from their experiential knowledge do not explicitly address the issues associated with uncertainty.  Failure to account for the uncertainty inherent in theater level operations creates many of the problems that have been associated with using SMEs, primarily lack of repeatability and disagreements between individual SME opinions.  Recognizing the uncertainty associated with the predictions of future theater level operations suggests an immediate resolution to these problems.The approach that we propose applying to the JWARS results validation differs from traditional validation of theater level simulations in two ways:•	It uses SMEs solely to build a referent against which the JWARS simulation results are compared to determine the accuracy of those results.•	It explicitly characterizes the uncertainty associated with the SME predictions and makes that characterization part of the referent against which the stochastic behavior of the simulation is compared.Motivation for the first difference comes from trying to decouple the various SME contributions to the traditional validation process.  This decoupling separates the SME as a source of referent knowledge, as an interpreter of user requirements, and as an evaluator of simulation results into individual components.  Further, our approach emphasizes the importance of the SME as a source of referent knowledge.  This strengthens the objectivity of the validation results and creates clearer visibility of the causes of any validity problems.Motivation for the second difference comes from the rationale presented above.  Our approach recognizes that the predictions of theater level operations are inherently uncertain and it explicitly associates the variability among SME predictions with that uncertainty.  This association enables quantifying the uncertainty associated with each prediction.  Having characterized the nature of the prediction uncertainties provides another set of points against which to compare the simulation results and judge the validity of those results from that comparison.  This improves the likely correctness of the validation results, permits estimation of the confidences of simulation to referent correlations, and strengthens the credibility of the results validation process as a whole.5.1  Construction of Referent from SME OpinionsOur approach begins with the design of a set of vignettes, each of which represents a narrow operational situation.  Each vignette defines •	A set of operational components that participate and interact (e.g., Blue tank-heavy battalion with air support, Red armored brigade, intelligence assets to support each),•	A set of conditions under which the vignette components must operate (e.g., mountainous terrain, rainy weather, night time),•	A set of constraints that limit the extent of the operations (e.g., Blue and Red missions, troop conditions, supply situations, time available), and•	The variables against which the simulation results will be measured (e.g., terrain gained, Blue and Red losses for each asset type, expenditures of supplies).The entire set of vignettes redundantly covers the important representational space of the JWARS simulation at its current state of development.  Also, each vignette may explore the influences of multiple dimensions of warfare (e.g., maneuver, engagement, logistics, combat engineering, intelligence) upon the outcomes.In the next step, we develop questionnaires, based upon the vignettes that extract the expert knowledge from the SMEs.  The design of these questionnaires draws from well-understood interviewing techniques.  Each questionnaire describes the vignettes associated with it and asks a number of questions about these vignettes.  These questions elicit three types of information from the participating SMEs:•	Their understanding of the vignette situation and the factors that affect its outcomes,•	Their predictions of the likely outcomes of the vignette situations, and•	The area and level of expertise of the SME.Many questions seek the same information but from different perspectives.  This redundancy provides the initial means from which to characterize the uncertainty associated with the SME predictions.  The questionnaires include questions that encourage the SME to characterize their own areas and levels of expertise as well as questions that indirectly support the characterizations from these self reports (e.g., queries about the subject vignettes that indicate the SME’s areas of expertise).  This redundancy gives some measure of the consistency of the SME opinions and predictions.  We realize that this redundancy does not necessarily improve the accuracy of our assessments but it does make available the mechanisms by which we can identify and characterize error sources [Youden].  Considerable attention will be spent to design questions that lead to predictions with uncertainties described by normal distributions wherever possible.  The psychological and sociological testing literature represents a rich source of information about constructing valid questionnaires and interview techniques from which we will certainly draw.We will administer these questionnaires to as large a sample of SMEs as possible to improve our understanding of the prediction variability associated with each vignette.  This can be done in person and via the Internet.  Internet administration has several advantages including •	Reducing the burden on the SME, •	Increasing the convenience to the SME, •	Reducing travel costs, •	Increasing the likely number of participants, and, possibly, •	Improving the independence of the responses.Despite its advantages, the Internet survey must be more carefully prepared since immediate clarification of respondent problems will not likely be possible.  Initially, we will try both survey approaches and evaluate the effectiveness of each for this purpose.The responses to these questionnaires will provide a rich source of raw data from which we will construct the JWARS simulation referent.  We will analyze this data in a number of ways.Any experimental effort must characterize the error sources and the magnitude of the effects of those errors upon the measurements [Youden]. To this end, we have identified two potentially large sources of errors that could contaminate the referent built from these SME responses:•	SME interdependence from their interactions and•	Expertise biases.The first of these error sources results from the possible interactions between collocated SMEs responding to the questionnaire via the Internet.  The lack of supervision during the survey administration creates the possibility that collocated SMEs might cooperate, even casually.  We have assumed that each responding SME acts as an independent source of knowledge and any cooperation could introduce coherence into the predictions that does not actually exist.The second error source that could bring in bias is SME area and level of expertise.  SMEs responding to situations outside their boundaries of expertise could introduce greater uncertainty in the compiled predictions than that derived from knowledgeable responses.  Further, inexperience also tends to introduce bias towards particular answers.  Both of these error sources could contaminate part or all of the data derived from these questionnaires.As a result, we will analyze the raw responses to assess the contributions of these error sources, determine the magnitude of their effects, and correct for them when possible.  To this end, we will •	Identify collocated groups of SMEs from their responses and our understanding of their current assignments and duty stations, and •	Characterize respondent SME areas and levels of expertise from their self report and from the questions that indirectly test their expertise.We will then apply Chi-squared tests [Bulmer, Mandel] to the raw data we receive to assess the independence of that data from these error sources.  We will examine any data that does not pass these tests and determine if quantifiable biases affect those data.  If we identify any biases in the data then we will attempt to localize those biases in the data sets, quantify them and normalize the affected data sets appropriately.  That data that we cannot normalize will simply be removed from the referent data set.Having corrected the raw data for the effects of errors, we will then proceed to build the referent.  This involves computing the arithmetic means of the SME predictions.  If necessary, we will compute separate means for the different levels of expertise represented in the respondent sample.  We will also compute several characteristics of the uncertainty associated with those means including•	Standard deviation (second moment),•	Skewness (third moment), and•	Kurtosis (fourth moment).We intend to compute skewness and kurtosis in their nondimensional forms [Bulmer].  We will then assess the skewness of each result to assess the impact of the distribution’s deviation from a normal distribution.  If the skewness shows moderate or large deviation (i.e., skewness > 0.5) then we shall adjust the means and standard deviations to more correctly reflect the nature of the uncertainties [Bulmer].  We have, in this analysis, also assumed all distributions to be unimodal.  We will examine the raw data carefully to determine the likelihood of any violations of this assumption and will correct the data appropriately if such cases are detected.  We will also examine all of the statistical results for any anomalies that may have manifested in these refined data.  If we discover these then we will attempt to identify the nature of the error sources and correct or annotate the data appropriately.  Further, we will use the statistical characteristics of the individual SME predictions to compute the variances of the standard deviations, skewnesses and kurtoses themselves.  This gives us another set of measures of the statistical behavior of the data set against which to compare the simulation results.Finally, we will analyze the raw and statistical data for any correlations between the prediction results and the vignette conditions using multivariate analysis [Mandel, Bulmer].  This analysis will identify any dependencies between the vignette conditions and the prediction outcomes as well as the nature and significance of these correlations.  We intend to initially identify the degree of correlation by the correlation coefficient and compute correlation significance from the significance of that coefficient [Mandel].  When we discover a significant correlation, we will attempt to determine if a causal relationship exists between the variables or if the actual causal relationship exists between the examined variables sharing another common variable.  From this information, we will attempt to statistically characterize any causal relationships that we discover.  If we fail to identify the causal relationships underlying any correlations then we will characterize the correlations alone.  We will include, in the referent, the most significant correlations and descriptions of the causal relationships that we discover in the data.The results of this analysis of the raw SME predictions will establish the referent for the JWARS simulation.  This referent will describe the mean predictions as a function of SME expertise level and the combined mean predictions.  It will also describe the nature of the uncertainties and the correlations associated with each of these prediction sets.  In addition, this referent will identify areas where SME opinions do not agree.  This disagreement highlights the areas where the referent is weakest.  Such weaknesses may reflect the need to collect further information through additional surveys if possible.  With descriptions of strengths and weaknesses combined, the integrated result of all this information will be a referent far richer and more objective than could be derived subjectively from a small set of SMEs.5.2  Validation of Simulation ResultsThe next step in our validation process is to execute the vignettes in the JWARS simulation.  Executions, repeated a statistically significant number of times, will generate the distributions of the simulation’s predictions.  We will then analyze the results from these executions to•	Determine the prediction means,•	Compute the statistical characteristics (i.e., second, third and fourth moments) of the distributions around those means, and•	Identify the correlations between the simulation’s predictions and the vignette conditions as well as the significance of those correlations.In this effort, we will reproduce the data in the referent from the simulation instead of the SMEs.  Using the same vignettes and the same analysis procedures as used to build the referent will make the comparison of the simulation results against the referent trivial.  Given that both the referent and the JWARS simulation represent uncertain events, we expect the simulation results and the referent to vary in places.  The question is whether this variance is significant or not.  We will test significance by applying the Student’s t-test [Bulmer, Mandel] to these comparisons.  We have chosen this significance test over others because we only have the means and the standard deviations of the sampled data available [Bulmer].  This test will give us a measure of the confidence that the simulation results agree with the referent.  We will apply this test to each of the prediction means, statistical characteristics and correlations described in the referent.  Each factor comparison will result in a difference between simulation results and referent and a confidence that the two results agree statistically.This analysis takes validation beyond its current state by delivering not only a comparison of how closely the simulation results agree with the referent but also a measure of the confidence of that agreement at each point of the comparison.6.  SummaryIn this paper, we propose an approach for validating the JWARS simulation results that applies to all simulations that must rely upon SMEs for the referent knowledge.  This approach uniquely•	Decouples the functions that SMEs serve and•	Explicitly recognizes the existence and influence of uncertainty in SME predictions and simulation results.We propose using SMEs only to build the referent knowledge and extract the uncertainty associated with that referent from the variance in the SME predictions.  We then statistically compare the simulation results against both the mean predictions and the uncertainty characteristics.  This results in measures of the deviation of the simulation results from the referent mean predictions (i.e., error) as well as of the deviation of the simulation’s representation of uncertainty from the actual uncertainty associated with the referent means.  Further, this statistical treatment permits us to estimate the confidence with which the simulation results agree with the referent.This approach has several advantages over current techniques:•	It reduces the subjectivity commonly associated with employing SMEs for simulation validation.•	It reinterprets disparities and disagreements between individual SME opinions as statistical variations due to uncertainty thus turning those problems into explicit measures associated with the referent.•	It quantifies and employs uncertainty in both the referent and the simulation results and incorporates that factor as an integral part of the validation process.•	It provides the means to determine and assess the magnitude of any error sources that affect the referent knowledge.•	It applies well understood statistical techniques to treating both referent and simulation data and this significantly improves the credibility of the validation results.•	It quantitatively identifies the boundaries of significance in comparisons of the simulation results against the referent and, in doing so, it creates measures of confidence of their agreement to support simulation accreditation decisions.We intend to demonstrate this approach in the results validation of the JWARS simulation.  This application should give us the opportunity to gain many lessons learned that we will report in a later paper.6. ReferencesNote:  website URLs were accurate on 1 July 2001.DMSO (1996). Department of Defense Verification, Validation, and Accreditation Recommended Practices Guide. November 1996.DMSO (2000). Department of Defense Verification, Validation, and Accreditation Recommended Practices Guide. Build 1 (and future completed editions) available at:   HYPERLINK "http://www.msiac.dmso.mil/vva/" http://www.msiac.dmso.mil/vva/.Joint Staff J-8 (1998).  Joint Warfare System (JWARS) Operational Requirements Document (ORD), 27 August 1998.  Available at:  HYPERLINK "https://www.jointmodels.army.mil/jwars/library.html" https://www.jointmodels.army.mil/jwars/library.htmlJWARS (1998).  Joint Warfare System (JWARS) Verification and Validation Plan, Version 3.0, 13 August 1998.  Available at:  HYPERLINK "https://www.jointmodels.army.mil/jwars/library.html" https://www.jointmodels.army.mil/jwars/library.htmlJWARS (2000). Joint Warfare System (JWARS) Overview Briefing, July 20, 2000.  Available at  HYPERLINK "https://www.jointmodels.army.mil/jwars/library.html" https://www.jointmodels.army.mil/jwars/library.html.Pace (2000).  Dale K. Pace. “Use of Subject Matter Experts (SMEs) in Simulation Evaluation,” 2000 Fall Simulation Interoperability Workshop Papers, September 2000. Available at:   HYPERLINK http://www.sisostds.org http://www.sisostds.org.Bulmer (1979).  M.G. Bulmer, Principles of Statistics, Dover Publications, New York, NY, 1979.Mandel (1984).  John Mandel, The Statistical Analysis of Experimental Data, Dover Publications, New York, NY, 1979.Youden (1998).  W.J. Youden, Experimentation and Measurement, Dover Publications, New York, NY, 1998. Author BiographiesMICHAEL METZ, Technical Director of the Joint Warfare System (JWARS) Verification and Validation Program at Innovative Management Concepts, Inc., is a specialist in the design, development, verification, validation, and accreditation of simulations.  He is a member of the Defense Modeling and Simulation Office’s Verification, Validation and Accreditation (VV&A) Technical Working Group and the VV&A Technical Support Team.  SCOTT HARMON, president of Zetetix, a small business specializing in modeling complex information systems.  Mr. Harmon has been developing rigorous techniques for the validation of simulation federations and human behavior representations.PAGE  PAGE  2