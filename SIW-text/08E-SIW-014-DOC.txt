On the evaluation of simulation results in complex and value-critical decision environments by statistical measuresMarko A. HofmannInstitute for Technology of Intelligent Systems (ITIS)University of the Federal Armed Forces MunichHeisenbergweg 39Germany - 85577 Neubiberg0049 89 6004 3242 HYPERLINK "mailto:marko.hofmann@ unibw.de" marko.hofmann@ unibw.deKeywords:Uncertainty, complex decision environments,  simulation supported decision making, statistics, measures of success and effectiveness, insight versus prediction ABSTRACT: Military decision making is generally a paramount example for a complex and value-critical decision environment (which is often even time-critical). Using simulations in order to support decision makers in such environments demands a self-critical reflection of the simulation results, since no single simulation run can be of much predictive validity (at least ex ante). Therefore, one tries to capture the complexity and uncertainty of the “real world” with stochastic simulation models. Their evaluation is based on the statistical analysis of a sequence of N simulation runs. This standard approach leads to a sample mean and variance which are subsequently the central basis for every advice based on the simulation model. There are several critical questions about this approach, some of them are epistemological, like, for example: It is really sufficient to capture the uncertainty of a complex real world system by stochastic variation? Other questions are more pragmatic: What is a sensible measure of effectiveness for the use of such aggregated simulation results in military practice. The acid test of such an approach will always be prediction, regardless of the importance of the slogan “insight not numbers”. Hence, we must ask, whether these results help to reduce the rate of error in forecasting or not?  And if they do, to what extent?In statistics such questions are answered by so called PRE-measures (proportional reduction in error). The paper – which presents work in progress –  examines how such measures can be constructed and used for simulation supported military decision making. Taking epistemological consideration into account it demonstrates the possibilities and (narrow) limits of this approach. The basic intention of the paper is to raise further discussions on the topic.1. IntroductionMilitary decision making is probably one of the most challenging endeavors of all human trying to govern the future. Most military decisions attempt to affect extremely complex systems, in which the outcome and consequences of an option are almost unforeseeable. This is mainly due to the fact that “war is the realm of uncertainty” [1]. It is therefore obvious, that exact predictions of military conflicts are, in general, impossible and that the competent leader must be flexible. However, prediction need not to be perfect in order to be useful, as is well known from weather forecasts, for example. With respect to statistical measures the prediction quality has only to be better than what is currently used. The value of a special prediction method or tool has therefore to be assessed in comparison with other methods or tools. Hence, if simulation systems are used to make predictions of military conflicts, the paramount question is, whether these predictions are statistically better than predictions made without them. We can even go a step further and ask precisely to what extend such simulation-based predictions are better than conventional predictions. In descriptive statistics, special measures of correlation are used to express this extent as the proportional reduction in error (PRE). PRE measures are composed of two error rates of prediction. Traditional PRE measures, for example,  Goodmans’ and Kruskals’ λ [2], are based on a comparison between predictions made on the distribution of only one or two variables. This idea can be transferred to simulation based military decision making. Here, the error rates of prediction with and without simulation can be used to calculate a PRE measure. The paper is meant as a first attempt towards the empirical evaluation of decision support tools for complex and value critical environments in general and at the same time as a critique of the whole endeavor of simulation supported decision making based on statistic measures. Most of the empirical testing of this arguments is, however, work in progress.The remainder of this paper is organized as follows: Section 2 discusses the challenge of military decision making, the problem of uncertainty and how simulation can be used to deal with it. Section 3 presents PRE (proportional reduction in error) measures from descriptive statistics. Section 4 transfers the concept of PRE measures to the evaluation of stochastic simulation results. Section 5 presents some preliminary results and discuses them critically. Section 6 concludes the paper by reiterating its contributions and suggests some conclusions and future research directions. 2. Simulation support in military decision making2.1 Military decision makingMilitary leadership is to a large extent decision making in concrete, critical, complex and controversial situations (C4-situations), which means that a problem has to be solved which isa currently unsolved real world problem without academic simplifications or generalizing abstractions (concrete),time critical (in the sense that the time available for decision making is scarce with respect to the accessible, problem relevant data, and also in the sense that the time horizon for which the results of the decision have to be evaluated is a priori unclear and therefore likely to be underestimated), and value critical (personal danger, damage, costs etc.) (critical)situated in an environment with nonlinear relations and different kinds of feedback loops between causes and effects (complex),differently perceived by at least two opposing social groups, with, at least, incompatible goals (controversial).It is also generally assumed that the decision making process is based on the evaluation of possible consequences of different available options.Examples for C4-situations are not only military combat but also all kinds of military and civilian evacuation operations, time restricted social and political controversies as well as direct economic competition. The supreme attribute of such decision environments is ubiquitous uncertainty. 2.2 UncertaintyMilitary uncertainty includes commanders’ imperfect intelligence regarding their enemy’s numbers, disposition, capabilities, and, especially, intent, regarding features of the terrain and environment, and even including inaccurate knowledge of the state of their own forces. Uncertainty is not only caused by the limits of reconnaissance, by the enemy’s feints and disinformation, by delays in receiving intelligence and difficulties passing orders, and by the difficult task of forming a cogent and consistent picture from a large amount of diverse and doubtful data, it is also caused by a general impossibility to predict the reasoning of single human minds and the behavior of agitated groups. During actual combat the uncertainty of war and the chaos of the battlefield even increase, since chance and imponderabilities like fear, hate and despair become of increasing importance. Taking this considerations serious it is obvious that dealing appropriately with uncertainty is a vital function of all military decision making.There are two classes of uncertainty distinguished in scientific modeling: parametric and structural uncertainty. Parametric uncertainty means that we know the relevant factors for a given phenomenon, but miss the exact (initial) values of these factors. In other words, we have good empirical evidence that the causal reasoning of the model we use is a adequate representation of the relations in the real world. What is sometimes hard to find are the "right" parameters for the model. Structural uncertainty means that we are not sure if we know all the relevant factors and that we most probably do not know their interdependencies. Or, in other words, there are serious reasons to believe that the model we use to represent the phenomenon is at least incomplete. Via stochastic models it is rather easy to take parametric uncertainty into consideration, whereas structural uncertainty is much harder to cope with (see “Dealing with structural uncertainty in tactical wargaming”, also submitted at the Euro-SIW 2008 conference). This problem is deferred to the discussion of the whole endeavor in section 5.The default treatment of uncertainty in military planning is to build reserves on multiple echelons and to prepare some general contingency plans. However, even with these provisions, the planner has to decide between a couple of possible courses of action. This can be done by intuition, rational analysis, general rules (from field manuals, for example) or a mixture of them. From a scientific point of view the quality of decision making has to be proven by empirical evidence. This is difficult within the military domain, because the outcome of the decision making depends from many factors independent from the decision making itself (ranging from sheer luck over technological issues to human factors). Moreover, since every conflict has some unique features, it is barely possible to deduce much from success and failure. Nevertheless, it would be desirable to have some measures on which the quality of decision making could be assessed, since otherwise only anecdotic evidence based, in best case, on exploratory analysis could be given in support of any decision strategy. In the following, we sketch how such a quantitative approach could look like for a simulation based decision support.2.3. Simulation supported military decision makingSimulation systems are sometimes considered to be the ideal military decision support tools (as simulation-based war games for example [3]), since they can capture a plethora of different factors without explicitly knowing their interdependencies. The macro-phenomenon to be explained (the outcome of a combat) is not addressed directly, but via better understood micro-phenomena or elementary processes (movements, attrition, reconnaissance, communication etc.), which are calibrated to produce the observed macro-phenomenon [4]. 2.3.1 Stochastic simulation Due to the massive uncertainty discussed in section 2.2 it is hardly conceivable that deterministic or single stochastic simulation runs can be regarded as useful predictions of reality. The standard method of simulation support is therefore to use stochastic simulation runs and calculated some stochastic parameters on their basis [5,6]. With respect to predictions the process can be summarized as follows:The current situation is initialized in the simulation system. Own and supposed enemy course of action (COA) are implemented by rules or agents. (iterated for different COAs)N simulation runs are made with different random numbers (seeds).Statistical parameters (mean and variance, for example) are calculated using the N simulation results.A most probable trajectory is derived from these results.The selected own course of action might be, for example, the one with the most promising worst case trajectory.The central question is, if the most probable trajectory is valid as a representation of the real future trajectory? With other words, can the (stochastically condensed) simulation results be used for prediction? One is intended to instantly negate these questions in the light of uncertainty. However, even if the predictions are bad they might still be better than everything else we have. The benefit becomes obvious it the focus is not accuracy but error reduction. Even if the absolute error rate remains high, a proportional error reduction might still be significant. This idea is intensively used in modern descriptive statistics, where – in recent years -so called PRE (proportional reduction in error) measures have gained tremendous importance. 3. PRE MeasuresGenerally stated, the method of PRE measures requires to make two different predictions about the scores of cases [7, 8, 9]. In the first prediction, we ignore information about the independent variable  A and, therefore, make many errors in predicting the score on the dependent variable B. In the second prediction, we take account of the score of the case on the independent variable A to help predict the score on the dependent variable B. If there is an association between the variables A and B we will make fewer errors when taking the independent variable into account. PRE measures consequently express the proportional reduction in errors between the two predictions. In short, we ask, whether the knowledge of A proportionally reduces the probable error in predicting B. The following very simple example will make the logic clearer.A random sample of students (all are or are going to be officers) at the University of the German Armed Forces has been classified into two groups: students of natural or technical sciences on the one hand and students of human sciences on the other hand. Subjects have been asked whether they intend to stay with the Forces for lifetime (“lifer”) or leave after 12 years (“temporary”). Does the knowledge of the scientific field helps to reduce the error in predicting intentions of a randomly chosen person? Table 3.1 Academic Studies and Intentions of Officers (Example for PRE measures)Academic StudiesIntentionslifertemporaryTotalsNatural/technical106070humanities503080Totals6090150Looking at table 3.1, suppose that we are told only that an individual is a student at the university of the German Armed Forces. How should we guess (predict) whether he intends to become a lifer or temporary career volunteer only? Since 60 of the individuals in the table intend to become lifers and 90 intend to stay temporary soldiers, on balance we would be better off guessing that he intends to stay a non-lifer. That way, without additional knowledge, we would be wrong in 60 of 150 cases. Suppose know, however, that we are given the added information about the academic domain of his study. If he is a student of natural or technical sciences our best guess is now that he intends to stay a temporary career soldier, making an error only in 10 cases. If he is a student of humanities our best guess would be that he intends to become a lifer, making an error in 30 cases. Summing up, with the additional information we would be wrong in 40 of 150 cases. Thus, if error E1occurs in 60 cases and error E2 occurs in 40 cases the proportional reduction in error is:(E1 –E2)/E1= (60-40)/60= 0.33.This measure is called λ. It has been introduced by Goodman and Kruskal [2]. A lambda of 0.33 indicates that we would make 33 % fewer errors in predicting intentions from study type, as opposed to predicting intentions while ignoring the additional information. PRE measures have been developed for nominal, ordinal and metric level variables and are considered to be the most practical statistical measures of association, just because they can be easily interpreted as error reduction rates, whereas the semantic interpretation of traditional measures like the phi-coefficient φ or Cramer’s V, for example, which are based on χ2-values is rather difficult if not impossible [7, 9].4. Towards a PRE measure for simulation supported decision makingThe general idea to use PRE measures for simulation evaluation is straightforward. One simply compares error rates of predictions that are solely based on the current situation with predictions that take the additional information of a stochastic simulation into account. We can then ask, if we, in fact, make fewer errors in predicting the actual course of action.Unfortunately, there are some serious problems with the implementation of this approach. First, the many degrees of freedom for error definition of military predictions make it hard to find a universal transformation of PRE measures from standard statistics to simulation results. Second, even the measures of success in reality are sometimes difficult to define for military endeavors. Third, it might be, that a simulation has provided excellent predictions in the past but is nevertheless useless for the future. Fourth, there is, generally, too much work in adapting a simulation model to the specific features of dozens of different real world examples.Putting these problems aside for the moment (they are going to discussed in section 5) or, more precise,  circumventing them, a sensible first attempt of application could look like this:Within a combat maneuver training center (for example the  US NTC, CMTC or the German Combat training center GÜTZ near Magdeburg) exercises start with a given initial situation on a standard terrain (this reduces the amount of work for simulation adaptation). The members of a group A of military experts are independently asked to describe the outcome of such exercises by numerical measures of success (loss ratio, time to reach the objective etc.). They are only allowed to use the current initial situation. A second group B of experts makes predictions for the same exercises but is allowed to use the results of a stochastic simulation. After the first half of the overall exercise number the groups are switched. At the end, not the groups are evaluated but the traditional and simulation based predictions. Errors are simply defined by deviations from the numerical measures greater than given limits.With such a setting it is possible to avoid the problems mentioned above to a certain extent, without solving them. We have academically experimented with this and similar settings without actually going into the training centers (reasons see below). However, even this first and rather limited experiments (based on some past exercises and the simulation system GESI/SIRA) showed, that some major refinements have to be made on the general concept.5. Preliminary findings and critical discussion 5.1 Preliminary findingsDuring the first preliminary experiments it became clear that the predictive validity of the simulation system for a specific (non-statistical) forecasts is extremely low, whereas the calibration of combat simulation models on past exercises is quite easy. Moreover, in many experiments not the average trajectory seemed to our military cooperators interesting, but the extreme ones. In discussion we found out, that there is a widespread mistrust in the predictive power of military simulation systems in general. There are several motives for this  HYPERLINK "http://dict.leo.org/ende?lp=ende&p=eL4jU.&search=suspiciousness" suspiciousness, some of them rooted deeply within the military self-conception as managers of the unexpected. However, we could also establish a much simpler explanation. All  military advisers familiar with simulation systems are very aware of the complicated and sometimes arbitrary calibrations necessary to fine tune a combat simulation system. Necessarily, such questionable preselections are also unavoidable for predictive simulations. The bottom line is,  that military experts think, that with such simulation systems you only get what you want.Taking this objections into consideration we tried to replay some past exercises without specific calibrations and quickly learned that it did not work at all. We also realized that calculating statistical measures only did not suffice for a deeper analysis, because too much information gets lost by the aggregation. Therefore, we slightly extended the setting for the experiments, using five different cases. The decision maker is provided withthe statistical result (end state of some main measures of success (MOS)) of N simulation runs done for each pair of own and enemy COA. a trajectory which reflects the statistically most probable developing of the MOS. the individual trajectories of MOS of all simulation runs.a graphical visualization of the m (m << N) worst and best simulation runs.a graphical visualization of each simulation run.So far, we could not sample enough data from exercises in order to make a final empirical judgment on these options. Moreover, it gets more and more doubtful if such an empirical proof will be possible (see below). However, from all subject matter experts (SME) we interviewed up to now, we can deduce that they, in fact, appreciate options e) and d) much more than a), b) and even c). If time is limited option d) is also preferred to option e).5.2 Critical discussionUnfortunately, the answers of SME are always biased and our original goal was a statistic measure of error reduction not a questionnaire on personal satisfaction (as valuable as that might be).However, in the meantime, we seriously doubt whether the whole approach of empirical validation of such an error reduction is feasible as a general solution. Returning  to the problems mentioned in section 4 we make the following caveats.First, our attempts to find a definition of military success and consequently error that satisfied all SME failed in many cases. It seems to us, that most modern military conflicts have to be evaluated by an individually changing unique vector of MOS. This vector has to incorporate social, economic and political aspects as well as a host of military measures of effectiveness. It would be possible to construct a generalized vector for academic purposes, but it would be of little use. Moreover, for practical purposes, the composition of each individual MOS vector belongs to the field of so called trans-science [10]. Second, for C4-decision environments, the calculation of deviations between forecasts and reality is much more difficult than for physical or technical environments, since some of the MOS are value-laden and can hardly be operationalized. Third, even if convincing predictions one some MOS could be made with a simulation system in the past, it seems hazardous the believe that the system will be right the next time. The main reason is, that C4 systems are always self-adaptive. Any victory changes the behavior of the defeated and others who want to learn from it. Fourth, using a family of slightly different models it would be easy to demonstrate an error reducing effect for at least one simulation model in particular, only for statistical reasons, especially if rather aggregated or binary MOS are used (which reduce the probability space).6. Summary and ConclusionThe use of simulation for decision support in complex and value-critical domains (in this paper specified as C4 situations) has always been controversial. The predictive validity of the simulation systems could never been raised on the level generally  required in natural or technical  sciences. However, too much emphasis has been placed on absolute validity. The introduction of PRE (proportional reduction in error) measures can overcome this problem by using a relative approach. Unfortunately, the simple comparison of error rates in the forecasts of traditional and simulation based planning is much more difficult as it may seem.On the other hand, it would be a great mistake to give up all attempts to statistically measure the quality of simulation based prediction in C4 decision environments. Although explorative simulation for insight will remain the main task of such simulation systems, they would eminently benefit from every, even the slightest empiric confirmation of predictive validity. The concept of PRE measures is a “minimalistic” approach for such corroboration. We are convinced, that it will be definitely possible to calculate sensible  PRE measures for simulation based decision making even in complex environments in some particular cases.However, the whole endeavor has to discussed in the community of expert simulation users and developers in order to set sensible goals for the future.Future workThe preliminary findings mentioned within this paper have to be corroborated by more empirical data. 7. References[1]	Clausewitz, C.v. Vom Kriege. Reinbek: Rowohlt Tb. 1978.[2]	Goodman, L. A. and Kruskal, W. H. Measures of Association for Cross Classification. Journal of the American Statistical Association, 49 (p. 732-764), 1954.[3]	G.D. Brewer and M. Shubik. The War Game. Harvard University Press. 1979.[4]	H. W. Hofmann, M. Hofmann On the Development of Command & Control Modules for Combat Simulation Models on Batallion down to Single Item Level. NATO-Symposium “New Information Processing Techniques for Military Systems”, RTO Meeting Proceeding MP-049, Neuilly-sur-Seine, Cedex; Frankreic, 2001.[5]	S. M. Ross. Simulation. Academic Press. San Diego. 2002[6]	J. Banks. Handbook of Simulation. Wiley and Sons. 1998.[7]	 H. Benninghaus. Deskriptive Statistik. Verlag für Sozialwissenschaften. 2007[8]	H. L. Costner. Criteria for Measures of Association. American Sociological Review 30 (p. 341-353). 1965.[9]	J. F. Healey Statistics: A tool for social research. Wadsworth 2004.[10]	A. M. Weinberg. Science and trans-science. Minerva 10 (209-222). 1972.Author BiographyMARKO HOFMANN is Project Manager at the Institute for Technology of Intelligent Systems (ITIS), Neubiberg, Germany. After his studies of computer science at the University of the Federal Armed Forces in Munich he served two years in an army battalion staff. From 1995 to 2000 he was research assistant at the Institute for Applied System Analysis and Operations Research (IASFOR), where he got his Ph. D. in computer science. Since April 2000 he is responsible for basic research in applied computer science at ITIS. He gives lectures at the University of the Federal Armed Forces in Munich and at the University of Applied Sciences in Kufstein.