Simulation Validation Quality and Validation Process MaturityS.Y. HarmonZetetixP.O. Box 705, Myrtle Creek, OR  97457(541) 863-4639 HYPERLINK "mailto:harmon@zetetix.com" harmon@zetetix.comS.M. YoungbloodDefense Modeling and Simulation Office1901 N. Beauregard Street, Alexandria, VA  22311(703) 824-3436 HYPERLINK "mailto:syoungblood@dmso.mil" syoungblood@dmso.milKeywords:validation, process maturity, fidelityABSTRACT: This paper builds upon the validation process maturity model proposed at the Spring 2003 Simulation Interoperability Workshop.  It recommends key parameters for measuring the quality of the products from a simulation validation process.  These parameters come from the notions for information quality introduced by the process improvement and traditional scientific communities.  They consist of assessments of information completeness, correctness and confidence.  Completeness gauges the degree to which a simulation can generate enough of the right information to adequately serve a purpose.  Correctness assesses whether the information that the simulation generates is sufficiently accurate to serve that purpose.  Confidence describes the degree of uncertainty that validation agents have in their assessments of simulation completeness and correctness.  We have assembled these metrics of validation quality into six levels that vary from no quality to the highest possible level of quality, one that contains all of the key information quality parameters.  We have also slightly re-organized the original levels of validation process maturity to reflect production of these varying levels of validation product quality.  As a result, the new structure describes both process quality and the quality of information that one can expect from each level of process quality.1.  IntroductionLast year we proposed a model for assessing the maturity of simulation validation processes [1].  The Carnegie Mellon University Software Engineering Institute’s (SEI’s) Capability Maturity Model (CMM) for software and systems [2, 3] inspired this effort.  But, we did not attempt to map the SEI CMM directly onto validation processes as proposed by some authors [4, 5].  Instead, we identified the basic reasoning underlying the CMM and developed the notion of validation process maturity from its first principles following an analogous reasoning.  Our derivation depended upon the assertion that the primary measure for improving any information process, such as simulation validation, is improvement in the quality of the information that process produces [1].  While we discussed the notion of validation information quality and connected it to the objectivity of the process, we did not recommend any specific parameters by which to measure validation information quality.This paper remedies that shortcoming and, as a result, restructures our original concept of simulation validation process maturity slightly to align with these new ideas.2.  Validation QualityOur earlier paper proposed the real measure of validation information quality as truthfulness and continued by noting that actually measuring the truthfulness of information ranges from difficult to impossible.  Measurements of truthfulness suffer from the same problems as measurements of randomness.  In order to overcome this problem, we adopted a scientific model of truth and, from that, asserted that a purely objective process (i.e., one that produced results independent of the observers) was more likely to supply truthful information than a subjective one.  We then created a structure with six levels of validation process objectivity [1].Following this path, we skirted the issue of information quality and its evaluation.  This choice created a structure of validation process maturity without a strong connection between the maturity of the process and the quality of the validation information that it produces.  The model we proposed rested upon the assumption that increasing objectivity produced validation information of increasing quality.  While considerable historical empirical evidence supports the veracity of this assumption, no formal argument exists to justify its soundness.  Further, our approach has left us without key measures of validation information quality.We remedy this shortcoming by revisiting the notion of information quality as applied to validation.  However before proceeding to consider validation information quality, we must define what we mean when we refer to simulation validity:simulation validity – the simulation’s representation of the simuland is fit enough to serve a particular purposeThis definition avoids many of the problems incurred with other definitions of validity (e.g., linking validity with only accuracy).  By fit enough, we mean that the simulation represents the simuland with sufficient fidelity to serve a user’s purpose.  We interpret fidelity as defined in the SISO Fidelity Conceptual Model [6].  This definition of validity implies that any assessment of simulation validity must consider all of the aspects of representational quality and not just accuracy.  This definition of validity says nothing about the capabilities normally addressed by conventional systems engineering (e.g., human factors, reliability, support requirements).  This is not to say that these factors are unimportant to a practical simulation but only that the simulation’s representational capabilities distinguish it from all other system types.  With this understanding of the meaning of validity, we can safely proceed to examining the properties of information quality.People have given considerable thought to the issue of information quality, especially in the context of evaluating the information presented on websites [7].  Many of these sources seem to confuse the issues of information quality with those of the credibility of the source.  While credibility and truth can correlate, they are not identical.  Nevertheless, the consensus of opinion on the attributes of information quality seems to include the following aspects:Objectivity,Repeatability, Timeliness,Completeness, andAccuracy [8-11].We have already argued for the importance of objectivity and repeatability in simulation validity assessments but these characteristics do not lend themselves to ready measurement and result more from the process used to obtain the information than from the information itself [1].  Timeliness is important in simulation validation in two regards, relevance to a use and relevance in the face of the simulation’s change.  Validity assessments inherently incorporate the relevance to a use by determining a simulation’s fitness for a particular purpose.  Validity assessments can become dated in the face of change only when the capabilities of the simulation change.  If the simulation remains static then an old assessment should contain identical information to a more recent assessment.  Nevertheless, some measure of a validity assessment’s timeliness can be valuable so that someone considering using that information could determine its currency.Physical experimentation typically attaches the notions of uncertainty and confidence to measurements of any physical property state [12].  The error bars commonly used when plotting physical measurements or data derived from measurements explicitly reflect the uncertainty associated with the measurement and implicitly suggest the confidence associated with the error interval in its confidence interval (actually, the confidence interval delimits the probability that the actual mean lies within the error bound) [13].  Logan and Nitta also propose evaluating the quality of simulation validation assessments through the information that they provide on simulation error and confidence.  They argue for the need for confidence in relating simulation validity to the risk of using the simulation for a purpose [14].We therefore suggest that a complete characterization of simulation representational capabilities will contain the following information:Enumeration of the entities, attributes and dependencies that the simulation represents,Characterization of the upper bounds of the errors at which the simulation represents the states of the entities that it represents, and Estimates of the confidences with which those errors are known.This proposition implies that the process of validating a simulation begins by determining the minimum representational capabilities sufficient to serve a purpose in these terms.  The process then determines the actual representational capabilities of the subject simulation in these terms.  Finally, a validator can assess the simulation’s validity for the purpose by comparing the representational capabilities against the representational requirements.  In the simplest case, a simulation possesses sufficient capabilities to completely meet the purpose’s requirements.  More likely, differences will exist and the validator can only identify those differences, letting either the user or their representative (e.g., the accreditation agent) decide whether those differences make the simulation inappropriate for the purpose.The idea of expecting that simulation requirements will be stated in such detailed terms may appear unrealistic considering the present state of requirements engineering for simulations.  However, these requirements need only define the characteristics of the output needed to address the user’s purpose.  A simulation may involve many more entity types, attributes and dependencies in order to represent sufficient causality to achieve the required accuracy of the output.  However, the actual representational requirements need only specify that accuracy and not the accuracies of all of the components necessary to achieve it explicitly.  This greatly simplifies requirements specification as well as simulation validation.Therefore, any simulation validation assessment will identify theEntities, attributes and dependencies needed to produce the output that the user requires to serve their purpose,Maximum errors in the output attribute values tolerable to the user and the attribute value ranges over which they need this maximum tolerable error to apply,Confidences that the user requires to reduce the use risk to achieve their goals, Entities, attributes and dependencies that the simulation actually represents,Maximum errors in the attribute values that the simulation produces as output and the ranges of over which those maximum errors can be guaranteed, Confidences in the observations of the output characteristics, andDifferences between the requirements and the simulation capabilities that may exist.The validator must get, as input, or interpret, from the input they get, the representational requirements and must observe or measure the simulation’s actual representational capabilities.  In a sense, a simulation creates an artificial causality through its execution of its dependencies.  Assessing the validity of a simulation determines whether its representation of causality adequately serves its purpose.  Ideally, an independent observer should be able to obtain this same information through measurements on the same simulation and given the same purpose.Through this line of consideration, we can define the quality of simulation validation information by its possession of the following three necessary components:Completeness – A validation assessment must determine if the simulation represents all of the things that the user requires to pursue their purpose.  It should also identify the required things that the simulation does not represent.  Identifying those things that the simulation represents but that the user does not specify as requirements is optional and should probably be selected by the user or their representative.Correctness – A validation assessment must identify if the simulation’s representation matches the simuland’s behavior sufficiently to adequately serve the user’s purposes.  It should also identify those attributes where the simulation’s representation has excessive maximum error.  Again, identifying those areas where the simulation has more accuracy than needed is optional.  Unnecessary representational capability can increase the costs of use so the user or their representative should determine if they need this additional information.  Further, computation of representational errors requires a referent to define the standard of the simuland’s behavior.  Referent choice can significantly affect the credibility of the validation assessment.  Therefore, the validation assessment must identify the referent used to support error computations.Confidence – A validation assessment must explicitly characterize the confidence that the user can place in its information, particularly the error estimates.  It should assign confidence intervals to each error estimate in such a way as to represent all of the sources of uncertainty associated with the validation measurements (i.e., not with the representational requirements as they come from the user).  It should also identify areas where either the simulation or the validation assessment cannot provide sufficient confidence to meet the user’s requirements and suggest the means through which to increase that confidence (e.g., performing more results testing, improving the referent information).In thinking about accuracy and error, one is really saying that if I give the simulation input (in the form of various data sets (e.g., terrain databases, entity characteristics, user input) with a certain maximum error then I can expect that the error of the output (as compared with the referent) will not exceed specified maxima.  The absolute error (again, as compared with the referent) can only be determined for a completely self-contained simulation (i.e., one that does not require any externally derived data or user input).  In that case, the contributions of all of the resident data contributing to the simulation’s error are well known.Confidence has many interpretations [13, 14].  We intend that the assessed confidences reflect the totality of uncertainties associated with making a validity determination.  For instance, measures of confidence are particularly necessary when the simulation uses stochastic approximations of causality.  These representations substitute randomness to represent complex causal situations.  In stochastic representations, the accuracy of the representation can only be known to a certain prescribed confidence.  This non-determinism built into the simulation complicates the measurement of its errors.  Further, confidence can measure the degree of confidence that one has in the truthfulness of the validity assessment.  Several factors can decrease this confidence from unity including the completeness of coverage of the behavior space, completeness of the requirements, and completeness of the referent.  Logan and Nitta describe more about the sources and influences of these different sources of stochasticity [14].  The coverage of the results behavior space becomes important when the validity assessment can only sample the entire space.  As a result, the assessed confidence should also include the effects of the uncertainty implied by incomplete results sampling of a simulation’s behavior space.A validation assessment may also need to identify the sources of information upon which it depends for credibility purposes.Recognizing these attributes of a validation assessment enables our proposing the levels of validation shown in below in Table 1.  In this structuring, we argue that successively adding information to the validation assessment, as described above, requires more effort but also improves the value of the assessment to the user.  For example, assessment of a simulation’s functionality and error limits identifies what the user can do with the simulation while still operating beneath specified error limits.  Identifying the confidences contributes to the user’s assessments of use risk.  Therefore, the increasing levels of validation conform to increasing effort as well as increasing value to the user.No validation of the simulation is done at Level 0.  As in the validation process maturity model, this level establishes the baseline for validation levels [1].At Level 1, one or more persons with credibility to the user have evaluated the simulation’s behavior informally and determined that it suits the user’s purposes.  They reflect this assessment with a simple statement that the simulation is either valid or invalid.  The strength of this declaration of validity depends entirely upon the reputations of the individuals offering the assessment.At Level 2, someone has examined the simulation to determine that it represents enough of the entities and their attributes to accomplish the user’s purpose.  This suggests that someone has articulated the user’s representational requirements in terms of the needed entities and their attributes.  This level effectively provides a static view of the simulation’s capabilities and its validity at this state of assessment.At Level 3, someone has examined the simulation to determine that the attributes of the entities change in a way sufficient to meet the user’s needs, i.e., that the simulation represents enough of the dependencies between the entity attributes to satisfy the user’s needs.  This assumes that someone has interpreted the user’s representational needs enough to identify the dependencies needed to meet those needs.  This begins the process of describing the simulation’s dynamical characteristics and their relevance to its validity.At Level 4, someone has examined the simulation to determine that the accuracies of the representation and the ranges of state over which those accuracies apply are sufficient to meet the user’s needs.  In this case, the accuracies refer to those of the attribute representations but ultimately of the dependencies between attributes.  This assumes that someone has interpreted the user’s needs to determine the minimum accuracies that they require (or, more properly, the maximum errors that they can tolerate) to achieve their goals.  This also assumes that someone has identified a credible referent against which to measure simulation errors.At Level 5, someone has evaluated the sources of uncertainty and computed the confidence with which the simulation’s behavior is understood and characterized by the functional and error assessments.  Several sources contribute uncertainty to a validity assessment including the requirements, the simulation evaluation and the referent.The notions of validation quality that Table 1 reflect permits restructuring the validation process maturity model as described below.3.  Validation Process MaturityThe simulation validation process maturity model proposed in Reference [1] was derived without direct consideration of any model of simulation validity, a significant shortcoming of this maturity model.  Knowing more about the characteristics of simulation validity enables improving the companion process maturity model.Table 2 presents the modified simulation validation process maturity model.  Actually, the proposed validity model has changed the process maturity model relatively little.  Levels 0 and 1 remain completely unchanged and generate assessments of validity at Levels 0 and 1, respectively.   A Level 2 process only produces information at the Level 3 of validity, skipping over Level 2 by producing validity assessments including both static and dynamic information.  In effect, the Level 2 of process maturity could be partitioned into Level 2a and Level 2b with one including only a static assessment and the following partition including both static and dynamic information.  Level 3 has been modified slightly to permit referents derived from a single source of information.  This information is sufficient if uncertainties are not estimated.  When one assesses the confidences then multiple correlated and statistically characterized referents are needed.  Thus, Level 4 produces validity at Level 5.  The slight skew between the process maturity model and the validity model comes from a strictly subjective desire to retain five levels in both but to progressively increase the level of objectivity in the process maturity model.  Thus, Level 5 in the process maturity model remains unchanged from the original.  The increasing level of automation will increase the objectivity of the validity assessment and minimize the uncertainties associated with that assessment.Repeatability really describes the attributes of a process.  It implies that any assessment produced by the process is independent of the specific people executing that process.  In other words, two people executing the process correctly on the same simulation for the same purpose will produce identical assessments of validity.  In Table 2, we have assumed that the process’s repeatability improves with the increasing level of objectivity.4.  ConclusionsWe have proposed that a simulation’s validation can be assigned into one of six levels depending upon the kind of information that the validation assessment contains.  Increasing the level of validation can increase the level of effort required to assess validity and can increase the level of rigor required in the validation process.  However, increasing the level of validation also supplies more and more information to the user and eventually supports computing the use risk.This understanding of validation level has inspired slight modifications to the validation process maturity model so that higher levels of process maturity produce higher quality validation assessments.5.  References[1]	S.Y. Harmon & S.M. Youngblood, “A Proposed Model for Simulation Validation Process Maturity,” Proc. SISO Spring 2003 Simulation Interoperability Workshop, Orlando, FL, April 2003, np.[2]	M.C. Paulk, C.V. Weber, W. Curtis & M.B. Chrissis (eds.), The Capability Maturity Model: Guidelines for Improving the Software, Addison-Wesley Publishing Company, Reading, MA, 1995.[3]	R. Bate et al., A Systems Engineering Capability Maturity Model, Version 1.1, CMU/SEI-95-MM-03, Carnegie Mellon University Software Engineering Institute, November 1995.[4]	H. Scholten & A.J. Udink ten Cate, “Quality Assessment of the Simulation Modeling Process, Computers and Electronics in Agriculture, 22, 1999, pp199-208,[5]	C.L. Conwell, R. Enright & M.A. Stutzman, “Capability Maturity Models Support of Modeling and Simulation Verification, Validation, and Accreditation,” Proc. 32nd SCS Winter Simulation Conf., Orlando, FL, December 2000, pp 819-828.[6]	D.C. Gross et al., “Report from the Fidelity Implementation Study Group,” Paper # S99-SIW-167, Proc. Spring 1999 Simulation Interoperability Workshop, Orlando, FL, 14-19 March 1999, np.[7]	A. Smith, “Evaluation of Information Sources,” The World-Wide Web Virtual Library, Victoria University of Wellington, Wellington, New Zealand, 23 October 2003.  (at  HYPERLINK "http://www.vuw.ac.nz/staff/alastair_smith/evaln/evaln.htm" http://www.vuw.ac.nz/staff/alastair_smith/evaln/evaln.htm)[8]	G. Tyburski, “Assessing the Quality of Information at a Web Site,” The Virtual Chase, Ballard, Spahr, Andrews and Ingersoll, LLP, Washington, DC, 13 February 2003.  (at  HYPERLINK "http://www.virtualchase.com/howto/assess_quality.html" http://www.virtualchase.com/howto/assess_quality.html)[9]	Showme Media, Ltd., Quick, The Quality Information Checklist, Health Development Agency, London, United Kingdom, August 1999.  (at  HYPERLINK "http://www.quick.org.uk/" http://www.quick.org.uk/)[10]	S.J. Fenton, Information Quality: Is The Truth Out There?, Univ. of North Carolina, Chapel Hill, NC, 29 May 1997.  (at  HYPERLINK "http://www.ils.unc.edu/~fents/310/" http://www.ils.unc.edu/~fents/310/)[11]	S. Adams, “Information Quality, Liability, and Corrections,” E-Media Live, 27 (5), September/October 2003, np.  (at  HYPERLINK "http://www.infotoday.com/online/sep03/adams.shtml" http://www.infotoday.com/online/sep03/adams.shtml)[12]	W.J. Youden, Experimentation and Measurement, Dover Publications, Inc., Mineola, NY, 1998.[13]	J. Mandel, The Statistical Analysis of Experimental Data, Dover Publications, Inc., New York, NY, 1964.[14]	R.W. Logan & C.K. Nitta, Validation, Uncertainty and Quantitative Reliability at Confidence (QRC), AIAA-2003-1337, January 2003, np.Note:  The URLs provided in these references are current as of 10 February 2004.6.  AcknowledgementsThe Validation, Verification and Accreditation Program at the Defense Modeling and Simulation Office generously supported this work.Author BiographiesSCOTT HARMON is president of Zetetix, a small business specializing in modeling complex information systems.  Mr. Harmon has been developing rigorous techniques for the validation of simulation federations and human behavior representations.SIMONE YOUNGBLOOD is a member of the Senior Professional Staff at the Johns Hopkins University Applied Physics Laboratory.  She is currently on assignment at the Defense Modeling and Simulation Office (DMSO) serving as the Technical Director for VV&A.  Ms. Youngblood currently serves as chair for the Simulation Interoperability Workshop's (SIW) VV&A Forum and co-chair for the DMSO VV&A Technical Working Group. Ms. Youngblood has supported several VV&A efforts, including the Navy's Naval Simulation System, the Joint Warfare System (JWARS), and the development of the current DoD VV&A Recommended Practices Guide.Table 1.  Proposed Levels of Simulation ValidationLevel of ValidationSupporting InformationInformal Validity Statement0NothingI have no idea1Simple statement of validityIt works; trust me.2Required entities & attributes compared against the entities & attributes that the simulation representsIt represents the right entities and attributes.3Required entities, attributes & dependencies compared against entities, attributes & dependencies representedIt does the right things; its representations are complete enough. 4Required entities, attributes, dependencies & dependency errors compared against entities, attributes & dependencies represented & representation errorsFor what it does, its representations are accurate enough.5Required entities, attributes, dependencies, dependency errors & confidences in assessment compared against represented entities, attributes, dependencies, representation errors & assessment confidencesI’m this confident that this simulation is valid.Table 2.  Revised Levels of Simulation Validation Process Maturity.LevelValidation CriteriaReferentConceptual ModelDevelopment ProductsSimulation Results0none derivednone chosennone formulatedverified only enough to support  developmentnot validated at all1represented by SME opinionrepresented by SME opinionnone formulatedverified only enough to support  developmentvalidated by SME observing simulation results2determined from user statements in terms of entities represented, their attributes & the dependencies between themrepresented solely by SME opinionvalidated against the validation criteria by the SMEverified against the conceptual model inventoryvalidated by SME against the validation criteria3determined from user statements in terms including attribute ranges, domains & errorsderived from a single sourcevalidated by objective party from validation criteria & referentverified against the conceptual modelevaluated by objective party from validation criteria & referent4determined from user statements in terms of entities, attributes, ranges, domains, errors and confidencessampled from multiple independent sources & correlated statistically with estimates of uncertaintiesvalidated by objective party from validation criteria & referent; analyzed to suggest results sampling space & estimate the confidence associated with that samplingverified against the conceptual model; provides information to guide results sampling & estimate the confidence associated with that samplingsampled from guidance developed from CM & verification results analysis; validated by objective party from validation criteria & referent5formally derived from user statements using causality argumentsformally derived from multiple independent sources & characterized statistically with estimates of uncertaintiesformally stated & validated by automatically from validation criteria & referent; analyzed to define results validation sample spaceverified against conceptual model & used to define results validation sample space & the confidence associated with that samplingautomatically sampled from guidance defined from CM & verification results analysis; validated by automatically from validation criteria & referent