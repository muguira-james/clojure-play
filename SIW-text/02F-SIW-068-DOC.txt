Analysis of Independent Throughput and Latency Benchmarks for Multiple RTI ImplementationsMs. Pamela Knight, Mr. Ron Liedel, Ms. Melanie Klinner, Ms. Jacqueline SteeleU.S. Army SMDCP.O. Box 1500106 Wynn DriveHuntsville, AL  35807-3801Tel. 256-955-3300 HYPERLINK mailto:Pamela.knight@smdc.army.mil pamela.knight@smdc.army.mil, ron.liedel@smdc.army.mil, melanie.klinner@smdc.army.mil, jackie.steele@smdc.army.mil Mr. Ray Drake, Mr. Paul Agarwal, Dr. Edwin Núñez,Mr. Mario Espinosa, Ms. Jessica Giddens, Ms. Carol JenkinsCOLSA CorporationAdvanced Research Center6726 Odyssey DriveHuntsville, AL  35806Tel. 256-922-1512  HYPERLINK mailto:rdrake@colsa.com rdrake@colsa.com,  HYPERLINK mailto:pagarwal@colsa.com pagarwal@colsa.com,  HYPERLINK mailto:enunez@colsa.com enunez@colsa.com,  HYPERLINK "mailto:mespinosa@colsa.com" mespinosa@colsa.com,  HYPERLINK "jgiddens@colsa.com" jgiddens@colsa.com,  HYPERLINK mailto:cjenkins@colsa.com cjenkins@colsa.comKeywords: Run-Time Infrastructure, benchmarks, HWIL, latency, throughput, HLA, distributed simulation, networkingABSTRACT: In the Fall 2001 and Spring 2002 SISO Conferences, we presented papers 01F-SIW-033 and 02S-SIW-081 describing the design and implementation of a series of benchmarks to test latency and throughput characteristics of several Run-Time Infrastructure (RTI) implementations.  The Spring 2002 conference paper provided a detailed overview of the benchmark design logic and presented an extensive series of benchmark results for the following RTIs: (1) RTI-1.3NG4, (2) MÄK RTI 1.3.5-ngc, and (3) Georgia Tech Parallel and Distributed Simulation (PADS) Federated Simulations Development Kit (FDK) 3.0 Detailed RTI.  Only a subset of all available benchmark data was actually published due to conference paper length limits.  The same restrictions limited the analysis and discussion of benchmark results.  The objective of the present paper is to provide an in-depth analysis and interpretation of the previous latency and throughput results.The performance of RTIs is characterized by using the established procedure of exercising the unit under test (i.e., the RTI), in a controlled configuration where a set of key, independent variables are stimulated across a specified range of values.  The common element between distributed simulations is the network.  Therefore, network performance characteristics provide relevant data for quantifying the expectations of a generalized distributed simulation.  By definition, only one element, the RTI is guaranteed to be common between federates in a federation.  Like a network switch, one goal of the RTI is to intelligently route data from sender to receiver while introducing little or no overhead.  These measurements of raw RTI performance should serve as valuable comparison tools for the wider modeling and simulation community.The benchmark activity was sponsored by the Wide Bandwidth Technology (WBT) Program and conducted at the U.S. Army Space and Missile Defense Command (SMDC) Federation Analysis Support Technology Laboratory (FAST Lab) in Huntsville, Alabama.  SMDC is the executing agent of the WBT program and SAIC is the Prime Contractor.DISTRIBUTION A. Approved for public release; distribution unlimited.IntroductionThis paper extends the analyses of latency and throughput capabilities of different RTIs (Run Time Infrastructure) in two papers presented at earlier SISO Conferences: 01F-SIW-033 and 02S-SIW-081.  The first paper, presented at the Fall 2001 SISO Conference, introduced our RTI Benchmark (RTIBM) effort.  The Spring 2002 conference paper provided a detailed overview of the benchmark design logic and some representative graphs from an extensive series of RTI benchmark results.  This paper expands on the analysis of the earlier results and the results of new tests conducted since the Spring 2002 SISO Conference.  Section 2 briefly describes the general approach for our benchmark tests and Section 3 touches on the context of benchmark evaluation.  Section 4 describes the RTIBM metrics, illustrating the delays that occur during the life cycle of roundtrip latency performance data.  Section 5 provides a detailed description of the series of tests performed for this benchmarking effort.  Section 6 provides a description of the RTIBM graphs.  A detailed analysis of throughput for the different RTIs is given isn Section 7.  Section 8 provides a similar analysis for latency.  This section also includes the latency results for the stress tests conducted.  Section 9 proposes several areas for future study.  Section 10 gives a summary of conclusions.  It should be noted that it is impossible to present in this paper all graphs generated during this extensive series of benchmark tests.  A full set of graphs will be available during the conference for the examination by any interested party.General Benchmarking ApproachFor our RTI benchmarking efforts, we use the established procedure of exercising the RTI under test in a controlled configuration, where one of a set of independent variables is exercised across a specified range of values.  Generally, an RTI is tested in its default configuration for latency and throughput tests.  Features like reliable Federation Object Model (FOM) data delivery, that are required by the benchmark federates are enabled. Conversely, features like the Management Object Model (MOM), which are not required, are disabled when possible.  Only one configuration is used for both latency and throughput tests.  In other words, the RTI can be optimized for latency or throughput, or a balance between the two.  Performance data is delivered or exchanged between exclusive federate pairs.  This capability allows tests to be conducted using multiple “send” federates, a feature not available with the DMSO (Defense Modeling and Simulation Office) benchmark.Evaluation ContextIt is critical that benchmark results are evaluated in context.  Overall performance of an RTI should not be based on the results from a single test, test suite, or test configuration.  When evaluating the relative performance of an RTI with respect to one of the independent variables, such as “buffer size,” both the throughput and latency results must be considered.  This is essential because of the relationship between latency and throughput.  To an extent, it is possible to reduce latency by decreasing buffer sizes and propagation timeouts at the expense of throughput.  Conversely, larger buffer sizes may increase throughput at the expense of latency.  Also, note our most recent suite of benchmark tests does not address scalability.  One must not necessarily assume that the RTI with the best overall performance in this two-system two-federate test suite will have the best performance in a large-scale federation.  Efforts are underway to include meaningful scalability tests in future benchmark studies.Benchmark MetricsThe RTIBM essentially measures RTI latency and throughput in the same manner as the DMSO benchmark. Throughput is measured as two rates from the perspective of the “sender” and “receiver” federate.  The “sender” federate’s throughput is expressed as the mean UAV rate for object attributes because it uses the RTI method, updateAttributeValues, to deliver attribute updates.  The “receiver” federate’s throughput is expressed as the mean RAV rate because it uses the RTI method, reflectAttributeValues, to accept incoming attribute reflections.  The RTIBM also supports throughput performance for interactions, so we will follow a similar naming convention as used for objects.  For interactions, the “sender” federate’s throughput is expressed as the mean SI rate because it uses the RTI method, sendInteraction, to send interaction parameters.  The “receiver” federate’s throughput is expressed as the mean RI rate because it uses the RTI method, receiveInteraction, to accept the interaction parameters.Latency is measured by “sender” federates using roundtrip attribute updates or interactions.  The “sender” federate delivers a timestamp to a “returnee” federate. In turn, it provides the same timestamp back to the originating “sender” federate which records the difference between the current time and the initial timestamp as roundtrip latency.  This measure is more specifically stated as the mean roundtrip (RT) RAV or RT RI latency on the graphs for attributes and interactions respectively.The average update frequency is another measure of interest when observing latency results.  For a given test, a RTI demonstrating a higher update rate would be more desirable when comparing two RTIs showing equal latency.  We conducted latency tests using limited and unlimited update rates and expressed the frequency as the mean RT RAV rate for object attribute based tests and the mean RT RI rate for interaction based tests.Delays (dn) incurred during roundtrip attribute updates are shown in detail in Figure 1.  The local RTI component is denoted with LRC and the federate ambassador is denoted with FA.  RTIs with an asynchronous model may combine delays (d3, d4) and (d10, d11).  Note that (d1) is not included directly in the period for a roundtrip update.  The duration of (dn) will fluctuate based on system and network load.A detailed list of the delays shown in Figure 1 appears below for two federates, F1 and F2.d1 = Time F1 main() spends accessing local proxy object (Osn) from list and getting the roundtrip start time (Ts)d2 = Time F1 main() spends creating one AHVPS (Sn) with (An) updates of value (Ts).  (createAttributeHandleValuePairSet())d3 = Time RTI spends copying (Sn) from F1 main () to F1LRC.  (updateAttributeValues())d4 = Time F1LRC spends processing and initiating delivery of (Sn).  May depend on tick() and include RTI bundling overhead.d5 = Time network layer spends moving (Sn) from F1LRC to F2LRC.  Includes transmission medium and protocol overhead.  May also include packet fragmenting and reassembly, retransmission delays, etc.d6 = Time F2LRC spends processing and providing (Sn) to F2FA. May also include RTI bundle splitting.d7 = Time F2 spends processing (Sn) reflection in F2FA including copying reflected values to corresponding local returnee proxy attributes and queuing a return reflection to occur in F2 main().d8 = Time F2 main() spends processing return reflection queue and accessing local proxy object (Orn).d9 = Time F2 main() spends creating one AHVPS (Sn) with (An) return updates of original value (Ts) stored in local proxy attribute buffer.  (createAttributeHandleValuePairSet())d10 = Time RTI spends copying (Sn) from F2 main() to F2LRC.  (updateAttributeValues())d11 = Time F2LRC spends processing and initiating delivery of (Sn).  May depend on tick() and include RTI bundling overhead.d12 = Time network layer spends moving (Sn) from F1LRC to F2LRC.  Includes transmission medium and protocol overhead.  May also include packet fragmenting, reassembly, retransmission delays, etc.d13 = Time F1LRC spends processing and providing (Sn) to F1FA. May also include RTI bundle splitting.d14 = Time F1 spends processing Sn return reflection in F1FA including getting the current roundtrip end time (Te) and storing the roundtrip period (Rn = Te - Ts) in a list for post processing in F1 main() following completion of the critical benchmark.The delays for a roundtrip interaction are essentially the same as those shown for a roundtrip attribute in Figure 1.  Considerable time is required for a “returnee” federate to process and return the original time-stamped attribute or interaction back to the originating “sender” federate.  We feel that it is more accurate to state the latency exactly as it was measured which is why we use the average roundtrip latency metric rather than average one-way latency.  This delay is magnified for tests in which the number of attributes per object or parameters per interaction is increased.  As such, the latency cost as well as the benefit of grouping the data is represented.  This overhead is greatly reduced by using a mapping of “send” object instance handles to the corresponding proxy return instance objects.  Likewise, a similar mapping is used for interactions.Test DescriptionsAs in our Spring 2002 SISO Conference paper, this study involves benchmarks of the following RTIs: (1) RTI-1.3NG4, (2) MÄK RTI 1.3.5-ngc, and (3) Georgia Tech Parallel and Distributed Simulation Federated Simulations Development Kit (FDK) 3.0 Detailed RTI.  For the remainder of the paper, these RTIs will be referred to as the DMSO, MÄK, and FDK RTIs, respectively.Benchmark tests conducted for this effort have been organized into Test Blocks 1 through 20 as shown in Table 1.  Blocks 1 through 10 represent the tests conducted for the Spring 2002 benchmark effort.  We have since repeated all roundtrip latency tests, focusing now on congestion avoidance.  These new tests are shown in Test Blocks 11 through 20.  The purpose of the tests in each test block is to measure the performance of the RTI for throughput and latency while varying a specific independent variable.  The independent variables exercised in these tests are attribute/parameter size, instance count, number of different object/interaction types, and the number of attributes per   object or parameters per interaction.  Four types of tests are conducted, one-way object throughput (OWOT), roundtrip object latency (RTOL), one-way interaction throughput (OWIT), and roundtrip interaction latency (RTIL).  Bundling was not used in any of these tests.  Also, only the required object attributes and interactions were published and updated or sent.The tests for the MÄK RTI in Blocks 1 through 8 were repeated using a patched version we received that resolved stability issues noted in earlier tests.  The stress tests in Blocks 9 and 10 did not have to be repeated because those tests had already been conducted with the patched version.  MÄK pointed out that our two-system, two-federate configuration was not optimal for testing their reliable transport mode because of their use of a TCP forwarder design.  MÄK has stated that their use of a TCP forwarder design realizes better performance when the "rtiexec" process is not co-located with a federate.  Benefits and design are discussed in more detail in [6].  Since this was intended to be a two-system configuration, we chose to repeat the test in the same configuration.  The original suite of tests as presented in our first SISO paper (01F-SIW-033) would have revealed the optimal “rtiexec” placement, but after reviewing other papers stating that “rtiexec” placement had no effect on the DMSO RTI, we wrongly assumed these tests to be irrelevant to other RTIs.  The “rtiexec” is placed on a separate system for the congestion avoidance tests conducted in Blocks 11 - 20.The FDK RTI was recompiled with time.tv_sec and time.tv_usec in the prefetch routine initialized to 0 instead of 10 in FEDSIM/RTIKIT/FM/SRC/TCP  /fm.c as advised by the developer.Test Blocks 1 through 4, 9, 11 through 14, and 19 use the reliable transport mode and Test Blocks 5 through 8, 10, 15 through 18 and 20 use best-effort.In Test Blocks 1, 5, 11 and 15 the size of the attribute or parameter is varied in powers of 2 from 8 to 1024 bytes.In Test Blocks 2, 6, 12 and 16, the number of instances per send federate is varied in powers of 2 from 1 to 512 and includes 1000 instances, a common limit for RTIs.  Attribute size is fixed at 256 bytes.  Interaction-based tests are not applicable because there is no instance persistence.In Test Blocks 3, 7, 13 and 17 the number of object or interaction types is varied from 1 to 32 in powers of 2.  The attribute/parameter size is fixed at 256 bytes.  For objects, one instance with one attribute is published and updated for each type.  For interactions, one parameter is sent per each type of interaction.In Test Blocks 4, 8, 14, and 18, the number of attributes per object or parameters per interaction is varied from 1 to 32 in powers of 2.  Only one object instance or interaction is used.  The attribute / parameter size is fixed at 256 bytes.Transport:  Best-effort - UnbundledCongestion AvoidanceStress TestsTest TypeDMSOMÄKFDKBlock 20Buffer SizeRTOL((Transport:  Reliable - UnbundledCongestion AvoidanceStress TestsTest TypeDMSOMÄKFDKBlock19Buffer SizeRTOL(((Transport:  Best-effort - UnbundledCongestionAvoidanceTest TypeDMSOMÄKFDKBlock 15Buffer SizeRTOL((RTIL((Block 16Instance CountRTOL((Block 17Type CountRTOL((RTIL((Block18Subtype CountRTOL((RTIL((Transport:  Reliable - UnbundledStress TestsTest TypeDMSOMÄKFDKBlock 9Buffer SizeOWOT(((RTOL(((Transport:  Reliable - UnbundledCongestion AvoidanceTest TypeDMSOMÄKFDKBlock 11Buffer SizeRTOL(((RTIL(((Block12Instance CountRTOL(((Block 13Type CountRTOL(((RTIL(((Block 14Subtype CountRTOL(((RTIL(((Transport:  Reliable – UnbundledRate UnlimitedTest TypeDMSOMÄKFDKBlock 1Buffer SizeOWOT(((RTOL(((OWIT(((RTIL(((Block 2Instance CountOWOT(((RTOL(((Block 3Type CountOWOT(((RTOL(((OWIT(((RTIL(((Block 4Subtype CountOWOT(((RTOL(((OWIT(((RTIL(((Transport:  Best-effort - UnbundledRate UnlimitedTest TypeDMSOMÄKFDKBlock 5Buffer SizeOWOT((RTOL((OWIT((RTIL((Block 6Instance CountOWOT((RTOL((Block 7Type CountOWOT((RTOL((OWIT((RTIL((Block 8Subtype CountOWOT((RTOL((OWIT((RTIL((Transport:  Best-effort - UnbundledStress TestsTest TypeDMSOMÄKFDKBlock 10Buffer SizeOWOT((RTOL(( Test Blocks 9, 10, 19 and 20 represent stress tests using reliable and best-effort transport modes.  In these tests, 25 different object types are updated.  The number of instances per type varies from 4 to 40 in steps of 4, which results in total instance counts ranging from 100 (25*4) to 1000 (25*40).  A fixed attribute size of 256 bytes is used for all stress tests.Graph DescriptionsTo reduce the gap between tests and analysis, we have developed a semi-automated process of generating the benchmark graphs.  We now populate a database from the raw benchmark data (ASCII text files) and use a program that queries the database to automatically generate all standard graphs using the GNUPLOT utility. We use five types of graphs named Type 1 through Type 4 and Type 6.Type 1 graphs, denoted T1-1 through T1-160, show the time and frequency for each set of data per RTI for all tests.  Two graph series were created for OWOT and OWIT tests, one for the send federate and another for the receiver federate results.  A similar graph series was created for the RTOL and RTIL send federate test results.Type 2 graphs, denoted T2-1 through T2-48, are used to compare the results of the three different RTIs with each other within the same test block series.Type 3 graphs, denoted T3-1 through T3-40, show the relationship between the rates of “sender“ federates and “receiver” federates. These graphs apply only to OWOT and OWIT tests.Type 4 graphs, denoted T4-1 through T4-64, compare results between tests using best-effort versus the reliable transport modes.  Results from the same RTI are compared in different test blocks.Type 5 graphs are not currently used.  We plan on using this type in the future to compare the results of a repeated test in a before/after situation.Type 6 graphs, denoted T6-1 through T6-20, compare object attribute updates with interactions.  These graphs are only used in test blocks in which the RTI type is varied.Throughput AnalysisIn graph T2-1 (#161) below, we compare the results of the RAV rates for the RTIs with tests using a varied attribute size.  The results, as expected, are fairly flat because there is no bundling of updates and the largest buffer size of 1024 bytes is 436 bytes less than the Transmission Control Protocol (TCP) maximum segment size (MSS) of 1460 bytes for Ethernet.  On bsd BSD (Berkeley Software Design) derived systems, TCP sends 12 bytes of options, so its actual MSS is actually 1448 bytes [4].  The benchmark data, RTI header, and TCP/IP headers should fit in a single Ethernet packet.  In this graph, it appears that better performance is accompanied by somewhat greater variation.In graph T2-15 (#175), we compare RTIs using the same test conducted with best-effort instead of the reliable transport mode.  Recall that the FDK RTI we tested doesn’t support best-effort transport.   We observe that MÄK’s RTI has the best performance with this mode.  It also appears that the RAV rate was better for both RTIs in the previous graph using reliable transport.  We verify this is true by comparing the reliable versus best-effort modes directly.  The results are shown in graph T4-2 (#250) for the DMSO RTI and T4-4 (#252) for the MÄK RTI. We observed in all sixteen graphs for OWOT and OWIT that reliable transport demonstrated better throughput performance than best-effort for both the DMSO and MÄK RTIs.  The smallest gap in performance between these modes occurred in tests where the number of attributes per object or parameters per interaction was increased.  As more buffers are grouped together, the performance of the two modes converges as seen in graphs T4-32 (#280) and T4-34 (#282).In Type 3 graphs we examine the gap between the sender and receiver federate for each point.  On some of the graphs the receiver federate rate is faster than the sender federate rate.   After seeing this we discovered a logic error in our receiver federate in which the start time of the test sample is set to the time the first update is received.  The update is counted but no time is charged to it.  Buffering of data could also account in some cases for this seemingly counter-intuitive result.  The next three graphs T3-4 (#212), T3-5 (#213), and T3-6 (#214) show the sender/receiver gap for interaction throughput with a varied parameter size.  Graph T3-4 shows the receiver rate higher than the sender rate for the DMSO RTI.  Graph T3-5 of MÄK’s RTI is more characteristic of what we expected to see. The initial gap in graph T3-6 suggests that data buffering is taking place somewhere in the FDK RTI.  The receiver RI rates of the RTIs are compared in graph T2-3 (#163). The next three graphs compare the performance of interactions with attributes.  Graphs T6-2 (#314), T6-4 (#316), and T6-5 (#317) correspond to the DMSO, MÄK, and FDK RTIs, respectively.  We expected the performance of the interactions to be better than that of attributes due to assumed attribute persistence overhead.  This was true for the MÄK RTI.  Attribute/parameter performance was close with to that of the Georgia Tech RTI.  However, the attribute updates outperformed interactions in every test case for the DMSO RTI.Similar observations of better interaction performance have been noted before [2].  Of the three RTIs tested, only DMSO’s uses the CORBA middleware.  A possible explanation is that this middleware represents object attributes more efficiently than interactions.Latency AnalysisGraph T1-7 (#7) shows the DMSO round–trip latency for RTOL with varied attribute size and reliable transport.  This graph shows the generally expected behavior from the latency tests.  Latency is expected to increase in proportion to attribute size.  As the latency increases, we expect to see the rate of the RT RAV rate decrease.  This inverse relationship is illustrated perfectly in the graph below.  It should be noted that this test was conducted without using a fixed rate, updating the attribute as fast as possible.  We did not want to place an arbitrary limit on the RTI’s capability.The same graphs for MÄK and FDK RTIs are shown in T1-8 (#8) and T1-9 (#9).  These two graphs show unanticipated results.  The rate and latency for the MÄK RTI in graph T1-8 are approximately parallel to each other.  In graph T1-9 there is an inverse relationship between the rate and latency for the FDK RTI, but it does not follow the curve we expected.  These graphs and others led us to believe that congestion was occurring in the “returnee” federates network interface card (NIC), thus causing the observed peculiar behavior.  Based on this assumption, we conducted a test for each RTI to roughly gauge the effect that a reasonable update rate would have on latency.  The effect of update rate variation was substantial, but since these tests were not performed in an isolated environment, we elected to not show them here.In paper 02S-SIW-081 we conducted tests to select the optimal method of invoking tick for each RTI.  At the time it was not understood why ticking the MÄK and FDK RTIs with, “usleep (1)” and “tick ()” which we called Version 2, consistently yielded better results.  The DMSO RTI yielded better results using Version 1, “tick (min, max)”.  We believe the hard-coded “usleep (1)” in Version 2 was providing congestion avoidance by limiting the amount of data being sent.  The DMSO RTI was sending data at a lesser rate so there was enough time for the “returnee” federate to process the data.In light of this hypothesis, we decided to implement a congestion avoidance strategy and repeat all the roundtrip latency tests.We took the following steps:Modified the benchmark program so that subsequent reliable updates or interactions are not issued until the expected return reflections or interactions have completed or a timeout occurs.Ticked the RTI every update, since the only work performed by the federates is the benchmark activity.Limited the update rate for roundtrip latency test to 100 Hz.  This was deemed reasonable for our configuration. Repeated the optimal tick version selection tests and modified all benchmark code to use Version 1 based on Table 2 below.Placed “Rrtiexec” process was placed on a system not co-located with a federate.Table 2 contains the results from the tests conducted to select the optimal method of ticking the RTI using our congestion avoidance strategy.  The column on the left describes the test configurations.  Systems are abbreviated as “SYS”.  The bottom line for each test describes the type of latency test performed and the version used (V1 or V2).  RTL denotes roundtrip latency and Hz denotes the measured rate.  The optimal values are shaded.Table 2.(Configuration)TYPE - VersionDMSO  MÄKFDKRTLHzRTLHzRTLHz2 SYS 2 CPUs RTOL   V13.8499.172.2799.820.5799.912 SYS 2 CPUsRTIL   V15.9199.161.8099.820.5699.942 SYS 2 CPUs RTOL V25.8199.193.9199.822.4699.932 SYS 2 CPUs  RTIL   V27.8299.143.4399.822.4899.93 1 SYS 2 CPUs RTOL V13.9299.182.7099.310.4299.811 SYS 2 CPUs  RTIL   V15.9599.152.1899.330.4299.821 SYS 2 CPUs RTOL V25.9099.173.1899.822.2899.931 SYS 2 CPUsRTIL   V27.7799.112.9599.822.2999.931 SYS 1 CPU   RTOL V17.3298.7039.9224.9340.0424.951 SYS 1 CPU  RTIL   V110.3893.0339.9124.9440.0424.951 SYS 1 CPU   RTOL V28.3997.473.2699.702.2599.711 SYS 1 CPU  RTIL   V212.3578.003.0999.682.3799.71We observe there are significant differences between these tests and the same tests shown in paper 02S-SIW-081.  First of all, there are mixed results for the best version in ticking the MÄK and FDK RTIs.  In the first two configurations, processor contention is not an issue and these two RTIs experience better performance using V1.  In the third configuration, only one processor is available for both the sender and receiver federates as well as the other processes. With V1, congestion occurs as a result of contention for a single processor, increasing latency.  In V2, the “usleep (1)” in the sender federate forces it to suspend execution temporarily, providing the receiver federate sufficient processor time to initiate the second half of the roundtrip data exchange.  Henceforth, we will use V1 for all RTI tests.  The DMSO RTI achieves better performance using V1 in all configurations.  It does n’ot suffer from processor contention and congestion in the single processor configuration because of its use of an asynchronous I/O process model [3].The latency results discussed earlier in graphs T1-7 (#7), T1-8 (#8), and T1-9 (#9) were not generated with our congestion avoidance (CA) efforts.  Below, we will examine the results of the same tests, but conducted with CA.  The corresponding CA graphs are T1-121 (#121), T1-122 (#122), and T-123 (#123), respectively.We observe that the DMSO RTI’s latency is slightly better for all attribute sizes.  These graphs allow us to make another point.  The send rate of 100 Hz used for the updates and the measured rates are essentially the same.  It is easy to misread the graphs when there is a large difference in vertical scale ranges.  So, although the rate shows a slight downward trend, it is essentially flat.  The MÄK and FDK RTIs realized a substantial reduction in latency (at the expense of throughput).In graphs T2-2 (#162) and T2-33 (#193) we compare the latency of the RTIs directly, without CA in the first, and with CA in the second.  As the graphs show, all RTIs had improved performance, especially the MÄK RTI when CA was used.Graphs T2-35 (#195) and T2-6 (#166) compare the latency of the RTIs directly with respect to a varied instance count, with and without CA, respectively.  As before, we observe a large performance gain as a result of CA.  It appears we may have to increase the instance counts substantially to detect an upward trend when using CA.In graph T2-38 (#198), we compare latency between the different RTIs when the number of attributes per object is varied and updated using reliable transport mode.  We see the same trend with the exception of the spike for the MÄK RTI.  We had expected a larger spike from all the RTIs when the attribute count reached a value of 8 because the amount of data (8 * 256) exceeds the bsd BSD TCP MSS (1448).  This implies packet fragmenting and reassembly is taking take placerequires the data stream to span multiple packets.Graph T2-45 (#205) shows the same test conducted with best-effort.  The performance of the DMSO and MÄK RTIs is similar when both graphs are compared.The last set of graphs discussed examines the results of the stress tests.  Graph T4-63 (#311) shows an example for the DMSO RTI where the reliable transport mode results in slightly less latency than best-effort.  The increase seen at 600 instances is sustained thereafter.  The smaller observed latency on a LAN for the reliable transport mode, when compared with best-effort, has been noted in an earlier study [1]. Graphs T2-47 (#207) and T2-30 (#190). compare the results between the stress tests with and without CA respectively.There is a dramatic benefit in using the CA strategy in the stress tests.  We found that the DMSO RTI hung in the first stress test during the register/discover instance phase.  As per their DMSO’s V6 release notes, we inserted a tick before the registerObjectInstance method and encountered no subsequent problems during the CA tests. Future StudiesBenchmarking work of great importance to the Modeling and Simulation (M&S) community remains to be done in several areas.  It is well known that RTI performance is substantially affected by the number and distribution of federates and the network architecture linking them.  One of the issues we are interested in pursuing further is scalability testing.    We will attempt to move in this direction in the next few months.  We also intend to evaluate the affect of time management services on throughput and latency.  We are interested in participating in other RTI performance characterization efforts including the standardization of benchmarking procedures, metrics, and methods [5].It is our desire to make our RTI benchmark data available to the M&S community through the Internet.  Initial work to support this task is under way.ConclusionsOur analysis showed the Georgia Tech (PADS) (FDK) 3.0 Detailed RTI to have the best overall performance in our test configuration. Please keep in mind that scalability was not addressed in this study and that this RTI is a partial implementation of the HLA (High Level Architecture) specification.  The MÄAK RTI had the best performance using best-effort transport.  The DMSO RTI had the least variance but incurred greater latency and less throughput.We did not anticipate the benefits of using Congestion Avoidance measures to be as significant as realized because we predetermined that optimal communication between just two federates on a LAN would be achieved by adequate flow control.  In addition to seeing a gain in performance, we saw less variance and increased stability.  All RTIs completed all CA tests without the loss of any data.  Typically, problems incurred on a LAN are only magnified on a WAN.  Mr. Jon Snader states “An application that injects datagrams into the network without regard for congestion control can easily cause severe throughput degradation for all network users, and can even cause network failure.” [4]   The data distribution management (DDM) is a form of CA for the RTI.  This service can be used to effectively decrease the amount of extraneous data on the wire.  However, publishing federates may send data at a rate higher than the network or receiving federates can sustain.  It would be beneficial to the whole federation if the rate or amount of data being sent were reduced before congestion or failure occurs.By nNecessarilyity, the RTI is a complex application that fulfills the functionality as specifdescribeied in the HLA InterfaceF specification.  Even so, overall performance and stability of a large-scale federation may be realized enhanced by giving the RTI another capability.  A study should be conducted to evaluate RTI-based congestion avoidance that limits the send rate of specific federates just before congestion occurs and lifts the restriction when congestion is no longer detected.  We are interested in participating in other RTI performance characterization efforts including the standardization of benchmarking procedures, methods as in [5] and metrics.Research SponsorsThe benchmark activity was sponsored by the Wide Bandwidth Technology (WBT) Program and conducted at the U.S. Army Space and Missile Defense Command (SMDC) Federation Analysis Support Technology Laboratory (FAST Lab) in Huntsville, Alabama.  SMDC is the executing agent of the WBT program and SAIC is the Prime Contractor.References[1] Maximo Lorenzo, Mike Muuss, Mike Caruso, and Bill Riggs: SISO paper “RTI Latency Testing over the Defense Research and Engineering Network”[2] Maximo Lorenzo, Don Tidrow, John Langworthy, Bill Riggs, Gilbert Gonzalez: SISO paper “A Performance Analysis of the PST Federation Using RTI NG Versions 3.2 and 4.0”[3] Rodger D. Wuerfel, Frank J. Hodum: SISO paper “RTI-NG Process Model”[4] Jon C. Snader: Effective TCP/IP Programming.  Addison-Wesley, October 2000[5] Brad Fitzgibbons, Thom McLean, Richard Fujimoto: SISO paper “RTI Benchmark Studies”[6] Douglas D. Wood, Len Granowetter: SISO paper “Rational and Design of the MÄK Real-Time RTI”Author BiographiesPAMELA KNIGHT is the Acting Director for the Space and Missile Defense Command's Information Science & Technology Directorate.  Ms. Knight has more than 20 years of professional experience in physics, engineering, test and evaluations, system engineering, information technology and management. MELANIE KLINNER is an Electrical Engineer in the Space & Missile Defense Command's Information Science & Technology Directorate.  She has more than 10 years of experience in DoD programs including Modeling and Simulation, Information Technology and Information Management, Systems Administration, Software Development and Systems Integration Analysis.RON LIEDEL is a senior engineer with the Space and Missile Defense Battle Lab (SMDBL) located in Huntsville, AL, and has authored numerous SMDC and BMDO BMDO policy forming software documents.  These include the SMDC Software Development Plan, the SMDBL 10 Year Software Plan, as well as the Command Software Mission and Function Statement.  Mr. Liedel founded and chaired the SMDC/SDIO Computer Resource Working Group and has presented several papers nationally on Software Sizing for Mega Systems. He serves as the FAST Lab Director for SMDC.JACQUELINE "JACKIE" STEELE is Chief of the Computer Resources Division, Space and Missile Defense Battle Lab, U.S. Army Space and Missile Defense Command (USASMDC) with over 20 years experience in engineering and simulation in government, industry, and academics.  Ms. Steele oversees the Advanced Research Center and the Simulation Center, and manages the DOD High Performance Computing Management Program (HPCMP) Shared Resource Center located at SMDC.  She holds a BME and BTE from Auburn University and MSM from University of Alabama-Huntsville.RAY DRAKE is a Software Engineer for COLSA Corporation in Huntsville, AL.  He has over 15 years of experience in system administration, optimization, and programming across various UNIX platforms.  He is currently COLSA's technical lead for the Federation Analysis Support Technology (FAST) Lab at the Advanced Research Center (ARC) in Huntsville, AL.  He is responsible for developing the RTI benchmark software used to conduct the timing studies.DR. EDWIN NÚÑEZ is a scientist for Research, Development, Test & Evaluation at COLSA Corporation in Huntsville, AL.  He has been involved with WBII, WBT, and projects requiring the application of innovative technologies to algorithm development.  Dr. Núñez provided the Design of Experiments for the test cases run for the RTI benchmark testing.PAUL AGARWAL is the COLSA Corporation Advanced Research Center (ARC) FAST Lab Program Manager.  The FAST Lab facility hosts the computational resources used in the WBT RTI Benchmark effort.  He has over 20 years experience in public and private sector computer software and systems analysis, design, development, evaluation, implementation, and program management.CAROL JENKINS is a Senior Systems Engineer with COLSA Corporation.  She has over 20 years of professional experience in DoD programs including Modeling and Simulation, Design of Experiments, Statistical Data Analysis, Software Development, Independent Verification and Validation, Communication Analysis and Weapon System Test and Evaluation.  She earned her Bachelor of Industrial Engineering degree in 1981 and her MS in Engineering from the University of Alabama in Huntsville.  She is responsible for data analysis following the RTI benchmark testing.JESSICA GIDDENS is a Systems Analyst for COLSA Corporation in Huntsville, AL.  She received her B.S. in Mathematics from the University of Kansas.  As a member of COLSA's software engineering department, she has participated ion analytical, engineering, and testing simulations in support of the U.S. missile defense effort and has been involved in modeling exercises and software development.  She has also been responsible for data analysis following RTI benchmark testing.MARIO ESPINOSA is a Senior Software Engineer at COLSA Corporation in Huntsville, AL.  He has over 10 years of experience in system administration, optimization, and programming, across a number of hardware platforms.  He has worked on both commercial and government-focused software and networking projects.  He assisted with the review, analysis, and interpretation of the RTI benchmark data.Table 1.  Test Block SummaryTable 1: TEST BLOCKSd8d14d13d11d10d9d12Os2A1A2d2d4d3F1LRCF1FAS2d6d7S2Or2A1A2F2LRCF2FAd5Os1A1A2d14d13d12d11d10d8d2d4d5d6d9d3d7S1Or1A1A2F1LRCF1FAF2LRCF2FAS1d1F1F2Figure 1 – Roundtrip RAV Delays