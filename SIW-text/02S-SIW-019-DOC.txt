Providing Authoritative Equipment Characteristics and Performance Data toSimulations using Data Interchange Format StandardsLee W. LacyMichael PuhlmannGabriel AvilesDynamics Research Corporation3505 Lake Lynda Drive, Suite 100Orlando, Florida  32817407-380-1200 x104LLacy@DRC.comPeter RiganoApplied Research Associates, Inc.(937) 886-0233rigatoni@woh.rr.comCaptain Joel PawloskiU.S. Army TRAC-Monterey(831) 656-4056PawloskJ@trac.nps.navy.milABSTRACT: Simulation applications require authoritative data to provide credible representations of military equipment and weapons effectiveness.  This data comes from a variety of sources with their own unique representation mechanisms and export formats.  Differences in representations have complicated the use of the data.  A standard Data Interchange Format (DIF) for Equipment Characteristics and Performance (C&P) data can improve this data provisioning problem.  Recent research sponsored by the Army Modeling and Simulation Office (AMSO) has resulted in an equipment C&P DIF based on the Extensible Markup Language (XML).  This standard was developed by specifying the requirements of consuming simulation applications, understanding how producers store and export their data, designing the DIF data model, and representing the data model with an XML Document Type Definition (DTD) and XML Schema.  The DIF has been tested with sample data sets and has been demonstrated to potential users.  Issues related to promulgating the standard are now being explored.IntroductionSimulation applications require authoritative data to provide credible representations of military equipment and weapons effectiveness.  This data comes from a variety of sources with their own unique representation mechanisms and export formats.  Differences in representations have complicated the use of the data.  A standard Data Interchange Format (DIF) for Equipment Characteristics and Performance (C&P) data can improve this data provisioning problem.  Recent research sponsored by the Army Modeling and Simulation Office (AMSO) has resulted in an equipment C&P DIF based on the Extensible Markup Language (XML).  This standard was developed by specifying the requirements of consuming simulation applications, understanding how producers store and export their data, designing the DIF data model, and defining the data model with an XML Document Type Definition (DTD) and XML Schema.  The DIF has been tested with various sample data sets and demonstrated to potential users.  Issues related to promulgating the standard are now being explored.The standard must satisfy the data requirements of consuming simulation applications.  The Extensible Markup Language (XML) is being used by the simulation community to standardize data interchange [1].  A standard Data Interchange Format (DIF) for Equipment Characteristics and Performance (C&P) data can improve this data provisioning problem.  BackgroundStandardizing Equipment Data InterchangeRecent research sponsored by the Army Modeling and Simulation Office (AMSO) has resulted in an equipment C&P DIF based on XML [2].  This research initially focused on ground platforms and their direct fire weapons.  The scope is being expanded to include indirect fire, communications, aircraft, sensors, and other equipment-related data.  Sample producers and (AMSAA, NGIC) and sample consumers (CXXI, OTB) were selected for detailed investigation (see Figure 1).  The selected producers were the Army Materiel Systems Analysis Activity (AMSAA) and the National Ground Intelligence Center (NGIC).  The selected consumers were the Army’s Combat XXI (CXXI) and OneSAF Testbed Baseline (OTB) simulations.Figure 1. Equipment Data InterchangeDIF Development ProcessThe first steps in developing the C&P DIF standard were to specify the requirements of consuming simulation applications and to evaluate producers’ systems to understand how they store and export their data.  The DIF data model was then designed, incorporating all consumer requirements and analysis of the producers.  This design was then implemented by specifying an XML Document Type Definition (DTD) and an XML Schema.  The DIF has been tested with sample data sets and has been demonstrated to potential users.  Issues related to promulgating the standard are now being explored.Specifying DIF RequirementsDetailed requirements were specified to guide the development of the DIF.  These requirements considered the scope, level of detail, and types of data being represented (metadata), etc.Select platforms within the program’s scope were analyzed from the OTB libraries and COMBATXXI  tables.  An important finding that directly affected the overall DIF design was that the data elements that make up the libraries can be grouped into the following three categories of data structures:Simple Structures – contain primitive datatypes (e.g., float, integer, Boolean)Arrays – contain user defined lists of itemsComplex Structures – contain user-defined types extending combinations of data structure categories.These data structures became the model for the basic data constructs in the DIF.  Producer AnalysisData producers’ systems must be understood in order to develop a DIF that is useable.  Two analyses were performed on the producers’ data models.   The first was a data inventory on tables provided by AMSAA and NGIC to identify the types of data available from these sources within the DIF’s scope.  The inventory analysis led to a more detailed and thorough data format analysis of the producers’ data.  This analysis investigated the types of formats and specific kinds of data that the producers provide within the scope of the DIF.AMSAA Data FormatAMSAA develops and maintains methodologies, models and databases to analyze item and system level performance of Army materiel systems.  They store their performance data in a relational database and typically provide this data via their proprietary report format.  AMSAA helped identify 18 reports that were within the scope of this analysis.  These reports fell into the following categories:Air Defense,Armor, andInfantry.The reports contained information for the following types of weapons performance:Firing Times,Accuracy Information,Probability of Hit (PH)/ Probability of Kill (PK) Data,Sensed Miss Information, andExpected Casualties for Area Targets.The key finding from the AMSAA analysis is that most of their data depends on the context of a key hierarchy of system / mount / weapon / munition / target identifiers.  Capturing the structure of this key hierarchy is critical for the DIF design.  Without it, the data values would be meaningless.NGIC Data FormatNGIC produces all-source integrated intelligence on foreign ground forces and supporting combat technologies.  They maintain their data in a database that is called the Spirit database.  This Spirit database is a relational database but the data is maintained with an additional level of abstraction, where the table fields are meta-elements that contain the actual elements, sub-elements and their data values.  This non-explicit method of modeling data is referred to as a meta-model. The data within this meta-model eis organized in a hierarchical fashion.  For example at the top level there is “group”, with items such as “Armored Fighting Vehicle”, that contains “Types”, such as “Tanks” and “Tracked Light Armor Vehicles”, that then contains “Models”, such as “T-72” and “BMP-2”.  The “Models” then contain “Components”, such as “Primary Weapon”.  The “Components” then contain name/value pairs that describe characteristics of that specific “Component” with information, such as “caliber” and “range”with specific models and is stored in their relational database as parameter/value pairs.  NGIC’s database contains fields representing hierarchical groups. . highest levelAlthough there is often inconsistency of information within similar items of this data (e.g., “Components” of “Main Gun” and “Primary Weapon” both exist, but represent the same thing), this meta-model does easily lend itself to populating a DIF that is also set up as a meta-model.  However, because the meta-model does not enforce consistency within component and parameter names, there is a possibility that there will be more of a burden for the consumer to understand this data if producers, such as NGIC, do not enforce standards or naming conventions for their data.Data Model DesignData engineering is necessary to model the structure of the data to be interchanged with the DIF.  A key design consideration is to select either an explicit or meta-model approach.Analysis showed producers and consumers have considerable variation in their needs for the DIF.  A meta-model design scheme provides extreme flexibility and was chosen to allow the DIF to handle this variation.  However, some optional explicit constructs were included within the design to help guide consistent population of the DIF. From a structural standpoint the DIF is conceptually grouped into four different areas:Root Elements,Meta-data Elements,Basic Constructs Elements, andExplicit Content Elements.Root ElementThe Root Element section is the root of the XML document and is explicitly tagged as “EquipmentCPData”.  This root name was chosen to reflect what the content of a populated instance of a DIF n XPOD would contain rather than just “DIF”, which is the format standard.All other sections of the DIF may exist directly under the root tag.  Additionally, an optional “Version” tag exists here to allow for providing the version of the DIF as an XML tagged item rather than just a comment.Meta-data ElementsMeta-data elements contain descriptive information that apply to contents of the DIF.  Examples of general Meta-data items are   “Certification” and “Date”.  However, there is a specific global meta-data section immediately below the root level with an explicitly tagged element called “MetaData” that contains all the descriptive data pertaining to all the contents of the DIF.  Items specific to this “MetaData” section are  “Provider” and “Point of Contact”.Basic Construct ElementsBasic construct elements are code-like structures that exist at the lowest level of the DIF.  They are the simplest building blocks of the DIF and together with explicit content elements encapsulate the critical DIF data.  The three basic constructs are: Simple structures (lowest level data item),Array structures (discrete list of items with the same simple structure type), andComplex Structures (made up of simple, array, and complex structures).Explicit Content ElementsExplicit content elements are the explicitly tagged constructs that provide a mechanism to address previously mentioned considerations, such as the AMSAA hierarchy.  They are intended only as a guide to DIF population and thus are all optional.  This also helps maintain the overall flexibility of the DIF.  They exist at an intermediate level between the Root and Basic constructs.  XML SpecificationsThe DIF was formally defined in an XML DTD and an XML Schema.  These machine-readable specifications allow software tools to automatically validate files for consistency with the standard [3].Testing and DemonstrationInstance files (i.e., XML Populated DIF files or XPoDs) were developed to test the structure of the formal XML specifications.  These files were primarily generated from AMSAA and NGIC data sources.  However, data from other sources such as the Universal Threat System for Simulators (UTSS) were also used.  Using multiple sources for test data proved beneficial because of the variations in their format and content.  For example, when translating some UTSS data into an XPoD, it was determined that there were cardinality mistakes on some of the DIF’s explicit content elements.AMSAA Sample DataAlthough AMSAA maintains their data in relational database tables, AMSAA typically provides data in customizable report formats.  For this effort, AMSAA exported sample data for the chosen platforms into an XML report format.  This format is a standard export view that AMSAA will maintain for their reports. AMSAA provided sample data for all applicable reports. Extensible Stylesheet Language Transformation (XSLT) routines were then developed to migrate data from the AMSAA report format into XPoDs.NGIC Sample DataTypically, the NGIC Spirit database may be accessed from the Defense Intelligence Modeling and Simulation Resource Repository (DIMSRR) website.  However, a copy of the Spirit database was provided for this effort, to allow direct access and evaluation.Visual Basic (VB) provides an interface to Oracle databases through the Open Database Connectivity (ODBC) Standard.  VB code was written to access the Spirit database and capture the selected sample data.  The code captured applicable data from the database and generated data in the DIF format.  The VB code also translated component and parameter names into standard names as needed.An earlier research effort helped identify NGIC XML export issues [4].  Eventually, NGIC intends to provide their data using XML DIFs [5].Data MappingResolving the differences between producer and consumer data is called data mapping.  The data mapping process involves identifying data elements from the producer and identifying the relationships to the consumer data elements.  The process is complicated because there is seldom a one-to-one mapping between data elements of producers and consumers.  Often, there is many-to-one, one-to-many, or possibly no mapping.To demonstrate data migration from the XPoDs to the consumers, data mapping from the sample XPoDs to CXXI and OTB was performed by an ad hoc, brute force process.  This process involved going through each element in the XPoDs and identifying the associated data element in the consumers’ formats.  This mapping or cross-reference of elements was captured in XSLT routines.Specifics of this data translation from the XPoDs to CXXI and OTB are discussed in the following two sections.CXXI TranslationThe CXXI equipment data is stored in a Microsoft Access database.  The XP version of Microsoft Access provides an XML import capability.  This capability was utilized to import data from the XPoDs to the CXXI database.The DTD for the XML format for CXXI export tables is explicit with tags for table fields.  Thus, each table has a unique DTD.  XSLT code was created for each table to map from the XPoDs to the CXXI table.  This XLST code contained the Data Map from the XPoD to the CXXI tables.OTB TranslationOTB imports parametric initialization files, called reader files, to make the system data-driven.  XSLT routines were used to generate the reader files directly from the XPoD.  Multiple transformations were needed due to the complex structure of the OTB reader files.  The generated reader files are flat ASCII text files.Data Map MethodologyDefining how consumers interpret data from an XPoD is essential.  For current demonstration purposes, an informal ad hoc process was utilized.  Mapping information was simply captured as conditional statements embedded in import routines (XSLT). However, this definition may be more formally captured in a Data Map document.  This document would contain the cross reference of the data elements.  The results from the Data Map may then be used to transform the XPoDs into consumer formats.  Additionally, a common dictionary of elements to which producers and consumers could map, would greatly simplify the mapping process.   A common dictionary is our term for a negotiated agreement on the semantic expectations of the DIF content (see Figure 2).  Initially, additional effort will be required for the producers to map their data to the dictionary before populating an XPoD.  Consumers would benefit because the dictionary will allow them to map from one common standard source to their format. In the absence of a common dictionary, data consumers will need to generate Data Maps based on the meta-data contained in the XPoDs.  Consumers’ import routines would be tailored to their formats and to specific producers.  Figure 2. Data Mapping with a Common DictionaryImplementing the use of DIFsThe primary challenges in implementing the use of DIFs are social rather than technical ones.  The tools required to implement DIFs are available today and will mature over the near term. The approaches are typically not technically complicated because XML itself is not complicated.  The main hurdle to overcome is getting decision makers to recognize the advantages that are offered by DIFs and to change the underlying business rules and processes that guide data provisioning.  This makes implementation more difficult for legacy producers and consumers who have established relationships and agreements with their major partners.  The focus should be on the DIF’s benefits to the community-at-large since the pool of data consumers grows larger everyday.The benefits are consistent, understandable, and flexible formats.  From a consumer’s perspective, this enables them to incorporate new data sources without have to overhaul parsing code.  From a producer perspective, they have a single export view to maintain.  All producers export data in this unified view and all consumers import from it.  The end result is a single transformation process for consumers and producers that is standardized by the C&P DIF.ImplementationThe implementation problem begins with the producers. If producers do not find a solution that is palatable and supportable by them, the customer will not find any available data that conforms to the standard.  The producers and consumers must commit to evolve their business processes to use the standard. This project has addressed these implementation issues by studying two potential implementations that are common in the Army’s analytical community.  One implementation option is an "off-line" batch type tool that supports large-scale studies (e.g., CXXI, OneSAF).  Another option is an on-line system that provides information to more detailed tools performing one-on-one analyses that look at a particular weapon attacking a particular target (e.g., targeting tools used by the Joint staff and other users).Bulk Data Provisioning Use CaseImagine that the U. S. Army Field Artillery Schools wants to investigate the use of new tactics for artillery engagements.  The study contains 100 systems for the “friendly” force and 107 for the opposing force.  When all is said and done, they require data for about 8000 system / weapon / mount / munition / target combinations.  They request data from AMSAA to support this study.	AMSAA queries their database to see what data they do and do not have for these combinations.  Their analysts produce estimates for the missing items.  When AMSAA is finished generating data, they will produce files with the required data in their native XML format.  These files are sent to the C&P DIF manager who marries them with corresponding data from NGIC.  The data are then translated into XPoDs and passed on to the Field Artillery School who use XSLT to translate them to their native formats, loads the data into their database, and performs the analysis.	This use case complements the existing business processes associated with data provisioning by providing a flexible, open standard.  The current standards employed are not flexible and are highly tailored to specific producer-customer combinations.  This tends to complicate data reuse due to semantic and format differences.On-line Targeting Toolbox Use Case	  The “on-line targeting toolbox” use case is a special case of the data provisioning use case.  The main differences are that data are required for a single system / weapon / mount / munition / target combination instead of multiple ones and that the consumer is an on-line computer application instead of an organization.  Following is an example of this use case:	Imagine that the Joint Staff at the Pentagon gets a call from Central Command asking whether a target complex should be engaged using a tactical missile or with an aircraft.  They need the answer within an hour.	The analyst starts up an on-line targeting tool and builds a mini-scenario that portrays the problem.  He inputs data including the target’s location, the local environment and any other pertinent factors.  He then selects the shooters and targets to be studied.  The tool fetches the required characteristics and performance data for these items from the “C&P Data Server” in the C&P DIF format.   The targeting tool automatically translates the C&P DIF formatted data into its native format and displays it to the analyst.  After reviewing the data the analyst selects the “analyze” button and reviews the results.  He performs sensitivity analyses and then communicates the results back to the Central Command.	Since the C&P DIF is an XML-based neutral format, it can be used to support web-based applications like the “targeting toolbox”.  The tools can be developed against the C&P DIF schema or DTD and use XSLT or other techniques to reformat the data prior to processing it.SummaryEquipment characteristic and performance (C&P) data required by simulations can be provided using a standard Data Interchange Format (DIF).  An XML-based DIF for C&P data has been developed for AMSO.  The DIF’s development was driven by requirements that constrained a data modeling and XML specification effort.  The resulting DIF was demonstrated and tested.  Implementing a DIF in the simulation community will require education and support for organizations that chose to adopt the standard.References[1]	Lacy, Lee, Dugone, Theodore, “Utilizing the eXtensible Markup Language (XML) to Support Simulation Data Interchange”, 69th MORS Symposium, June 12–14, 2001, US Naval Academy, Annapolis, MD[2]	Lacy, Lee, Dugone, Theodore, Youngren, Robert W., “Standard Data Exchange Methods for Equipment Characteristics And Performance Data”, Proceedings of the Interservice/Industry Training, Simulation and Education Conference (I/ITSEC), November 26-29, 2001, Orlando, FL.[3]	Dodds, David, et al, Professional XML Meta Data, Wrox Press, 2001.[4]	Lacy, Lee W., King, Dany, Burnett, Kay, “Equipment Characteristics and Performance Data Interchange for Simulations using XML”, March 25-30, 2001, Orlando, FL.[5]	Grant, BG Nick, presentation at the 2001 Simulation and Modeling for Acquisition, Requirements, and Training (SMART) Conference, Orlando, Florida, April 16, 2001.Author BiographiesLEE LACY is the Director of Orlando Operations for Dynamics Research Corporation.  He has worked on major simulation programs including WARSIM 2000 and the Close Combat Tactical Trainer (CCTT).  He serves on the Board of Directors of the National Center for Simulation. His research areas include XML and the Semantic Web.  He received an M.S. and B.S in Computer Science from the University of Central Florida.MICHAEL PUHLMANN is a Senior Software Engineer.  His primary focus has been data and systems modeling.  He has worked on several major simulation systems including the Close Combat Tactical Trainer (CCTT) and the International Space Station (ISS) Training Facility.  He received a B.S. in Mechanical Engineering from Clarkson University and an M.S. in Computer Science from the University of Houston - Clear Lake.GABRIEL AVILES is an independent software consultant with nine years of experience designing and developing solutions for modeling and simulation applications.  His most recent work is focused on semantic web technology.  He received a B.S. in Electrical Engineering from Boston University.PETER RIGANO is a Senior Engineer with Applied Research Associates Incorporated.  He has worked on Army data provisioning for many years and has served as the Category Coordinator for the Army Model Improvement Program’s Data Standards Category.  His current research focus is on enabling business solutions through database technologies.  He received a B.S. in Chemical Engineering from the Ohio State University and an M.S. in Systems Management from the Florida Institute of Technology. He is also an Oracle Certified Professional.CAPTAIN JOEL S. PAWLOSKI is an Army officer with over fourteen years of active service. He graduated with a B.S. in Aviation Technology from Embry-Riddle Aeronautical University in 1991 and was commissioned in the Armor branch. His assignments include Armored Scout Platoon Leader, Air Cavalry Troop Commander, and Brigade Logistics Officer in Bosnia.  CPT Pawloski completed an M.S. in Modeling, Virtual Environments and Simulation from the Naval Postgraduate School in 2001. He is currently assigned to the TRADOC Research Analysis Center in Monterey, California.PAGE \# "'Page: '#''"  This paragraph also exists in the intro.  PAGE \# "'Page: '#''"  Spell out  AMSAA and NGIC for first timeU. S. Army Materiel Systems Analysis ActivityNational Ground Intelligence CenterPAGE \# "'Page: '#''"  Spell out CXXI and OTB for first timeCombined Arms Analysis Tool for the XXIst CenturyOneSAF Test BedPAGE \# "'Page: '#''"  May with to describe what metadata means before using it.