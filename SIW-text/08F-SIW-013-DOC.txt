Mathematical Means for Assessing Military Aircraft Pilot Performance as Applied to Training Effectiveness Ingrida ValiusaityteEshan RajaballySystems Engineering Innovation CentreLoughborough, LE11 3TUUnited KingdomI.Valiusaityte2@lboro.co.uk, E.Rajabally@lboro.co.ukTom PageDepartment of Design and TechnologyLoughborough UniversityLoughborough, LE11 3TUUnited Kingdomt.page@ lboro.co.ukKeywords:Simulation-based technology, pilot performance assessment, mathematical techniquesABSTRACT: Modern simulation-based training techniques use virtual or mixed realities to achieve cost-effective military aircraft pilot training. Pros and cons of simulation-based versus live training are briefly described in the paper. Determining the optimum usage of advanced simulation or choosing the best mixture of diverse techniques hinges upon the reliable assessment of training effectiveness. To this purpose, current methodologies adapt a straightforward approach of comparing trainees’ performance before and after the training. Previous attempts to quantify trainee performance have been modestly successful, but lack objectivity, are limited to basic measures chosen for the particular training technique, and struggle to incorporate measures that are subjective/objective, qualitative/quantitative, and psychological/demographical. An assessment of pilot performance is increasingly complex given the sophistication of modern military aircraft and the changing nature of military operations. This is reflected by interest in pilots’ assessment against “Mission Essential Competencies” (MECs) courtesy of the United States Air Force Research Laboratory and Air Combat Command. In addition to further explaining the motivation for research into mathematical means for assessing pilot performance ranging from probabilistic approaches to Artificial Neural Networks (ANNs), this paper provides a critique of such means based upon experiences in other domains and illustrated by the MECs. Critical factors for choosing the right mathematical approach include the fusion of heterogeneous data, the ability to handle uncertain and incomplete data, and the ease of implementation.IntroductionThe need for cost- and resource- effective military aircraft pilot training has led to an interest in blending Live, Virtual and Constructive (LVC) techniques. Today, fully immersive as well as less complex computer simulations are used more and more heavily to supplement live flight exercises. Advanced simulation enables trainees to experience situations with degrees of precision that were impossible with previous generation technologies. However, if used exclusively, simulation-based and traditional live training have both pros and cons, and determining the optimum LVC mix hinges upon the reliable assessment of training effectiveness. To this purpose, a straightforward approach of comparing trainees’ performance pre and post training is often adopted. Previous attempts to quantify trainee performance have been modestly successful, but lack objectivity, are limited to basic measures (e.g., response time, duration, accuracy, etc.), and struggle to combine heterogeneous measures. Such measures may be subjective / objective, qualitative / quantitative, or psychological / demographical. In order to enhance the effectiveness of training regardless of technologies involved, it is first necessary to resolve issues surrounding pilot performance assessment. This paper concerns a viable way ahead – usage of mathematical means for solving these issues. Trainee assessments are increasingly complex given the sophistication of modern military aircraft and the changing nature of military operations. Thus, the “Mission Essential Competencies” (MECs1) as opposed to conventional airmanship skills are used as a basis for the problem illustration. This paper aims to provide a critique of mathematical means ranging from the classical probabilistic approach to Artificial Neural Networks (ANNs) in the light of this context and is based largely upon experiences in other domains. Firstly however, we briefly introduce the nature of modern aircrew training, summarize current thinking in both training effectiveness and pilot performance assessment, and introduce MECs1. The Nature of Modern Aircrew TrainingIn the past, teaching aircrew flying skills and necessary procedures for combat readiness has relied on a combination of live flying and simulated exercises. For instance, live flight exercises have primarily been used for training basic flying skills while simulators have been used for training emergency procedures. Then given the push for limited flying hours and low yearly flying rates, the simulator sessions were targeted to use hours in the aircraft more effectively  REF _Ref200444067 \r \h [1]. In any case, simply throwing more live flying hours at gaps in training would not have been a viable solution across the spectrum of core activities  REF _Ref200444074 \r \h [2] due to high operating and support costs, safety factors, mission complexity, airspace and range restrictions, weather, aircraft fatigue and even artificial termination of flight at the onset of unsafe conditions  REF _Ref200444079 \r \h [3],  REF _Ref200444085 \r \h [4]. Nowadays fighter pilots must deal with multiple tasks whilst perhaps flying at twice the speed of sound. Accordingly, over the years, a number of technologies have been developed to ensure that training adequately addresses the critical competencies required to do this effectively. Major development is accredited to the dramatic improvement of simulation-based training  REF _Ref200444113 \r \h [5].The long history of use of simulators  REF _Ref200444119 \r \h [6] has established enough data about their effectiveness as well as shortfalls compared to the actual experience. It is beyond the scope of this paper to provide a detailed review of the use and effectiveness of simulators for military training, thus only major features of simulation are mentioned. The key benefits include control of the environment, repeatability, measurability  REF _Ref200444127 \r \h [7], safety, accessibility and cost reduction  REF _Ref200444119 \r \h [6],  REF _Ref200444149 \r \h [8],  REF _Ref200444144 \r \h [9]. With respect to effectiveness, a validation exercise performed by the South African Army  REF _Ref200444113 \r \h [5] showed that students going through the gunnery training simulator achieved 30% to 40% quicker reaction time and 14% better first hit results than those who did not use the simulator. Simulators also allow better assessment of trainees since measurability – greater opportunity for data collection than the real world – is one of the features simulations can provide. There is a wide range of rationale stated by different authors for simulation-based training such as affordability, practicality in providing training for tomorrow’s multinational, multi-force, joint/coalition operations, and flexibility for these operations. As a result, improvements of the existing training technologies have been made and new training technologies have been developed over the years. Graesser and King  REF _Ref200444168 \r \h [10] have identified ten advanced training environments that hold potential for technology-based training. These include computer-based training, multimedia training, interactive simulation, hypertext and hypermedia, intelligent tutoring systems, inquiry-based information retrieval, animated pedagogical agents, virtual environments with agents, serious games, and computer-supported collaborative learning.Although benefits of simulations cannot be underestimated, their obvious limitation is a lack of realism  REF _Ref200444174 \r \h [11], which means that results from simulations may not always hold practical usefulness in the real world. Other limitations include motion sickness due to differing response characteristics of visual displays and of motion platforms, risk of losing confidence in the use of actual equipment and even instilling habits of incorrect use of such equipment  REF _Ref200444179 \r \h [12]. Moreover, environments in which military aircraft pilots are, and will in the future be, required to operate are evolving continuously and no single training technology can continually provide combat ready aircrew  REF _Ref200444184 \r \h [13]. Today, therefore, simultaneous use and convergence of multiple technologies and strategies is increasingly becoming popular in addressing the training gap  REF _Ref200444196 \r \h [14]. There are already many examples of convergence within training environments such as the use of Web-based courses  REF _Ref200444202 \r \h [15] to access simulations in which an online chat is embedded and MyBase  REF _Ref200444207 \r \h [16] – a virtual, exploratory and interactive environment and architecture that supports both continuous and precision learning. The goal of MyBase is to assist in recruiting the brightest and educating them “with agile minds and cutting edge skills”  REF _Ref200444207 \r \h [16] in order to produce future airmen. With the emergence of mixed and virtual reality-based simulations  REF _Ref200444212 \r \h [17],  REF _Ref200444217 \r \h [18], more complex interactions are possible, including visualization, audio, and touch technologies. In recent years, serious games  REF _Ref200444224 \r \h [19] have also emerged from the gaming sector to provide a solution for training anytime-anywhere, but lack true training as they generally omit objectives, skills and knowledge assessment  REF _Ref200444079 \r \h [3],  REF _Ref200444241 \r \h [20]. Interoperability applied to simulation has made a major contribution towards providing collective training. In a networked simulation used for close air support training, Orlansky et al.  REF _Ref200444245 \r \h [21] found that collective training can increase the number of close air support kills, reduce the mean time between bomb releases, and improve fire support and manoeuvre synchronization while reducing the costs of training by about one-third. A related technology, Distributed Mission Training (DMT) provides increased training frequency and flexibility in training scenarios involving the real-time interaction with multiple players at multiple sites  REF _Ref200444196 \r \h [14],  REF _Ref200444261 \r \h [22].Training EffectivenessAdvances in training technology are outpacing advances in an underlying science of training effectiveness, in part because the field is vendor and not science driven  REF _Ref200444270 \r \h [23]. This is despite the observation by Estock et al.  REF _Ref200444085 \r \h [4] that "Effective training develops capabilities that can mean the difference between life and death for air combat fighters." However, no matter what technologies are used to train, the balance between training effectiveness and cost-efficiency must be well controlled in order to use them productively  REF _Ref200444119 \r \h [6],  REF _Ref200444282 \r \h [24]. From a simplistic view, there are two approaches that can be taken to achieve the “best” mix of live-synthetic training, one is to minimize costs while holding effectiveness constant whilst the other is to maximize effectiveness while holding costs constant  REF _Ref200444298 \r \h [25].Both approaches require evaluation of training effectiveness, more specifically an evaluation of skills acquisition and a Transfer Effectiveness Evaluation (TEE)  REF _Ref200444304 \r \h [26],  REF _Ref200444309 \r \h [27]. These both require pilot performance assessment either in real or simulated conditions. The evaluation of skill acquisition seeks to quantify the amount of learning regardless of its transfer to a different environment. It may then be inferred by comparing performance pre and post training  REF _Ref200444304 \r \h [26].A TEE concerns “the ability for a skilled behaviour which has been learned in one situation to be carried over to another”  REF _Ref200444319 \r \h [28] and is commonly measured by subjectively assessing trainee’s performance in real world conditions, which however detracts from the validity of a TEE  REF _Ref200444304 \r \h [26]. A value equal to 1.0 of transfer of training can be achieved, if the amount of training time saved in the air equals the amount of time spent in the simulator  REF _Ref200444074 \r \h [2]. However, Provenmire and Roscoe  REF _Ref200444345 \r \h [29] point out that not all simulator hours are equal since early hours in the simulator may save more flying hours than later ones. Military Aircraft Pilot Performance AssessmentThe primary purpose of military aircraft pilot performance assessment is to identify strengths and weaknesses in the competencies necessary for successful air combat so that existing deficiencies can be addressed during the training  REF _Ref200444270 \r \h [23]. For this reason, performance assessment must exhibit adaptability to the available data, accumulate and fuse heterogeneous data varying in type, accuracy and frequency, and must be carried out regularly in order to determine individual training success at specific time intervals.Adaptability is a desired feature since in some cases there is good analytical modelling data to use, in others real operational experience may be available, whilst for the remainder, subjective judgements may only be available. Due to such diversity of data, pilot performance assessment has several limitations. First, only one aspect of competence or a small number of behaviour indicators associated with one competence is typically considered, for example  REF _Ref202328466 \r \h [14]. Second, data of only one type is considered at a time such as subjective or objective, or quantitative or qualitative. Finally, an assumption must often be made by Subject Matter Experts (SMEs) regarding the suitability for purpose of the available data. In general, pilot performance is assessed using predefined performance measures, which can be categorized into two general types: those based on actual performance (objective) and those derived from expert judgment (subjective)  REF _Ref200444304 \r \h [26]. While objective measures provide data that is difficult to argue with and reports exactly what happened in terms of relevant to air combat competencies  REF _Ref200444385 \r \h [30], subjective measures provide results that, by their very nature, can be influenced by human factors. The difficulty with subjective measures is that they are often hard to define and update, and lack means for consistent application. Adding Rigour to Performance AssessmentThere have been many attempts in history to make pilot performance assessment less subjective by introducing objective measures where possible which are then combined using SME judgement. For example, when considering mission end results, the observed pilot behaviour is compared with the actions that would be expected by an expert  REF _Ref200444393 \r \h [31] as decided by SMEs, and by assigning a numeric score depending on the deviation. With the advent of new technologies, simple mission end results have evolved into more complex ones such as coordination measures  REF _Ref200444402 \r \h [32], Pairwise Escape-G  REF _Ref200444408 \r \h [33] measure or the All Aspect Manoeuvring Index (AAMI)  REF _Ref200444196 \r \h [14],  REF _Ref200444408 \r \h [33]. Physiological measurements have also been introduced to pilot performance assessment  REF _Ref200444427 \r \h [34],  REF _Ref200444433 \r \h [35],  REF _Ref200444440 \r \h [36] to add information from different perspectives. Many more similar measures have been developed throughout the years. However, they continued to be focused on one aspect only – a specific skill, knowledge or competency, but not the overall performance as a combination of these specific abilities. Since an overall assessment is a complex combination of different abilities, pilot performance assessment should accommodate a subtle transition from one level of competence to another and provide a corresponding explanation.With new technologies, new ways of assessing pilot performance that to some extent involved fusion of operational data have been published in the literature and developed as commercial tools as described below. Proposed approaches can be classified as subjective, objective or mixed based on the type of measures employed in the performance assessment. Subjective techniques include post-participation questionnaires  REF _Ref200444427 \r \h [34], Behaviourally Anchored Rating Scales (BARS) and Scenario-based Performance Observation Tool for Learning in Team Environments (SPOTLITE)  REF _Ref200444127 \r \h [7],  REF _Ref200444643 \r \h [37]. Examples of objective techniques are Automatic Performance Assessment (APA)  REF _Ref200444393 \r \h [31], and Performance Effectiveness and Evaluation Tracking System (PETS)  REF _Ref200444196 \r \h [14]. Newer techniques  REF _Ref200444393 \r \h [31],  REF _Ref200444676 \r \h [38],  REF _Ref200444683 \r \h [39] often fall into the final category of ‘mixed’ by employing both, subjective and objective measures. These techniques also attempt to mitigate subjectivity more by providing SMEs with the evidence they need, when needed and in a form that allows rapid assessment  REF _Ref200444393 \r \h [31]. However, mitigating subjectivity may not work with an increasing number of real-time decisions that must be made and with excesssive amounts of evidence provided for SMEs’ assistance. This is very likely to happen when several technologies with embedded performance assessments are used for training  REF _Ref200444224 \r \h [19]. Handling uncertainty is another issue that is of considerable significance in pilot performance assessment. Uncertainty impacts throughout an assessment, such as the particular choice of skills, knowledge and attitudes necessary for tasks, the subjective scoring of these tasks and the accuracy and validity of measurements. Sources of uncertainty commonly listed in literature include lack of information, conflicting evidence, ambiguity, measurement error, subjective belief, and unpredictability  REF _Ref200444754 \r \h [40],  REF _Ref200444760 \r \h [41],  REF _Ref200444768 \r \h [42]. As a result, a key requirement of pilot performance assessment is the robustness to such uncertainty. Introduction to Mission Essential Competencies (MECs1)Brief mention is now made of latest thinking in relation to defining the performance required of military pilots. Mission Essential Competencies (MECs1)  REF _Ref200444261 \r \h [22],  REF _Ref200444776 \r \h [43],  REF _Ref200444782 \r \h [44] have been pioneered in the U.S. although wider application has occurred such as the UK Composite Air Operations (COMAO) experiments for training using distributed networks  REF _Ref200444067 \r \h [1]. According to Schreiber et al.  REF _Ref200444385 \r \h [30] the MECs1 for an air-to-air mission process define which skills constitute a proficient fighter pilot in combat, that are readily applicable to a realistic environment. The hierarchical structure of the MEC1 model consists of the MECs1 at the top level, the Supporting Competencies (SCs) at the next level and Knowledge and Skills (SKs) at the lowest level. Standardized abilities can be determined using different work analysis methods such as Hierarchical Task Analysis (HTA) or the Mission Essential Task List (METL)  REF _Ref200444875 \r \h [45]. Metrics for each of the abilities are beyond the scope of this paper as our goal is only to introduce a new way of combining those measures, once defined, in order to provide useful results. The MECs1 aim to provide a richer picture of aircrew competency than offered by the dated perspective of gauging skills and knowledge acquired through the completion of flying hours alone. The MECs1 therefore present more of an assessment challenge given the range and combination of abilities concerned and the disparate nature of evidence that could be considered. .Applying Mathematical Means to Performance AssessmentPrevious sections were intended to expose desirable features to mathematical means in contributing objectivity to performance assessment. These features are proposed to be the abilities to:fuse heterogeneous data in a rigorous manner,provide traceability between results and evidence, handle uncertain and incomplete information,handle quantitative and qualitative measures,accommodate various performance decompositions such as hierarchical,reflect real-world complexity such as subtle transitions in competency and many-to-many relationships,be implemented and used with relative ease.With respect to training, performance assessment based on mathematical means may be beneficial to both aircrew and instructors. Aircrew-centric benefits include powerful debriefing, identification of task deficiencies and potential optimisation of training regime. Instructor-centric benefits include assistance in pilot grading and certification, assessment of pilot readiness for airborne deployment and assistance in role assignment according to operational requirements.The following subsections critically assess six mathematical means in the context of human performance assessment, namely the classical probabilistic approach, Bayesian Belief Networks (BBN), Response Surface Method (RSM), fuzzy logic, Artificial Neural Networks (ANN) and Genetic Algorithms (GAs). The selection of these means has been based on a history of wide application in other related domains, particularly uncertainty reasoning. Classical Probabilistic ApproachThe classical probabilistic approach is “the most widely used technique for uncertainty analysis”  REF _Ref200444886 \r \h [46]. Probabilities in this approach are associated with events whose occurrence or failure to occur is random and where an event corresponds to any possible outcome of a given problem  REF _Ref200444901 \r \h [47]. In applying the probabilistic approach, it has long been recognized that the assumptions representing the connection to reality are often effected through observation and statistical methods. Issues that are typically cited include: the difficulty to justify the assumptions made to infer probabilistic descriptions,the difficulty to obtain information in real application settings such that it would justify these descriptions, and finally,the sensitivity of the output descriptions to the assumed input probabilistic information.Despite the difficulties, the classic probabilistic approach is ideal for representing any degree of intra-system dependency and handling multi-parameter and highly coupled designs regardless of sample size  REF _Ref200444901 \r \h [47]. However, algorithms required for uncertainty representation and manipulation are computationally demanding  REF _Ref200444760 \r \h [41],  REF _Ref200444927 \r \h [48]. In most problems, particularly pilot performance assessment, it is not sufficient to introduce random variables without the consideration of doubt. Performance for air mission success is often affected by various uncertainties, which cannot be described by traditional probability alone.Bayesian Belief Networks Bayesian Belief Networks (BBNs) are a vital tool in probabilistic modelling and permit a combination of probability interpreted as a belief. This combination makes BBNs a powerful methodology within machine learning and statistics. An important fact to realize about BBNs is that they are particularly useful when the information about the past and/or the current situation is uncertain  REF _Ref200444768 \r \h [42]. BBNs require the definition of prior probabilities although complete prior information is rarely available and the application of BBNs is thus limited  REF _Ref200444927 \r \h [48],  REF _Ref200444976 \r \h [49]. BBNs are founded on Bayes’ theorem: EMBED Equation.DSMT4  where P(A) and P(C) denote the prior probabilities of events A and C, respectively; the term P(A|C) gives the probability of event A assuming the hypothesised event C is true; and the term P(C|A) denotes the posterior probability of event C after the effect of event A is considered. The prior probability is generally very subjective and in fact, according to Niedermayer  REF _Ref200444768 \r \h [42], a Bayesian probability is only as useful as this prior knowledge is reliable. In the context of MECs1, BBNs provide a “candidate approach for assigning proficiency levels to knowledge and skill competencies based on observed performance” and has already been implemented by Carolan et al.  REF _Ref200445005 \r \h [50].  In their approach:prior probabilities are used to represent the pilot’s competencies profile prior to the exercise,conditional probabilities are used to  represent the relationship between competencies and performance actions, evidence is used to represent deviation between observed and expected performance during the exercise, and posterior probabilities are used to represent profile of pilot’s competencies after the exercise. A BBN is formed for each MEC1 and each participant in order to evaluate training effectiveness on a comparative basis. That is, when a predefined action is observed during a test of a pilot’s performance, his competencies’ posterior probabilities are updated. At the end of the test, a new competencies profile is created and can be used for comparison. Probabilities of any branch of the network can also be calculated thus providing traceability from results back to evidence. However, while calculating the probability of one branch of the network, computational difficulty may be experienced because first all branches must be calculated.Carolan’s et al.  REF _Ref200445005 \r \h [50] approach has been simplified to binary performance and competency variables, which in reality are seldom enough. Simplification is made since the sum and product rules of probability theory for scenarios with a large number of discrete random variables can greatly increase the difficulty of calculations. In addition, to use the approach, SMEs are required to define SK requirements for maintaining performance standards on each task. However, it is often impossible to identify a distinct value of an appropriate indicator (e.g., root mean square value of acceleration) that would separate satisfactory from unsatisfactory performance, if such a simplistic binary relationship is adopted. Typically, a transition region is observed, where a pilot is gradually losing his/her ability to perform adequately. Despite the limitations, humans are excellent at vague linguistic representations of knowledge (e.g., “it will probably rain tomorrow”), and less adept at providing specific estimates. Hence the insight afforded by BBNs despite vagaries in the input information is particularly advantageous. With respect to BBNs, Niedermayer  REF _Ref200444768 \r \h [42] and Zimmerman  REF _Ref200445035 \r \h [51] comment on the risk of violation of the distribution of probabilities upon which a model is built due to its inability to capture previously unforeseen events. As an example, unanticipated pilot’s actions might put equipment and/or people in danger and yet allow achievement of the highest evaluation of performance against the predefined performance indicators. Furthermore, the additivity assumption valid for conditional probabilities is found to be too constraining for many problems  REF _Ref200445043 \r \h [52]. This assumption states that if a piece of evidence is only partially in favour of a hypothesis, it would also have to partially support the negation of that hypothesis to satisfy the requirement P(A|B) + P(Ā|B) = 1. Under the additivity assumption and given binary variables, if a pilot’s action suggested the presence of a particular competency, then the omission of that action indicates the absence of the competency. However, the latter may not be true in all contexts.Response Surface Method Myers and Montgomery  REF _Ref200445050 \r \h [53] describe the Response Surface Method (RSM) as “a collection of statistical and mathematical techniques useful for developing, improving, and optimizing processes”. Using RSM, approximating relationships between the response (output) and the process variables (input), called the response functions f, are first developed, and then the values of the variables that produce desirable responses can be found using optimization methods  REF _Ref200445060 \r \h [54]. Although these tools were developed for applications in the chemical industry, RSM may be applied in the domain of human performance assessment. When combined with Design of Experiments (DOE), this approach allows investigation of design space using relatively few point designs. And the contour plot – a visualization of the entire design space – provides an easily understandable display of that space.In the context of the MEC1 model, RSM may be employed to model relationships between a MEC1 and a set of performance measures of SKs. Once desired competencies are identified, RSM can be used to determine the skill and knowledge levels that will simultaneously satisfy the requirements, thus, providing a guideline for training. Moreover, RSM would also be capable of providing the optimum combination of SKs that would yield a desired MEC1. In addition, the SME could not only pinpoint the SKs that need to be improved to meet the requirements, but also estimate how much each would have to be enhanced by, thus providing good feedback to trainees. On the other hand, the size of RSM model greatly increases with increasing number of variables, thus the method is effective to apply only if a small number of SKs (variables) influence the MEC1 (response). The size of RSM model is also affected by limits of variables. A poor choice of these limits would result in an incorrect optimum due to large errors or constraints. Moreover, if the design space is identified incorrectly, a simple choice of polynomial model might do a poor job of predicting aircrew performance outside this space. Furthermore, to address uncertainty, RSM requires combination with an uncertainty analysis. Fuzzy Logic based Approach Fuzzy logic or fuzzy set theory is relatively young but, over the past few decades, has succeeded in spreading across a wide range of problem domains from process control to pattern recognition and classification  REF _Ref200445073 \r \h [55]. Fuzzy logic excels in mimicking human decision making given an ability to describe problems in linguistic terms, rather than in terms of relationships between precise numerical values. Fuzzy logic can also generate precise solutions from both certain and uncertain information  REF _Ref200445083 \r \h [56],  REF _Ref200445089 \r \h [57] as well as incomplete and ambiguous data  REF _Ref200445073 \r \h [55]. These features are highly beneficial in many applications including human performance assessment since in the real world not everything is so strictly true or false.Fuzzy sets have the ability to represent uncertainty  REF _Ref200445116 \r \h [58] since a blurred set boundary is assumed as opposed to the crisp boundary in classical sets theory. Thus, unlike members of a classical set, a fuzzy set member is attributed with a membership function  REF _Ref200445126 \r \h  \* MERGEFORMAT [59] that describes its degree of belonging to that set. A fuzzy set member can also belong to more than one set with different degrees of membership and this allows a gradual transition between adjacent sets  REF _Ref200445073 \r \h [55]. This is especially important in the context of competencies where subjective measures are needed but are hard to quantify precisely. Gradual transition from one set to another provides required robustness and non-sensitivity to small changes. There are many ways that fuzzy membership functions can be determined and one of the popular ones is the heuristic method where predefined shapes are chosen by an expert to represent certain linguistic terms. Triangular and trapezoidal membership functions are often chosen due to their computational efficiency  REF _Ref200445142 \r \h [60]. Yet, considerable difficulty and a lack of common approaches have been observed in generating these membership functions  REF _Ref200444976 \r \h [49],  REF _Ref200445043 \r \h [52]. Therefore, the choice of membership function ultimately remains a question of subjective preference. The whole fuzzy system consists of an input that is fuzzified, a means of inference, an output that is defuzzified and a feedback connection from the output to the input  REF _Ref200445083 \r \h [56]. Fuzzification transforms real input data into fuzzy sets by means of linguistic variables with fuzzy states. The inference component consists of a fuzzy rule base and a method for the aggregation of the resulting fuzzy set. The rules are based on the expert’s domain knowledge thus allowing experts to express their opinion in an intuitive way and can be used at any level of a hierarchical structure such as in the MEC1 model. For the designer who understands the system, these rules are easy to write, and as many rules as necessary can be supplied to describe the system adequately  REF _Ref200445177 \r \h [61]. Finally, the output is attained from the fuzzy results by defuzzification methods such as centre of gravity  REF _Ref200445073 \r \h [55]. On the downside, fuzzy sets use heuristic algorithms for defuzzification, rule evaluation, and antecedent (if-part) rule processing, which cannot handle unforeseen conditions. Moreover, with increasing complexity of relationships between SKs and MECs1, the generation of membership functions and rules becomes difficult and time consuming. Also, once the rules are determined they remain fixed, although adaptive fuzzy systems allow some flexibility. As a result, if fuzzy logic is chosen for pilot performance assessment there are disadvantages that must be carefully considered. Despite this, a fuzzy expert system has been used to develop a Performance Evaluation Methodology (PEM) to assess combat readiness  REF _Ref200445196 \r \h [62]. It was found that such system is not only suitable for quantitative and qualitative assessment but is also tolerant to imprecise data.Genetic AlgorithmThe Genetic Algorithm (GA) is a popular technique in evolutionary computation research  REF _Ref200445202 \r \h [63]. It has been used in many different domains for solving scientific and engineering problems as well as for modelling economic, ecological or social systems. However, for the most part, applications of GA tend to be rather specialized; they are only rarely seen in more general-purpose applications  REF _Ref200445177 \r \h [61]. The GA  REF _Ref200445083 \r \h [56],  REF _Ref200445177 \r \h [61] is an adaptive heuristic search algorithm premised on the ideas of natural selection and genetics. The basic concept of the GA is maintaining a population of candidate solutions that are tested against a given problem. Based on performance level each solution is assigned a fitness value. As the evolutionary process based on chosen operators continues, inferior traits tend to die out due to lack of reproduction. Meanwhile, stronger traits tend to combine with other strong traits producing better performing solutions. The final solution is the one that maximises/minimises the fitness function.The major benefit of GAs still comes from their simplicity and elegance even when the search space is characterised as large, complex or poorly understood. The search space of relationships between SKs and MECs1 in the MEC1 model for pilot performance assessment, certainly exhibits these characteristics. If the data is available, the optimal solution, i.e., well predicted relationships between SKs and MECs1, can be determined by using this approach. The GA works also quite well even when initial requirements are not very accurate, which is very likely in the domain of assessing pilot competencies. What is more, the GA possesses the ability to explore and learn from its domain  REF _Ref200445177 \r \h [61], thus assisting when the expert’s domain knowledge is limited. When solving multi-objective problems, the GA has the ability to generate many satisfactory solutions in terms of the objectives, allowing the decision maker to select the best alternative.Despite the benefits of the GA in discovering good solutions by extracting hidden information from several solutions, the search speed and accuracy can suffer from premature convergence due to a poor choice of GA operators. Low search speed and inaccuracy can also result since the chosen operators with their parameters are held fixed throughout the evolutionary process, and because varying environments are not considered in the algorithm  REF _Ref200445083 \r \h [56]. The selection of operators and parameters remains a subjective and manual process leading to the increased running time of the GA if any settings need to be changed. Up to now, different techniques have been investigated to solve such problems. For example, to solve the GA parameter adjustment problem McClintock et al.  REF _Ref200445083 \r \h [56] employ a fuzzy system that dynamically controls selected GA parameters. Such hybrid GA systems could also permit the explicit consideration of uncertainty.   Artificial Neural NetworksAn Artificial Neural Network (ANN) is an information-processing paradigm that is inspired by the way biological nervous systems, such as the brain, process information. ANNs have a history of successful applications in many different areas such as predicting financial time series, diagnosing medical conditions, identifying fraudulent credit card transactions and hand-written character recognition  REF _Ref200445252 \r \h [64]. Their success is associated with the ability of ANNs to handle various types of uncertainty that is clearly relevant to human performance assessment.An ANN consists of interconnected neurons where each neuron has the ability to compute an output signal based on all the incoming signals from the neurons connected to it in the previous stage. This output signal is then multiplied by the corresponding weights (the connection strength) before being transmitted to the neuron on the next stage. The neural network weights are determined during a learning process for a specific application based upon underlying data. Indeed, an ANN requires a number of data sets representing the output as a function of the inputs, covering the entire range of operation. The learning capability enables ANN to address nonlinear, time variant problems effectively, even under noisy conditions. The accuracy of the solution provided by an ANN can furthermore be specified although an ANN may be expensive to implement particularly if compared to fuzzy logic. Once the training is completed, an ANN can determine a solution from any input applied no matter how complex and answer “what if” questions  REF _Ref200445258 \r \h [65] given different situations of interest. Given such features with the ability to retain some of them in case of partial destruction of the network (i.e. fault tolerance), an ANN offers a candidate approach for pilot performance assessment.The ANN, however, does have a number of disadvantages. The major issues of concern today include “the scalability problem, testing, verification, and integration of neural network systems into the modern environment”  REF _Ref200445258 \r \h [65]. In addition, the ANN needs a long training phase and the network architecture together with other parameters must be set up by an expert in the use of ANNs. In the context of the MEC1 model, the relationships between a MEC1 and SKs as already mentioned, may be large and complex. Thus it would be difficult to determine the proper size and structure of an ANN to find a solution. With increasing complexity, manipulation of learning parameters, as well as convergence, becomes increasingly difficult. Yet, the significant problem with an ANN is their so called “black box” nature  REF _Ref200445267 \r \h [66]. This means that the nature by which a specific solution is determined from a given set of inputs is unknown. As a result, it would be impossible to establish the precise reasons for a particular pilot assessment and therefore provide significant feedback.Concluding Remarks With modern technology, the feasibility of training military aircrew using means other than live flying has greatly increased. Despite a lack of realism, such training means have clear benefits in terms of cost, flexibility and practicality. Amongst other factors, the best mix of training media hinges upon a clear understanding of the effectiveness of such media. This understanding requires an objective measurement of aircrew performance pre and post training in order that the effectiveness of such training can be rigorously assessed. However, as modern day Mission Essential Competencies (MECs1) suggest, the capability of current aircraft along with the diverse and networked nature of military operations, require a complex combination of knowledge, skills and abilities. This complexity motivated a study of the applicability of mathematical techniques to aircrew performance assessment as summarised in this paper.    The literature based review of relevant mathematical techniques suggested a number of pros and cons to their application. For example,The probabilistic approach has a long successful history in uncertainty reasoning but struggles to deal with subjective doubt or ambiguity.Bayesian Belief Networks permit the treatment of uncertainty as subjective belief but require considerable prior data.The Response Surface Method can provide useful visualizations of a response variable but is limited to simple relationships.Fuzzy logic can be used to represent human decision making although membership functions can be difficult to define.Genetic algorithms have enjoyed considerable success as a search technique although fitness functions are difficult to define.Artificial Neural Networks can represent complex relationships although provide limited insight.Many find that no technique in isolation can adequately handle the real world  REF _Ref200444976 \r \h [49],  REF _Ref200445116 \r \h [58],  REF _Ref200445284 \r \h [67]. However, the mentioned techniques hold the potential of applicability to performance assessment due to their main abilities to fuse diversity of data and constrain the impact of subjective inputs. Future research will concern the test application of mathematical techniques to aircrew performance assessment and the potential development of hybrid techniques.ReferencesMaj D.B. Cochrane: “The Evolution of Simulation. The Necessity for Distributed Mission Training (DMT) in meeting future Air Force Flying Requirements“, Canadian Forces College, MDS Research Project CSC 30, pp. 1-94, 2004. S.R. McGrath: "Leveraging Simulation Against the F-16 Flying Training Gap", Center for Strategy and Technology, Air War College, Air University, Research Paper No. ADA463470, pp. 353-382, November 2005.A. Nussbaum: "The Advantage of Low Cost Training & Simulation Solutions", In. Proceedings of SimTecT 2007 Conference, Australia, June 2007. J.L. Estock, A.L. Alexander, K.M. Gildea and M. Nash: "A Model-based Approach to Simulator Fidelity and Training Effectiveness", Interservice/Industry Training Simulation, and Education Conference (I/ITSEC), No. 2794, pp. 1-9, 2006. B.C. Kiat: "Advancement in Simulation Technology and its Military Application", Journal of the Singapore Armed Forces, Vol. 25, No. 2, April-June 1999. J.D. Fletcher: “Measuring the Cost, Effectiveness, and Value of Simulation Used for Military Training”, In. Proceedings of SimTecT 1998 Conference, Adelaide, Australia, 1998. J. MacMillan, E.B. Entin, R. Morley and W. Bennett: “Measuring Team Performance in Complex and Dynamic Military Environments: The SPOTLITE Method”, Aptima, Inc., Draft, pp. 1-32, 2006.  G. Eves: “Virtual Reality Based Training for Industry, a Cognitive Process Approach“, VR Solutions, 2007. B. Otto, “U.S. Air Force Training Requirements”, Military Flight Training 2008 Conference, London, UK, February 2008. C. Graesser and B. King: “Technology-Based Training”, Human Behavior in Military Contexts, The National Academies Press, Washington, D.C., 2008. J.J. Saleem: “Multi-method Approach to understand pilot performance in sociotechnical aviation system”, Dissertation (Doctor of Philosophy). Virginia Polytechnic Institute and State University, 2003. J. Orlansky, C.J. Dahlman, C.P. Hammon, J. Metzko, H.L. Taylor and C. Youngblut: “The Value of Simulation for Training”, Institute for Defense Analysis, Alexandria, VA, IDA Report No. P-2982, DTIC No. ADA289174, 1994. S. Parr and R. MacDonald: "Train Like They Fight: An Introduction to the Maritime Warfare Training System", SISO Fall SIW 2005 Conference, No. 05F-SIW-014, May 2005. B.T. Schreiber, E. Watz, W. Bennett and A.M. Portrey: “Development of a Distributed Mission Training Automated Performance Tracking System”, BRIMS 2003 Conference, U.S., No. 03-BRIMS-062, pp. 1-8, May 2003. S.P. Mihal, A.A. Vidali, T.C. Christenberry and S. Kirkley: “Continuous Learning Environments: Incorporating Performance Metrics into Next Generation Simulation-based eLearning Environments for Military and Law Enforcement”, Institute for Operational Readiness and Continuous Education in Security, White Paper, pp.1-31, 2003. E.F. Lessel III, M.D. Mattison and J.S. Werchan: “On Learning: The Future of Air Force Education and Training”, Air Education and Training Command, U.S., January 2008. P. Milgram, H. Takemura, A. Utsumi and F. Kishino: "Augmented Reality: A class of displays on the reality-virtuality continuum", Telemanipulator and Telepresence Technologies, Vol. 2351, pp. 282-292, 1994. S.E. Kirkley and J.R. Kirkley, “Creating next generation blended learning environments using mixed reality, video games and simulations”, TechTrends: Linking Research & Practice to Improve Learning, Vol. 49, No. 3, pp. 42-53, 2005. S. Kirkley, S. Tomblin and J. Kirkley: “Instructional Design Authoring Support for the Development of Serious Games and Mixed Reality Training”, Interservice/Industry Training, Simulation, and Education Conference (I/ITSEC), No. 2420, pp. 1-11, 2005. D. Serfaty, F. Diedrich, K. Hess, J. Beaubien, J. Sidman and T. Brunyé: “Engineering “Force Multipliers” for Training and Education” Aptima, Inc., Science of Leaning Workshop, Washington, D.C., July 2006. J. Orlansky, H.L. Taylor, D.B. Levine and J.G. Honig: "The Cost and Effectiveness of the Multi-Service Distributed Training Testbed (MDT2) for Training Close Air Support", Final Report Sep 95 - Dec 96, IDA Paper, No. P-3284, Institute for Defense Analyses Alexandria VA, pp. 68, 1997. W. Bennett, B.T. Schreiber and D.H. Andrews: "Developing competency-based methods for near-real-time air combat problem solving assessment", Computers in Human Behavior, Vol. 18, pp. 773-782, 2002. Human Behavior in Military Contexts, The National Academies Press, Washington, D.C., 2008. R.J. Atkins, M.R. Smith, A.R. Mildred and H.P. Pfister: “Towards Managing the Learning Process. Evaluation of simulation training”, The University of Newcastle, 2005. G.J. Holman: "Training Effectiveness of the CH-47 Flight Simulator", Research Report, Army Research Institute for the Behavioral and Social Sciences, Alexandria, VA, No. ARI-RR-1209, pp. 1-91, 1979. J.E. Morrison and C. Hammon: “On measuring the effectiveness of large-scale training simulations”, Institute for Defense Analysis, Alexandria, VA, IDA Paper No. P-3570, DTIC No. ADA394491, 2000. R. Breaux, M. Martin and S. Jones: “Cognitive Precursors to Virtual Reality Applications”, In. Proceedings of SimTecT 1998 Conference, Adelaide, Australia, 1998. J.M. Rolfe and K.J. Staples: “Flight Simulation”, Cambridge UK, Cambridge Press, pp. 1-247, 1994. H.K. Povenmire and S.N. Roscoe: "The Incremental Transfer Effectiveness of a Ground-Based General Aviation Trainer", Technical Report, Illinois University at Urbana-Champaign Savoy Aviation Research Lab, No.  ARL-72-9/AFOSR-72-4, No. AD-754 213, pp. 1-26, 1972. B.T. Schreiber, W.A. Stock and W. Bennett: “Distributed Mission Operations Within-Simulator Training Effectiveness Baseline Study: Metric Development and Objectively Quantifying the Degree of Learning”, Air Force Research Laboratory Warfighter, Readiness Research Division, Technical Report No.2, pp. 1-42, 2006. P.H. Radtke, J.H. Johnston, E. Biddle and T.F. Carolan, "Integrating and Presenting Performance Information in Simulation-Based Air Warfare Scenarios", The Interservice/Industry Training, Simulation & Education Conference (I/ITSEC), 2007. K.A. Smith-Jentsch, J.H. Johnston and S. Payne: “Measuring team-related expertise in complex environments”, In: J. Cannon-Bowers and E. Salas, Editors: “Making decisions under stress. Implications for individual and team training”, American Psychological Association, Washington, D.C., pp. 61–87, 1998. A.M. Portrey, B. Schreiber and W. Bennett: “The pairwise Escape-G metric: a measure for air combat maneuvering performance“, Proceedings of the 2005 Winter Simulation Conference, pp. 1-8, 2005. M. Barlow, E. Lewis and J. Keir: “Heart-Rate and Immersion in a First Person Simulation”, In. Proceedings of SimTecT 2006 Conference, Melbourne, Australia, May 2006. S.Y. Sohn, Y.K. Jo and S.O.Choy: "Classification models for sequential flight test results for selecting air force pilot trainee", Expert Systems with Applications, Vol. 26, pp. 591-599, 2004. B.K. Siegel and K.J. Keller: “Pilot task monitoring using neural networks”, Aerospace and Electronics Conference, NAECON, Proceedings of the IEEE 1992 National, Vol. 2, pp. 709-71, 2005. M. Krusmark, B.T. Schreiber and W. Bennett: “The Effectiveness of a Traditional Gradesheet for Measuring Air Combat Team Performance in Simulated Distributed Mission Operations”, Air Force Research Laboratory Warfighter, Readiness Research Division, Technical Report No. AFRL-HE-AZ-TR-2004-0090, pp. 1-80, 2004. B.T. Schreiber, P. DiSalvo, W.A. Stock and W. Bennett: “Distributed Mission Operations Within-Simulator Training Effectiveness Baseline Study. Using the Pathfinder Methodology to Assess Pilot”, Final Report Mar 2002-Jul 2005, No. A653264, Vol. 5, pp. 1-30, 2006. Y. Shub, A. Kushnir and J. Frenkel:” Pilot Evaluation System”, IEEE, Vol. 2, pp. 734-741, 1994. H.L. McManus and D.E. Hastings: "A Framework for Understanding Uncertainty and its Mitigation and Exploitation in Complex Systems", Proceedings of the 15th Annual Symposium of the International Council On Systems Engineering (INCOSE), Rochester, U.S., pp. 1-19, 2004. S. Wang, W. Shi, H. Yuan and G. Chen: "Attribute Uncertainty in GIS Data", Lecture Notes in Computer Science, pp. 614-623, 2005.D. Niedermayer: “An Introduction to Bayesian Networks and Their Contemporary Applications”, Artificial Intelligence: Belief Networks, 2003. C.M. Colegrove and G.M. Alliger: "Mission Essential Competencies: Defining Combat Mission Readiness in a Novel Way", SAS-038 Symposium, Brussels, Belgium, No. RTO-MP-099, April 2002. S. Symons, F. Michael, J. Bell and W. Bennett: “Linking Knowledge and Skills to Mission Essential Competency-Based Syllabus Development for Distributed Mission Operations”, Air Force Research Laboratory, Warfighter Readiness Research Division, Technical Report No. AFRL-HE-AZ-TR-2006-0041, pp. 1-18, 2006. P. S. Best, F.C. Gentner, P.H. Cunningham, T.C. Tiller and A.W. Schopper: “Evaluating Aircrew and Maintainer Warfighter Performance in Aeronautical Systems using Mission-Oriented Measures of Effectiveness”, Final Report Oct 1995-Apr 1991, University of Dayton Research Institute, U.S., No. ADA398683, pp.1-250, February 2001. S.S. Isukapalli: “Uncertainty Analysis of Transport-Transformation Models”, Probabilistic Approach for Uncertainty Analysis, Dissertation, New Brunswick, New Jersey, January 1999. P. Rutledge and B. Buchbinder: “A Quantitative, Probabilistic Approach to Human-Rating of Space Systems”, Proceedings Annual Reliability and Maintainability Symposium, 216-221, 1994. J.C. Pomerol: “Artificial intelligence and human decision making”, European Journal of Operational Research, No. 99, pp. 3-25, 1997. S.J. Henkind and M.C. Harrison: “An Analysis of Four Uncertainty Calculi”, IEEE (Institute of Electrical and Electronics Engineers) Transactions on Systems, Man, and Cybernetics, Vol. 18, No. 5, pp. 700-714, 1988. T.F. Carolan, I. Schrig and W. Bennett: “Evaluating Competencies: A Probabilistic Approach”, 12th BRIMS Conference, pp. 1-38, 2003. H.J. Zimmerman: “Problems and Tools to Model Uncertainty in Expert- and Decision Support Systems”, Mathematical and Computer Modelling, Vol. 14, pp. 8-20, 1990. P.J. Krause and D.A. Clark: “Representing Uncertain Knowledge: An Artificial Intelligence Approach”, Intellect, Oxford, UK, 1993. R.H. Myers and D.C. Montgomery: “Response Surface Methodology: Process and Product Optimization Using Designed Experiment”, 2nd Edition, Wiley-Interscience, 2002. K.M. Carley, N.Y. Kamneva and J. Reminga: “Response Surface Methodology”, Institute of Software Research International, Carnegie Mellon University, Pittsburgh, Report No. CMU-ISRI-04-136, pp. 1-31, October 2004. S. Ammar and R. Wright: “Applying fuzzy-set theory to performance evaluation”, Socio-Economic Planning Sciences, Vol. 34, pp. 285-302, 2000. S. McClintock, T. Lunney and A. Hashim: “Using Fuzzy Logic to Optimize Genetic Algorithm Performance”, IEEE International Conference on Intelligent Engineering Systems, pp. 271-275, 1997. M. Holický: “Fuzzy probabilistic optimisation of building performance”, Automation in Construction, Vol. 8, No. 4, pp. 437-443, April 1999. D. Pang, J. Bigham and E.H. Mamdani: “Reasoning with uncertain information”, IEE (Institute of Electrical Engineers) Proceedings, Vol. 134, Pt.D, No. 4, pp. 231-237, 1987. S.A. Aziz and J. Parthiban: “Fuzzy Logic and Its Uses”, Surprise Journal, Imperial College of Science, Technology and Medicine, London, 1996. K.A. Rasmani and Q. Shen: “Data-driven Fuzzy Rule Generation and its Application for Student Academic Performance Evaluation”, Applied Intelligence, Vol. 25, No. 3, pp. 302-319, 2006. T. Wong and H. Wong: “Genetic Algorithms”, Surprise Journal, Imperial College of Science, Technology and Medicine, London, Report No. 4, 1996. D. Filippidis and V. Puri: “Performance Measurement Modelling: A Methodology for the Objective Assessment of Trainee Performance”, Land Operations Division, DSTO, Australia, 2007. P.J. Angeline and K.E. Kinnear: “Genetic Programming’s continued evolution “, Advances in Genetic Programming, MIT Press, Vol. 2, pp. 1-20, 1996. L. Henesey: “Intelligence, Software Agents, Creativity, Implementing and Integrating for Management”, School of Engineering, Blekinge Institute of Technology, Lecture No. 6, pp. 1-31, 2007. C. Stergiou and D. Siganos: “Neural Networks”, Surprise Journal, Imperial College of Science, Technology and Medicine, London, 1996. E. Turban, J. Aronson, and T. Liang: “Advanced Intelligent Systems“, Decision Support Systems and Intelligent Systems, 7th Edition, Prentice Hall, pp. 1-20, 2005. S. Parsons: “Qualitative Methods for Reasoning under Uncertainty”, MIT (Massachusetts Institute of Technology) Press, U.S., 2001. Author BiographiesINGRIDA VALIUSAITYTE is a Mathematical Modeller at the Systems Engineering Innovation Centre (SEIC). She holds a MSc. in Industrial Mathematical Modelling from Loughborough University in UK and B.A. in Applied Mathematics from Kaunas University of Technology in Lithuania.ESHAN RAJABALLY is a Senior Scientist for BAE Systems located at the Systems Engineering Innovation Centre (SEIC). He holds a PhD in the field of systems modelling acceptance.TOM PAGE is a lecturer in Electronic Product Design in the department of Design and Technology at Loughborough University.  He is co-founder of the European Society for Virtual Reality Learning Environment Research and Development and holds a PhD in Electronics Systems Design.1courtesy of the United States Air Force Research Laboratory and Air Combat Command1courtesy of the United States Air Force Research Laboratory and Air Combat Command